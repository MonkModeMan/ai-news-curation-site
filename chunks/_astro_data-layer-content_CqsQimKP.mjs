const _astro_dataLayerContent = [["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.9.2","content-config-digest","a191d85ec3e0cb2a","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://raisex-llc.github.io/ai-news-curation-site/\",\"compressHTML\":true,\"base\":\"/ai-news-curation-site/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"experimentalDefaultStyles\":true},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"prism\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","posts",["Map",11,12,89,90,116,117,137,138,158,159,185,186,206,207,459,460,480,481,501,502,546,547,567,568,606,607,627,628,653,654,673,674,694,695,837,838],"2025-06-20-lora-fine-tuning-flux1-dev-on-consumer-hardware-c5eb6e",{id:11,data:13,body:23,filePath:24,digest:25,rendered:26,legacyId:88},{title:14,summary:15,pubDate:16,media:17,tags:18,link:22,thumbnail:15},"(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware","",["Date","2025-06-19T00:00:00.000Z"],"Hugging Face Blog",[19,20,21],"huggingface","transformers","nlp","https://huggingface.co/blog/flux-qlora","(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware\r\nIn our previous post, Exploring Quantization Backends in Diffusers, we dived into how various quantization techniques can shrink diffusion models like FLUX.1-dev, making them significantly more accessible for inference without drastically compromising performance. We saw how bitsandbytes\r\n, torchao\r\n, and others reduce memory footprints for generating images.\r\nPerforming inference is cool, but to make these models truly our own, we also need to be able to fine-tune them. Therefore, in this post, we tackle efficient fine-tuning of these models with peak memory use under ~10 GB of VRAM on a single GPU. This post will guide you through fine-tuning FLUX.1-dev using QLoRA with the diffusers\r\nlibrary. We'll showcase results from an NVIDIA RTX 4090. We'll also highlight how FP8 training with torchao\r\ncan further optimize speed on compatible hardware.\r\nTable of Contents\r\n- Dataset\r\n- FLUX Architecture\r\n- QLoRA Fine-tuning FLUX.1-dev with diffusers\r\n- FP8 Fine-tuning with\r\ntorchao\r\n- Inference with Trained LoRA Adapters\r\n- Running on Google Colab\r\n- Conclusion\r\nDataset\r\nWe aim to fine-tune black-forest-labs/FLUX.1-dev\r\nto adopt the artistic style of Alphonse Mucha, using a small dataset.\r\nFLUX Architecture\r\nThe model consists of three main components:\r\n- Text Encoders (CLIP and T5)\r\n- Transformer (Main Model - Flux Transformer)\r\n- Variational Auto-Encoder (VAE)\r\nIn our QLoRA approach, we focus exclusively on fine-tuning the transformer component. The text encoders and VAE remain frozen throughout training.\r\nQLoRA Fine-tuning FLUX.1-dev with diffusers\r\nWe used a diffusers\r\ntraining script (slightly modified from https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/train_dreambooth_lora_flux_miniature.py) designed for DreamBooth-style LoRA fine-tuning of FLUX models. Also, a shortened version to reproduce the results in this blogpost (and used in the Google Colab) is available here. Let's examine the crucial parts for QLoRA and memory efficiency:\r\nKey Optimization Techniques\r\nLoRA (Low-Rank Adaptation) Deep Dive: LoRA makes model training more efficient by keeping track of the weight updates with low-rank matrices. Instead of updating the full weight matrix , LoRA learns two smaller matrices and . The update to the weights for the model is , where and . The number (called rank) is much smaller than the original dimensions, which means less parameters to update. Lastly, is a scaling factor for the LoRA activations. This affects how much LoRA affects the updates, and is often set to the same value as the or a multiple of it. It helps balance the influence of the pre-trained model and the LoRA adapter. For a general introduction to the concept, check out our previous blog post: Using LoRA for Efficient Stable Diffusion Fine-Tuning.\r\nQLoRA: The Efficiency Powerhouse: QLoRA enhances LoRA by first loading the pre-trained base model in a quantized format (typically 4-bit via bitsandbytes\r\n), drastically cutting the base model's memory footprint. It then trains LoRA adapters (usually in FP16/BF16) on top of this quantized base. This dramatically lowers the VRAM needed to hold the base model.\r\nFor instance, in the DreamBooth training script for HiDream 4-bit quantization with bitsandbytes reduces the peak memory usage of a LoRA fine-tune from ~60GB down to ~37GB with negligible-to-none quality degradation. The very same principle is what we apply here to fine-tune FLUX.1 on a consumer-grade hardware.\r\n8-bit Optimizer (AdamW): Standard AdamW optimizer maintains first and second moment estimates for each parameter in 32-bit (FP32), which consumes a lot of memory. The 8-bit AdamW uses block-wise quantization to store optimizer states in 8-bit precision, while maintaining training stability. This technique can reduce optimizer memory usage by ~75% compared to standard FP32 AdamW. Enabling it in the script is straightforward:\r\n# Check for the --use_8bit_adam flag\r\nif args.use_8bit_adam:\r\noptimizer_class = bnb.optim.AdamW8bit\r\nelse:\r\noptimizer_class = torch.optim.AdamW\r\noptimizer = optimizer_class(\r\nparams_to_optimize,\r\nbetas=(args.adam_beta1, args.adam_beta2),\r\nweight_decay=args.adam_weight_decay,\r\neps=args.adam_epsilon,\r\n)\r\nGradient Checkpointing: During forward pass, intermediate activations are typically stored for backward pass gradient computation. Gradient checkpointing trades computation for memory by only storing certain checkpoint activations and recomputing others during backpropagation.\r\nif args.gradient_checkpointing:\r\ntransformer.enable_gradient_checkpointing()\r\nCache Latents: This optimization technique pre-processes all training images through the VAE encoder before the beginning of the training. It stores the resulting latent representations in memory. During the training, instead of encoding images on-the-fly, the cached latents are directly used. This approach offers two main benefits:\r\n- eliminates redundant VAE encoding computations during training, speeding up each training step\r\n- allows the VAE to be completely removed from GPU memory after caching. The trade-off is increased RAM usage to store all cached latents, but this is typically manageable for small datasets.\r\n# Cache latents before training if the flag is set\r\nif args.cache_latents:\r\nlatents_cache = []\r\nfor batch in tqdm(train_dataloader, desc=\"Caching latents\"):\r\nwith torch.no_grad():\r\nbatch[\"pixel_values\"] = batch[\"pixel_values\"].to(\r\naccelerator.device, non_blocking=True, dtype=weight_dtype\r\n)\r\nlatents_cache.append(vae.encode(batch[\"pixel_values\"]).latent_dist)\r\n# VAE is no longer needed, free up its memory\r\ndel vae\r\nfree_memory()\r\nSetting up 4-bit Quantization (BitsAndBytesConfig\r\n):\r\nThis section demonstrates the QLoRA configuration for the base model:\r\n# Determine compute dtype based on mixed precision\r\nbnb_4bit_compute_dtype = torch.float32\r\nif args.mixed_precision == \"fp16\":\r\nbnb_4bit_compute_dtype = torch.float16\r\nelif args.mixed_precision == \"bf16\":\r\nbnb_4bit_compute_dtype = torch.bfloat16\r\nnf4_config = BitsAndBytesConfig(\r\nload_in_4bit=True,\r\nbnb_4bit_quant_type=\"nf4\",\r\nbnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\r\n)\r\ntransformer = FluxTransformer2DModel.from_pretrained(\r\nargs.pretrained_model_name_or_path,\r\nsubfolder=\"transformer\",\r\nquantization_config=nf4_config,\r\ntorch_dtype=bnb_4bit_compute_dtype,\r\n)\r\n# Prepare model for k-bit training\r\ntransformer = prepare_model_for_kbit_training(transformer, use_gradient_checkpointing=False)\r\n# Gradient checkpointing is enabled later via transformer.enable_gradient_checkpointing() if arg is set\r\nDefining LoRA Configuration (LoraConfig\r\n):\r\nAdapters are added to the quantized transformer:\r\ntransformer_lora_config = LoraConfig(\r\nr=args.rank,\r\nlora_alpha=args.rank,\r\ninit_lora_weights=\"gaussian\",\r\ntarget_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"], # FLUX attention blocks\r\n)\r\ntransformer.add_adapter(transformer_lora_config)\r\nprint(f\"trainable params: {transformer.num_parameters(only_trainable=True)} || all params: {transformer.num_parameters()}\")\r\n# trainable params: 4,669,440 || all params: 11,906,077,760\r\nOnly these LoRA parameters become trainable.\r\nPre-computing Text Embeddings (CLIP/T5)\r\nBefore we launch the QLoRA fine-tune we can save a huge chunk of VRAM and wall-clock time by caching outputs of text encoders once.\r\nAt training time the dataloader simply reads the cached embeddings instead of re-encoding the caption, so the CLIP/T5 encoder never has to sit in GPU memory.\r\nCode\r\n# https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/compute_embeddings.py\r\nimport argparse\r\nimport pandas as pd\r\nimport torch\r\nfrom datasets import load_dataset\r\nfrom huggingface_hub.utils import insecure_hashlib\r\nfrom tqdm.auto import tqdm\r\nfrom transformers import T5EncoderModel\r\nfrom diffusers import FluxPipeline\r\nMAX_SEQ_LENGTH = 77\r\nOUTPUT_PATH = \"embeddings.parquet\"\r\ndef generate_image_hash(image):\r\nreturn insecure_hashlib.sha256(image.tobytes()).hexdigest()\r\ndef load_flux_dev_pipeline():\r\nid = \"black-forest-labs/FLUX.1-dev\"\r\ntext_encoder = T5EncoderModel.from_pretrained(id, subfolder=\"text_encoder_2\", load_in_8bit=True, device_map=\"auto\")\r\npipeline = FluxPipeline.from_pretrained(\r\nid, text_encoder_2=text_encoder, transformer=None, vae=None, device_map=\"balanced\"\r\n)\r\nreturn pipeline\r\n@torch.no_grad()\r\ndef compute_embeddings(pipeline, prompts, max_sequence_length):\r\nall_prompt_embeds = []\r\nall_pooled_prompt_embeds = []\r\nall_text_ids = []\r\nfor prompt in tqdm(prompts, desc=\"Encoding prompts.\"):\r\n(\r\nprompt_embeds,\r\npooled_prompt_embeds,\r\ntext_ids,\r\n) = pipeline.encode_prompt(prompt=prompt, prompt_2=None, max_sequence_length=max_sequence_length)\r\nall_prompt_embeds.append(prompt_embeds)\r\nall_pooled_prompt_embeds.append(pooled_prompt_embeds)\r\nall_text_ids.append(text_ids)\r\nmax_memory = torch.cuda.max_memory_allocated() / 1024 / 1024 / 1024\r\nprint(f\"Max memory allocated: {max_memory:.3f} GB\")\r\nreturn all_prompt_embeds, all_pooled_prompt_embeds, all_text_ids\r\ndef run(args):\r\ndataset = load_dataset(\"Norod78/Yarn-art-style\", split=\"train\")\r\nimage_prompts = {generate_image_hash(sample[\"image\"]): sample[\"text\"] for sample in dataset}\r\nall_prompts = list(image_prompts.values())\r\nprint(f\"{len(all_prompts)=}\")\r\npipeline = load_flux_dev_pipeline()\r\nall_prompt_embeds, all_pooled_prompt_embeds, all_text_ids = compute_embeddings(\r\npipeline, all_prompts, args.max_sequence_length\r\n)\r\ndata = []\r\nfor i, (image_hash, _) in enumerate(image_prompts.items()):\r\ndata.append((image_hash, all_prompt_embeds[i], all_pooled_prompt_embeds[i], all_text_ids[i]))\r\nprint(f\"{len(data)=}\")\r\n# Create a DataFrame\r\nembedding_cols = [\"prompt_embeds\", \"pooled_prompt_embeds\", \"text_ids\"]\r\ndf = pd.DataFrame(data, columns=[\"image_hash\"] + embedding_cols)\r\nprint(f\"{len(df)=}\")\r\n# Convert embedding lists to arrays (for proper storage in parquet)\r\nfor col in embedding_cols:\r\ndf[col] = df[col].apply(lambda x: x.cpu().numpy().flatten().tolist())\r\n# Save the dataframe to a parquet file\r\ndf.to_parquet(args.output_path)\r\nprint(f\"Data successfully serialized to {args.output_path}\")\r\nif __name__ == \"__main__\":\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\r\n\"--max_sequence_length\",\r\ntype=int,\r\ndefault=MAX_SEQ_LENGTH,\r\nhelp=\"Maximum sequence length to use for computing the embeddings. The more the higher computational costs.\",\r\n)\r\nparser.add_argument(\"--output_path\", type=str, default=OUTPUT_PATH, help=\"Path to serialize the parquet file.\")\r\nargs = parser.parse_args()\r\nrun(args)\r\nHow to use it\r\npython compute_embeddings.py \\\r\n--max_sequence_length 77 \\\r\n--output_path embeddings_alphonse_mucha.parquet\r\nBy combining this with cached VAE latents (--cache_latents\r\n) you whittle the active model down to just the quantized transformer + LoRA adapters, keeping the whole fine-tune comfortably under 10 GB of GPU memory.\r\nSetup & Results\r\nFor this demonstration, we leveraged an NVIDIA RTX 4090 (24GB VRAM) to explore its performance. The full training command using accelerate\r\nis shown below.\r\n# You need to pre-compute the text embeddings first. See the diffusers repo.\r\n# https://github.com/huggingface/diffusers/tree/main/examples/research_projects/flux_lora_quantization\r\naccelerate launch --config_file=accelerate.yaml \\\r\ntrain_dreambooth_lora_flux_miniature.py \\\r\n--pretrained_model_name_or_path=\"black-forest-labs/FLUX.1-dev\" \\\r\n--data_df_path=\"embeddings_alphonse_mucha.parquet\" \\\r\n--output_dir=\"alphonse_mucha_lora_flux_nf4\" \\\r\n--mixed_precision=\"bf16\" \\\r\n--use_8bit_adam \\\r\n--weighting_scheme=\"none\" \\\r\n--width=512 \\\r\n--height=768 \\\r\n--train_batch_size=1 \\\r\n--repeats=1 \\\r\n--learning_rate=1e-4 \\\r\n--guidance_scale=1 \\\r\n--report_to=\"wandb\" \\\r\n--gradient_accumulation_steps=4 \\\r\n--gradient_checkpointing \\ # can drop checkpointing when HW has more than 16 GB.\r\n--lr_scheduler=\"constant\" \\\r\n--lr_warmup_steps=0 \\\r\n--cache_latents \\\r\n--rank=4 \\\r\n--max_train_steps=700 \\\r\n--seed=\"0\"\r\nConfiguration for RTX 4090:\r\nOn our RTX 4090, we used a train_batch_size\r\nof 1, gradient_accumulation_steps\r\nof 4, mixed_precision=\"bf16\"\r\n, gradient_checkpointing=True\r\n, use_8bit_adam=True\r\n, a LoRA rank\r\nof 4, and resolution of 512x768. Latents were cached with cache_latents=True\r\n.\r\nMemory Footprint (RTX 4090):\r\n- QLoRA: Peak VRAM usage for QLoRA fine-tuning was approximately 9GB.\r\n- BF16 LoRA: Running standard LoRA (with the base FLUX.1-dev in FP16) on the same setup consumed 26 GB VRAM.\r\n- BF16 full finetuning: An estimate would be ~120 GB VRAM with no memory optimizations.\r\nTraining Time (RTX 4090):\r\nFine-tuning for 700 steps on the Alphonse Mucha dataset took approximately 41 minutes on the RTX 4090 with train_batch_size\r\nof 1 and resolution of 512x768.\r\nOutput Quality: The ultimate measure is the generated art. Here are samples from our QLoRA fine-tuned model on the derekl35/alphonse-mucha-style dataset:\r\nThis table compares the primary bf16\r\nprecision results. The goal of the fine-tuning was to teach the model the distinct style of Alphonse Mucha.\r\nThe fine-tuned model nicely captured Mucha's iconic art nouveau style, evident in the decorative motifs and distinct color palette. The QLoRA process maintained excellent fidelity while learning the new style.\r\nClick to see the fp16 comparison\r\nThe results are nearly identical, showing that QLoRA performs effectively with both fp16\r\nand bf16\r\nmixed precision.\r\nModel Comparison: Base vs. QLoRA Fine-tuned (fp16)\r\nFP8 Fine-tuning with torchao\r\nFor users with NVIDIA GPUs possessing compute capability 8.9 or greater (such as the H100, RTX 4090), even greater speed efficiencies can be achieved by leveraging FP8 training via the torchao\r\nlibrary.\r\nWe fine-tuned FLUX.1-dev LoRA on an H100 SXM GPU slightly modifieddiffusers-torchao\r\ntraining scripts. The following command was used:\r\naccelerate launch train_dreambooth_lora_flux.py \\\r\n--pretrained_model_name_or_path=black-forest-labs/FLUX.1-dev \\\r\n--dataset_name=derekl35/alphonse-mucha-style --instance_prompt=\"a woman, alphonse mucha style\" --caption_column=\"text\" \\\r\n--output_dir=alphonse_mucha_fp8_lora_flux \\\r\n--mixed_precision=bf16 --use_8bit_adam \\\r\n--weighting_scheme=none \\\r\n--height=768 --width=512 --train_batch_size=1 --repeats=1 \\\r\n--learning_rate=1e-4 --guidance_scale=1 --report_to=wandb \\\r\n--gradient_accumulation_steps=1 --gradient_checkpointing \\\r\n--lr_scheduler=constant --lr_warmup_steps=0 --rank=4 \\\r\n--max_train_steps=700 --checkpointing_steps=600 --seed=0 \\\r\n--do_fp8_training --push_to_hub\r\nThe training run had a peak memory usage of 36.57 GB and completed in approximately 20 minutes.\r\nQualitative results from this FP8 fine-tuned model are also available:\r\nKey steps to enable FP8 training with torchao\r\ninvolve:\r\n- Injecting FP8 layers into the model using\r\nconvert_to_float8_training\r\nfromtorchao.float8\r\n. - Defining a\r\nmodule_filter_fn\r\nto specify which modules should and should not be converted to FP8.\r\nFor a more detailed guide and code snippets, please refer to this gist and the diffusers-torchao\r\nrepository.\r\nInference with Trained LoRA Adapters\r\nAfter training your LoRA adapters, you have two main approaches for inference.\r\nOption 1: Loading LoRA Adapters\r\nOne approach is to load your trained LoRA adapters on top of the base model.\r\nBenefits of Loading LoRA:\r\n- Flexibility: Easily switch between different LoRA adapters without reloading the base model\r\n- Experimentation: Test multiple artistic styles or concepts by swapping adapters\r\n- Modularity: Combine multiple LoRA adapters using\r\nset_adapters()\r\nfor creative blending - Storage efficiency: Keep a single base model and multiple small adapter files\r\nCode\r\nfrom diffusers import FluxPipeline, FluxTransformer2DModel, BitsAndBytesConfig\r\nimport torch\r\nckpt_id = \"black-forest-labs/FLUX.1-dev\"\r\npipeline = FluxPipeline.from_pretrained(\r\nckpt_id, torch_dtype=torch.float16\r\n)\r\npipeline.load_lora_weights(\"derekl35/alphonse_mucha_qlora_flux\", weight_name=\"pytorch_lora_weights.safetensors\")\r\npipeline.enable_model_cpu_offload()\r\nimage = pipeline(\r\n\"a puppy in a pond, alphonse mucha style\", num_inference_steps=28, guidance_scale=3.5, height=768, width=512, generator=torch.manual_seed(0)\r\n).images[0]\r\nimage.save(\"alphonse_mucha.png\")\r\nOption 2: Merging LoRA into Base Model\r\nFor when you want maximum efficiency with a single style, you can merge the LoRA weights into the base model.\r\nBenefits of Merging LoRA:\r\n- VRAM efficiency: No additional memory overhead from adapter weights during inference\r\n- Speed: Slightly faster inference as there's no need to apply adapter computations\r\n- Quantization compatibility: Can re-quantize the merged model for maximum memory efficiency\r\nCode\r\nfrom diffusers import FluxPipeline, AutoPipelineForText2Image, FluxTransformer2DModel, BitsAndBytesConfig\r\nimport torch\r\nckpt_id = \"black-forest-labs/FLUX.1-dev\"\r\npipeline = FluxPipeline.from_pretrained(\r\nckpt_id, text_encoder=None, text_encoder_2=None, torch_dtype=torch.float16\r\n)\r\npipeline.load_lora_weights(\"derekl35/alphonse_mucha_qlora_flux\", weight_name=\"pytorch_lora_weights.safetensors\")\r\npipeline.fuse_lora()\r\npipeline.unload_lora_weights()\r\npipeline.transformer.save_pretrained(\"fused_transformer\")\r\nbnb_4bit_compute_dtype = torch.bfloat16\r\nnf4_config = BitsAndBytesConfig(\r\nload_in_4bit=True,\r\nbnb_4bit_quant_type=\"nf4\",\r\nbnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\r\n)\r\ntransformer = FluxTransformer2DModel.from_pretrained(\r\n\"fused_transformer\",\r\nquantization_config=nf4_config,\r\ntorch_dtype=bnb_4bit_compute_dtype,\r\n)\r\npipeline = AutoPipelineForText2Image.from_pretrained(\r\nckpt_id, transformer=transformer, torch_dtype=bnb_4bit_compute_dtype\r\n)\r\npipeline.enable_model_cpu_offload()\r\nimage = pipeline(\r\n\"a puppy in a pond, alphonse mucha style\", num_inference_steps=28, guidance_scale=3.5, height=768, width=512, generator=torch.manual_seed(0)\r\n).images[0]\r\nimage.save(\"alphonse_mucha_merged.png\")\r\nRunning on Google Colab\r\nWhile we showcased results on an RTX 4090, the same code can be run on more accessible hardware like the T4 GPU available in Google Colab for free.\r\nOn a T4, you can expect the fine-tuning process to take significantly longer around 4 hours for the same number of steps. This is a trade-off for accessibility, but it makes custom fine-tuning possible without high-end hardware. Be mindful of usage limits if running on Colab, as a 4-hour training run might push them.\r\nConclusion\r\nQLoRA, coupled with the diffusers\r\nlibrary, significantly democratizes the ability to customize state-of-the-art models like FLUX.1-dev. As demonstrated on an RTX 4090, efficient fine-tuning is well within reach, yielding high-quality stylistic adaptations. Furthermore, for users with the latest NVIDIA hardware, torchao\r\nenables even faster training through FP8 precision.\r\nShare your creations on the Hub!\r\nSharing your fine-tuned LoRA adapters is a fantastic way to contribute to the open-source community. It allows others to easily try out your styles, build on your work, and helps create a vibrant ecosystem of creative AI tools.\r\nIf you've trained a LoRA for FLUX.1-dev, we encourage you to share it. The easiest way is to add the --push_to_hub flag to the training script. Alternatively, if you have already trained a model and want to upload it, you can use the following snippet.\r\n# Prereqs:\r\n# - pip install huggingface_hub diffusers\r\n# - Run `huggingface-cli login` (or set HF_TOKEN env-var) once.\r\n# - save model\r\nfrom huggingface_hub import create_repo, upload_folder\r\nrepo_id = \"your-username/alphonse_mucha_qlora_flux\"\r\ncreate_repo(repo_id, exist_ok=True)\r\nupload_folder(\r\nrepo_id=repo_id,\r\nfolder_path=\"alphonse_mucha_qlora_flux\",\r\ncommit_message=\"Add Alphonse Mucha LoRA adapter\"\r\n)\r\nCheck out our Mucha QLoRA https://huggingface.co/derekl35/alphonse_mucha_qlora_flux FP8 LoRA https://huggingface.co/derekl35/alphonse_mucha_fp8_lora_flux. You can find both, plus other adapters, in this collection as an example.\r\nWe can't wait to see what you create!","src/content/posts/2025-06-20-(lora)-fine-tuning-flux.1-dev-on-consumer-hardware-c5eb6e.md","05094d6c0c37e705",{html:27,metadata:28},"<p>(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware\r\nIn our previous post, Exploring Quantization Backends in Diffusers, we dived into how various quantization techniques can shrink diffusion models like FLUX.1-dev, making them significantly more accessible for inference without drastically compromising performance. We saw how bitsandbytes\r\n, torchao\r\n, and others reduce memory footprints for generating images.\r\nPerforming inference is cool, but to make these models truly our own, we also need to be able to fine-tune them. Therefore, in this post, we tackle efficient fine-tuning of these models with peak memory use under ~10 GB of VRAM on a single GPU. This post will guide you through fine-tuning FLUX.1-dev using QLoRA with the diffusers\r\nlibrary. We’ll showcase results from an NVIDIA RTX 4090. We’ll also highlight how FP8 training with torchao\r\ncan further optimize speed on compatible hardware.\r\nTable of Contents</p>\n<ul>\n<li>Dataset</li>\n<li>FLUX Architecture</li>\n<li>QLoRA Fine-tuning FLUX.1-dev with diffusers</li>\n<li>FP8 Fine-tuning with\r\ntorchao</li>\n<li>Inference with Trained LoRA Adapters</li>\n<li>Running on Google Colab</li>\n<li>Conclusion\r\nDataset\r\nWe aim to fine-tune black-forest-labs/FLUX.1-dev\r\nto adopt the artistic style of Alphonse Mucha, using a small dataset.\r\nFLUX Architecture\r\nThe model consists of three main components:</li>\n<li>Text Encoders (CLIP and T5)</li>\n<li>Transformer (Main Model - Flux Transformer)</li>\n<li>Variational Auto-Encoder (VAE)\r\nIn our QLoRA approach, we focus exclusively on fine-tuning the transformer component. The text encoders and VAE remain frozen throughout training.\r\nQLoRA Fine-tuning FLUX.1-dev with diffusers\r\nWe used a diffusers\r\ntraining script (slightly modified from <a href=\"https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/train_dreambooth_lora_flux_miniature.py\">https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/train_dreambooth_lora_flux_miniature.py</a>) designed for DreamBooth-style LoRA fine-tuning of FLUX models. Also, a shortened version to reproduce the results in this blogpost (and used in the Google Colab) is available here. Let’s examine the crucial parts for QLoRA and memory efficiency:\r\nKey Optimization Techniques\r\nLoRA (Low-Rank Adaptation) Deep Dive: LoRA makes model training more efficient by keeping track of the weight updates with low-rank matrices. Instead of updating the full weight matrix , LoRA learns two smaller matrices and . The update to the weights for the model is , where and . The number (called rank) is much smaller than the original dimensions, which means less parameters to update. Lastly, is a scaling factor for the LoRA activations. This affects how much LoRA affects the updates, and is often set to the same value as the or a multiple of it. It helps balance the influence of the pre-trained model and the LoRA adapter. For a general introduction to the concept, check out our previous blog post: Using LoRA for Efficient Stable Diffusion Fine-Tuning.\r\nQLoRA: The Efficiency Powerhouse: QLoRA enhances LoRA by first loading the pre-trained base model in a quantized format (typically 4-bit via bitsandbytes\r\n), drastically cutting the base model’s memory footprint. It then trains LoRA adapters (usually in FP16/BF16) on top of this quantized base. This dramatically lowers the VRAM needed to hold the base model.\r\nFor instance, in the DreamBooth training script for HiDream 4-bit quantization with bitsandbytes reduces the peak memory usage of a LoRA fine-tune from ~60GB down to ~37GB with negligible-to-none quality degradation. The very same principle is what we apply here to fine-tune FLUX.1 on a consumer-grade hardware.\r\n8-bit Optimizer (AdamW): Standard AdamW optimizer maintains first and second moment estimates for each parameter in 32-bit (FP32), which consumes a lot of memory. The 8-bit AdamW uses block-wise quantization to store optimizer states in 8-bit precision, while maintaining training stability. This technique can reduce optimizer memory usage by ~75% compared to standard FP32 AdamW. Enabling it in the script is straightforward:</li>\n</ul>\n<h1 id=\"check-for-the-use_8bit_adam-flag\">Check for the —use_8bit_adam flag</h1>\n<p>if args.use_8bit_adam:\r\noptimizer_class = bnb.optim.AdamW8bit\r\nelse:\r\noptimizer_class = torch.optim.AdamW\r\noptimizer = optimizer_class(\r\nparams_to_optimize,\r\nbetas=(args.adam_beta1, args.adam_beta2),\r\nweight_decay=args.adam_weight_decay,\r\neps=args.adam_epsilon,\r\n)\r\nGradient Checkpointing: During forward pass, intermediate activations are typically stored for backward pass gradient computation. Gradient checkpointing trades computation for memory by only storing certain checkpoint activations and recomputing others during backpropagation.\r\nif args.gradient_checkpointing:\r\ntransformer.enable_gradient_checkpointing()\r\nCache Latents: This optimization technique pre-processes all training images through the VAE encoder before the beginning of the training. It stores the resulting latent representations in memory. During the training, instead of encoding images on-the-fly, the cached latents are directly used. This approach offers two main benefits:</p>\n<ul>\n<li>eliminates redundant VAE encoding computations during training, speeding up each training step</li>\n<li>allows the VAE to be completely removed from GPU memory after caching. The trade-off is increased RAM usage to store all cached latents, but this is typically manageable for small datasets.</li>\n</ul>\n<h1 id=\"cache-latents-before-training-if-the-flag-is-set\">Cache latents before training if the flag is set</h1>\n<p>if args.cache_latents:\r\nlatents_cache = []\r\nfor batch in tqdm(train_dataloader, desc=“Caching latents”):\r\nwith torch.no_grad():\r\nbatch[“pixel_values”] = batch[“pixel_values”].to(\r\naccelerator.device, non_blocking=True, dtype=weight_dtype\r\n)\r\nlatents_cache.append(vae.encode(batch[“pixel_values”]).latent_dist)</p>\n<h1 id=\"vae-is-no-longer-needed-free-up-its-memory\">VAE is no longer needed, free up its memory</h1>\n<p>del vae\r\nfree_memory()\r\nSetting up 4-bit Quantization (BitsAndBytesConfig\r\n):\r\nThis section demonstrates the QLoRA configuration for the base model:</p>\n<h1 id=\"determine-compute-dtype-based-on-mixed-precision\">Determine compute dtype based on mixed precision</h1>\n<p>bnb_4bit_compute_dtype = torch.float32\r\nif args.mixed_precision == “fp16”:\r\nbnb_4bit_compute_dtype = torch.float16\r\nelif args.mixed_precision == “bf16”:\r\nbnb_4bit_compute_dtype = torch.bfloat16\r\nnf4_config = BitsAndBytesConfig(\r\nload_in_4bit=True,\r\nbnb_4bit_quant_type=“nf4”,\r\nbnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\r\n)\r\ntransformer = FluxTransformer2DModel.from_pretrained(\r\nargs.pretrained_model_name_or_path,\r\nsubfolder=“transformer”,\r\nquantization_config=nf4_config,\r\ntorch_dtype=bnb_4bit_compute_dtype,\r\n)</p>\n<h1 id=\"prepare-model-for-k-bit-training\">Prepare model for k-bit training</h1>\n<p>transformer = prepare_model_for_kbit_training(transformer, use_gradient_checkpointing=False)</p>\n<h1 id=\"gradient-checkpointing-is-enabled-later-via-transformerenable_gradient_checkpointing-if-arg-is-set\">Gradient checkpointing is enabled later via transformer.enable_gradient_checkpointing() if arg is set</h1>\n<p>Defining LoRA Configuration (LoraConfig\r\n):\r\nAdapters are added to the quantized transformer:\r\ntransformer_lora_config = LoraConfig(\r\nr=args.rank,\r\nlora_alpha=args.rank,\r\ninit_lora_weights=“gaussian”,\r\ntarget_modules=[“to_k”, “to_q”, “to_v”, “to_out.0”], # FLUX attention blocks\r\n)\r\ntransformer.add_adapter(transformer_lora_config)\r\nprint(f”trainable params: {transformer.num_parameters(only_trainable=True)} || all params: {transformer.num_parameters()}“)</p>\n<h1 id=\"trainable-params-4669440--all-params-11906077760\">trainable params: 4,669,440 || all params: 11,906,077,760</h1>\n<p>Only these LoRA parameters become trainable.\r\nPre-computing Text Embeddings (CLIP/T5)\r\nBefore we launch the QLoRA fine-tune we can save a huge chunk of VRAM and wall-clock time by caching outputs of text encoders once.\r\nAt training time the dataloader simply reads the cached embeddings instead of re-encoding the caption, so the CLIP/T5 encoder never has to sit in GPU memory.\r\nCode</p>\n<h1 id=\"httpsgithubcomhuggingfacediffusersblobmainexamplesresearch_projectsflux_lora_quantizationcompute_embeddingspy\"><a href=\"https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/compute_embeddings.py\">https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/compute_embeddings.py</a></h1>\n<p>import argparse\r\nimport pandas as pd\r\nimport torch\r\nfrom datasets import load_dataset\r\nfrom huggingface_hub.utils import insecure_hashlib\r\nfrom tqdm.auto import tqdm\r\nfrom transformers import T5EncoderModel\r\nfrom diffusers import FluxPipeline\r\nMAX_SEQ_LENGTH = 77\r\nOUTPUT_PATH = “embeddings.parquet”\r\ndef generate_image_hash(image):\r\nreturn insecure_hashlib.sha256(image.tobytes()).hexdigest()\r\ndef load_flux_dev_pipeline():\r\nid = “black-forest-labs/FLUX.1-dev”\r\ntext_encoder = T5EncoderModel.from_pretrained(id, subfolder=“text_encoder_2”, load_in_8bit=True, device_map=“auto”)\r\npipeline = FluxPipeline.from_pretrained(\r\nid, text_encoder_2=text_encoder, transformer=None, vae=None, device_map=“balanced”\r\n)\r\nreturn pipeline\r\n@torch.no_grad()\r\ndef compute_embeddings(pipeline, prompts, max_sequence_length):\r\nall_prompt_embeds = []\r\nall_pooled_prompt_embeds = []\r\nall_text_ids = []\r\nfor prompt in tqdm(prompts, desc=“Encoding prompts.”):\r\n(\r\nprompt_embeds,\r\npooled_prompt_embeds,\r\ntext_ids,\r\n) = pipeline.encode_prompt(prompt=prompt, prompt_2=None, max_sequence_length=max_sequence_length)\r\nall_prompt_embeds.append(prompt_embeds)\r\nall_pooled_prompt_embeds.append(pooled_prompt_embeds)\r\nall_text_ids.append(text_ids)\r\nmax_memory = torch.cuda.max_memory_allocated() / 1024 / 1024 / 1024\r\nprint(f”Max memory allocated: {max_memory:.3f} GB”)\r\nreturn all_prompt_embeds, all_pooled_prompt_embeds, all_text_ids\r\ndef run(args):\r\ndataset = load_dataset(“Norod78/Yarn-art-style”, split=“train”)\r\nimage_prompts = {generate_image_hash(sample[“image”]): sample[“text”] for sample in dataset}\r\nall_prompts = list(image_prompts.values())\r\nprint(f”{len(all_prompts)=}”)\r\npipeline = load_flux_dev_pipeline()\r\nall_prompt_embeds, all_pooled_prompt_embeds, all_text_ids = compute_embeddings(\r\npipeline, all_prompts, args.max_sequence_length\r\n)\r\ndata = []\r\nfor i, (image_hash, _) in enumerate(image_prompts.items()):\r\ndata.append((image_hash, all_prompt_embeds[i], all_pooled_prompt_embeds[i], all_text_ids[i]))\r\nprint(f”{len(data)=}“)</p>\n<h1 id=\"create-a-dataframe\">Create a DataFrame</h1>\n<p>embedding_cols = [“prompt_embeds”, “pooled_prompt_embeds”, “text_ids”]\r\ndf = pd.DataFrame(data, columns=[“image_hash”] + embedding_cols)\r\nprint(f”{len(df)=}“)</p>\n<h1 id=\"convert-embedding-lists-to-arrays-for-proper-storage-in-parquet\">Convert embedding lists to arrays (for proper storage in parquet)</h1>\n<p>for col in embedding_cols:\r\ndf[col] = df[col].apply(lambda x: x.cpu().numpy().flatten().tolist())</p>\n<h1 id=\"save-the-dataframe-to-a-parquet-file\">Save the dataframe to a parquet file</h1>\n<p>df.to_parquet(args.output_path)\r\nprint(f”Data successfully serialized to {args.output_path}”)\r\nif <strong>name</strong> == “<strong>main</strong>”:\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\r\n“—max_sequence_length”,\r\ntype=int,\r\ndefault=MAX_SEQ_LENGTH,\r\nhelp=“Maximum sequence length to use for computing the embeddings. The more the higher computational costs.”,\r\n)\r\nparser.add_argument(“—output_path”, type=str, default=OUTPUT_PATH, help=“Path to serialize the parquet file.”)\r\nargs = parser.parse_args()\r\nrun(args)\r\nHow to use it\r\npython compute_embeddings.py <br>\n—max_sequence_length 77 <br>\n—output_path embeddings_alphonse_mucha.parquet\r\nBy combining this with cached VAE latents (—cache_latents\r\n) you whittle the active model down to just the quantized transformer + LoRA adapters, keeping the whole fine-tune comfortably under 10 GB of GPU memory.\r\nSetup &#x26; Results\r\nFor this demonstration, we leveraged an NVIDIA RTX 4090 (24GB VRAM) to explore its performance. The full training command using accelerate\r\nis shown below.</p>\n<h1 id=\"you-need-to-pre-compute-the-text-embeddings-first-see-the-diffusers-repo\">You need to pre-compute the text embeddings first. See the diffusers repo.</h1>\n<h1 id=\"httpsgithubcomhuggingfacediffuserstreemainexamplesresearch_projectsflux_lora_quantization\"><a href=\"https://github.com/huggingface/diffusers/tree/main/examples/research_projects/flux_lora_quantization\">https://github.com/huggingface/diffusers/tree/main/examples/research_projects/flux_lora_quantization</a></h1>\n<p>accelerate launch —config_file=accelerate.yaml <br>\ntrain_dreambooth_lora_flux_miniature.py <br>\n—pretrained_model_name_or_path=“black-forest-labs/FLUX.1-dev” <br>\n—data_df_path=“embeddings_alphonse_mucha.parquet” <br>\n—output_dir=“alphonse_mucha_lora_flux_nf4” <br>\n—mixed_precision=“bf16” <br>\n—use_8bit_adam <br>\n—weighting_scheme=“none” <br>\n—width=512 <br>\n—height=768 <br>\n—train_batch_size=1 <br>\n—repeats=1 <br>\n—learning_rate=1e-4 <br>\n—guidance_scale=1 <br>\n—report_to=“wandb” <br>\n—gradient_accumulation_steps=4 <br>\n—gradient_checkpointing \\ # can drop checkpointing when HW has more than 16 GB.\r\n—lr_scheduler=“constant” <br>\n—lr_warmup_steps=0 <br>\n—cache_latents <br>\n—rank=4 <br>\n—max_train_steps=700 <br>\n—seed=“0”\r\nConfiguration for RTX 4090:\r\nOn our RTX 4090, we used a train_batch_size\r\nof 1, gradient_accumulation_steps\r\nof 4, mixed_precision=“bf16”\r\n, gradient_checkpointing=True\r\n, use_8bit_adam=True\r\n, a LoRA rank\r\nof 4, and resolution of 512x768. Latents were cached with cache_latents=True\r\n.\r\nMemory Footprint (RTX 4090):</p>\n<ul>\n<li>QLoRA: Peak VRAM usage for QLoRA fine-tuning was approximately 9GB.</li>\n<li>BF16 LoRA: Running standard LoRA (with the base FLUX.1-dev in FP16) on the same setup consumed 26 GB VRAM.</li>\n<li>BF16 full finetuning: An estimate would be ~120 GB VRAM with no memory optimizations.\r\nTraining Time (RTX 4090):\r\nFine-tuning for 700 steps on the Alphonse Mucha dataset took approximately 41 minutes on the RTX 4090 with train_batch_size\r\nof 1 and resolution of 512x768.\r\nOutput Quality: The ultimate measure is the generated art. Here are samples from our QLoRA fine-tuned model on the derekl35/alphonse-mucha-style dataset:\r\nThis table compares the primary bf16\r\nprecision results. The goal of the fine-tuning was to teach the model the distinct style of Alphonse Mucha.\r\nThe fine-tuned model nicely captured Mucha’s iconic art nouveau style, evident in the decorative motifs and distinct color palette. The QLoRA process maintained excellent fidelity while learning the new style.\r\nClick to see the fp16 comparison\r\nThe results are nearly identical, showing that QLoRA performs effectively with both fp16\r\nand bf16\r\nmixed precision.\r\nModel Comparison: Base vs. QLoRA Fine-tuned (fp16)\r\nFP8 Fine-tuning with torchao\r\nFor users with NVIDIA GPUs possessing compute capability 8.9 or greater (such as the H100, RTX 4090), even greater speed efficiencies can be achieved by leveraging FP8 training via the torchao\r\nlibrary.\r\nWe fine-tuned FLUX.1-dev LoRA on an H100 SXM GPU slightly modifieddiffusers-torchao\r\ntraining scripts. The following command was used:\r\naccelerate launch train_dreambooth_lora_flux.py <br>\n—pretrained_model_name_or_path=black-forest-labs/FLUX.1-dev <br>\n—dataset_name=derekl35/alphonse-mucha-style —instance_prompt=“a woman, alphonse mucha style” —caption_column=“text” <br>\n—output_dir=alphonse_mucha_fp8_lora_flux <br>\n—mixed_precision=bf16 —use_8bit_adam <br>\n—weighting_scheme=none <br>\n—height=768 —width=512 —train_batch_size=1 —repeats=1 <br>\n—learning_rate=1e-4 —guidance_scale=1 —report_to=wandb <br>\n—gradient_accumulation_steps=1 —gradient_checkpointing <br>\n—lr_scheduler=constant —lr_warmup_steps=0 —rank=4 <br>\n—max_train_steps=700 —checkpointing_steps=600 —seed=0 <br>\n—do_fp8_training —push_to_hub\r\nThe training run had a peak memory usage of 36.57 GB and completed in approximately 20 minutes.\r\nQualitative results from this FP8 fine-tuned model are also available:\r\nKey steps to enable FP8 training with torchao\r\ninvolve:</li>\n<li>Injecting FP8 layers into the model using\r\nconvert_to_float8_training\r\nfromtorchao.float8\r\n. - Defining a\r\nmodule_filter_fn\r\nto specify which modules should and should not be converted to FP8.\r\nFor a more detailed guide and code snippets, please refer to this gist and the diffusers-torchao\r\nrepository.\r\nInference with Trained LoRA Adapters\r\nAfter training your LoRA adapters, you have two main approaches for inference.\r\nOption 1: Loading LoRA Adapters\r\nOne approach is to load your trained LoRA adapters on top of the base model.\r\nBenefits of Loading LoRA:</li>\n<li>Flexibility: Easily switch between different LoRA adapters without reloading the base model</li>\n<li>Experimentation: Test multiple artistic styles or concepts by swapping adapters</li>\n<li>Modularity: Combine multiple LoRA adapters using\r\nset_adapters()\r\nfor creative blending - Storage efficiency: Keep a single base model and multiple small adapter files\r\nCode\r\nfrom diffusers import FluxPipeline, FluxTransformer2DModel, BitsAndBytesConfig\r\nimport torch\r\nckpt_id = “black-forest-labs/FLUX.1-dev”\r\npipeline = FluxPipeline.from_pretrained(\r\nckpt_id, torch_dtype=torch.float16\r\n)\r\npipeline.load_lora_weights(“derekl35/alphonse_mucha_qlora_flux”, weight_name=“pytorch_lora_weights.safetensors”)\r\npipeline.enable_model_cpu_offload()\r\nimage = pipeline(\r\n“a puppy in a pond, alphonse mucha style”, num_inference_steps=28, guidance_scale=3.5, height=768, width=512, generator=torch.manual_seed(0)\r\n).images[0]\r\nimage.save(“alphonse_mucha.png”)\r\nOption 2: Merging LoRA into Base Model\r\nFor when you want maximum efficiency with a single style, you can merge the LoRA weights into the base model.\r\nBenefits of Merging LoRA:</li>\n<li>VRAM efficiency: No additional memory overhead from adapter weights during inference</li>\n<li>Speed: Slightly faster inference as there’s no need to apply adapter computations</li>\n<li>Quantization compatibility: Can re-quantize the merged model for maximum memory efficiency\r\nCode\r\nfrom diffusers import FluxPipeline, AutoPipelineForText2Image, FluxTransformer2DModel, BitsAndBytesConfig\r\nimport torch\r\nckpt_id = “black-forest-labs/FLUX.1-dev”\r\npipeline = FluxPipeline.from_pretrained(\r\nckpt_id, text_encoder=None, text_encoder_2=None, torch_dtype=torch.float16\r\n)\r\npipeline.load_lora_weights(“derekl35/alphonse_mucha_qlora_flux”, weight_name=“pytorch_lora_weights.safetensors”)\r\npipeline.fuse_lora()\r\npipeline.unload_lora_weights()\r\npipeline.transformer.save_pretrained(“fused_transformer”)\r\nbnb_4bit_compute_dtype = torch.bfloat16\r\nnf4_config = BitsAndBytesConfig(\r\nload_in_4bit=True,\r\nbnb_4bit_quant_type=“nf4”,\r\nbnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\r\n)\r\ntransformer = FluxTransformer2DModel.from_pretrained(\r\n“fused_transformer”,\r\nquantization_config=nf4_config,\r\ntorch_dtype=bnb_4bit_compute_dtype,\r\n)\r\npipeline = AutoPipelineForText2Image.from_pretrained(\r\nckpt_id, transformer=transformer, torch_dtype=bnb_4bit_compute_dtype\r\n)\r\npipeline.enable_model_cpu_offload()\r\nimage = pipeline(\r\n“a puppy in a pond, alphonse mucha style”, num_inference_steps=28, guidance_scale=3.5, height=768, width=512, generator=torch.manual_seed(0)\r\n).images[0]\r\nimage.save(“alphonse_mucha_merged.png”)\r\nRunning on Google Colab\r\nWhile we showcased results on an RTX 4090, the same code can be run on more accessible hardware like the T4 GPU available in Google Colab for free.\r\nOn a T4, you can expect the fine-tuning process to take significantly longer around 4 hours for the same number of steps. This is a trade-off for accessibility, but it makes custom fine-tuning possible without high-end hardware. Be mindful of usage limits if running on Colab, as a 4-hour training run might push them.\r\nConclusion\r\nQLoRA, coupled with the diffusers\r\nlibrary, significantly democratizes the ability to customize state-of-the-art models like FLUX.1-dev. As demonstrated on an RTX 4090, efficient fine-tuning is well within reach, yielding high-quality stylistic adaptations. Furthermore, for users with the latest NVIDIA hardware, torchao\r\nenables even faster training through FP8 precision.\r\nShare your creations on the Hub!\r\nSharing your fine-tuned LoRA adapters is a fantastic way to contribute to the open-source community. It allows others to easily try out your styles, build on your work, and helps create a vibrant ecosystem of creative AI tools.\r\nIf you’ve trained a LoRA for FLUX.1-dev, we encourage you to share it. The easiest way is to add the —push_to_hub flag to the training script. Alternatively, if you have already trained a model and want to upload it, you can use the following snippet.</li>\n</ul>\n<h1 id=\"prereqs\">Prereqs:</h1>\n<h1 id=\"--pip-install-huggingface_hub-diffusers\">- pip install huggingface_hub diffusers</h1>\n<h1 id=\"--run-huggingface-cli-login-or-set-hf_token-env-var-once\">- Run <code>huggingface-cli login</code> (or set HF_TOKEN env-var) once.</h1>\n<h1 id=\"--save-model\">- save model</h1>\n<p>from huggingface_hub import create_repo, upload_folder\r\nrepo_id = “your-username/alphonse_mucha_qlora_flux”\r\ncreate_repo(repo_id, exist_ok=True)\r\nupload_folder(\r\nrepo_id=repo_id,\r\nfolder_path=“alphonse_mucha_qlora_flux”,\r\ncommit_message=“Add Alphonse Mucha LoRA adapter”\r\n)\r\nCheck out our Mucha QLoRA <a href=\"https://huggingface.co/derekl35/alphonse_mucha_qlora_flux\">https://huggingface.co/derekl35/alphonse_mucha_qlora_flux</a> FP8 LoRA <a href=\"https://huggingface.co/derekl35/alphonse_mucha_fp8_lora_flux\">https://huggingface.co/derekl35/alphonse_mucha_fp8_lora_flux</a>. You can find both, plus other adapters, in this collection as an example.\r\nWe can’t wait to see what you create!</p>",{headings:29,localImagePaths:82,remoteImagePaths:83,frontmatter:84,imagePaths:87},[30,34,37,40,43,46,49,52,55,58,61,64,67,70,73,76,79],{depth:31,slug:32,text:33},1,"check-for-the-use_8bit_adam-flag","Check for the —use_8bit_adam flag",{depth:31,slug:35,text:36},"cache-latents-before-training-if-the-flag-is-set","Cache latents before training if the flag is set",{depth:31,slug:38,text:39},"vae-is-no-longer-needed-free-up-its-memory","VAE is no longer needed, free up its memory",{depth:31,slug:41,text:42},"determine-compute-dtype-based-on-mixed-precision","Determine compute dtype based on mixed precision",{depth:31,slug:44,text:45},"prepare-model-for-k-bit-training","Prepare model for k-bit training",{depth:31,slug:47,text:48},"gradient-checkpointing-is-enabled-later-via-transformerenable_gradient_checkpointing-if-arg-is-set","Gradient checkpointing is enabled later via transformer.enable_gradient_checkpointing() if arg is set",{depth:31,slug:50,text:51},"trainable-params-4669440--all-params-11906077760","trainable params: 4,669,440 || all params: 11,906,077,760",{depth:31,slug:53,text:54},"httpsgithubcomhuggingfacediffusersblobmainexamplesresearch_projectsflux_lora_quantizationcompute_embeddingspy","https://github.com/huggingface/diffusers/blob/main/examples/research_projects/flux_lora_quantization/compute_embeddings.py",{depth:31,slug:56,text:57},"create-a-dataframe","Create a DataFrame",{depth:31,slug:59,text:60},"convert-embedding-lists-to-arrays-for-proper-storage-in-parquet","Convert embedding lists to arrays (for proper storage in parquet)",{depth:31,slug:62,text:63},"save-the-dataframe-to-a-parquet-file","Save the dataframe to a parquet file",{depth:31,slug:65,text:66},"you-need-to-pre-compute-the-text-embeddings-first-see-the-diffusers-repo","You need to pre-compute the text embeddings first. See the diffusers repo.",{depth:31,slug:68,text:69},"httpsgithubcomhuggingfacediffuserstreemainexamplesresearch_projectsflux_lora_quantization","https://github.com/huggingface/diffusers/tree/main/examples/research_projects/flux_lora_quantization",{depth:31,slug:71,text:72},"prereqs","Prereqs:",{depth:31,slug:74,text:75},"--pip-install-huggingface_hub-diffusers","- pip install huggingface_hub diffusers",{depth:31,slug:77,text:78},"--run-huggingface-cli-login-or-set-hf_token-env-var-once","- Run huggingface-cli login (or set HF_TOKEN env-var) once.",{depth:31,slug:80,text:81},"--save-model","- save model",[],[],{title:14,summary:15,pubDate:85,media:17,tags:86,link:22,thumbnail:15},"Thu, 19 Jun 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-(lora)-fine-tuning-flux.1-dev-on-consumer-hardware-c5eb6e.md","2025-06-20-codeagents--structure-a-better-way-to-execute-actions-ec6688",{id:89,data:91,body:96,filePath:97,digest:98,rendered:99,legacyId:115},{title:92,summary:15,pubDate:93,media:17,tags:94,link:95,thumbnail:15},"CodeAgents + Structure: A Better Way to Execute Actions",["Date","2025-05-28T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/structured-codeagent","CodeAgents + Structure: A Better Way to Execute Actions\r\nToday we're sharing research that bridges two powerful paradigms in AI agent design: the expressiveness of code-based actions and the reliability of structured generation. Our findings show that forcing CodeAgents to generate both thoughts and code in a structured JSON format can significantly outperform traditional approaches across multiple benchmarks.\r\nFigure 1: Accuracy comparison of three approaches: Structured CodeAgent (blue), CodeAgent (orange), and ToolCallingAgent (gray) on SmolBench (GAIA, MATH, SimpleQA, and Frames). Error bars represent 95% Confidence Intervals.\r\n🤔 The Evolution of Agent Actions\r\nAI agents need to take actions in the world - whether that's calling APIs, processing data, or reasoning through complex problems. How agents express these actions has evolved through several paradigms:\r\nTraditional JSON Agent: Agents generate structured JSON to call tools.\r\n{\"tool\": \"get_weather\", \"arguments\": {\"city\": \"Paris\"}}\r\nThese agents operate by selecting from a list of predefined tools and generating JSON-formatted calls. This method for calling tools has been popularized by OpenAI's function calling API, and has since then been the most widely used method to call tools.\r\nIt is reliable, but limited by:\r\n- A limited set of actions: The actions the agent can take are expressed only through predefined tools which limit its functionality.\r\n- Lack of composability: If the task requires composing information from multiple sources, JSON agents struggle because they lack support for maintaining intermediate state across tool calls. While some models support parallel tool calls, they can't easily handle scenarios where one tool's output determines the next action or where results need to be compared and processed together.\r\n- Rigid structure: Very limited in handling cases where tools do not match exactly what needs to be done.\r\nCode Agents: Agents make use of their innate coding ability and write executable Python code directly.\r\n# We can get the average temperature in 3 cities in 1 model call.\r\ntemperature_sum = 0\r\nfor city in [\"Paris\", \"Tokyo\", \"New York\"]:\r\ntemp = get_weather(city)\r\ntemperature_sum += temp\r\nprint(f\"Average temperature: {temperature_sum / 3:.1f}°C\")\r\nThis shift, first presented as CodeAct in the paper “Executable Code Actions Elicit Better LLM Agents” gave AI agents the flexibility to write arbitrary executable Python code in addition to tool-calling.\r\nThe key insight here is that tools are called directly from within the code, making variables and state management much more reliable. Agents can call tools within loops, functions, and conditional statements - essentially generating a dynamic graph of tool execution in each action!\r\nPros of using a CodeAgent:\r\n- Smart tool use: Agents decide which tools to use based on what’s happening in the moment.\r\n- Unlimited flexibility: Can use any Python functionality to achieve a goal.\r\n- Ability to test thoughts: Agents can hypothesize and test, leading to more flexibility in their actions\r\nHowever, parsing code from markdown can be error-prone which leads us to a proposition: why not use structured generation to generate code actions?\r\n➡️ Adding Structured outputs to Code Agent\r\nWith Structured outputs, you can force the LLM to generate explicit thoughts and code as a JSON blob:\r\n// The \"code\" block gets parsed into executable Python\r\n{\r\n\"thoughts\": \"I want to find the average temperature across 3 cities.\",\r\n\"code\": \"temperature_sum = 0\\nfor city in [\\\"Paris\\\", \\\"Tokyo\\\", \\\"New York\\\"]:\\n temp = get_weather(city)\\n temperature_sum += temp\\n\\nprint(f\\\"Average temperature: {temperature_sum / 3:.1f}°C\\\")\"\r\n}\r\nThe key difference is that the generation is enforced: basically, now instead of just being prompted to output thoughts, then code, the usage of structured outputs forces it to respect the structure.\r\nThis approach adds the reliability of structured generation to the flexibility of code execution, thus getting the best of both worlds.\r\n- Explicit reasoning: The\r\nthoughts\r\nfield forces the agent to reason right before it takes an action. - Reliable parsing: JSON structure eliminates markdown parsing errors\r\n- Full code expressiveness: The\r\ncode\r\nfield maintains all the flexibility of code agents - Better separation: Clear separation between planning and execution\r\n🧪 Benchmark Results\r\nWe compared these three paradigms across multiple benchmarks including GAIA, MATH, SimpleQA, and Frames. The results show a clear pattern: Code actions + structured generation consistently improves performance for capable models.\r\nAcross most capable models, the structured approach consistently outperformed the regular CodeAgent approach by 2-7 percentage points on average.\r\n- OpenAI models: Show the largest improvements with structure, particularly on reasoning-heavy tasks\r\n- Claude models: Benefit from structure, with Claude 3.7 Sonnet showing especially strong results\r\n- Qwen models: Generally improve with structure, though “structure tax” (see in next section) creeps in for smaller models.\r\n💡 Why Structure (Generally) Helps\r\nThe Parsing Problem is Real\r\nOur implementation of CodeAgent in smolagents extracts Python code from the LLM output, which can fail when:\r\n- Code block formulation in markdown is incomplete or incorrectly formatted\r\n- Multiple code blocks appear in a single response\r\nStructured generation eliminates these issues with reliable JSON parsing.\r\nTo understand why structured generation matters, we analyzed 15,724 agent traces across our benchmarks. The results are striking:\r\n- 2.4% of traces had parsing errors in their first call\r\n- Traces with first call parsing errors: 42.3% success rate\r\n- Traces without first call parsing errors: 51.3% success rate\r\nAgent traces without parsing errors succeed 21.3% more often than those with parsing errors.\r\nThis isn't just about convenience - parsing errors create a cascade of failures that significantly impact overall agent performance. When an agent can't execute its first action due to malformed code, it often struggles to recover, leading to suboptimal problem-solving paths.\r\nFigure 2: Parsing errors in the first step reduce success rates of the agent by 21.3% and increase average steps taken from 3.18 to 4.63.\r\nAdditionally: Enforced Reasoning Process\r\nThe use of structured generation and explicit thoughts\r\nnot just prompts, but forces agents to articulate their reasoning before acting. This leads to:\r\n- Better planning: Agents think through problems more systematically\r\n- Enhanced reliability: Explicit reasoning catches logical errors early\r\nThe Structure Tax\r\nOur results also reveal a clear capability threshold: models need sufficient instruction-following ability and JSON coverage in their pre-training data to benefit from structured generation. This suggests that structured approaches work best with:\r\n- Large, well-trained models\r\n- Models with strong instruction-following capabilities\r\n- Models fine-tuned on structured generation.\r\nWhen Structure Breaks: A Real Example\r\nHere's what happens when a smaller model (e.g mistralai/Mistral-7B-Instruct-v0.3\r\n) tries to generate structured code - the cognitive load becomes too much:\r\n{\r\n\"thought\": \"I need to find the height...\",\r\n\"code\": \"web_search(query=\\\"Eiffel Tower height\\\")\\\", \"\r\n}\r\nThe model generates syntactically broken Python code: web_search(query=\"Eiffel Tower height\")\",\r\n- notice the malformed string with an extra quote and comma. This leads to an immediate SyntaxError and execution failure.\r\nThis illustrates the \"structure tax\": smaller models struggle to simultaneously handle JSON formatting, Python syntax, and the actual problem-solving logic. The cognitive overhead of structured generation can overwhelm models that would otherwise perform reasonably well with simpler markdown-based code generation.\r\n🚀 When to Use Structured CodeAgents\r\n✅ Use Structured CodeAgents when:\r\n- Working with capable models (32B+ parameters or frontier models)\r\n- Tasks require complex reasoning and code execution\r\n- You need reliable parsing of agent outputs\r\n⚠️ Consider alternatives when:\r\n- Working with smaller models that struggle with structured generation\r\n- Simple, predefined workflows are sufficient\r\nHow to use with smolagents:\r\nIt’s super simple! Just enable it with use_structured_outputs_internally:\r\nfrom smolagents import CodeAgent, InferenceClientModel, GoogleSearchTool\r\n# Configure agent for structured generation\r\nagent = CodeAgent(\r\ntools=[GoogleSearchTool(provider=\"serper\")],\r\nmodel=InferenceClientModel(\"Qwen/Qwen3-235B-A22B\", provider='nebius'),\r\nuse_structured_outputs_internally=True # Enable structured output\r\n)\r\nresult = agent.run(\"Calculate the time for a cheetah to run across the Golden Gate Bridge\")\r\nThe LLM will generate something like this:\r\n{\r\n\"thoughts\": \"I need to find the length of the Golden Gate Bridge and the top speed of a cheetah, then calculate the time.\",\r\n\"code\": \"bridge_info = web_search('Golden Gate Bridge length meters')\\ncheetah_speed = web_search('Cheetah top speed') ...\"\r\n}\r\nThen the \"code\" part gets executed by the agent as usual : this is the standard CodeAgent, but now it has 100% parsing reliability!\r\nImplementation Tips\r\n- Clear prompting: Ensure your prompts clearly specify the expected JSON structure\r\n- Model selection: Choose models with strong structured generation capabilities\r\n- Select the right provider: Some API providers like OpenAI or Anthropic support structured generation out of the box. If you're using Inference providers through Hugging Face, the support of structured generation varies across providers. Here is a list of providers that support structured generation: Structured generation support for Models in smolagents‣\r\nThe Bigger Picture - What's Next?\r\nThis research suggests we're moving toward a more nuanced understanding of agent architectures. It's not just about \"what can the agent do?\" but \"how should the agent think about what it's doing?\"\r\nMaybe making the reasoning process more explicit helps the model stay on track. Or maybe it's just easier to parse. Either way, it’s a win.\r\nBut this is just the beginning. There are so many questions left to explore:\r\n- What other structural improvements could help?\r\n- How do we make this work better across different model architectures, specifically smol models?\r\n- What does this tell us about the nature of AI reasoning?\r\nFor now, if you're using smolagents (or building your own CodeAgent system), consider giving structured output a try. Your parsing errors will thank you, and you might just see a nice increase in performance!","src/content/posts/2025-06-20-codeagents-+-structure-a-better-way-to-execute-actions-ec6688.md","4aa78c00fdd990f7",{html:100,metadata:101},"<p>CodeAgents + Structure: A Better Way to Execute Actions\r\nToday we’re sharing research that bridges two powerful paradigms in AI agent design: the expressiveness of code-based actions and the reliability of structured generation. Our findings show that forcing CodeAgents to generate both thoughts and code in a structured JSON format can significantly outperform traditional approaches across multiple benchmarks.\r\nFigure 1: Accuracy comparison of three approaches: Structured CodeAgent (blue), CodeAgent (orange), and ToolCallingAgent (gray) on SmolBench (GAIA, MATH, SimpleQA, and Frames). Error bars represent 95% Confidence Intervals.\r\n🤔 The Evolution of Agent Actions\r\nAI agents need to take actions in the world - whether that’s calling APIs, processing data, or reasoning through complex problems. How agents express these actions has evolved through several paradigms:\r\nTraditional JSON Agent: Agents generate structured JSON to call tools.\r\n{“tool”: “get_weather”, “arguments”: {“city”: “Paris”}}\r\nThese agents operate by selecting from a list of predefined tools and generating JSON-formatted calls. This method for calling tools has been popularized by OpenAI’s function calling API, and has since then been the most widely used method to call tools.\r\nIt is reliable, but limited by:</p>\n<ul>\n<li>A limited set of actions: The actions the agent can take are expressed only through predefined tools which limit its functionality.</li>\n<li>Lack of composability: If the task requires composing information from multiple sources, JSON agents struggle because they lack support for maintaining intermediate state across tool calls. While some models support parallel tool calls, they can’t easily handle scenarios where one tool’s output determines the next action or where results need to be compared and processed together.</li>\n<li>Rigid structure: Very limited in handling cases where tools do not match exactly what needs to be done.\r\nCode Agents: Agents make use of their innate coding ability and write executable Python code directly.</li>\n</ul>\n<h1 id=\"we-can-get-the-average-temperature-in-3-cities-in-1-model-call\">We can get the average temperature in 3 cities in 1 model call.</h1>\n<p>temperature_sum = 0\r\nfor city in [“Paris”, “Tokyo”, “New York”]:\r\ntemp = get_weather(city)\r\ntemperature_sum += temp\r\nprint(f”Average temperature: {temperature_sum / 3:.1f}°C”)\r\nThis shift, first presented as CodeAct in the paper “Executable Code Actions Elicit Better LLM Agents” gave AI agents the flexibility to write arbitrary executable Python code in addition to tool-calling.\r\nThe key insight here is that tools are called directly from within the code, making variables and state management much more reliable. Agents can call tools within loops, functions, and conditional statements - essentially generating a dynamic graph of tool execution in each action!\r\nPros of using a CodeAgent:</p>\n<ul>\n<li>Smart tool use: Agents decide which tools to use based on what’s happening in the moment.</li>\n<li>Unlimited flexibility: Can use any Python functionality to achieve a goal.</li>\n<li>Ability to test thoughts: Agents can hypothesize and test, leading to more flexibility in their actions\r\nHowever, parsing code from markdown can be error-prone which leads us to a proposition: why not use structured generation to generate code actions?\r\n➡️ Adding Structured outputs to Code Agent\r\nWith Structured outputs, you can force the LLM to generate explicit thoughts and code as a JSON blob:\r\n// The “code” block gets parsed into executable Python\r\n{\r\n“thoughts”: “I want to find the average temperature across 3 cities.”,\r\n“code”: “temperature_sum = 0\\nfor city in [“Paris”, “Tokyo”, “New York”]:\\n temp = get_weather(city)\\n temperature_sum += temp\\n\\nprint(f”Average temperature: {temperature_sum / 3:.1f}°C”)”\r\n}\r\nThe key difference is that the generation is enforced: basically, now instead of just being prompted to output thoughts, then code, the usage of structured outputs forces it to respect the structure.\r\nThis approach adds the reliability of structured generation to the flexibility of code execution, thus getting the best of both worlds.</li>\n<li>Explicit reasoning: The\r\nthoughts\r\nfield forces the agent to reason right before it takes an action. - Reliable parsing: JSON structure eliminates markdown parsing errors</li>\n<li>Full code expressiveness: The\r\ncode\r\nfield maintains all the flexibility of code agents - Better separation: Clear separation between planning and execution\r\n🧪 Benchmark Results\r\nWe compared these three paradigms across multiple benchmarks including GAIA, MATH, SimpleQA, and Frames. The results show a clear pattern: Code actions + structured generation consistently improves performance for capable models.\r\nAcross most capable models, the structured approach consistently outperformed the regular CodeAgent approach by 2-7 percentage points on average.</li>\n<li>OpenAI models: Show the largest improvements with structure, particularly on reasoning-heavy tasks</li>\n<li>Claude models: Benefit from structure, with Claude 3.7 Sonnet showing especially strong results</li>\n<li>Qwen models: Generally improve with structure, though “structure tax” (see in next section) creeps in for smaller models.\r\n💡 Why Structure (Generally) Helps\r\nThe Parsing Problem is Real\r\nOur implementation of CodeAgent in smolagents extracts Python code from the LLM output, which can fail when:</li>\n<li>Code block formulation in markdown is incomplete or incorrectly formatted</li>\n<li>Multiple code blocks appear in a single response\r\nStructured generation eliminates these issues with reliable JSON parsing.\r\nTo understand why structured generation matters, we analyzed 15,724 agent traces across our benchmarks. The results are striking:</li>\n<li>2.4% of traces had parsing errors in their first call</li>\n<li>Traces with first call parsing errors: 42.3% success rate</li>\n<li>Traces without first call parsing errors: 51.3% success rate\r\nAgent traces without parsing errors succeed 21.3% more often than those with parsing errors.\r\nThis isn’t just about convenience - parsing errors create a cascade of failures that significantly impact overall agent performance. When an agent can’t execute its first action due to malformed code, it often struggles to recover, leading to suboptimal problem-solving paths.\r\nFigure 2: Parsing errors in the first step reduce success rates of the agent by 21.3% and increase average steps taken from 3.18 to 4.63.\r\nAdditionally: Enforced Reasoning Process\r\nThe use of structured generation and explicit thoughts\r\nnot just prompts, but forces agents to articulate their reasoning before acting. This leads to:</li>\n<li>Better planning: Agents think through problems more systematically</li>\n<li>Enhanced reliability: Explicit reasoning catches logical errors early\r\nThe Structure Tax\r\nOur results also reveal a clear capability threshold: models need sufficient instruction-following ability and JSON coverage in their pre-training data to benefit from structured generation. This suggests that structured approaches work best with:</li>\n<li>Large, well-trained models</li>\n<li>Models with strong instruction-following capabilities</li>\n<li>Models fine-tuned on structured generation.\r\nWhen Structure Breaks: A Real Example\r\nHere’s what happens when a smaller model (e.g mistralai/Mistral-7B-Instruct-v0.3\r\n) tries to generate structured code - the cognitive load becomes too much:\r\n{\r\n“thought”: “I need to find the height…”,\r\n“code”: “web_search(query=“Eiffel Tower height”)”, ”\r\n}\r\nThe model generates syntactically broken Python code: web_search(query=“Eiffel Tower height”)”,</li>\n<li>notice the malformed string with an extra quote and comma. This leads to an immediate SyntaxError and execution failure.\r\nThis illustrates the “structure tax”: smaller models struggle to simultaneously handle JSON formatting, Python syntax, and the actual problem-solving logic. The cognitive overhead of structured generation can overwhelm models that would otherwise perform reasonably well with simpler markdown-based code generation.\r\n🚀 When to Use Structured CodeAgents\r\n✅ Use Structured CodeAgents when:</li>\n<li>Working with capable models (32B+ parameters or frontier models)</li>\n<li>Tasks require complex reasoning and code execution</li>\n<li>You need reliable parsing of agent outputs\r\n⚠️ Consider alternatives when:</li>\n<li>Working with smaller models that struggle with structured generation</li>\n<li>Simple, predefined workflows are sufficient\r\nHow to use with smolagents:\r\nIt’s super simple! Just enable it with use_structured_outputs_internally:\r\nfrom smolagents import CodeAgent, InferenceClientModel, GoogleSearchTool</li>\n</ul>\n<h1 id=\"configure-agent-for-structured-generation\">Configure agent for structured generation</h1>\n<p>agent = CodeAgent(\r\ntools=[GoogleSearchTool(provider=“serper”)],\r\nmodel=InferenceClientModel(“Qwen/Qwen3-235B-A22B”, provider=‘nebius’),\r\nuse_structured_outputs_internally=True # Enable structured output\r\n)\r\nresult = agent.run(“Calculate the time for a cheetah to run across the Golden Gate Bridge”)\r\nThe LLM will generate something like this:\r\n{\r\n“thoughts”: “I need to find the length of the Golden Gate Bridge and the top speed of a cheetah, then calculate the time.”,\r\n“code”: “bridge_info = web_search(‘Golden Gate Bridge length meters’)\\ncheetah_speed = web_search(‘Cheetah top speed’) …”\r\n}\r\nThen the “code” part gets executed by the agent as usual : this is the standard CodeAgent, but now it has 100% parsing reliability!\r\nImplementation Tips</p>\n<ul>\n<li>Clear prompting: Ensure your prompts clearly specify the expected JSON structure</li>\n<li>Model selection: Choose models with strong structured generation capabilities</li>\n<li>Select the right provider: Some API providers like OpenAI or Anthropic support structured generation out of the box. If you’re using Inference providers through Hugging Face, the support of structured generation varies across providers. Here is a list of providers that support structured generation: Structured generation support for Models in smolagents‣\r\nThe Bigger Picture - What’s Next?\r\nThis research suggests we’re moving toward a more nuanced understanding of agent architectures. It’s not just about “what can the agent do?” but “how should the agent think about what it’s doing?”\r\nMaybe making the reasoning process more explicit helps the model stay on track. Or maybe it’s just easier to parse. Either way, it’s a win.\r\nBut this is just the beginning. There are so many questions left to explore:</li>\n<li>What other structural improvements could help?</li>\n<li>How do we make this work better across different model architectures, specifically smol models?</li>\n<li>What does this tell us about the nature of AI reasoning?\r\nFor now, if you’re using smolagents (or building your own CodeAgent system), consider giving structured output a try. Your parsing errors will thank you, and you might just see a nice increase in performance!</li>\n</ul>",{headings:102,localImagePaths:109,remoteImagePaths:110,frontmatter:111,imagePaths:114},[103,106],{depth:31,slug:104,text:105},"we-can-get-the-average-temperature-in-3-cities-in-1-model-call","We can get the average temperature in 3 cities in 1 model call.",{depth:31,slug:107,text:108},"configure-agent-for-structured-generation","Configure agent for structured generation",[],[],{title:92,summary:15,pubDate:112,media:17,tags:113,link:95,thumbnail:15},"Wed, 28 May 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-codeagents-+-structure-a-better-way-to-execute-actions-ec6688.md","2025-06-20-dell-enterprise-hub-is-all-you-need-to-build-ai-on-premises-979866",{id:116,data:118,body:123,filePath:124,digest:125,rendered:126,legacyId:136},{title:119,summary:15,pubDate:120,media:17,tags:121,link:122,thumbnail:15},"Dell Enterprise Hub is all you need to build AI on premises",["Date","2025-05-23T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/dell-ai-applications","Dell Enterprise Hub is all you need to build AI on premises\r\nThis week at Dell Tech World, we announced the new version of Dell Enterprise Hub, with a complete suite of models and applications to easily build AI running on premises with Dell AI servers and AI PCs.\r\nModels Ready for Action\r\nIf you go to the Dell Enterprise Hub today, you can find some of the most popular models, like Meta Llama 4 Maverick, DeepSeek R1 or Google Gemma 3, available for deployment and training in a few clicks.\r\nBut what you get is much more than a model, it’s a fully tested container optimized for specific Dell AI Server Platforms, with easy instructions to deploy on-premises using Docker and Kubernetes.\r\nMeta Llama 4 Maverick can be deployed on NVIDIA H200 or AMD MI300X Dell PowerEdge servers\r\nWe continuously work with Dell CTIO and Engineering teams to make the latest and greatest models ready, tested and optimized for Dell AI Server platforms as quickly as possible - Llama 4 models were available on the Dell Enterprise Hub within 1 hour of their public release by Meta!\r\nIntroducing AI Applications\r\nThe Dell Enterprise Hub now features ready-to-deploy AI Applications!\r\nIf models are engines, then applications are the cars that make them useful so you can actually go places. With the new Application Catalog you can build powerful applications that run entirely on-premises for your employees and use your internal data and services.\r\nThe new Application Catalog makes it easy to deploy leading open source applications within your private network, including OpenWebUI and AnythingLLM.\r\nOpenWebUI makes it easy to deploy on-premises chatbot assistants that connect to your internal data and services via MCP, to build agentic experiences that can search the web, retrieve internal data with vector databases and storage for RAG use cases.\r\nAnythingLLM makes it easy to build powerful agentic assistants connecting to multiple MCP servers so you can connect your internal systems or even external services. It includes features to enable multiple models, working with images, documents and set role-based access controls for your internal users.\r\nThese applications are easy to deploy using the provided, customizable helm charts so your MCP servers are registered from the get go.\r\nPowered by NVIDIA, AMD and Intel\r\nDell Enterprise Hub is the only platform in the world that offers ready-to-use model deployment solutions for the latest AI Accelerator hardware:\r\n- NVIDIA H100 and H200 GPU powered Dell platforms\r\n- AMD MI300X powered Dell platforms\r\n- Intel Gaudi 3 powered Dell platforms\r\nWe work directly with Dell, NVIDIA, AMD and Intel so that when you deploy a container on your system, it’s all configured and ready to go, has been fully tested and benchmarked so it runs with the best performance out of the box on your Dell AI Server platform.\r\nOn-Device Models for Dell AI PC\r\nThe new Dell Enterprise Hub now provides support for models to run on-device on Dell AI PCs in addition to AI Servers!\r\nThese models enable on-device speech transcription (OpenAI whisper), chat assistants (Microsoft Phi and Qwen 2.5), upscaling images and generating embeddings.\r\nTo deploy a model, you can follow specific instructions for the Dell AI PC of your choice, powered by Intel or Qualcomm NPUs, using the new Dell Pro AI Studio. Coupled with PC fleet management systems like Microsoft Intune, it’s a complete solution for IT organizations to enable employees with on-device AI capabilities.\r\nNow with CLI and Python SDK\r\nDell Enterprise Hub offers an online portal into AI capabilities for Dell AI Server platforms and AI PCs. But what if you want to work directly from your development environment?\r\nIntroducing the new dell-ai open source library with a Python SDK and CLI, so you can use Dell Enterprise Hub within your environment directly from your terminal or code - just pip install dell-ai\r\nWrapping up\r\nWith Models and Applications, for AI Servers and AI PCs, easily installable using Docker, Kubernetes and Dell Pro AI Studio, Dell Enterprise Hub is a complete toolkit to deploy Gen AI applications in the enterprise, fully secure and on-premises.\r\nAs a Dell customer, that means you can very quickly, within an hour instead of weeks:\r\n- roll out an in-network chat assistant powered by the latest open LLMs, and connect it to your internal storage systems (ex. Dell PowerScale) using MCP, all in an air gapped environment\r\n- give access to complex agentic systems, with granular access controls and SSO, that can work with internal text, code, images, audio and documents and access the web for current context\r\n- set up employees with on-device, private transcription powered by a fleet of Dell AI PCs in a fully managed way\r\nIf you are using Dell Enterprise Hub today, we would love to hear from you in the comments!","src/content/posts/2025-06-20-dell-enterprise-hub-is-all-you-need-to-build-ai-on-premises-979866.md","8e694c50a46df3fc",{html:127,metadata:128},"<p>Dell Enterprise Hub is all you need to build AI on premises\r\nThis week at Dell Tech World, we announced the new version of Dell Enterprise Hub, with a complete suite of models and applications to easily build AI running on premises with Dell AI servers and AI PCs.\r\nModels Ready for Action\r\nIf you go to the Dell Enterprise Hub today, you can find some of the most popular models, like Meta Llama 4 Maverick, DeepSeek R1 or Google Gemma 3, available for deployment and training in a few clicks.\r\nBut what you get is much more than a model, it’s a fully tested container optimized for specific Dell AI Server Platforms, with easy instructions to deploy on-premises using Docker and Kubernetes.\r\nMeta Llama 4 Maverick can be deployed on NVIDIA H200 or AMD MI300X Dell PowerEdge servers\r\nWe continuously work with Dell CTIO and Engineering teams to make the latest and greatest models ready, tested and optimized for Dell AI Server platforms as quickly as possible - Llama 4 models were available on the Dell Enterprise Hub within 1 hour of their public release by Meta!\r\nIntroducing AI Applications\r\nThe Dell Enterprise Hub now features ready-to-deploy AI Applications!\r\nIf models are engines, then applications are the cars that make them useful so you can actually go places. With the new Application Catalog you can build powerful applications that run entirely on-premises for your employees and use your internal data and services.\r\nThe new Application Catalog makes it easy to deploy leading open source applications within your private network, including OpenWebUI and AnythingLLM.\r\nOpenWebUI makes it easy to deploy on-premises chatbot assistants that connect to your internal data and services via MCP, to build agentic experiences that can search the web, retrieve internal data with vector databases and storage for RAG use cases.\r\nAnythingLLM makes it easy to build powerful agentic assistants connecting to multiple MCP servers so you can connect your internal systems or even external services. It includes features to enable multiple models, working with images, documents and set role-based access controls for your internal users.\r\nThese applications are easy to deploy using the provided, customizable helm charts so your MCP servers are registered from the get go.\r\nPowered by NVIDIA, AMD and Intel\r\nDell Enterprise Hub is the only platform in the world that offers ready-to-use model deployment solutions for the latest AI Accelerator hardware:</p>\n<ul>\n<li>NVIDIA H100 and H200 GPU powered Dell platforms</li>\n<li>AMD MI300X powered Dell platforms</li>\n<li>Intel Gaudi 3 powered Dell platforms\r\nWe work directly with Dell, NVIDIA, AMD and Intel so that when you deploy a container on your system, it’s all configured and ready to go, has been fully tested and benchmarked so it runs with the best performance out of the box on your Dell AI Server platform.\r\nOn-Device Models for Dell AI PC\r\nThe new Dell Enterprise Hub now provides support for models to run on-device on Dell AI PCs in addition to AI Servers!\r\nThese models enable on-device speech transcription (OpenAI whisper), chat assistants (Microsoft Phi and Qwen 2.5), upscaling images and generating embeddings.\r\nTo deploy a model, you can follow specific instructions for the Dell AI PC of your choice, powered by Intel or Qualcomm NPUs, using the new Dell Pro AI Studio. Coupled with PC fleet management systems like Microsoft Intune, it’s a complete solution for IT organizations to enable employees with on-device AI capabilities.\r\nNow with CLI and Python SDK\r\nDell Enterprise Hub offers an online portal into AI capabilities for Dell AI Server platforms and AI PCs. But what if you want to work directly from your development environment?\r\nIntroducing the new dell-ai open source library with a Python SDK and CLI, so you can use Dell Enterprise Hub within your environment directly from your terminal or code - just pip install dell-ai\r\nWrapping up\r\nWith Models and Applications, for AI Servers and AI PCs, easily installable using Docker, Kubernetes and Dell Pro AI Studio, Dell Enterprise Hub is a complete toolkit to deploy Gen AI applications in the enterprise, fully secure and on-premises.\r\nAs a Dell customer, that means you can very quickly, within an hour instead of weeks:</li>\n<li>roll out an in-network chat assistant powered by the latest open LLMs, and connect it to your internal storage systems (ex. Dell PowerScale) using MCP, all in an air gapped environment</li>\n<li>give access to complex agentic systems, with granular access controls and SSO, that can work with internal text, code, images, audio and documents and access the web for current context</li>\n<li>set up employees with on-device, private transcription powered by a fleet of Dell AI PCs in a fully managed way\r\nIf you are using Dell Enterprise Hub today, we would love to hear from you in the comments!</li>\n</ul>",{headings:129,localImagePaths:130,remoteImagePaths:131,frontmatter:132,imagePaths:135},[],[],[],{title:119,summary:15,pubDate:133,media:17,tags:134,link:122,thumbnail:15},"Fri, 23 May 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-dell-enterprise-hub-is-all-you-need-to-build-ai-on-premises-979866.md","2025-06-20-featherless-ai-on-hugging-face-inference-providers--1b86cd",{id:137,data:139,body:144,filePath:145,digest:146,rendered:147,legacyId:157},{title:140,summary:15,pubDate:141,media:17,tags:142,link:143,thumbnail:15},"Featherless AI on Hugging Face Inference Providers 🔥",["Date","2025-06-12T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/inference-providers-featherless","Featherless AI on Hugging Face Inference Providers 🔥\r\nWe're thrilled to share that Featherless AI is now a supported Inference Provider on the Hugging Face Hub! Featherless AI joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub’s model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\r\nFeatherless AI supports a wide variety of text and conversational models, including the latest open-source models from DeepSeek, Meta, Google, Qwen, and much more.\r\nFeatherless AI is a serverless AI inference provider with unique model loading and GPU orchestration abilities that makes an exceptionally large catalog of models available for users. Providers often offer either a low cost of access to a limited set of models, or an unlimited range of models with users managing servers and the associated costs of operation. Featherless provides the best of both worlds offering unmatched model range and variety but with serverless pricing. Find the full list of supported models on the models page.\r\nWe're super excited to see what you'll build with this new provider!\r\nRead more about how to use Featherless as an Inference Provider in its dedicated documentation page.\r\nHow it works\r\nIn the website UI\r\n- In your user account settings, you are able to:\r\n- Set your own API keys for the providers you’ve signed up with. If no custom key is set, your requests will be routed through HF. Learn more about request types in the docs.\r\n- Order providers by preference. This applies to the widget and code snippets in the model pages.\r\n- As mentioned, there are two modes when calling Inference Providers:\r\n- Custom key (calls go directly to the inference provider, using your own API key of the corresponding inference provider)\r\n- Routed by HF (in that case, you don't need a token from the provider, and the charges are applied directly to your HF account rather than the provider's account)\r\n- Model pages showcase third-party inference providers (the ones that are compatible with the current model, sorted by user preference)\r\nFrom the client SDKs\r\nfrom Python, using huggingface_hub\r\nThe following example shows how to use DeepSeek-R1 using Featherless AI as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own Featherless AI API key if you have one.\r\nInstall or upgrade huggingface_hub\r\nto ensure you have version v0.33.0 or better: pip install --upgrade huggingface-hub\r\nimport os\r\nfrom huggingface_hub import InferenceClient\r\nclient = InferenceClient(\r\nprovider=\"featherless-ai\",\r\napi_key=os.environ[\"HF_TOKEN\"]\r\n)\r\nmessages = [\r\n{\r\n\"role\": \"user\",\r\n\"content\": \"What is the capital of France?\"\r\n}\r\n]\r\ncompletion = client.chat.completions.create(\r\nmodel=\"deepseek-ai/DeepSeek-R1-0528\",\r\nmessages=messages,\r\n)\r\nprint(completion.choices[0].message)\r\nfrom JS using @huggingface/inference\r\nimport { InferenceClient } from \"@huggingface/inference\";\r\nconst client = new InferenceClient(process.env.HF_TOKEN);\r\nconst chatCompletion = await client.chatCompletion({\r\nmodel: \"deepseek-ai/DeepSeek-R1-0528\",\r\nmessages: [\r\n{\r\nrole: \"user\",\r\ncontent: \"What is the capital of France?\"\r\n}\r\n],\r\nprovider: \"featherless-ai\",\r\n});\r\nconsole.log(chatCompletion.choices[0].message);\r\nBilling\r\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use a Featherless AI API key you're billed on your Featherless AI account.\r\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you'll only pay the standard provider API rates. There's no additional markup from us, we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\r\nImportant Note ‼️ PRO users get $2 worth of Inference credits every month. You can use them across providers. 🔥\r\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\r\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\r\nFeedback and next steps\r\nWe would love to get your feedback! Share your thoughts and/or comments here: https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49","src/content/posts/2025-06-20-featherless-ai-on-hugging-face-inference-providers-🔥-1b86cd.md","cd36db453e68c98a",{html:148,metadata:149},"<p>Featherless AI on Hugging Face Inference Providers 🔥\r\nWe’re thrilled to share that Featherless AI is now a supported Inference Provider on the Hugging Face Hub! Featherless AI joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub’s model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\r\nFeatherless AI supports a wide variety of text and conversational models, including the latest open-source models from DeepSeek, Meta, Google, Qwen, and much more.\r\nFeatherless AI is a serverless AI inference provider with unique model loading and GPU orchestration abilities that makes an exceptionally large catalog of models available for users. Providers often offer either a low cost of access to a limited set of models, or an unlimited range of models with users managing servers and the associated costs of operation. Featherless provides the best of both worlds offering unmatched model range and variety but with serverless pricing. Find the full list of supported models on the models page.\r\nWe’re super excited to see what you’ll build with this new provider!\r\nRead more about how to use Featherless as an Inference Provider in its dedicated documentation page.\r\nHow it works\r\nIn the website UI</p>\n<ul>\n<li>In your user account settings, you are able to:</li>\n<li>Set your own API keys for the providers you’ve signed up with. If no custom key is set, your requests will be routed through HF. Learn more about request types in the docs.</li>\n<li>Order providers by preference. This applies to the widget and code snippets in the model pages.</li>\n<li>As mentioned, there are two modes when calling Inference Providers:</li>\n<li>Custom key (calls go directly to the inference provider, using your own API key of the corresponding inference provider)</li>\n<li>Routed by HF (in that case, you don’t need a token from the provider, and the charges are applied directly to your HF account rather than the provider’s account)</li>\n<li>Model pages showcase third-party inference providers (the ones that are compatible with the current model, sorted by user preference)\r\nFrom the client SDKs\r\nfrom Python, using huggingface_hub\r\nThe following example shows how to use DeepSeek-R1 using Featherless AI as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own Featherless AI API key if you have one.\r\nInstall or upgrade huggingface_hub\r\nto ensure you have version v0.33.0 or better: pip install —upgrade huggingface-hub\r\nimport os\r\nfrom huggingface_hub import InferenceClient\r\nclient = InferenceClient(\r\nprovider=“featherless-ai”,\r\napi_key=os.environ[“HF_TOKEN”]\r\n)\r\nmessages = [\r\n{\r\n“role”: “user”,\r\n“content”: “What is the capital of France?”\r\n}\r\n]\r\ncompletion = client.chat.completions.create(\r\nmodel=“deepseek-ai/DeepSeek-R1-0528”,\r\nmessages=messages,\r\n)\r\nprint(completion.choices[0].message)\r\nfrom JS using @huggingface/inference\r\nimport { InferenceClient } from “@huggingface/inference”;\r\nconst client = new InferenceClient(process.env.HF_TOKEN);\r\nconst chatCompletion = await client.chatCompletion({\r\nmodel: “deepseek-ai/DeepSeek-R1-0528”,\r\nmessages: [\r\n{\r\nrole: “user”,\r\ncontent: “What is the capital of France?”\r\n}\r\n],\r\nprovider: “featherless-ai”,\r\n});\r\nconsole.log(chatCompletion.choices[0].message);\r\nBilling\r\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use a Featherless AI API key you’re billed on your Featherless AI account.\r\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you’ll only pay the standard provider API rates. There’s no additional markup from us, we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\r\nImportant Note ‼️ PRO users get $2 worth of Inference credits every month. You can use them across providers. 🔥\r\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\r\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\r\nFeedback and next steps\r\nWe would love to get your feedback! Share your thoughts and/or comments here: <a href=\"https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49\">https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49</a></li>\n</ul>",{headings:150,localImagePaths:151,remoteImagePaths:152,frontmatter:153,imagePaths:156},[],[],[],{title:140,summary:15,pubDate:154,media:17,tags:155,link:143,thumbnail:15},"Thu, 12 Jun 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-featherless-ai-on-hugging-face-inference-providers-🔥-1b86cd.md","2025-06-20-exploring-quantization-backends-in-diffusers-bbb537",{id:158,data:160,body:165,filePath:166,digest:167,rendered:168,legacyId:184},{title:161,summary:15,pubDate:162,media:17,tags:163,link:164,thumbnail:15},"Exploring Quantization Backends in Diffusers",["Date","2025-05-21T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/diffusers-quantization","Exploring Quantization Backends in Diffusers\r\nLarge diffusion models like Flux (a flow-based text-to-image generation model) can create stunning images, but their size can be a hurdle, demanding significant memory and compute resources. Quantization offers a powerful solution, shrinking these models to make them more accessible without drastically compromising performance. But the big question always is: can you actually tell the difference in the final image?\r\nBefore we dive into the technical details of how various quantization backends in Hugging Face Diffusers work, why not test your own perception?\r\nSpot The Quantized Model\r\nWe created a setup where you can provide a prompt, and we generate results using both the original, high-precision model (e.g., Flux-dev in BF16) and several quantized versions (BnB 4-bit, BnB 8-bit). The generated images are then presented to you and your challenge is to identify which ones came from the quantized models.\r\nTry it out here or below!\r\nOften, especially with 8-bit quantization, the differences are subtle and may not be noticeable without close inspection. More aggressive quantization like 4-bit or lower might be more noticeable, but the results can still be good, especially considering the massive memory savings. NF4 often gives the best trade-off though.\r\nNow, let's dive deeper.\r\nQuantization Backends in Diffusers\r\nBuilding on our previous post, \"Memory-efficient Diffusion Transformers with Quanto and Diffusers\", this post explores the diverse quantization backends integrated directly into Hugging Face Diffusers. We'll examine how bitsandbytes, GGUF, torchao, Quanto and native FP8 support make large and powerful models more accessible, demonstrating their use with Flux.\r\nBefore diving into the quantization backends, let's introduce the FluxPipeline (using the black-forest-labs/FLUX.1-dev checkpoint) and its components, which we'll be quantizing. Loading the full FLUX.1-dev\r\nmodel in BF16 precision requires approximately 31.447 GB of memory. The main components are:\r\n- Text Encoders (CLIP and T5):\r\n- Function: Process input text prompts. FLUX-dev uses CLIP for initial understanding and a larger T5 for nuanced comprehension and better text rendering.\r\n- Memory: T5 - 9.52 GB; CLIP - 246 MB (in BF16)\r\n- Transformer (Main Model - MMDiT):\r\n- Function: Core generative part (Multimodal Diffusion Transformer). Generates images in latent space from text embeddings.\r\n- Memory: 23.8 GB (in BF16)\r\n- Variational Auto-Encoder (VAE):\r\n- Function: Translates images between pixel and latent space. Decodes generated latent representation to a pixel-based image.\r\n- Memory: 168 MB (in BF16)\r\n- Focus of Quantization: Examples will primarily focus on the\r\ntransformer\r\nandtext_encoder_2\r\n(T5) for the most substantial memory savings.\r\nprompts = [\r\n\"Baroque style, a lavish palace interior with ornate gilded ceilings, intricate tapestries, and dramatic lighting over a grand staircase.\",\r\n\"Futurist style, a dynamic spaceport with sleek silver starships docked at angular platforms, surrounded by distant planets and glowing energy lines.\",\r\n\"Noir style, a shadowy alleyway with flickering street lamps and a solitary trench-coated figure, framed by rain-soaked cobblestones and darkened storefronts.\",\r\n]\r\nbitsandbytes (BnB)\r\nbitsandbytes\r\nis a popular and user-friendly library for 8-bit and 4-bit quantization, widely used for LLMs and QLoRA fine-tuning. We can use it for transformer-based diffusion and flow models, too.\r\n|\r\nBF16 |\r\nBnB 4-bit |\r\nBnB 8-bit |\r\n| Visual comparison of Flux-dev model outputs using BF16 (left), BnB 4-bit (center), and BnB 8-bit (right) quantization. (Click on an image to zoom) |\r\n| Precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| BF16 | ~31.447 GB | 36.166 GB | 12 seconds |\r\n| 4-bit | 12.584 GB | 17.281 GB | 12 seconds |\r\n| 8-bit | 19.273 GB | 24.432 GB | 27 seconds |\r\nAll benchmarks performed on 1x NVIDIA H100 80GB GPU\r\nExample (Flux-dev with BnB 4-bit):\r\nimport torch\r\nfrom diffusers import FluxPipeline\r\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\r\nfrom diffusers.quantizers import PipelineQuantizationConfig\r\nfrom transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\r\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\r\npipeline_quant_config = PipelineQuantizationConfig(\r\nquant_mapping={\r\n\"transformer\": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\r\n\"text_encoder_2\": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\r\n}\r\n)\r\npipe = FluxPipeline.from_pretrained(\r\nmodel_id,\r\nquantization_config=pipeline_quant_config,\r\ntorch_dtype=torch.bfloat16\r\n)\r\npipe.to(\"cuda\")\r\nprompt = \"Baroque style, a lavish palace interior with ornate gilded ceilings, intricate tapestries, and dramatic lighting over a grand staircase.\"\r\npipe_kwargs = {\r\n\"prompt\": prompt,\r\n\"height\": 1024,\r\n\"width\": 1024,\r\n\"guidance_scale\": 3.5,\r\n\"num_inference_steps\": 50,\r\n\"max_sequence_length\": 512,\r\n}\r\nprint(f\"Pipeline memory usage: {torch.cuda.max_memory_reserved() / 1024**3:.3f} GB\")\r\nimage = pipe(\r\n**pipe_kwargs, generator=torch.manual_seed(0),\r\n).images[0]\r\nprint(f\"Pipeline memory usage: {torch.cuda.max_memory_reserved() / 1024**3:.3f} GB\")\r\nimage.save(\"flux-dev_bnb_4bit.png\")\r\nNote: When using\r\nPipelineQuantizationConfig\r\nwithbitsandbytes\r\n, you need to importDiffusersBitsAndBytesConfig\r\nfromdiffusers\r\nandTransformersBitsAndBytesConfig\r\nfromtransformers\r\nseparately. This is because these components originate from different libraries. If you prefer a simpler setup without managing these distinct imports, you can use an alternative approach for pipeline-level quantization, an example of this method is in the Diffusers documentation on Pipeline-level quantization.\r\nFor more information check out the bitsandbytes docs.\r\ntorchao\r\ntorchao\r\nis a PyTorch-native library for architecture optimization, offering quantization, sparsity, and custom data types, designed for compatibility with torch.compile\r\nand FSDP. Diffusers supports a wide range of torchao\r\n's exotic data types, enabling fine-grained control over model optimization.\r\n|\r\nint4_weight_only |\r\nint8_weight_only |\r\nfloat8_weight_only |\r\n| Visual comparison of Flux-dev model outputs using torchao int4_weight_only (left), int8_weight_only (center), and float8_weight_only (right) quantization. (Click on an image to zoom) |\r\n| torchao Precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| int4_weight_only | 10.635 GB | 14.654 GB | 109 seconds |\r\n| int8_weight_only | 17.020 GB | 21.482 GB | 15 seconds |\r\n| float8_weight_only | 17.016 GB | 21.488 GB | 15 seconds |\r\nExample (Flux-dev with torchao INT8 weight-only):\r\n@@\r\n- from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\r\n+ from diffusers import TorchAoConfig as DiffusersTorchAoConfig\r\n- from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\r\n+ from transformers import TorchAoConfig as TransformersTorchAoConfig\r\n@@\r\npipeline_quant_config = PipelineQuantizationConfig(\r\nquant_mapping={\r\n- \"transformer\": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\r\n- \"text_encoder_2\": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\r\n+ \"transformer\": DiffusersTorchAoConfig(\"int8_weight_only\"),\r\n+ \"text_encoder_2\": TransformersTorchAoConfig(\"int8_weight_only\"),\r\n}\r\n)\r\nExample (Flux-dev with torchao INT4 weight-only):\r\n@@\r\n- from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\r\n+ from diffusers import TorchAoConfig as DiffusersTorchAoConfig\r\n- from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\r\n+ from transformers import TorchAoConfig as TransformersTorchAoConfig\r\n@@\r\npipeline_quant_config = PipelineQuantizationConfig(\r\nquant_mapping={\r\n- \"transformer\": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\r\n- \"text_encoder_2\": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\r\n+ \"transformer\": DiffusersTorchAoConfig(\"int4_weight_only\"),\r\n+ \"text_encoder_2\": TransformersTorchAoConfig(\"int4_weight_only\"),\r\n}\r\n)\r\npipe = FluxPipeline.from_pretrained(\r\nmodel_id,\r\nquantization_config=pipeline_quant_config,\r\ntorch_dtype=torch.bfloat16,\r\n+ device_map=\"balanced\"\r\n)\r\n- pipe.to(\"cuda\")\r\nFor more information check out the torchao docs.\r\nQuanto\r\nQuanto is a quantization library integrated with the Hugging Face ecosystem via the optimum\r\nlibrary.\r\n|\r\nINT4 |\r\nINT8 |\r\nFP8 |\r\n| Visual comparison of Flux-dev model outputs using Quanto INT4 (left), INT8 (center), and FP8 (right) quantization. (Click on an image to zoom) |\r\n| quanto Precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| INT4 | 12.254 GB | 16.139 GB | 109 seconds |\r\n| INT8 | 17.330 GB | 21.814 GB | 15 seconds |\r\n| FP8 | 16.395 GB | 20.898 GB | 16 seconds |\r\nExample (Flux-dev with quanto INT8 weight-only):\r\n@@\r\n- from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\r\n+ from diffusers import QuantoConfig as DiffusersQuantoConfig\r\n- from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\r\n+ from transformers import QuantoConfig as TransformersQuantoConfig\r\n@@\r\npipeline_quant_config = PipelineQuantizationConfig(\r\nquant_mapping={\r\n- \"transformer\": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\r\n- \"text_encoder_2\": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\r\n+ \"transformer\": DiffusersQuantoConfig(weights_dtype=\"int8\"),\r\n+ \"text_encoder_2\": TransformersQuantoConfig(weights_dtype=\"int8\"),\r\n}\r\n)\r\nNote: At the time of writing, for float8 support with Quanto, you'll need\r\noptimum-quanto<0.2.5\r\nand use quanto directly. We will be working on fixing this.\r\nExample (Flux-dev with quanto FP8 weight-only)\r\nimport torch\r\nfrom diffusers import AutoModel, FluxPipeline\r\nfrom transformers import T5EncoderModel\r\nfrom optimum.quanto import freeze, qfloat8, quantize\r\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\r\ntext_encoder_2 = T5EncoderModel.from_pretrained(\r\nmodel_id,\r\nsubfolder=\"text_encoder_2\",\r\ntorch_dtype=torch.bfloat16,\r\n)\r\nquantize(text_encoder_2, weights=qfloat8)\r\nfreeze(text_encoder_2)\r\ntransformer = AutoModel.from_pretrained(\r\nmodel_id,\r\nsubfolder=\"transformer\",\r\ntorch_dtype=torch.bfloat16,\r\n)\r\nquantize(transformer, weights=qfloat8)\r\nfreeze(transformer)\r\npipe = FluxPipeline.from_pretrained(\r\nmodel_id,\r\ntransformer=transformer,\r\ntext_encoder_2=text_encoder_2,\r\ntorch_dtype=torch.bfloat16\r\n).to(\"cuda\")\r\nFor more information check out the Quanto docs.\r\nGGUF\r\nGGUF is a file format popular in the llama.cpp community for storing quantized models.\r\n|\r\nQ2_k |\r\nQ4_1 |\r\nQ8_0 |\r\n| Visual comparison of Flux-dev model outputs using GGUF Q2_k (left), Q4_1 (center), and Q8_0 (right) quantization. (Click on an image to zoom) |\r\n| GGUF Precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| Q2_k | 13.264 GB | 17.752 GB | 26 seconds |\r\n| Q4_1 | 16.838 GB | 21.326 GB | 23 seconds |\r\n| Q8_0 | 21.502 GB | 25.973 GB | 15 seconds |\r\nExample (Flux-dev with GGUF Q4_1)\r\nimport torch\r\nfrom diffusers import FluxPipeline, FluxTransformer2DModel, GGUFQuantizationConfig\r\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\r\n# Path to a pre-quantized GGUF file\r\nckpt_path = \"https://huggingface.co/city96/FLUX.1-dev-gguf/resolve/main/flux1-dev-Q4_1.gguf\"\r\ntransformer = FluxTransformer2DModel.from_single_file(\r\nckpt_path,\r\nquantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\r\ntorch_dtype=torch.bfloat16,\r\n)\r\npipe = FluxPipeline.from_pretrained(\r\nmodel_id,\r\ntransformer=transformer,\r\ntorch_dtype=torch.bfloat16,\r\n)\r\npipe.to(\"cuda\")\r\nFor more information check out the GGUF docs.\r\nFP8 Layerwise Casting (enable_layerwise_casting\r\n)\r\nFP8 Layerwise Casting is a memory optimization technique. It works by storing the model's weights in the compact FP8 (8-bit floating point) format, which uses roughly half the memory of standard FP16 or BF16 precision. Just before a layer performs its calculations, its weights are dynamically cast up to a higher compute precision (like FP16/BF16). Immediately afterward, the weights are cast back down to FP8 for efficient storage. This approach works because the core computations retain high precision, and layers particularly sensitive to quantization (like normalization) are typically skipped. This technique can also be combined with group offloading for further memory savings.\r\n|\r\nFP8 (e4m3) |\r\n| Visual output of Flux-dev model using FP8 Layerwise Casting (e4m3) quantization. |\r\n| precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| FP8 (e4m3) | 23.682 GB | 28.451 GB | 13 seconds |\r\nimport torch\r\nfrom diffusers import AutoModel, FluxPipeline\r\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\r\ntransformer = AutoModel.from_pretrained(\r\nmodel_id,\r\nsubfolder=\"transformer\",\r\ntorch_dtype=torch.bfloat16\r\n)\r\ntransformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.bfloat16)\r\npipe = FluxPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.bfloat16)\r\npipe.to(\"cuda\")\r\nFor more information check out the Layerwise casting docs.\r\nCombining with More Memory Optimizations and torch.compile\r\nMost of these quantization backends can be combined with the memory optimization techniques offered in Diffusers. Let's explore CPU offloading, group offloading, and torch.compile\r\n. You can learn more about these techniques in the Diffusers documentation.\r\nNote: At the time of writing, bnb +\r\ntorch.compile\r\nalso works if bnb is installed from source and using pytorch nightly or with fullgraph=False.\r\nExample (Flux-dev with BnB 4-bit + enable_model_cpu_offload):\r\nimport torch\r\nfrom diffusers import FluxPipeline\r\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\r\nfrom diffusers.quantizers import PipelineQuantizationConfig\r\nfrom transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\r\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\r\npipeline_quant_config = PipelineQuantizationConfig(\r\nquant_mapping={\r\n\"transformer\": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\r\n\"text_encoder_2\": TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\r\n}\r\n)\r\npipe = FluxPipeline.from_pretrained(\r\nmodel_id,\r\nquantization_config=pipeline_quant_config,\r\ntorch_dtype=torch.bfloat16\r\n)\r\n- pipe.to(\"cuda\")\r\n+ pipe.enable_model_cpu_offload()\r\nModel CPU Offloading (enable_model_cpu_offload\r\n): This method moves entire model components (like the UNet, text encoders, or VAE) between the CPU and GPU during the inference pipeline. It offers substantial VRAM savings and is generally faster than more granular offloading because it involves fewer, larger data transfers.\r\nbnb + enable_model_cpu_offload\r\n:\r\n| Precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| 4-bit | 12.383 GB | 12.383 GB | 17 seconds |\r\n| 8-bit | 19.182 GB | 23.428 GB | 27 seconds |\r\nExample (Flux-dev with fp8 layerwise casting + group offloading):\r\nimport torch\r\nfrom diffusers import FluxPipeline, AutoModel\r\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\r\ntransformer = AutoModel.from_pretrained(\r\nmodel_id,\r\nsubfolder=\"transformer\",\r\ntorch_dtype=torch.bfloat16,\r\n# device_map=\"cuda\"\r\n)\r\ntransformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.bfloat16)\r\n+ transformer.enable_group_offload(onload_device=torch.device(\"cuda\"), offload_device=torch.device(\"cpu\"), offload_type=\"leaf_level\", use_stream=True)\r\npipe = FluxPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.bfloat16)\r\n- pipe.to(\"cuda\")\r\nGroup offloading (enable_group_offload\r\nfor diffusers\r\ncomponents or apply_group_offloading\r\nfor generic torch.nn.Module\r\ns): It moves groups of internal model layers (like torch.nn.ModuleList\r\nor torch.nn.Sequential\r\ninstances) to the CPU. This approach is typically more memory-efficient than full model offloading and faster than sequential offloading.\r\nFP8 layerwise casting + group offloading:\r\n| precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| FP8 (e4m3) | 9.264 GB | 14.232 GB | 58 seconds |\r\nExample (Flux-dev with torchao 4-bit + torch.compile):\r\nimport torch\r\nfrom diffusers import FluxPipeline\r\nfrom diffusers import TorchAoConfig as DiffusersTorchAoConfig\r\nfrom diffusers.quantizers import PipelineQuantizationConfig\r\nfrom transformers import TorchAoConfig as TransformersTorchAoConfig\r\nfrom torchao.quantization import Float8WeightOnlyConfig\r\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\r\ndtype = torch.bfloat16\r\npipeline_quant_config = PipelineQuantizationConfig(\r\nquant_mapping={\r\n\"transformer\":DiffusersTorchAoConfig(\"int4_weight_only\"),\r\n\"text_encoder_2\": TransformersTorchAoConfig(\"int4_weight_only\"),\r\n}\r\n)\r\npipe = FluxPipeline.from_pretrained(\r\nmodel_id,\r\nquantization_config=pipeline_quant_config,\r\ntorch_dtype=torch.bfloat16,\r\ndevice_map=\"balanced\"\r\n)\r\n+ pipe.transformer = torch.compile(pipe.transformer, mode=\"max-autotune\", fullgraph=True)\r\nNote:\r\ntorch.compile\r\ncan introduce subtle numerical differences, leading to changes in image output\r\ntorch.compile: Another complementary approach is to accelerate the execution of your model with PyTorch 2.x’s torch.compile() feature. Compiling the model doesn’t directly lower memory, but it can significantly speed up inference. PyTorch 2.0’s compile (Torch Dynamo) works by tracing and optimizing the model graph ahead-of-time.\r\ntorchao + torch.compile\r\n:\r\n| torchao Precision | Memory after loading | Peak memory | Inference time | Compile Time |\r\n|---|---|---|---|---|\r\n| int4_weight_only | 10.635 GB | 15.238 GB | 6 seconds | ~285 seconds |\r\n| int8_weight_only | 17.020 GB | 22.473 GB | 8 seconds | ~851 seconds |\r\n| float8_weight_only | 17.016 GB | 22.115 GB | 8 seconds | ~545 seconds |\r\nExplore some benchmarking results here:\r\nReady to use quantized checkpoints\r\nYou can find bitsandbytes\r\nand torchao\r\nquantized models from this blog post in our Hugging Face collection: link to collection.\r\nConclusion\r\nHere's a quick guide to choosing a quantization backend:\r\n- Easiest Memory Savings (NVIDIA): Start with\r\nbitsandbytes\r\n4/8-bit. This can also be combined withtorch.compile()\r\nfor faster inference. - Prioritize Inference Speed:\r\ntorchao\r\n,GGUF\r\n, andbitsandbytes\r\ncan all be used withtorch.compile()\r\nto potentially boost inference speed. - For Hardware Flexibility (CPU/MPS), FP8 Precision:\r\nQuanto\r\ncan be a good option. - Simplicity (Hopper/Ada): Explore FP8 Layerwise Casting (\r\nenable_layerwise_casting\r\n). - For Using Existing GGUF Models: Use GGUF loading (\r\nfrom_single_file\r\n). - Curious about training with quantization? Stay tuned for a follow-up blog post on that topic!\r\nQuantization significantly lowers the barrier to entry for using large diffusion models. Experiment with these backends to find the best balance of memory, speed, and quality for your needs.\r\nAcknowledgements: Thanks to Chunte for providing the thumbnail for this post.","src/content/posts/2025-06-20-exploring-quantization-backends-in-diffusers-bbb537.md","672effc2a8c49cb9",{html:169,metadata:170},"<p>Exploring Quantization Backends in Diffusers\r\nLarge diffusion models like Flux (a flow-based text-to-image generation model) can create stunning images, but their size can be a hurdle, demanding significant memory and compute resources. Quantization offers a powerful solution, shrinking these models to make them more accessible without drastically compromising performance. But the big question always is: can you actually tell the difference in the final image?\r\nBefore we dive into the technical details of how various quantization backends in Hugging Face Diffusers work, why not test your own perception?\r\nSpot The Quantized Model\r\nWe created a setup where you can provide a prompt, and we generate results using both the original, high-precision model (e.g., Flux-dev in BF16) and several quantized versions (BnB 4-bit, BnB 8-bit). The generated images are then presented to you and your challenge is to identify which ones came from the quantized models.\r\nTry it out here or below!\r\nOften, especially with 8-bit quantization, the differences are subtle and may not be noticeable without close inspection. More aggressive quantization like 4-bit or lower might be more noticeable, but the results can still be good, especially considering the massive memory savings. NF4 often gives the best trade-off though.\r\nNow, let’s dive deeper.\r\nQuantization Backends in Diffusers\r\nBuilding on our previous post, “Memory-efficient Diffusion Transformers with Quanto and Diffusers”, this post explores the diverse quantization backends integrated directly into Hugging Face Diffusers. We’ll examine how bitsandbytes, GGUF, torchao, Quanto and native FP8 support make large and powerful models more accessible, demonstrating their use with Flux.\r\nBefore diving into the quantization backends, let’s introduce the FluxPipeline (using the black-forest-labs/FLUX.1-dev checkpoint) and its components, which we’ll be quantizing. Loading the full FLUX.1-dev\r\nmodel in BF16 precision requires approximately 31.447 GB of memory. The main components are:</p>\n<ul>\n<li>Text Encoders (CLIP and T5):</li>\n<li>Function: Process input text prompts. FLUX-dev uses CLIP for initial understanding and a larger T5 for nuanced comprehension and better text rendering.</li>\n<li>Memory: T5 - 9.52 GB; CLIP - 246 MB (in BF16)</li>\n<li>Transformer (Main Model - MMDiT):</li>\n<li>Function: Core generative part (Multimodal Diffusion Transformer). Generates images in latent space from text embeddings.</li>\n<li>Memory: 23.8 GB (in BF16)</li>\n<li>Variational Auto-Encoder (VAE):</li>\n<li>Function: Translates images between pixel and latent space. Decodes generated latent representation to a pixel-based image.</li>\n<li>Memory: 168 MB (in BF16)</li>\n<li>Focus of Quantization: Examples will primarily focus on the\r\ntransformer\r\nandtext_encoder_2\r\n(T5) for the most substantial memory savings.\r\nprompts = [\r\n“Baroque style, a lavish palace interior with ornate gilded ceilings, intricate tapestries, and dramatic lighting over a grand staircase.”,\r\n“Futurist style, a dynamic spaceport with sleek silver starships docked at angular platforms, surrounded by distant planets and glowing energy lines.”,\r\n“Noir style, a shadowy alleyway with flickering street lamps and a solitary trench-coated figure, framed by rain-soaked cobblestones and darkened storefronts.”,\r\n]\r\nbitsandbytes (BnB)\r\nbitsandbytes\r\nis a popular and user-friendly library for 8-bit and 4-bit quantization, widely used for LLMs and QLoRA fine-tuning. We can use it for transformer-based diffusion and flow models, too.\r\n|\r\nBF16 |\r\nBnB 4-bit |\r\nBnB 8-bit |\r\n| Visual comparison of Flux-dev model outputs using BF16 (left), BnB 4-bit (center), and BnB 8-bit (right) quantization. (Click on an image to zoom) |\r\n| Precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| BF16 | ~31.447 GB | 36.166 GB | 12 seconds |\r\n| 4-bit | 12.584 GB | 17.281 GB | 12 seconds |\r\n| 8-bit | 19.273 GB | 24.432 GB | 27 seconds |\r\nAll benchmarks performed on 1x NVIDIA H100 80GB GPU\r\nExample (Flux-dev with BnB 4-bit):\r\nimport torch\r\nfrom diffusers import FluxPipeline\r\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\r\nfrom diffusers.quantizers import PipelineQuantizationConfig\r\nfrom transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\r\nmodel_id = “black-forest-labs/FLUX.1-dev”\r\npipeline_quant_config = PipelineQuantizationConfig(\r\nquant_mapping={\r\n“transformer”: DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=“nf4”, bnb_4bit_compute_dtype=torch.bfloat16),\r\n“text_encoder_2”: TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=“nf4”, bnb_4bit_compute_dtype=torch.bfloat16),\r\n}\r\n)\r\npipe = FluxPipeline.from_pretrained(\r\nmodel_id,\r\nquantization_config=pipeline_quant_config,\r\ntorch_dtype=torch.bfloat16\r\n)\r\npipe.to(“cuda”)\r\nprompt = “Baroque style, a lavish palace interior with ornate gilded ceilings, intricate tapestries, and dramatic lighting over a grand staircase.”\r\npipe_kwargs = {\r\n“prompt”: prompt,\r\n“height”: 1024,\r\n“width”: 1024,\r\n“guidance_scale”: 3.5,\r\n“num_inference_steps”: 50,\r\n“max_sequence_length”: 512,\r\n}\r\nprint(f”Pipeline memory usage: {torch.cuda.max_memory_reserved() / 1024**3:.3f} GB”)\r\nimage = pipe(\r\n<strong>pipe_kwargs, generator=torch.manual_seed(0),\r\n).images[0]\r\nprint(f”Pipeline memory usage: {torch.cuda.max_memory_reserved() / 1024</strong>3:.3f} GB”)\r\nimage.save(“flux-dev_bnb_4bit.png”)\r\nNote: When using\r\nPipelineQuantizationConfig\r\nwithbitsandbytes\r\n, you need to importDiffusersBitsAndBytesConfig\r\nfromdiffusers\r\nandTransformersBitsAndBytesConfig\r\nfromtransformers\r\nseparately. This is because these components originate from different libraries. If you prefer a simpler setup without managing these distinct imports, you can use an alternative approach for pipeline-level quantization, an example of this method is in the Diffusers documentation on Pipeline-level quantization.\r\nFor more information check out the bitsandbytes docs.\r\ntorchao\r\ntorchao\r\nis a PyTorch-native library for architecture optimization, offering quantization, sparsity, and custom data types, designed for compatibility with torch.compile\r\nand FSDP. Diffusers supports a wide range of torchao\r\n‘s exotic data types, enabling fine-grained control over model optimization.\r\n|\r\nint4_weight_only |\r\nint8_weight_only |\r\nfloat8_weight_only |\r\n| Visual comparison of Flux-dev model outputs using torchao int4_weight_only (left), int8_weight_only (center), and float8_weight_only (right) quantization. (Click on an image to zoom) |\r\n| torchao Precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| int4_weight_only | 10.635 GB | 14.654 GB | 109 seconds |\r\n| int8_weight_only | 17.020 GB | 21.482 GB | 15 seconds |\r\n| float8_weight_only | 17.016 GB | 21.488 GB | 15 seconds |\r\nExample (Flux-dev with torchao INT8 weight-only):\r\n@@</li>\n<li>from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig</li>\n</ul>\n<ul>\n<li>from diffusers import TorchAoConfig as DiffusersTorchAoConfig</li>\n</ul>\n<ul>\n<li>from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig</li>\n</ul>\n<ul>\n<li>from transformers import TorchAoConfig as TransformersTorchAoConfig\r\n@@\r\npipeline_quant_config = PipelineQuantizationConfig(\r\nquant_mapping={</li>\n</ul>\n<ul>\n<li>“transformer”: DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=“nf4”, bnb_4bit_compute_dtype=torch.bfloat16),</li>\n<li>“text_encoder_2”: TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=“nf4”, bnb_4bit_compute_dtype=torch.bfloat16),</li>\n</ul>\n<ul>\n<li>“transformer”: DiffusersTorchAoConfig(“int8_weight_only”),</li>\n<li>“text_encoder_2”: TransformersTorchAoConfig(“int8_weight_only”),\r\n}\r\n)\r\nExample (Flux-dev with torchao INT4 weight-only):\r\n@@</li>\n</ul>\n<ul>\n<li>from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig</li>\n</ul>\n<ul>\n<li>from diffusers import TorchAoConfig as DiffusersTorchAoConfig</li>\n</ul>\n<ul>\n<li>from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig</li>\n</ul>\n<ul>\n<li>from transformers import TorchAoConfig as TransformersTorchAoConfig\r\n@@\r\npipeline_quant_config = PipelineQuantizationConfig(\r\nquant_mapping={</li>\n</ul>\n<ul>\n<li>“transformer”: DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=“nf4”, bnb_4bit_compute_dtype=torch.bfloat16),</li>\n<li>“text_encoder_2”: TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=“nf4”, bnb_4bit_compute_dtype=torch.bfloat16),</li>\n</ul>\n<ul>\n<li>“transformer”: DiffusersTorchAoConfig(“int4_weight_only”),</li>\n<li>“text_encoder_2”: TransformersTorchAoConfig(“int4_weight_only”),\r\n}\r\n)\r\npipe = FluxPipeline.from_pretrained(\r\nmodel_id,\r\nquantization_config=pipeline_quant_config,\r\ntorch_dtype=torch.bfloat16,</li>\n<li>device_map=“balanced”\r\n)</li>\n</ul>\n<ul>\n<li>pipe.to(“cuda”)\r\nFor more information check out the torchao docs.\r\nQuanto\r\nQuanto is a quantization library integrated with the Hugging Face ecosystem via the optimum\r\nlibrary.\r\n|\r\nINT4 |\r\nINT8 |\r\nFP8 |\r\n| Visual comparison of Flux-dev model outputs using Quanto INT4 (left), INT8 (center), and FP8 (right) quantization. (Click on an image to zoom) |\r\n| quanto Precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| INT4 | 12.254 GB | 16.139 GB | 109 seconds |\r\n| INT8 | 17.330 GB | 21.814 GB | 15 seconds |\r\n| FP8 | 16.395 GB | 20.898 GB | 16 seconds |\r\nExample (Flux-dev with quanto INT8 weight-only):\r\n@@</li>\n<li>from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig</li>\n</ul>\n<ul>\n<li>from diffusers import QuantoConfig as DiffusersQuantoConfig</li>\n</ul>\n<ul>\n<li>from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig</li>\n</ul>\n<ul>\n<li>from transformers import QuantoConfig as TransformersQuantoConfig\r\n@@\r\npipeline_quant_config = PipelineQuantizationConfig(\r\nquant_mapping={</li>\n</ul>\n<ul>\n<li>“transformer”: DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=“nf4”, bnb_4bit_compute_dtype=torch.bfloat16),</li>\n<li>“text_encoder_2”: TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=“nf4”, bnb_4bit_compute_dtype=torch.bfloat16),</li>\n</ul>\n<ul>\n<li>“transformer”: DiffusersQuantoConfig(weights_dtype=“int8”),</li>\n<li>“text_encoder_2”: TransformersQuantoConfig(weights_dtype=“int8”),\r\n}\r\n)\r\nNote: At the time of writing, for float8 support with Quanto, you’ll need\r\noptimum-quanto&#x3C;0.2.5\r\nand use quanto directly. We will be working on fixing this.\r\nExample (Flux-dev with quanto FP8 weight-only)\r\nimport torch\r\nfrom diffusers import AutoModel, FluxPipeline\r\nfrom transformers import T5EncoderModel\r\nfrom optimum.quanto import freeze, qfloat8, quantize\r\nmodel_id = “black-forest-labs/FLUX.1-dev”\r\ntext_encoder_2 = T5EncoderModel.from_pretrained(\r\nmodel_id,\r\nsubfolder=“text_encoder_2”,\r\ntorch_dtype=torch.bfloat16,\r\n)\r\nquantize(text_encoder_2, weights=qfloat8)\r\nfreeze(text_encoder_2)\r\ntransformer = AutoModel.from_pretrained(\r\nmodel_id,\r\nsubfolder=“transformer”,\r\ntorch_dtype=torch.bfloat16,\r\n)\r\nquantize(transformer, weights=qfloat8)\r\nfreeze(transformer)\r\npipe = FluxPipeline.from_pretrained(\r\nmodel_id,\r\ntransformer=transformer,\r\ntext_encoder_2=text_encoder_2,\r\ntorch_dtype=torch.bfloat16\r\n).to(“cuda”)\r\nFor more information check out the Quanto docs.\r\nGGUF\r\nGGUF is a file format popular in the llama.cpp community for storing quantized models.\r\n|\r\nQ2_k |\r\nQ4_1 |\r\nQ8_0 |\r\n| Visual comparison of Flux-dev model outputs using GGUF Q2_k (left), Q4_1 (center), and Q8_0 (right) quantization. (Click on an image to zoom) |\r\n| GGUF Precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| Q2_k | 13.264 GB | 17.752 GB | 26 seconds |\r\n| Q4_1 | 16.838 GB | 21.326 GB | 23 seconds |\r\n| Q8_0 | 21.502 GB | 25.973 GB | 15 seconds |\r\nExample (Flux-dev with GGUF Q4_1)\r\nimport torch\r\nfrom diffusers import FluxPipeline, FluxTransformer2DModel, GGUFQuantizationConfig\r\nmodel_id = “black-forest-labs/FLUX.1-dev”</li>\n</ul>\n<h1 id=\"path-to-a-pre-quantized-gguf-file\">Path to a pre-quantized GGUF file</h1>\n<p>ckpt_path = “<a href=\"https://huggingface.co/city96/FLUX.1-dev-gguf/resolve/main/flux1-dev-Q4_1.gguf\">https://huggingface.co/city96/FLUX.1-dev-gguf/resolve/main/flux1-dev-Q4_1.gguf</a>”\r\ntransformer = FluxTransformer2DModel.from_single_file(\r\nckpt_path,\r\nquantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\r\ntorch_dtype=torch.bfloat16,\r\n)\r\npipe = FluxPipeline.from_pretrained(\r\nmodel_id,\r\ntransformer=transformer,\r\ntorch_dtype=torch.bfloat16,\r\n)\r\npipe.to(“cuda”)\r\nFor more information check out the GGUF docs.\r\nFP8 Layerwise Casting (enable_layerwise_casting\r\n)\r\nFP8 Layerwise Casting is a memory optimization technique. It works by storing the model’s weights in the compact FP8 (8-bit floating point) format, which uses roughly half the memory of standard FP16 or BF16 precision. Just before a layer performs its calculations, its weights are dynamically cast up to a higher compute precision (like FP16/BF16). Immediately afterward, the weights are cast back down to FP8 for efficient storage. This approach works because the core computations retain high precision, and layers particularly sensitive to quantization (like normalization) are typically skipped. This technique can also be combined with group offloading for further memory savings.\r\n|\r\nFP8 (e4m3) |\r\n| Visual output of Flux-dev model using FP8 Layerwise Casting (e4m3) quantization. |</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>precision</th><th>Memory after loading</th><th>Peak memory</th><th>Inference time</th></tr></thead><tbody><tr><td>FP8 (e4m3)</td><td>23.682 GB</td><td>28.451 GB</td><td>13 seconds</td></tr><tr><td>import torch</td><td></td><td></td><td></td></tr><tr><td>from diffusers import AutoModel, FluxPipeline</td><td></td><td></td><td></td></tr><tr><td>model_id = “black-forest-labs/FLUX.1-dev”</td><td></td><td></td><td></td></tr><tr><td>transformer = AutoModel.from_pretrained(</td><td></td><td></td><td></td></tr><tr><td>model_id,</td><td></td><td></td><td></td></tr><tr><td>subfolder=“transformer”,</td><td></td><td></td><td></td></tr><tr><td>torch_dtype=torch.bfloat16</td><td></td><td></td><td></td></tr><tr><td>)</td><td></td><td></td><td></td></tr><tr><td>transformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.bfloat16)</td><td></td><td></td><td></td></tr><tr><td>pipe = FluxPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.bfloat16)</td><td></td><td></td><td></td></tr><tr><td>pipe.to(“cuda”)</td><td></td><td></td><td></td></tr><tr><td>For more information check out the Layerwise casting docs.</td><td></td><td></td><td></td></tr><tr><td>Combining with More Memory Optimizations and torch.compile</td><td></td><td></td><td></td></tr><tr><td>Most of these quantization backends can be combined with the memory optimization techniques offered in Diffusers. Let’s explore CPU offloading, group offloading, and torch.compile</td><td></td><td></td><td></td></tr><tr><td>. You can learn more about these techniques in the Diffusers documentation.</td><td></td><td></td><td></td></tr><tr><td>Note: At the time of writing, bnb +</td><td></td><td></td><td></td></tr><tr><td>torch.compile</td><td></td><td></td><td></td></tr><tr><td>also works if bnb is installed from source and using pytorch nightly or with fullgraph=False.</td><td></td><td></td><td></td></tr><tr><td>Example (Flux-dev with BnB 4-bit + enable_model_cpu_offload):</td><td></td><td></td><td></td></tr><tr><td>import torch</td><td></td><td></td><td></td></tr><tr><td>from diffusers import FluxPipeline</td><td></td><td></td><td></td></tr><tr><td>from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig</td><td></td><td></td><td></td></tr><tr><td>from diffusers.quantizers import PipelineQuantizationConfig</td><td></td><td></td><td></td></tr><tr><td>from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig</td><td></td><td></td><td></td></tr><tr><td>model_id = “black-forest-labs/FLUX.1-dev”</td><td></td><td></td><td></td></tr><tr><td>pipeline_quant_config = PipelineQuantizationConfig(</td><td></td><td></td><td></td></tr><tr><td>quant_mapping={</td><td></td><td></td><td></td></tr><tr><td>“transformer”: DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=“nf4”, bnb_4bit_compute_dtype=torch.bfloat16),</td><td></td><td></td><td></td></tr><tr><td>“text_encoder_2”: TransformersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=“nf4”, bnb_4bit_compute_dtype=torch.bfloat16),</td><td></td><td></td><td></td></tr><tr><td>}</td><td></td><td></td><td></td></tr><tr><td>)</td><td></td><td></td><td></td></tr><tr><td>pipe = FluxPipeline.from_pretrained(</td><td></td><td></td><td></td></tr><tr><td>model_id,</td><td></td><td></td><td></td></tr><tr><td>quantization_config=pipeline_quant_config,</td><td></td><td></td><td></td></tr><tr><td>torch_dtype=torch.bfloat16</td><td></td><td></td><td></td></tr><tr><td>)</td><td></td><td></td><td></td></tr></tbody></table>\n<ul>\n<li>pipe.to(“cuda”)</li>\n</ul>\n<ul>\n<li>pipe.enable_model_cpu_offload()\r\nModel CPU Offloading (enable_model_cpu_offload\r\n): This method moves entire model components (like the UNet, text encoders, or VAE) between the CPU and GPU during the inference pipeline. It offers substantial VRAM savings and is generally faster than more granular offloading because it involves fewer, larger data transfers.\r\nbnb + enable_model_cpu_offload\r\n:\r\n| Precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| 4-bit | 12.383 GB | 12.383 GB | 17 seconds |\r\n| 8-bit | 19.182 GB | 23.428 GB | 27 seconds |\r\nExample (Flux-dev with fp8 layerwise casting + group offloading):\r\nimport torch\r\nfrom diffusers import FluxPipeline, AutoModel\r\nmodel_id = “black-forest-labs/FLUX.1-dev”\r\ntransformer = AutoModel.from_pretrained(\r\nmodel_id,\r\nsubfolder=“transformer”,\r\ntorch_dtype=torch.bfloat16,</li>\n</ul>\n<h1 id=\"device_mapcuda\">device_map=“cuda”</h1>\n<p>)\r\ntransformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.bfloat16)</p>\n<ul>\n<li>transformer.enable_group_offload(onload_device=torch.device(“cuda”), offload_device=torch.device(“cpu”), offload_type=“leaf_level”, use_stream=True)\r\npipe = FluxPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.bfloat16)</li>\n</ul>\n<ul>\n<li>pipe.to(“cuda”)\r\nGroup offloading (enable_group_offload\r\nfor diffusers\r\ncomponents or apply_group_offloading\r\nfor generic torch.nn.Module\r\ns): It moves groups of internal model layers (like torch.nn.ModuleList\r\nor torch.nn.Sequential\r\ninstances) to the CPU. This approach is typically more memory-efficient than full model offloading and faster than sequential offloading.\r\nFP8 layerwise casting + group offloading:\r\n| precision | Memory after loading | Peak memory | Inference time |\r\n|---|---|---|---|\r\n| FP8 (e4m3) | 9.264 GB | 14.232 GB | 58 seconds |\r\nExample (Flux-dev with torchao 4-bit + torch.compile):\r\nimport torch\r\nfrom diffusers import FluxPipeline\r\nfrom diffusers import TorchAoConfig as DiffusersTorchAoConfig\r\nfrom diffusers.quantizers import PipelineQuantizationConfig\r\nfrom transformers import TorchAoConfig as TransformersTorchAoConfig\r\nfrom torchao.quantization import Float8WeightOnlyConfig\r\nmodel_id = “black-forest-labs/FLUX.1-dev”\r\ndtype = torch.bfloat16\r\npipeline_quant_config = PipelineQuantizationConfig(\r\nquant_mapping={\r\n“transformer”:DiffusersTorchAoConfig(“int4_weight_only”),\r\n“text_encoder_2”: TransformersTorchAoConfig(“int4_weight_only”),\r\n}\r\n)\r\npipe = FluxPipeline.from_pretrained(\r\nmodel_id,\r\nquantization_config=pipeline_quant_config,\r\ntorch_dtype=torch.bfloat16,\r\ndevice_map=“balanced”\r\n)</li>\n</ul>\n<ul>\n<li>pipe.transformer = torch.compile(pipe.transformer, mode=“max-autotune”, fullgraph=True)\r\nNote:\r\ntorch.compile\r\ncan introduce subtle numerical differences, leading to changes in image output\r\ntorch.compile: Another complementary approach is to accelerate the execution of your model with PyTorch 2.x’s torch.compile() feature. Compiling the model doesn’t directly lower memory, but it can significantly speed up inference. PyTorch 2.0’s compile (Torch Dynamo) works by tracing and optimizing the model graph ahead-of-time.\r\ntorchao + torch.compile\r\n:\r\n| torchao Precision | Memory after loading | Peak memory | Inference time | Compile Time |\r\n|---|---|---|---|---|\r\n| int4_weight_only | 10.635 GB | 15.238 GB | 6 seconds | ~285 seconds |\r\n| int8_weight_only | 17.020 GB | 22.473 GB | 8 seconds | ~851 seconds |\r\n| float8_weight_only | 17.016 GB | 22.115 GB | 8 seconds | ~545 seconds |\r\nExplore some benchmarking results here:\r\nReady to use quantized checkpoints\r\nYou can find bitsandbytes\r\nand torchao\r\nquantized models from this blog post in our Hugging Face collection: link to collection.\r\nConclusion\r\nHere’s a quick guide to choosing a quantization backend:</li>\n</ul>\n<ul>\n<li>Easiest Memory Savings (NVIDIA): Start with\r\nbitsandbytes\r\n4/8-bit. This can also be combined withtorch.compile()\r\nfor faster inference. - Prioritize Inference Speed:\r\ntorchao\r\n,GGUF\r\n, andbitsandbytes\r\ncan all be used withtorch.compile()\r\nto potentially boost inference speed. - For Hardware Flexibility (CPU/MPS), FP8 Precision:\r\nQuanto\r\ncan be a good option. - Simplicity (Hopper/Ada): Explore FP8 Layerwise Casting (\r\nenable_layerwise_casting\r\n). - For Using Existing GGUF Models: Use GGUF loading (\r\nfrom_single_file\r\n). - Curious about training with quantization? Stay tuned for a follow-up blog post on that topic!\r\nQuantization significantly lowers the barrier to entry for using large diffusion models. Experiment with these backends to find the best balance of memory, speed, and quality for your needs.\r\nAcknowledgements: Thanks to Chunte for providing the thumbnail for this post.</li>\n</ul>",{headings:171,localImagePaths:178,remoteImagePaths:179,frontmatter:180,imagePaths:183},[172,175],{depth:31,slug:173,text:174},"path-to-a-pre-quantized-gguf-file","Path to a pre-quantized GGUF file",{depth:31,slug:176,text:177},"device_mapcuda","device_map=“cuda”",[],[],{title:161,summary:15,pubDate:181,media:17,tags:182,link:164,thumbnail:15},"Wed, 21 May 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-exploring-quantization-backends-in-diffusers-bbb537.md","2025-06-20-groq-on-hugging-face-inference-providers--159ff2",{id:185,data:187,body:192,filePath:193,digest:194,rendered:195,legacyId:205},{title:188,summary:15,pubDate:189,media:17,tags:190,link:191,thumbnail:15},"Groq on Hugging Face Inference Providers 🔥",["Date","2025-06-16T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/inference-providers-groq","Groq on Hugging Face Inference Providers 🔥\r\nWe're thrilled to share that Groq is now a supported Inference Provider on the Hugging Face Hub! Groq joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub’s model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\r\nGroq supports a wide variety of text and conversational models, including the latest open-source models such as Meta's LLama 4, Qwen's QWQ-32B, ad many more.\r\nAt the heart of Groq's technology is the Language Processing Unit (LPU™), a new type of end-to-end processing unit system that provides the fastest inference for computationally intensive applications with a sequential component, such as Large Language Models (LLMs). LPUs are designed to overcome the limitations of GPUs for inference, offering significantly lower latency and higher throughput. This makes them ideal for real-time AI applications.\r\nGroq offers fast AI inference for openly-available models. They provide an API that allows developers to easily integrate these models into their applications. It offers an on-demand, pay-as-you-go model for accessing a wide range of openly-available LLMs.\r\nYou can now use Groq's Inference API as an Inference Provider on Huggingface. We're quite excited to see what you'll build with this new provider.\r\nRead more about how to use Groq as Inference Provider in its dedicated documentation page.\r\nSee the list of supported models here.\r\nHow it works\r\nIn the website UI\r\n- In your user account settings, you are able to:\r\n- Set your own API keys for the providers you’ve signed up with. If no custom key is set, your requests will be routed through HF.\r\n- Order providers by preference. This applies to the widget and code snippets in the model pages.\r\n- As mentioned, there are two modes when calling Inference Providers:\r\n- Custom key (calls go directly to the inference provider, using your own API key of the corresponding inference provider)\r\n- Routed by HF (in that case, you don't need a token from the provider, and the charges are applied directly to your HF account rather than the provider's account)\r\n- Model pages showcase third-party inference providers (the ones that are compatible with the current model, sorted by user preference)\r\nFrom the client SDKs\r\nfrom Python, using huggingface_hub\r\nThe following example shows how to use Meta's LLama 4 using Groq as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own Groq API key if you have one.\r\nInstall huggingface_hub\r\nfrom source (see instructions). Official support will be released soon in version v0.33.0.\r\nimport os\r\nfrom huggingface_hub import InferenceClient\r\nclient = InferenceClient(\r\nprovider=\"groq\",\r\napi_key=os.environ[\"HF_TOKEN\"],\r\n)\r\nmessages = [\r\n{\r\n\"role\": \"user\",\r\n\"content\": \"What is the capital of France?\"\r\n}\r\n]\r\ncompletion = client.chat.completions.create(\r\nmodel=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\r\nmessages=messages,\r\n)\r\nprint(completion.choices[0].message)\r\nfrom JS using @huggingface/inference\r\nimport { InferenceClient } from \"@huggingface/inference\";\r\nconst client = new InferenceClient(process.env.HF_TOKEN);\r\nconst chatCompletion = await client.chatCompletion({\r\nmodel: \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\r\nmessages: [\r\n{\r\nrole: \"user\",\r\ncontent: \"What is the capital of France?\",\r\n},\r\n],\r\nprovider: \"groq\",\r\n});\r\nconsole.log(chatCompletion.choices[0].message);\r\nBilling\r\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use a Groq API key you're billed on your Groq account.\r\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you'll only pay the standard provider API rates. There's no additional markup from us, we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\r\nImportant Note ‼️ PRO users get $2 worth of Inference credits every month. You can use them across providers. 🔥\r\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\r\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\r\nFeedback and next steps\r\nWe would love to get your feedback! Share your thoughts and/or comments here: https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49","src/content/posts/2025-06-20-groq-on-hugging-face-inference-providers-🔥-159ff2.md","757a511213b1e8d6",{html:196,metadata:197},"<p>Groq on Hugging Face Inference Providers 🔥\r\nWe’re thrilled to share that Groq is now a supported Inference Provider on the Hugging Face Hub! Groq joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub’s model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\r\nGroq supports a wide variety of text and conversational models, including the latest open-source models such as Meta’s LLama 4, Qwen’s QWQ-32B, ad many more.\r\nAt the heart of Groq’s technology is the Language Processing Unit (LPU™), a new type of end-to-end processing unit system that provides the fastest inference for computationally intensive applications with a sequential component, such as Large Language Models (LLMs). LPUs are designed to overcome the limitations of GPUs for inference, offering significantly lower latency and higher throughput. This makes them ideal for real-time AI applications.\r\nGroq offers fast AI inference for openly-available models. They provide an API that allows developers to easily integrate these models into their applications. It offers an on-demand, pay-as-you-go model for accessing a wide range of openly-available LLMs.\r\nYou can now use Groq’s Inference API as an Inference Provider on Huggingface. We’re quite excited to see what you’ll build with this new provider.\r\nRead more about how to use Groq as Inference Provider in its dedicated documentation page.\r\nSee the list of supported models here.\r\nHow it works\r\nIn the website UI</p>\n<ul>\n<li>In your user account settings, you are able to:</li>\n<li>Set your own API keys for the providers you’ve signed up with. If no custom key is set, your requests will be routed through HF.</li>\n<li>Order providers by preference. This applies to the widget and code snippets in the model pages.</li>\n<li>As mentioned, there are two modes when calling Inference Providers:</li>\n<li>Custom key (calls go directly to the inference provider, using your own API key of the corresponding inference provider)</li>\n<li>Routed by HF (in that case, you don’t need a token from the provider, and the charges are applied directly to your HF account rather than the provider’s account)</li>\n<li>Model pages showcase third-party inference providers (the ones that are compatible with the current model, sorted by user preference)\r\nFrom the client SDKs\r\nfrom Python, using huggingface_hub\r\nThe following example shows how to use Meta’s LLama 4 using Groq as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own Groq API key if you have one.\r\nInstall huggingface_hub\r\nfrom source (see instructions). Official support will be released soon in version v0.33.0.\r\nimport os\r\nfrom huggingface_hub import InferenceClient\r\nclient = InferenceClient(\r\nprovider=“groq”,\r\napi_key=os.environ[“HF_TOKEN”],\r\n)\r\nmessages = [\r\n{\r\n“role”: “user”,\r\n“content”: “What is the capital of France?”\r\n}\r\n]\r\ncompletion = client.chat.completions.create(\r\nmodel=“meta-llama/Llama-4-Scout-17B-16E-Instruct”,\r\nmessages=messages,\r\n)\r\nprint(completion.choices[0].message)\r\nfrom JS using @huggingface/inference\r\nimport { InferenceClient } from “@huggingface/inference”;\r\nconst client = new InferenceClient(process.env.HF_TOKEN);\r\nconst chatCompletion = await client.chatCompletion({\r\nmodel: “meta-llama/Llama-4-Scout-17B-16E-Instruct”,\r\nmessages: [\r\n{\r\nrole: “user”,\r\ncontent: “What is the capital of France?”,\r\n},\r\n],\r\nprovider: “groq”,\r\n});\r\nconsole.log(chatCompletion.choices[0].message);\r\nBilling\r\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use a Groq API key you’re billed on your Groq account.\r\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you’ll only pay the standard provider API rates. There’s no additional markup from us, we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\r\nImportant Note ‼️ PRO users get $2 worth of Inference credits every month. You can use them across providers. 🔥\r\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\r\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\r\nFeedback and next steps\r\nWe would love to get your feedback! Share your thoughts and/or comments here: <a href=\"https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49\">https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49</a></li>\n</ul>",{headings:198,localImagePaths:199,remoteImagePaths:200,frontmatter:201,imagePaths:204},[],[],[],{title:188,summary:15,pubDate:202,media:17,tags:203,link:191,thumbnail:15},"Mon, 16 Jun 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-groq-on-hugging-face-inference-providers-🔥-159ff2.md","2025-06-20-enhance-your-models-in-5-minutes-with-the-hugging-face-kernel-hub-6d932b",{id:206,data:208,body:213,filePath:214,digest:215,rendered:216,legacyId:458},{title:209,summary:15,pubDate:210,media:17,tags:211,link:212,thumbnail:15},"Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub",["Date","2025-06-12T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/hello-hf-kernels","🏎️ Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub\r\nBoost your model performance with pre-optimized kernels, easily loaded from the Hub.\r\nToday, we'll explore an exciting development from Hugging Face: the Kernel Hub! As ML practitioners, we know that maximizing performance often involves diving deep into optimized code, custom CUDA kernels, or complex build systems. The Kernel Hub simplifies this process dramatically!\r\nBelow is a short example of how to use a kernel in your code.\r\nimport torch\r\nfrom kernels import get_kernel\r\n# Download optimized kernels from the Hugging Face hub\r\nactivation = get_kernel(\"kernels-community/activation\")\r\n# Random tensor\r\nx = torch.randn((10, 10), dtype=torch.float16, device=\"cuda\")\r\n# Run the kernel\r\ny = torch.empty_like(x)\r\nactivation.gelu_fast(y, x)\r\nprint(y)\r\nIn the next sections we'll cover the following topics:\r\n- What is the Kernel Hub? - Understanding the core concept.\r\n- How to use the Kernel Hub - A quick code example.\r\n- Adding a Kernel to a Simple Model - A practical integration using RMSNorm.\r\n- Reviewing Performance Impact - Benchmarking the RMSNorm difference.\r\n- Real world use cases - Examples of how the kernels library is being used in other projects.\r\nWe'll introduce these concepts quickly – the core idea can be grasped in about 5 minutes (though experimenting and benchmarking might take a bit longer!).\r\n1. What is the Kernel Hub?\r\nThe Kernel Hub (👈 Check it out!) allows Python libraries and applications to load optimized compute kernels directly from the Hugging Face Hub. Think of it like the Model Hub, but for low-level, high-performance code snippets (kernels) that accelerate specific operations, often on GPUs.\r\nExamples include advanced attention mechanisms (like FlashAttention for dramatic speedups and memory savings). Custom quantization kernels (enabling efficient computation with lower-precision data types like INT8 or INT4). Specialized kernels required for complex architectures like Mixture of Experts (MoE) layers, which involve intricate routing and computation patterns. As well as activation functions, and normalization layers (like LayerNorm or RMSNorm).\r\nInstead of manually managing complex dependencies, wrestling with compilation flags, or building libraries like Triton or CUTLASS from source, you can use the kernels\r\nlibrary to instantly fetch and run pre-compiled, optimized kernels.\r\nFor example, to enable FlashAttention you need just one line—no builds, no flags:\r\nfrom kernels import get_kernel\r\nflash_attention = get_kernel(\"kernels-community/flash-attn\")\r\nkernels\r\ndetects your exact Python, PyTorch, and CUDA versions, then downloads the matching pre‑compiled binary—typically in seconds (or a minute or two on a slow connection).\r\nBy contrast, compiling FlashAttention yourself requires:\r\n- Cloning the repository and installing every dependency.\r\n- Configuring build flags and environment variables.\r\n- Reserving ~96 GB of RAM and plenty of CPU cores.\r\n- Waiting 10 minutes to several hours, depending on your hardware. (See the project’s own installation guide for details.)\r\nKernel Hub erases all that friction: one function call, instant acceleration.\r\nBenefits of the Kernel Hub:\r\n- Instant Access to Optimized Kernels: Load and run kernels optimized for various hardware starting with NVIDIA and AMD GPUs, without local compilation hassles.\r\n- Share and Reuse: Discover, share, and reuse kernels across different projects and the community.\r\n- Easy Updates: Stay up-to-date with the latest kernel improvements simply by pulling the latest version from the Hub.\r\n- Accelerate Development: Focus on your model architecture and logic, not on the intricacies of kernel compilation and deployment.\r\n- Improve Performance: Leverage kernels optimized by experts to potentially speed up training and inference.\r\n- Simplify Deployment: Reduce the complexity of your deployment environment by fetching kernels on demand.\r\n- Develop and Share Your Own Kernels: If you create optimized kernels, you can easily share them on the Hub for others to use. This encourages collaboration and knowledge sharing within the community.\r\nAs many machine learning developers know, managing dependencies and building low-level code from source can be a time-consuming and error-prone process. The Kernel Hub aims to simplify this by providing a centralized repository of optimized compute kernels that can be easily loaded and run.\r\nSpend more time building great models and less time fighting build systems!\r\n2. How to Use the Kernel Hub (Basic Example)\r\nUsing the Kernel Hub is designed to be straightforward. The kernels\r\nlibrary provides the main interface. Here's a quick example that loads an optimized GELU activation function kernel. (Later on, we'll see another example about how to integrate a kernel in our model).\r\nFile: activation_validation_example.py\r\n# /// script\r\n# dependencies = [\r\n# \"numpy\",\r\n# \"torch\",\r\n# \"kernels\",\r\n# ]\r\n# ///\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom kernels import get_kernel\r\nDEVICE = \"cuda\"\r\n# Make reproducible\r\ntorch.manual_seed(42)\r\n# Download optimized activation kernels from the Hub\r\nactivation_kernels = get_kernel(\"kernels-community/activation\")\r\n# Create a random tensor on the GPU\r\nx = torch.randn((4, 4), dtype=torch.float16, device=DEVICE)\r\n# Prepare an output tensor\r\ny = torch.empty_like(x)\r\n# Run the fast GELU kernel\r\nactivation_kernels.gelu_fast(y, x)\r\n# Get expected output using PyTorch's built-in GELU\r\nexpected = F.gelu(x)\r\n# Compare the kernel output with PyTorch's result\r\ntorch.testing.assert_close(y, expected, rtol=1e-2, atol=1e-2)\r\nprint(\"✅ Kernel output matches PyTorch GELU!\")\r\n# Optional: print both tensors for inspection\r\nprint(\"\\nInput tensor:\")\r\nprint(x)\r\nprint(\"\\nFast GELU kernel output:\")\r\nprint(y)\r\nprint(\"\\nPyTorch GELU output:\")\r\nprint(expected)\r\n# List available functions in the loaded kernel module\r\nprint(\"\\nAvailable functions in 'kernels-community/activation':\")\r\nprint(dir(activation_kernels))\r\n(Note: If you have uv\r\ninstalled, you can save this script as script.py\r\nand run uv run script.py\r\nto automatically handle dependencies.)\r\nWhat's happening here?\r\n- Import\r\nget_kernel\r\n: This function is the entry point to the Kernel Hub via thekernels\r\nlibrary. get_kernel(\"kernels-community/activation\")\r\n: This line looks for theactivation\r\nkernel repository under thekernels-community\r\norganization. It downloads, caches, and loads the appropriate pre-compiled kernel binary.- Prepare Tensors: We create input (\r\nx\r\n) and output (y\r\n) tensors on the GPU. activation_kernels.gelu_fast(y, x)\r\n: We call the specific optimized function (gelu_fast\r\n) provided by the loaded kernel module.- Verification: We check the output.\r\nThis simple example shows how easily you can fetch and execute highly optimized code. Now let's look at a more practical integration using RMS Normalization.\r\n3. Add a Kernel to a Simple Model\r\nLet's integrate an optimized RMS Normalization kernel into a basic model. We'll use the LlamaRMSNorm\r\nimplementation provided in the kernels-community/triton-layer-norm\r\nrepository (note: this repo contains various normalization kernels) and compare it against a baseline PyTorch implementation of RMSNorm.\r\nFirst, define a simple RMSNorm module in PyTorch and a baseline model using it:\r\nFile: rmsnorm_baseline.py\r\n# /// script\r\n# dependencies = [\r\n# \"numpy\",\r\n# \"torch\",\r\n# \"kernels\",\r\n# ]\r\n# ///\r\nimport torch\r\nimport torch.nn as nn\r\nDEVICE = \"cuda\"\r\nDTYPE = torch.float16 # Use float16 for better kernel performance potential\r\n# Simple PyTorch implementation of RMSNorm for baseline comparison\r\nclass RMSNorm(nn.Module):\r\ndef __init__(self, hidden_size, variance_epsilon=1e-5):\r\nsuper().__init__()\r\nself.weight = nn.Parameter(torch.ones(hidden_size))\r\nself.eps = variance_epsilon\r\nself.hidden_size = hidden_size\r\ndef forward(self, x):\r\n# Assumes x is (batch_size, ..., hidden_size)\r\ninput_dtype = x.dtype\r\n# Calculate variance in float32 for stability\r\nvariance = x.to(torch.float32).pow(2).mean(-1, keepdim=True)\r\nx = x * torch.rsqrt(variance + self.eps)\r\n# Apply weight and convert back to original dtype\r\nreturn (self.weight * x).to(input_dtype)\r\nclass BaselineModel(nn.Module):\r\ndef __init__(self, input_size, hidden_size, output_size, eps=1e-5):\r\nsuper().__init__()\r\nself.linear1 = nn.Linear(input_size, hidden_size)\r\nself.norm = RMSNorm(hidden_size, variance_epsilon=eps)\r\nself.activation = nn.GELU()\r\nself.linear2 = nn.Linear(hidden_size, output_size)\r\n# ensure all linear layers weights are 1 for testing\r\nwith torch.no_grad():\r\nself.linear1.weight.fill_(1)\r\nself.linear1.bias.fill_(0)\r\nself.linear2.weight.fill_(1)\r\nself.linear2.bias.fill_(0)\r\nself.norm.weight.fill_(1)\r\ndef forward(self, x):\r\nx = self.linear1(x)\r\nx = self.norm(x) # Apply RMSNorm\r\nx = self.activation(x)\r\nx = self.linear2(x)\r\nreturn x\r\n# Example usage\r\ninput_size = 128\r\nhidden_size = 256\r\noutput_size = 10\r\neps_val = 1e-5\r\nbaseline_model = (\r\nBaselineModel(input_size, hidden_size, output_size, eps=eps_val)\r\n.to(DEVICE)\r\n.to(DTYPE)\r\n)\r\ndummy_input = torch.randn(32, input_size, device=DEVICE, dtype=DTYPE) # Batch of 32\r\noutput = baseline_model(dummy_input)\r\nprint(\"Baseline RMSNorm model output shape:\", output.shape)\r\nNow, let's create a version using the LlamaRMSNorm\r\nkernel loaded via kernels\r\n.\r\nFile: rmsnorm_kernel.py\r\n# /// script\r\n# dependencies = [\r\n# \"numpy\",\r\n# \"torch\",\r\n# \"kernels\",\r\n# ]\r\n# ///\r\nimport torch\r\nimport torch.nn as nn\r\nfrom kernels import get_kernel, use_kernel_forward_from_hub\r\n# reuse the model from the previous snippet or copy the class\r\n# definition here to run this script independently\r\nfrom rmsnorm_baseline import BaselineModel\r\nDEVICE = \"cuda\"\r\nDTYPE = torch.float16 # Use float16 for better kernel performance potential\r\nlayer_norm_kernel_module = get_kernel(\"kernels-community/triton-layer-norm\")\r\n# Simply add the decorator to the LlamaRMSNorm class to automatically replace the forward function\r\n# with the optimized kernel version\r\n#\r\n# Note: note all kernels ship with layers already mapped, and would require calling the function directly\r\n# Howeber in this case, the LlamaRMSNorm class is already mapped to the kernel function. Otherwise we'd need to\r\n# call the function directly like this:\r\n# ```python\r\n# layer_norm_kernel_module.rms_norm_fn(\r\n# hidden_states,\r\n# self.weight,\r\n# bias=None,\r\n# residual=None,\r\n# eps=self.variance_epsilon,\r\n# dropout_p=0.0,\r\n# prenorm=False,\r\n# residual_in_fp32=False,\r\n# )\r\n# ```\r\n@use_kernel_forward_from_hub(\"LlamaRMSNorm\")\r\nclass OriginalRMSNorm(nn.Module):\r\ndef __init__(self, hidden_size, variance_epsilon=1e-5):\r\nsuper().__init__()\r\nself.weight = nn.Parameter(torch.ones(hidden_size))\r\nself.eps = variance_epsilon\r\nself.hidden_size = hidden_size\r\ndef forward(self, x):\r\n# Assumes x is (batch_size, ..., hidden_size)\r\ninput_dtype = x.dtype\r\n# Calculate variance in float32 for stability\r\nvariance = x.to(torch.float32).pow(2).mean(-1, keepdim=True)\r\nx = x * torch.rsqrt(variance + self.eps)\r\n# Apply weight and convert back to original dtype\r\nreturn (self.weight * x).to(input_dtype)\r\nclass KernelModel(nn.Module):\r\ndef __init__(\r\nself,\r\ninput_size,\r\nhidden_size,\r\noutput_size,\r\ndevice=\"cuda\",\r\ndtype=torch.float16,\r\neps=1e-5,\r\n):\r\nsuper().__init__()\r\nself.linear1 = nn.Linear(input_size, hidden_size)\r\n# OriginalRMSNorm will be replaced with the optimized kernel layer\r\n# when the model is loaded\r\nself.norm = OriginalRMSNorm(hidden_size, variance_epsilon=eps)\r\nself.activation = nn.GELU()\r\nself.linear2 = nn.Linear(hidden_size, output_size)\r\n# ensure all linear layers weights are 1 for testing\r\nwith torch.no_grad():\r\nself.linear1.weight.fill_(1)\r\nself.linear1.bias.fill_(0)\r\nself.linear2.weight.fill_(1)\r\nself.linear2.bias.fill_(0)\r\nself.norm.weight.fill_(1)\r\ndef forward(self, x):\r\nx = self.linear1(x)\r\nx = self.norm(x)\r\nx = self.activation(x)\r\nx = self.linear2(x)\r\nreturn x\r\n# Example usage\r\ninput_size = 128\r\nhidden_size = 256\r\noutput_size = 10\r\neps_val = 1e-5\r\nkernel_model = (\r\nKernelModel(\r\ninput_size, hidden_size, output_size, device=DEVICE, dtype=DTYPE, eps=eps_val\r\n)\r\n.to(DEVICE)\r\n.to(DTYPE)\r\n)\r\nbaseline_model = (\r\nBaselineModel(input_size, hidden_size, output_size, eps=eps_val)\r\n.to(DEVICE)\r\n.to(DTYPE)\r\n)\r\ndummy_input = torch.randn(32, input_size, device=DEVICE, dtype=DTYPE) # Batch of 32\r\noutput = baseline_model(dummy_input)\r\noutput_kernel = kernel_model(dummy_input)\r\nprint(\"Kernel RMSNorm model output shape:\", output_kernel.shape)\r\n# Verify outputs are close (RMSNorm implementations should be numerically close)\r\ntry:\r\ntorch.testing.assert_close(output, output_kernel, rtol=1e-2, atol=1e-2)\r\nprint(\"\\nBaseline and Kernel RMSNorm model outputs match!\")\r\nexcept AssertionError as e:\r\nprint(\"\\nBaseline and Kernel RMSNorm model outputs differ slightly:\")\r\nprint(e)\r\nexcept NameError:\r\nprint(\"\\nSkipping output comparison as kernel model output was not generated.\")\r\nImportant Notes on the KernelModel\r\n:\r\n- Kernel Inheritance: The\r\nKernelRMSNorm\r\nclass inherits fromlayer_norm_kernel_module.layers.LlamaRMSNorm\r\n, which is the RMSNorm implementation in the kernel. This allows us to use the optimized kernel directly. - Accessing the Function: The exact way to access the RMSNorm function (\r\nlayer_norm_kernel_module.layers.LlamaRMSNorm.forward\r\n,layer_norm_kernel_module.rms_norm_forward\r\n, or something else) depends entirely on how the kernel creator structured the repository on the Hub. You may need to inspect the loadedlayer_norm_kernel_module\r\nobject (e.g., usingdir()\r\n) or check the kernel's documentation on the Hub to find the correct function/method and its signature. I've usedrms_norm_forward\r\nas a plausible placeholder and added error handling. - Parameters: We now only define\r\nrms_norm_weight\r\n(no bias), consistent with RMSNorm.\r\n4. Benchmarking the Performance Impact\r\nHow much faster is the optimized Triton RMSNorm kernel compared to the standard PyTorch version? Let’s benchmark the forward pass to find out.\r\nFile: rmsnorm_benchmark.py\r\n# /// script\r\n# dependencies = [\r\n# \"numpy\",\r\n# \"torch\",\r\n# \"kernels\",\r\n# ]\r\n# ///\r\nimport torch\r\n# reuse the models from the previous snippets or copy the class\r\n# definitions here to run this script independently\r\nfrom rmsnorm_baseline import BaselineModel\r\nfrom rmsnorm_kernel import KernelModel\r\nDEVICE = \"cuda\"\r\nDTYPE = torch.float16 # Use float16 for better kernel performance potential\r\n# Use torch.cuda.Event for accurate GPU timing (ensure function is defined)\r\ndef benchmark_model(model, input_tensor, num_runs=100, warmup_runs=10):\r\nmodel.eval() # Set model to evaluation mode\r\ndtype = input_tensor.dtype\r\nmodel = model.to(input_tensor.device).to(dtype)\r\n# Warmup runs\r\nfor _ in range(warmup_runs):\r\n_ = model(input_tensor)\r\ntorch.cuda.synchronize()\r\n# Timed runs\r\nstart_event = torch.cuda.Event(enable_timing=True)\r\nend_event = torch.cuda.Event(enable_timing=True)\r\nstart_event.record()\r\nfor _ in range(num_runs):\r\n_ = model(input_tensor)\r\nend_event.record()\r\ntorch.cuda.synchronize()\r\nelapsed_time_ms = start_event.elapsed_time(end_event)\r\navg_time_ms = elapsed_time_ms / num_runs\r\nreturn avg_time_ms\r\ninput_size_bench = 4096\r\nhidden_size_bench = 4096 # RMSNorm performance is sensitive to this dimension\r\noutput_size_bench = 10\r\neps_val_bench = 1e-5\r\n# Create larger models and input for benchmark\r\n# Ensure both models are fully converted to the target DEVICE and DTYPE\r\nbaseline_model_bench = (\r\nBaselineModel(\r\ninput_size_bench, hidden_size_bench, output_size_bench, eps=eps_val_bench\r\n)\r\n.to(DEVICE)\r\n.to(DTYPE)\r\n)\r\nkernel_model_bench = (\r\nKernelModel(\r\ninput_size_bench,\r\nhidden_size_bench,\r\noutput_size_bench,\r\ndevice=DEVICE,\r\ndtype=DTYPE,\r\neps=eps_val_bench,\r\n)\r\n.to(DEVICE)\r\n.to(DTYPE)\r\n)\r\n# call both with larger batch sizes to warm up the GPU\r\n# and ensure the models are loaded\r\nwarmup_input = torch.randn(4096, input_size_bench, device=DEVICE, dtype=DTYPE)\r\n_ = kernel_model_bench(warmup_input)\r\n_ = baseline_model_bench(warmup_input)\r\nbatch_sizes = [\r\n256,\r\n512,\r\n1024,\r\n2048,\r\n4096,\r\n8192,\r\n16384,\r\n32768,\r\n]\r\nprint(\r\nf\"{'Batch Size':<12} | {'Baseline Time (ms)':<18} | {'Kernel Time (ms)':<18} | {'Speedup'}\"\r\n)\r\nprint(\"-\" * 74)\r\nfor batch_size in batch_sizes:\r\n# Call cuda synchronize to ensure all previous GPU operations are complete\r\ntorch.cuda.synchronize()\r\n# Create random input tensor\r\n# Ensure the input tensor is on the correct device and dtype\r\nbench_input = torch.randn(batch_size, input_size_bench, device=DEVICE, dtype=DTYPE)\r\n# Run benchmarks only if kernel was loaded successfully\r\nbaseline_time = benchmark_model(baseline_model_bench, bench_input)\r\nkernel_time = -1 # Sentinel value\r\nkernel_time = benchmark_model(kernel_model_bench, bench_input)\r\nbaseline_time = round(baseline_time, 4)\r\nkernel_time = round(kernel_time, 4)\r\nspeedup = round(baseline_time / kernel_time, 2) if kernel_time > 0 else \"N/A\"\r\nif kernel_time < baseline_time:\r\nspeedup = f\"{speedup:.2f}x\"\r\nelif kernel_time == baseline_time:\r\nspeedup = \"1.00x (identical)\"\r\nelse:\r\nspeedup = f\"{kernel_time / baseline_time:.2f}x slower\"\r\nprint(f\"{batch_size:<12} | {baseline_time:<18} | {kernel_time:<18} | {speedup}\")\r\nExpected Outcome:\r\nAs with LayerNorm, a well-tuned RMSNorm implementation using Triton can deliver substantial speedups over PyTorch’s default version—especially for memory-bound workloads on compatible hardware (e.g., NVIDIA Ampere or Hopper GPUs) and with low-precision types like float16\r\nor bfloat16\r\n.\r\nKeep in Mind:\r\n- Results may vary depending on your GPU, input size, and data type.\r\n- Microbenchmarks can misrepresent real-world performance.\r\n- Performance hinges on the quality of the kernel implementation.\r\n- Optimized kernels might not benefit small batch sizes due to overhead.\r\nActual results will depend on your hardware and the specific kernel implementation. Here's an example of what you might see (on a L4 GPU):\r\n| Batch Size | Baseline Time (ms) | Kernel Time (ms) | Speedup |\r\n|---|---|---|---|\r\n| 256 | 0.2122 | 0.2911 | 0.72x |\r\n| 512 | 0.4748 | 0.3312 | 1.43x |\r\n| 1024 | 0.8946 | 0.6864 | 1.30x |\r\n| 2048 | 2.0289 | 1.3889 | 1.46x |\r\n| 4096 | 4.4318 | 2.2467 | 1.97x |\r\n| 8192 | 9.2438 | 4.8497 | 1.91x |\r\n| 16384 | 18.6992 | 9.8805 | 1.89x |\r\n| 32768 | 37.079 | 19.9461 | 1.86x |\r\n| 65536 | 73.588 | 39.593 | 1.86x |\r\n5. Real World Use Cases\r\nThe kernels\r\nlibrary is still growing but is already being used in various real work projects, including:\r\n- Text Generation Inference: The TGI project uses the\r\nkernels\r\nlibrary to load optimized kernels for text generation tasks, improving performance and efficiency. - Transformers: The Transformers library has integrated the\r\nkernels\r\nlibrary to use drop in optimized layers without requiring any changes to the model code. This allows users to easily switch between standard and optimized implementations.\r\nGet Started and Next Steps!\r\nYou've seen how easy it is to fetch and use optimized kernels with the Hugging Face Kernel Hub. Ready to try it yourself?\r\nInstall the library:\r\npip install kernels torch numpy\r\nEnsure you have a compatible PyTorch version and gpu driver installed.\r\nBrowse the Hub: Explore available kernels on the Hugging Face Hub under the\r\nkernels\r\ntag or within organizations likekernels-community\r\n. Look for kernels relevant to your operations (activations, attention, normalization like LayerNorm/RMSNorm, etc.).Experiment: Try replacing components in your own models. Use\r\nget_kernel(\"user-or-org/kernel-name\")\r\n. Crucially, inspect the loaded kernel object (e.g.,print(dir(loaded_kernel))\r\n) or check its Hub repository documentation to understand how to correctly call its functions/methods and what parameters (weights, biases, inputs, epsilon) it expects.Benchmark: Measure the performance impact on your specific hardware and workload. Don't forget to check for numerical correctness (\r\ntorch.testing.assert_close\r\n).(Advanced) Contribute: If you develop optimized kernels, consider sharing them on the Hub!\r\nConclusion\r\nThe Hugging Face Kernel Hub provides a powerful yet simple way to access and leverage optimized compute kernels. By replacing standard PyTorch components with optimized versions for operations like RMS Normalization, you can potentially unlock significant performance improvements without the traditional complexities of custom builds. Remember to check the specifics of each kernel on the Hub for correct usage. Give it a try and see how it can accelerate your workflows!","src/content/posts/2025-06-20-enhance-your-models-in-5-minutes-with-the-hugging-face-kernel-hub-6d932b.md","527e226595b3b7ce",{html:217,metadata:218},"<p>🏎️ Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub\r\nBoost your model performance with pre-optimized kernels, easily loaded from the Hub.\r\nToday, we’ll explore an exciting development from Hugging Face: the Kernel Hub! As ML practitioners, we know that maximizing performance often involves diving deep into optimized code, custom CUDA kernels, or complex build systems. The Kernel Hub simplifies this process dramatically!\r\nBelow is a short example of how to use a kernel in your code.\r\nimport torch\r\nfrom kernels import get_kernel</p>\n<h1 id=\"download-optimized-kernels-from-the-hugging-face-hub\">Download optimized kernels from the Hugging Face hub</h1>\n<p>activation = get_kernel(“kernels-community/activation”)</p>\n<h1 id=\"random-tensor\">Random tensor</h1>\n<p>x = torch.randn((10, 10), dtype=torch.float16, device=“cuda”)</p>\n<h1 id=\"run-the-kernel\">Run the kernel</h1>\n<p>y = torch.empty_like(x)\r\nactivation.gelu_fast(y, x)\r\nprint(y)\r\nIn the next sections we’ll cover the following topics:</p>\n<ul>\n<li>What is the Kernel Hub? - Understanding the core concept.</li>\n<li>How to use the Kernel Hub - A quick code example.</li>\n<li>Adding a Kernel to a Simple Model - A practical integration using RMSNorm.</li>\n<li>Reviewing Performance Impact - Benchmarking the RMSNorm difference.</li>\n<li>Real world use cases - Examples of how the kernels library is being used in other projects.\r\nWe’ll introduce these concepts quickly – the core idea can be grasped in about 5 minutes (though experimenting and benchmarking might take a bit longer!).</li>\n</ul>\n<ol>\n<li>What is the Kernel Hub?\r\nThe Kernel Hub (👈 Check it out!) allows Python libraries and applications to load optimized compute kernels directly from the Hugging Face Hub. Think of it like the Model Hub, but for low-level, high-performance code snippets (kernels) that accelerate specific operations, often on GPUs.\r\nExamples include advanced attention mechanisms (like FlashAttention for dramatic speedups and memory savings). Custom quantization kernels (enabling efficient computation with lower-precision data types like INT8 or INT4). Specialized kernels required for complex architectures like Mixture of Experts (MoE) layers, which involve intricate routing and computation patterns. As well as activation functions, and normalization layers (like LayerNorm or RMSNorm).\r\nInstead of manually managing complex dependencies, wrestling with compilation flags, or building libraries like Triton or CUTLASS from source, you can use the kernels\r\nlibrary to instantly fetch and run pre-compiled, optimized kernels.\r\nFor example, to enable FlashAttention you need just one line—no builds, no flags:\r\nfrom kernels import get_kernel\r\nflash_attention = get_kernel(“kernels-community/flash-attn”)\r\nkernels\r\ndetects your exact Python, PyTorch, and CUDA versions, then downloads the matching pre‑compiled binary—typically in seconds (or a minute or two on a slow connection).\r\nBy contrast, compiling FlashAttention yourself requires:</li>\n</ol>\n<ul>\n<li>Cloning the repository and installing every dependency.</li>\n<li>Configuring build flags and environment variables.</li>\n<li>Reserving ~96 GB of RAM and plenty of CPU cores.</li>\n<li>Waiting 10 minutes to several hours, depending on your hardware. (See the project’s own installation guide for details.)\r\nKernel Hub erases all that friction: one function call, instant acceleration.\r\nBenefits of the Kernel Hub:</li>\n<li>Instant Access to Optimized Kernels: Load and run kernels optimized for various hardware starting with NVIDIA and AMD GPUs, without local compilation hassles.</li>\n<li>Share and Reuse: Discover, share, and reuse kernels across different projects and the community.</li>\n<li>Easy Updates: Stay up-to-date with the latest kernel improvements simply by pulling the latest version from the Hub.</li>\n<li>Accelerate Development: Focus on your model architecture and logic, not on the intricacies of kernel compilation and deployment.</li>\n<li>Improve Performance: Leverage kernels optimized by experts to potentially speed up training and inference.</li>\n<li>Simplify Deployment: Reduce the complexity of your deployment environment by fetching kernels on demand.</li>\n<li>Develop and Share Your Own Kernels: If you create optimized kernels, you can easily share them on the Hub for others to use. This encourages collaboration and knowledge sharing within the community.\r\nAs many machine learning developers know, managing dependencies and building low-level code from source can be a time-consuming and error-prone process. The Kernel Hub aims to simplify this by providing a centralized repository of optimized compute kernels that can be easily loaded and run.\r\nSpend more time building great models and less time fighting build systems!</li>\n</ul>\n<ol start=\"2\">\n<li>How to Use the Kernel Hub (Basic Example)\r\nUsing the Kernel Hub is designed to be straightforward. The kernels\r\nlibrary provides the main interface. Here’s a quick example that loads an optimized GELU activation function kernel. (Later on, we’ll see another example about how to integrate a kernel in our model).\r\nFile: activation_validation_example.py</li>\n</ol>\n<h1 id=\"-script\">/// script</h1>\n<h1 id=\"dependencies-\">dependencies = [</h1>\n<h1 id=\"numpy\">“numpy”,</h1>\n<h1 id=\"torch\">“torch”,</h1>\n<h1 id=\"kernels\">“kernels”,</h1>\n<h1 id=\"\">]</h1>\n<h1 id=\"-1\">///</h1>\n<p>import torch\r\nimport torch.nn.functional as F\r\nfrom kernels import get_kernel\r\nDEVICE = “cuda”</p>\n<h1 id=\"make-reproducible\">Make reproducible</h1>\n<p>torch.manual_seed(42)</p>\n<h1 id=\"download-optimized-activation-kernels-from-the-hub\">Download optimized activation kernels from the Hub</h1>\n<p>activation_kernels = get_kernel(“kernels-community/activation”)</p>\n<h1 id=\"create-a-random-tensor-on-the-gpu\">Create a random tensor on the GPU</h1>\n<p>x = torch.randn((4, 4), dtype=torch.float16, device=DEVICE)</p>\n<h1 id=\"prepare-an-output-tensor\">Prepare an output tensor</h1>\n<p>y = torch.empty_like(x)</p>\n<h1 id=\"run-the-fast-gelu-kernel\">Run the fast GELU kernel</h1>\n<p>activation_kernels.gelu_fast(y, x)</p>\n<h1 id=\"get-expected-output-using-pytorchs-built-in-gelu\">Get expected output using PyTorch’s built-in GELU</h1>\n<p>expected = F.gelu(x)</p>\n<h1 id=\"compare-the-kernel-output-with-pytorchs-result\">Compare the kernel output with PyTorch’s result</h1>\n<p>torch.testing.assert_close(y, expected, rtol=1e-2, atol=1e-2)\r\nprint(”✅ Kernel output matches PyTorch GELU!”)</p>\n<h1 id=\"optional-print-both-tensors-for-inspection\">Optional: print both tensors for inspection</h1>\n<p>print(“\\nInput tensor:”)\r\nprint(x)\r\nprint(“\\nFast GELU kernel output:”)\r\nprint(y)\r\nprint(“\\nPyTorch GELU output:”)\r\nprint(expected)</p>\n<h1 id=\"list-available-functions-in-the-loaded-kernel-module\">List available functions in the loaded kernel module</h1>\n<p>print(“\\nAvailable functions in ‘kernels-community/activation’:”)\r\nprint(dir(activation_kernels))\r\n(Note: If you have uv\r\ninstalled, you can save this script as script.py\r\nand run uv run script.py\r\nto automatically handle dependencies.)\r\nWhat’s happening here?</p>\n<ul>\n<li>Import\r\nget_kernel\r\n: This function is the entry point to the Kernel Hub via thekernels\r\nlibrary. get_kernel(“kernels-community/activation”)\r\n: This line looks for theactivation\r\nkernel repository under thekernels-community\r\norganization. It downloads, caches, and loads the appropriate pre-compiled kernel binary.- Prepare Tensors: We create input (\r\nx\r\n) and output (y\r\n) tensors on the GPU. activation_kernels.gelu_fast(y, x)\r\n: We call the specific optimized function (gelu_fast\r\n) provided by the loaded kernel module.- Verification: We check the output.\r\nThis simple example shows how easily you can fetch and execute highly optimized code. Now let’s look at a more practical integration using RMS Normalization.</li>\n</ul>\n<ol start=\"3\">\n<li>Add a Kernel to a Simple Model\r\nLet’s integrate an optimized RMS Normalization kernel into a basic model. We’ll use the LlamaRMSNorm\r\nimplementation provided in the kernels-community/triton-layer-norm\r\nrepository (note: this repo contains various normalization kernels) and compare it against a baseline PyTorch implementation of RMSNorm.\r\nFirst, define a simple RMSNorm module in PyTorch and a baseline model using it:\r\nFile: rmsnorm_baseline.py</li>\n</ol>\n<h1 id=\"-script-1\">/// script</h1>\n<h1 id=\"dependencies---1\">dependencies = [</h1>\n<h1 id=\"numpy-1\">“numpy”,</h1>\n<h1 id=\"torch-1\">“torch”,</h1>\n<h1 id=\"kernels-1\">“kernels”,</h1>\n<h1 id=\"-2\">]</h1>\n<h1 id=\"-3\">///</h1>\n<p>import torch\r\nimport torch.nn as nn\r\nDEVICE = “cuda”\r\nDTYPE = torch.float16 # Use float16 for better kernel performance potential</p>\n<h1 id=\"simple-pytorch-implementation-of-rmsnorm-for-baseline-comparison\">Simple PyTorch implementation of RMSNorm for baseline comparison</h1>\n<p>class RMSNorm(nn.Module):\r\ndef <strong>init</strong>(self, hidden_size, variance_epsilon=1e-5):\r\nsuper().<strong>init</strong>()\r\nself.weight = nn.Parameter(torch.ones(hidden_size))\r\nself.eps = variance_epsilon\r\nself.hidden_size = hidden_size\r\ndef forward(self, x):</p>\n<h1 id=\"assumes-x-is-batch_size--hidden_size\">Assumes x is (batch_size, …, hidden_size)</h1>\n<p>input_dtype = x.dtype</p>\n<h1 id=\"calculate-variance-in-float32-for-stability\">Calculate variance in float32 for stability</h1>\n<p>variance = x.to(torch.float32).pow(2).mean(-1, keepdim=True)\r\nx = x * torch.rsqrt(variance + self.eps)</p>\n<h1 id=\"apply-weight-and-convert-back-to-original-dtype\">Apply weight and convert back to original dtype</h1>\n<p>return (self.weight * x).to(input_dtype)\r\nclass BaselineModel(nn.Module):\r\ndef <strong>init</strong>(self, input_size, hidden_size, output_size, eps=1e-5):\r\nsuper().<strong>init</strong>()\r\nself.linear1 = nn.Linear(input_size, hidden_size)\r\nself.norm = RMSNorm(hidden_size, variance_epsilon=eps)\r\nself.activation = nn.GELU()\r\nself.linear2 = nn.Linear(hidden_size, output_size)</p>\n<h1 id=\"ensure-all-linear-layers-weights-are-1-for-testing\">ensure all linear layers weights are 1 for testing</h1>\n<p>with torch.no_grad():\r\nself.linear1.weight.fill_(1)\r\nself.linear1.bias.fill_(0)\r\nself.linear2.weight.fill_(1)\r\nself.linear2.bias.fill_(0)\r\nself.norm.weight.fill_(1)\r\ndef forward(self, x):\r\nx = self.linear1(x)\r\nx = self.norm(x) # Apply RMSNorm\r\nx = self.activation(x)\r\nx = self.linear2(x)\r\nreturn x</p>\n<h1 id=\"example-usage\">Example usage</h1>\n<p>input_size = 128\r\nhidden_size = 256\r\noutput_size = 10\r\neps_val = 1e-5\r\nbaseline_model = (\r\nBaselineModel(input_size, hidden_size, output_size, eps=eps_val)\r\n.to(DEVICE)\r\n.to(DTYPE)\r\n)\r\ndummy_input = torch.randn(32, input_size, device=DEVICE, dtype=DTYPE) # Batch of 32\r\noutput = baseline_model(dummy_input)\r\nprint(“Baseline RMSNorm model output shape:”, output.shape)\r\nNow, let’s create a version using the LlamaRMSNorm\r\nkernel loaded via kernels\r\n.\r\nFile: rmsnorm_kernel.py</p>\n<h1 id=\"-script-2\">/// script</h1>\n<h1 id=\"dependencies---2\">dependencies = [</h1>\n<h1 id=\"numpy-2\">“numpy”,</h1>\n<h1 id=\"torch-2\">“torch”,</h1>\n<h1 id=\"kernels-2\">“kernels”,</h1>\n<h1 id=\"-4\">]</h1>\n<h1 id=\"-5\">///</h1>\n<p>import torch\r\nimport torch.nn as nn\r\nfrom kernels import get_kernel, use_kernel_forward_from_hub</p>\n<h1 id=\"reuse-the-model-from-the-previous-snippet-or-copy-the-class\">reuse the model from the previous snippet or copy the class</h1>\n<h1 id=\"definition-here-to-run-this-script-independently\">definition here to run this script independently</h1>\n<p>from rmsnorm_baseline import BaselineModel\r\nDEVICE = “cuda”\r\nDTYPE = torch.float16 # Use float16 for better kernel performance potential\r\nlayer_norm_kernel_module = get_kernel(“kernels-community/triton-layer-norm”)</p>\n<h1 id=\"simply-add-the-decorator-to-the-llamarmsnorm-class-to-automatically-replace-the-forward-function\">Simply add the decorator to the LlamaRMSNorm class to automatically replace the forward function</h1>\n<h1 id=\"with-the-optimized-kernel-version\">with the optimized kernel version</h1>\n<h1 id=\"-6\"></h1>\n<h1 id=\"note-note-all-kernels-ship-with-layers-already-mapped-and-would-require-calling-the-function-directly\">Note: note all kernels ship with layers already mapped, and would require calling the function directly</h1>\n<h1 id=\"howeber-in-this-case-the-llamarmsnorm-class-is-already-mapped-to-the-kernel-function-otherwise-wed-need-to\">Howeber in this case, the LlamaRMSNorm class is already mapped to the kernel function. Otherwise we’d need to</h1>\n<h1 id=\"call-the-function-directly-like-this\">call the function directly like this:</h1>\n<h1 id=\"python\">```python</h1>\n<h1 id=\"layer_norm_kernel_modulerms_norm_fn\">layer_norm_kernel_module.rms_norm_fn(</h1>\n<h1 id=\"hidden_states\">hidden_states,</h1>\n<h1 id=\"selfweight\">self.weight,</h1>\n<h1 id=\"biasnone\">bias=None,</h1>\n<h1 id=\"residualnone\">residual=None,</h1>\n<h1 id=\"epsselfvariance_epsilon\">eps=self.variance_epsilon,</h1>\n<h1 id=\"dropout_p00\">dropout_p=0.0,</h1>\n<h1 id=\"prenormfalse\">prenorm=False,</h1>\n<h1 id=\"residual_in_fp32false\">residual_in_fp32=False,</h1>\n<h1 id=\"-7\">)</h1>\n<h1 id=\"-8\">```</h1>\n<p>@use_kernel_forward_from_hub(“LlamaRMSNorm”)\r\nclass OriginalRMSNorm(nn.Module):\r\ndef <strong>init</strong>(self, hidden_size, variance_epsilon=1e-5):\r\nsuper().<strong>init</strong>()\r\nself.weight = nn.Parameter(torch.ones(hidden_size))\r\nself.eps = variance_epsilon\r\nself.hidden_size = hidden_size\r\ndef forward(self, x):</p>\n<h1 id=\"assumes-x-is-batch_size--hidden_size-1\">Assumes x is (batch_size, …, hidden_size)</h1>\n<p>input_dtype = x.dtype</p>\n<h1 id=\"calculate-variance-in-float32-for-stability-1\">Calculate variance in float32 for stability</h1>\n<p>variance = x.to(torch.float32).pow(2).mean(-1, keepdim=True)\r\nx = x * torch.rsqrt(variance + self.eps)</p>\n<h1 id=\"apply-weight-and-convert-back-to-original-dtype-1\">Apply weight and convert back to original dtype</h1>\n<p>return (self.weight * x).to(input_dtype)\r\nclass KernelModel(nn.Module):\r\ndef <strong>init</strong>(\r\nself,\r\ninput_size,\r\nhidden_size,\r\noutput_size,\r\ndevice=“cuda”,\r\ndtype=torch.float16,\r\neps=1e-5,\r\n):\r\nsuper().<strong>init</strong>()\r\nself.linear1 = nn.Linear(input_size, hidden_size)</p>\n<h1 id=\"originalrmsnorm-will-be-replaced-with-the-optimized-kernel-layer\">OriginalRMSNorm will be replaced with the optimized kernel layer</h1>\n<h1 id=\"when-the-model-is-loaded\">when the model is loaded</h1>\n<p>self.norm = OriginalRMSNorm(hidden_size, variance_epsilon=eps)\r\nself.activation = nn.GELU()\r\nself.linear2 = nn.Linear(hidden_size, output_size)</p>\n<h1 id=\"ensure-all-linear-layers-weights-are-1-for-testing-1\">ensure all linear layers weights are 1 for testing</h1>\n<p>with torch.no_grad():\r\nself.linear1.weight.fill_(1)\r\nself.linear1.bias.fill_(0)\r\nself.linear2.weight.fill_(1)\r\nself.linear2.bias.fill_(0)\r\nself.norm.weight.fill_(1)\r\ndef forward(self, x):\r\nx = self.linear1(x)\r\nx = self.norm(x)\r\nx = self.activation(x)\r\nx = self.linear2(x)\r\nreturn x</p>\n<h1 id=\"example-usage-1\">Example usage</h1>\n<p>input_size = 128\r\nhidden_size = 256\r\noutput_size = 10\r\neps_val = 1e-5\r\nkernel_model = (\r\nKernelModel(\r\ninput_size, hidden_size, output_size, device=DEVICE, dtype=DTYPE, eps=eps_val\r\n)\r\n.to(DEVICE)\r\n.to(DTYPE)\r\n)\r\nbaseline_model = (\r\nBaselineModel(input_size, hidden_size, output_size, eps=eps_val)\r\n.to(DEVICE)\r\n.to(DTYPE)\r\n)\r\ndummy_input = torch.randn(32, input_size, device=DEVICE, dtype=DTYPE) # Batch of 32\r\noutput = baseline_model(dummy_input)\r\noutput_kernel = kernel_model(dummy_input)\r\nprint(“Kernel RMSNorm model output shape:”, output_kernel.shape)</p>\n<h1 id=\"verify-outputs-are-close-rmsnorm-implementations-should-be-numerically-close\">Verify outputs are close (RMSNorm implementations should be numerically close)</h1>\n<p>try:\r\ntorch.testing.assert_close(output, output_kernel, rtol=1e-2, atol=1e-2)\r\nprint(“\\nBaseline and Kernel RMSNorm model outputs match!”)\r\nexcept AssertionError as e:\r\nprint(“\\nBaseline and Kernel RMSNorm model outputs differ slightly:”)\r\nprint(e)\r\nexcept NameError:\r\nprint(“\\nSkipping output comparison as kernel model output was not generated.”)\r\nImportant Notes on the KernelModel\r\n:</p>\n<ul>\n<li>Kernel Inheritance: The\r\nKernelRMSNorm\r\nclass inherits fromlayer_norm_kernel_module.layers.LlamaRMSNorm\r\n, which is the RMSNorm implementation in the kernel. This allows us to use the optimized kernel directly. - Accessing the Function: The exact way to access the RMSNorm function (\r\nlayer_norm_kernel_module.layers.LlamaRMSNorm.forward\r\n,layer_norm_kernel_module.rms_norm_forward\r\n, or something else) depends entirely on how the kernel creator structured the repository on the Hub. You may need to inspect the loadedlayer_norm_kernel_module\r\nobject (e.g., usingdir()\r\n) or check the kernel’s documentation on the Hub to find the correct function/method and its signature. I’ve usedrms_norm_forward\r\nas a plausible placeholder and added error handling. - Parameters: We now only define\r\nrms_norm_weight\r\n(no bias), consistent with RMSNorm.</li>\n</ul>\n<ol start=\"4\">\n<li>Benchmarking the Performance Impact\r\nHow much faster is the optimized Triton RMSNorm kernel compared to the standard PyTorch version? Let’s benchmark the forward pass to find out.\r\nFile: rmsnorm_benchmark.py</li>\n</ol>\n<h1 id=\"-script-3\">/// script</h1>\n<h1 id=\"dependencies---3\">dependencies = [</h1>\n<h1 id=\"numpy-3\">“numpy”,</h1>\n<h1 id=\"torch-3\">“torch”,</h1>\n<h1 id=\"kernels-3\">“kernels”,</h1>\n<h1 id=\"-9\">]</h1>\n<h1 id=\"-10\">///</h1>\n<p>import torch</p>\n<h1 id=\"reuse-the-models-from-the-previous-snippets-or-copy-the-class\">reuse the models from the previous snippets or copy the class</h1>\n<h1 id=\"definitions-here-to-run-this-script-independently\">definitions here to run this script independently</h1>\n<p>from rmsnorm_baseline import BaselineModel\r\nfrom rmsnorm_kernel import KernelModel\r\nDEVICE = “cuda”\r\nDTYPE = torch.float16 # Use float16 for better kernel performance potential</p>\n<h1 id=\"use-torchcudaevent-for-accurate-gpu-timing-ensure-function-is-defined\">Use torch.cuda.Event for accurate GPU timing (ensure function is defined)</h1>\n<p>def benchmark_model(model, input_tensor, num_runs=100, warmup_runs=10):\r\nmodel.eval() # Set model to evaluation mode\r\ndtype = input_tensor.dtype\r\nmodel = model.to(input_tensor.device).to(dtype)</p>\n<h1 id=\"warmup-runs\">Warmup runs</h1>\n<p>for _ in range(warmup_runs):\r\n_ = model(input_tensor)\r\ntorch.cuda.synchronize()</p>\n<h1 id=\"timed-runs\">Timed runs</h1>\n<p>start_event = torch.cuda.Event(enable_timing=True)\r\nend_event = torch.cuda.Event(enable_timing=True)\r\nstart_event.record()\r\nfor _ in range(num_runs):\r\n_ = model(input_tensor)\r\nend_event.record()\r\ntorch.cuda.synchronize()\r\nelapsed_time_ms = start_event.elapsed_time(end_event)\r\navg_time_ms = elapsed_time_ms / num_runs\r\nreturn avg_time_ms\r\ninput_size_bench = 4096\r\nhidden_size_bench = 4096 # RMSNorm performance is sensitive to this dimension\r\noutput_size_bench = 10\r\neps_val_bench = 1e-5</p>\n<h1 id=\"create-larger-models-and-input-for-benchmark\">Create larger models and input for benchmark</h1>\n<h1 id=\"ensure-both-models-are-fully-converted-to-the-target-device-and-dtype\">Ensure both models are fully converted to the target DEVICE and DTYPE</h1>\n<p>baseline_model_bench = (\r\nBaselineModel(\r\ninput_size_bench, hidden_size_bench, output_size_bench, eps=eps_val_bench\r\n)\r\n.to(DEVICE)\r\n.to(DTYPE)\r\n)\r\nkernel_model_bench = (\r\nKernelModel(\r\ninput_size_bench,\r\nhidden_size_bench,\r\noutput_size_bench,\r\ndevice=DEVICE,\r\ndtype=DTYPE,\r\neps=eps_val_bench,\r\n)\r\n.to(DEVICE)\r\n.to(DTYPE)\r\n)</p>\n<h1 id=\"call-both-with-larger-batch-sizes-to-warm-up-the-gpu\">call both with larger batch sizes to warm up the GPU</h1>\n<h1 id=\"and-ensure-the-models-are-loaded\">and ensure the models are loaded</h1>\n<p>warmup_input = torch.randn(4096, input_size_bench, device=DEVICE, dtype=DTYPE)\r\n_ = kernel_model_bench(warmup_input)\r\n_ = baseline_model_bench(warmup_input)\r\nbatch_sizes = [\r\n256,\r\n512,\r\n1024,\r\n2048,\r\n4096,\r\n8192,\r\n16384,\r\n32768,\r\n]\r\nprint(\r\nf”{‘Batch Size’:&#x3C;12} | {‘Baseline Time (ms)’:&#x3C;18} | {‘Kernel Time (ms)’:&#x3C;18} | {‘Speedup’}”\r\n)\r\nprint(”-” * 74)\r\nfor batch_size in batch_sizes:</p>\n<h1 id=\"call-cuda-synchronize-to-ensure-all-previous-gpu-operations-are-complete\">Call cuda synchronize to ensure all previous GPU operations are complete</h1>\n<p>torch.cuda.synchronize()</p>\n<h1 id=\"create-random-input-tensor\">Create random input tensor</h1>\n<h1 id=\"ensure-the-input-tensor-is-on-the-correct-device-and-dtype\">Ensure the input tensor is on the correct device and dtype</h1>\n<p>bench_input = torch.randn(batch_size, input_size_bench, device=DEVICE, dtype=DTYPE)</p>\n<h1 id=\"run-benchmarks-only-if-kernel-was-loaded-successfully\">Run benchmarks only if kernel was loaded successfully</h1>\n<p>baseline_time = benchmark_model(baseline_model_bench, bench_input)\r\nkernel_time = -1 # Sentinel value\r\nkernel_time = benchmark_model(kernel_model_bench, bench_input)\r\nbaseline_time = round(baseline_time, 4)\r\nkernel_time = round(kernel_time, 4)\r\nspeedup = round(baseline_time / kernel_time, 2) if kernel_time > 0 else “N/A”\r\nif kernel_time &#x3C; baseline_time:\r\nspeedup = f”{speedup:.2f}x”\r\nelif kernel_time == baseline_time:\r\nspeedup = “1.00x (identical)”\r\nelse:\r\nspeedup = f”{kernel_time / baseline_time:.2f}x slower”\r\nprint(f”{batch_size:&#x3C;12} | {baseline_time:&#x3C;18} | {kernel_time:&#x3C;18} | {speedup}”)\r\nExpected Outcome:\r\nAs with LayerNorm, a well-tuned RMSNorm implementation using Triton can deliver substantial speedups over PyTorch’s default version—especially for memory-bound workloads on compatible hardware (e.g., NVIDIA Ampere or Hopper GPUs) and with low-precision types like float16\r\nor bfloat16\r\n.\r\nKeep in Mind:</p>\n<ul>\n<li>Results may vary depending on your GPU, input size, and data type.</li>\n<li>Microbenchmarks can misrepresent real-world performance.</li>\n<li>Performance hinges on the quality of the kernel implementation.</li>\n<li>Optimized kernels might not benefit small batch sizes due to overhead.\r\nActual results will depend on your hardware and the specific kernel implementation. Here’s an example of what you might see (on a L4 GPU):\r\n| Batch Size | Baseline Time (ms) | Kernel Time (ms) | Speedup |\r\n|---|---|---|---|\r\n| 256 | 0.2122 | 0.2911 | 0.72x |\r\n| 512 | 0.4748 | 0.3312 | 1.43x |\r\n| 1024 | 0.8946 | 0.6864 | 1.30x |\r\n| 2048 | 2.0289 | 1.3889 | 1.46x |\r\n| 4096 | 4.4318 | 2.2467 | 1.97x |\r\n| 8192 | 9.2438 | 4.8497 | 1.91x |\r\n| 16384 | 18.6992 | 9.8805 | 1.89x |\r\n| 32768 | 37.079 | 19.9461 | 1.86x |\r\n| 65536 | 73.588 | 39.593 | 1.86x |</li>\n</ul>\n<ol start=\"5\">\n<li>Real World Use Cases\r\nThe kernels\r\nlibrary is still growing but is already being used in various real work projects, including:</li>\n</ol>\n<ul>\n<li>Text Generation Inference: The TGI project uses the\r\nkernels\r\nlibrary to load optimized kernels for text generation tasks, improving performance and efficiency. - Transformers: The Transformers library has integrated the\r\nkernels\r\nlibrary to use drop in optimized layers without requiring any changes to the model code. This allows users to easily switch between standard and optimized implementations.\r\nGet Started and Next Steps!\r\nYou’ve seen how easy it is to fetch and use optimized kernels with the Hugging Face Kernel Hub. Ready to try it yourself?\r\nInstall the library:\r\npip install kernels torch numpy\r\nEnsure you have a compatible PyTorch version and gpu driver installed.\r\nBrowse the Hub: Explore available kernels on the Hugging Face Hub under the\r\nkernels\r\ntag or within organizations likekernels-community\r\n. Look for kernels relevant to your operations (activations, attention, normalization like LayerNorm/RMSNorm, etc.).Experiment: Try replacing components in your own models. Use\r\nget_kernel(“user-or-org/kernel-name”)\r\n. Crucially, inspect the loaded kernel object (e.g.,print(dir(loaded_kernel))\r\n) or check its Hub repository documentation to understand how to correctly call its functions/methods and what parameters (weights, biases, inputs, epsilon) it expects.Benchmark: Measure the performance impact on your specific hardware and workload. Don’t forget to check for numerical correctness (\r\ntorch.testing.assert_close\r\n).(Advanced) Contribute: If you develop optimized kernels, consider sharing them on the Hub!\r\nConclusion\r\nThe Hugging Face Kernel Hub provides a powerful yet simple way to access and leverage optimized compute kernels. By replacing standard PyTorch components with optimized versions for operations like RMS Normalization, you can potentially unlock significant performance improvements without the traditional complexities of custom builds. Remember to check the specifics of each kernel on the Hub for correct usage. Give it a try and see how it can accelerate your workflows!</li>\n</ul>",{headings:219,localImagePaths:453,remoteImagePaths:454,frontmatter:455,imagePaths:457},[220,223,226,229,232,235,238,241,244,246,249,252,255,258,261,264,267,270,273,276,278,280,282,284,286,288,290,293,296,299,302,305,308,310,312,314,316,318,320,322,325,328,331,334,336,339,342,345,348,351,354,357,360,363,366,369,372,375,378,381,383,385,387,390,393,395,397,400,402,404,406,408,410,412,414,417,420,423,426,429,432,435,438,441,444,447,450],{depth:31,slug:221,text:222},"download-optimized-kernels-from-the-hugging-face-hub","Download optimized kernels from the Hugging Face hub",{depth:31,slug:224,text:225},"random-tensor","Random tensor",{depth:31,slug:227,text:228},"run-the-kernel","Run the kernel",{depth:31,slug:230,text:231},"-script","/// script",{depth:31,slug:233,text:234},"dependencies-","dependencies = [",{depth:31,slug:236,text:237},"numpy","“numpy”,",{depth:31,slug:239,text:240},"torch","“torch”,",{depth:31,slug:242,text:243},"kernels","“kernels”,",{depth:31,slug:15,text:245},"]",{depth:31,slug:247,text:248},"-1","///",{depth:31,slug:250,text:251},"make-reproducible","Make reproducible",{depth:31,slug:253,text:254},"download-optimized-activation-kernels-from-the-hub","Download optimized activation kernels from the Hub",{depth:31,slug:256,text:257},"create-a-random-tensor-on-the-gpu","Create a random tensor on the GPU",{depth:31,slug:259,text:260},"prepare-an-output-tensor","Prepare an output tensor",{depth:31,slug:262,text:263},"run-the-fast-gelu-kernel","Run the fast GELU kernel",{depth:31,slug:265,text:266},"get-expected-output-using-pytorchs-built-in-gelu","Get expected output using PyTorch’s built-in GELU",{depth:31,slug:268,text:269},"compare-the-kernel-output-with-pytorchs-result","Compare the kernel output with PyTorch’s result",{depth:31,slug:271,text:272},"optional-print-both-tensors-for-inspection","Optional: print both tensors for inspection",{depth:31,slug:274,text:275},"list-available-functions-in-the-loaded-kernel-module","List available functions in the loaded kernel module",{depth:31,slug:277,text:231},"-script-1",{depth:31,slug:279,text:234},"dependencies---1",{depth:31,slug:281,text:237},"numpy-1",{depth:31,slug:283,text:240},"torch-1",{depth:31,slug:285,text:243},"kernels-1",{depth:31,slug:287,text:245},"-2",{depth:31,slug:289,text:248},"-3",{depth:31,slug:291,text:292},"simple-pytorch-implementation-of-rmsnorm-for-baseline-comparison","Simple PyTorch implementation of RMSNorm for baseline comparison",{depth:31,slug:294,text:295},"assumes-x-is-batch_size--hidden_size","Assumes x is (batch_size, …, hidden_size)",{depth:31,slug:297,text:298},"calculate-variance-in-float32-for-stability","Calculate variance in float32 for stability",{depth:31,slug:300,text:301},"apply-weight-and-convert-back-to-original-dtype","Apply weight and convert back to original dtype",{depth:31,slug:303,text:304},"ensure-all-linear-layers-weights-are-1-for-testing","ensure all linear layers weights are 1 for testing",{depth:31,slug:306,text:307},"example-usage","Example usage",{depth:31,slug:309,text:231},"-script-2",{depth:31,slug:311,text:234},"dependencies---2",{depth:31,slug:313,text:237},"numpy-2",{depth:31,slug:315,text:240},"torch-2",{depth:31,slug:317,text:243},"kernels-2",{depth:31,slug:319,text:245},"-4",{depth:31,slug:321,text:248},"-5",{depth:31,slug:323,text:324},"reuse-the-model-from-the-previous-snippet-or-copy-the-class","reuse the model from the previous snippet or copy the class",{depth:31,slug:326,text:327},"definition-here-to-run-this-script-independently","definition here to run this script independently",{depth:31,slug:329,text:330},"simply-add-the-decorator-to-the-llamarmsnorm-class-to-automatically-replace-the-forward-function","Simply add the decorator to the LlamaRMSNorm class to automatically replace the forward function",{depth:31,slug:332,text:333},"with-the-optimized-kernel-version","with the optimized kernel version",{depth:31,slug:335,text:15},"-6",{depth:31,slug:337,text:338},"note-note-all-kernels-ship-with-layers-already-mapped-and-would-require-calling-the-function-directly","Note: note all kernels ship with layers already mapped, and would require calling the function directly",{depth:31,slug:340,text:341},"howeber-in-this-case-the-llamarmsnorm-class-is-already-mapped-to-the-kernel-function-otherwise-wed-need-to","Howeber in this case, the LlamaRMSNorm class is already mapped to the kernel function. Otherwise we’d need to",{depth:31,slug:343,text:344},"call-the-function-directly-like-this","call the function directly like this:",{depth:31,slug:346,text:347},"python","```python",{depth:31,slug:349,text:350},"layer_norm_kernel_modulerms_norm_fn","layer_norm_kernel_module.rms_norm_fn(",{depth:31,slug:352,text:353},"hidden_states","hidden_states,",{depth:31,slug:355,text:356},"selfweight","self.weight,",{depth:31,slug:358,text:359},"biasnone","bias=None,",{depth:31,slug:361,text:362},"residualnone","residual=None,",{depth:31,slug:364,text:365},"epsselfvariance_epsilon","eps=self.variance_epsilon,",{depth:31,slug:367,text:368},"dropout_p00","dropout_p=0.0,",{depth:31,slug:370,text:371},"prenormfalse","prenorm=False,",{depth:31,slug:373,text:374},"residual_in_fp32false","residual_in_fp32=False,",{depth:31,slug:376,text:377},"-7",")",{depth:31,slug:379,text:380},"-8","```",{depth:31,slug:382,text:295},"assumes-x-is-batch_size--hidden_size-1",{depth:31,slug:384,text:298},"calculate-variance-in-float32-for-stability-1",{depth:31,slug:386,text:301},"apply-weight-and-convert-back-to-original-dtype-1",{depth:31,slug:388,text:389},"originalrmsnorm-will-be-replaced-with-the-optimized-kernel-layer","OriginalRMSNorm will be replaced with the optimized kernel layer",{depth:31,slug:391,text:392},"when-the-model-is-loaded","when the model is loaded",{depth:31,slug:394,text:304},"ensure-all-linear-layers-weights-are-1-for-testing-1",{depth:31,slug:396,text:307},"example-usage-1",{depth:31,slug:398,text:399},"verify-outputs-are-close-rmsnorm-implementations-should-be-numerically-close","Verify outputs are close (RMSNorm implementations should be numerically close)",{depth:31,slug:401,text:231},"-script-3",{depth:31,slug:403,text:234},"dependencies---3",{depth:31,slug:405,text:237},"numpy-3",{depth:31,slug:407,text:240},"torch-3",{depth:31,slug:409,text:243},"kernels-3",{depth:31,slug:411,text:245},"-9",{depth:31,slug:413,text:248},"-10",{depth:31,slug:415,text:416},"reuse-the-models-from-the-previous-snippets-or-copy-the-class","reuse the models from the previous snippets or copy the class",{depth:31,slug:418,text:419},"definitions-here-to-run-this-script-independently","definitions here to run this script independently",{depth:31,slug:421,text:422},"use-torchcudaevent-for-accurate-gpu-timing-ensure-function-is-defined","Use torch.cuda.Event for accurate GPU timing (ensure function is defined)",{depth:31,slug:424,text:425},"warmup-runs","Warmup runs",{depth:31,slug:427,text:428},"timed-runs","Timed runs",{depth:31,slug:430,text:431},"create-larger-models-and-input-for-benchmark","Create larger models and input for benchmark",{depth:31,slug:433,text:434},"ensure-both-models-are-fully-converted-to-the-target-device-and-dtype","Ensure both models are fully converted to the target DEVICE and DTYPE",{depth:31,slug:436,text:437},"call-both-with-larger-batch-sizes-to-warm-up-the-gpu","call both with larger batch sizes to warm up the GPU",{depth:31,slug:439,text:440},"and-ensure-the-models-are-loaded","and ensure the models are loaded",{depth:31,slug:442,text:443},"call-cuda-synchronize-to-ensure-all-previous-gpu-operations-are-complete","Call cuda synchronize to ensure all previous GPU operations are complete",{depth:31,slug:445,text:446},"create-random-input-tensor","Create random input tensor",{depth:31,slug:448,text:449},"ensure-the-input-tensor-is-on-the-correct-device-and-dtype","Ensure the input tensor is on the correct device and dtype",{depth:31,slug:451,text:452},"run-benchmarks-only-if-kernel-was-loaded-successfully","Run benchmarks only if kernel was loaded successfully",[],[],{title:209,summary:15,pubDate:154,media:17,tags:456,link:212,thumbnail:15},[19,20,21],[],"2025-06-20-enhance-your-models-in-5-minutes-with-the-hugging-face-kernel-hub-6d932b.md","2025-06-20-introducing-training-cluster-as-a-service---a-new-collaboration-with-nvidia-c20249",{id:459,data:461,body:466,filePath:467,digest:468,rendered:469,legacyId:479},{title:462,summary:15,pubDate:463,media:17,tags:464,link:465,thumbnail:15},"Introducing Training Cluster as a Service - a new collaboration with NVIDIA",["Date","2025-06-11T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/nvidia-training-cluster","Introducing Training Cluster as a Service - a new collaboration with NVIDIA\r\nToday at GTC Paris, we are excited to announce Training Cluster as a Service in collaboration with NVIDIA, to make large GPU clusters more easily accessible for research organizations all over the world, so they can train the foundational models of tomorrow in every domain.\r\nMaking GPU Clusters Accessible\r\nMany Gigawatt-size GPU supercluster projects are being built to train the next gen of AI models. This can make it seem that the compute gap between the “GPU poor” and the “GPU rich” is quickly widening. But the GPUs are out there, as hyperscalers, regional and AI-native cloud providers all quickly expand their capacity.\r\nHow do we then connect AI compute capacity with the researchers who need it? How do we enable universities, national research labs and companies all over the world to build their own models?\r\nThis is what Hugging Face and NVIDIA are tackling with Training Cluster as a Service - providing GPU cluster accessibility, with the flexibility to only pay for the duration of training runs.\r\nTo get started, any of the 250,000 organizations on Hugging Face can request the GPU cluster size they need, when they need it.\r\nHow it works\r\nTo get started, you can request a GPU cluster on behalf of your organization at hf.co/training-cluster\r\nTraining Cluster as a Service integrates key components from NVIDIA and Hugging Face into a complete solution:\r\n- NVIDIA Cloud Partners provide capacity for the latest NVIDIA accelerating computing like NVIDIA Hopper and NVIDIA GB200 in regional datacenters, all centralized within NVIDIA DGX Cloud\r\n- NVIDIA DGX Cloud Lepton - announced today at GTC Paris - provides easy access to the infrastructure provisioned for researchers, and enables training run scheduling and monitoring\r\n- Hugging Face developer resources and open source libraries make it easy to get training runs started.\r\nOnce your GPU cluster request is accepted, Hugging Face and NVIDIA will collaborate to source, price, provision and set up your GPU cluster per your size, region and duration requirements.\r\nClusters at Work\r\nAdvancing Rare Genetic Disease Research with TIGEM\r\nThe Telethon Institute of Genomics and Medicine - TIGEM for short - is a research center dedicated to understanding the molecular mechanisms behind rare genetic diseases and developing novel treatments. Training new AI models is a new path to predict the effect of pathogenic variants and for drug repositioning.\r\nAI offers new ways to research the causes of rare genetic diseases and to develop treatments, but our domain requires training new models. Training Cluster as a Service made it easy to procure the GPU capacity we needed, at the right time\r\n-- Diego di Bernardo, Coordinator of the Genomic Medicine program at TIGEM\r\nAdvancing AI for Mathematics with Numina\r\nNumina is a non-profit organization building open-source, open-dataset AI for reasoning in math - and won the 2024 AIMO progress prize.\r\nWe are tracking well on our objective to build open alternatives to the best closed-source models, such as Deepmind's AlphaProof. Computing resources is our bottleneck today - with Training Cluster as a Service we will be able to reach our goal!\r\n-- Yann Fleureau, cofounder of Project Numina\r\nAdvancing Material Science with Mirror Physics\r\nMirror Physics is a startup creating frontier AI systems for chemistry and materials science.\r\nTogether with the MACE team, we're working to push the limits of AI for chemistry. With Training Cluster as a Service, we're producing high-fidelity chemical models at unprecedented scale. This is going to be a significant step forward for the field.\r\n-- Sam Walton Norwood, CEO and founder at Mirror\r\nPowering the Diversity of AI Research\r\nTraining Cluster as a Service is a new collaboration between Hugging Face and NVIDIA to make AI compute more readily available to the global community of AI researchers.\r\nAccess to large-scale, high-performance compute is essential for building the next generation of AI models across every domain and language. Training Cluster as a Service will remove barriers for researchers and companies, unlocking the ability to train the most advanced models and push the boundaries of what’s possible in AI.\r\n-- Clément Delangue, cofounder and CEO of Hugging Face\r\nIntegrating DGX Cloud Lepton with Hugging Face’s Training Cluster as a Service gives developers and researchers a seamless way to access high-performance NVIDIA GPUs across a broad network of cloud providers. This collaboration makes it easier for AI researchers and organizations to scale their AI training workloads while using familiar tools on Hugging Face.\r\n-- Alexis Bjorlin, vice president of DGX Cloud at NVIDIA\r\nEnabling AI Builders with NVIDIA\r\nWe are excited to collaborate with NVIDIA to offer Training Cluster as a Service to Hugging Face organizations - you can get started today at hf.co/training-cluster\r\nToday at GTC Paris, NVIDIA announced many new contributions for Hugging Face users, from agents to robots!\r\n- NVIDIA DGX Cloud Lepton connects Europe’s developers to global NVIDIA compute ecosystem\r\n- NVIDIA AI customers can now deploy over 100k Hugging Face models with NIM\r\n- Hugging Face users can build custom Physical AI models with NVIDIA Cosmos Predict-2\r\n- NVIDIA Isaac GR00T N1.5 lands on Hugging Face to power humanoids robots","src/content/posts/2025-06-20-introducing-training-cluster-as-a-service---a-new-collaboration-with-nvidia-c20249.md","00239fc68d4ea012",{html:470,metadata:471},"<p>Introducing Training Cluster as a Service - a new collaboration with NVIDIA\r\nToday at GTC Paris, we are excited to announce Training Cluster as a Service in collaboration with NVIDIA, to make large GPU clusters more easily accessible for research organizations all over the world, so they can train the foundational models of tomorrow in every domain.\r\nMaking GPU Clusters Accessible\r\nMany Gigawatt-size GPU supercluster projects are being built to train the next gen of AI models. This can make it seem that the compute gap between the “GPU poor” and the “GPU rich” is quickly widening. But the GPUs are out there, as hyperscalers, regional and AI-native cloud providers all quickly expand their capacity.\r\nHow do we then connect AI compute capacity with the researchers who need it? How do we enable universities, national research labs and companies all over the world to build their own models?\r\nThis is what Hugging Face and NVIDIA are tackling with Training Cluster as a Service - providing GPU cluster accessibility, with the flexibility to only pay for the duration of training runs.\r\nTo get started, any of the 250,000 organizations on Hugging Face can request the GPU cluster size they need, when they need it.\r\nHow it works\r\nTo get started, you can request a GPU cluster on behalf of your organization at hf.co/training-cluster\r\nTraining Cluster as a Service integrates key components from NVIDIA and Hugging Face into a complete solution:</p>\n<ul>\n<li>NVIDIA Cloud Partners provide capacity for the latest NVIDIA accelerating computing like NVIDIA Hopper and NVIDIA GB200 in regional datacenters, all centralized within NVIDIA DGX Cloud</li>\n<li>NVIDIA DGX Cloud Lepton - announced today at GTC Paris - provides easy access to the infrastructure provisioned for researchers, and enables training run scheduling and monitoring</li>\n<li>Hugging Face developer resources and open source libraries make it easy to get training runs started.\r\nOnce your GPU cluster request is accepted, Hugging Face and NVIDIA will collaborate to source, price, provision and set up your GPU cluster per your size, region and duration requirements.\r\nClusters at Work\r\nAdvancing Rare Genetic Disease Research with TIGEM\r\nThe Telethon Institute of Genomics and Medicine - TIGEM for short - is a research center dedicated to understanding the molecular mechanisms behind rare genetic diseases and developing novel treatments. Training new AI models is a new path to predict the effect of pathogenic variants and for drug repositioning.\r\nAI offers new ways to research the causes of rare genetic diseases and to develop treatments, but our domain requires training new models. Training Cluster as a Service made it easy to procure the GPU capacity we needed, at the right time\r\n— Diego di Bernardo, Coordinator of the Genomic Medicine program at TIGEM\r\nAdvancing AI for Mathematics with Numina\r\nNumina is a non-profit organization building open-source, open-dataset AI for reasoning in math - and won the 2024 AIMO progress prize.\r\nWe are tracking well on our objective to build open alternatives to the best closed-source models, such as Deepmind’s AlphaProof. Computing resources is our bottleneck today - with Training Cluster as a Service we will be able to reach our goal!\r\n— Yann Fleureau, cofounder of Project Numina\r\nAdvancing Material Science with Mirror Physics\r\nMirror Physics is a startup creating frontier AI systems for chemistry and materials science.\r\nTogether with the MACE team, we’re working to push the limits of AI for chemistry. With Training Cluster as a Service, we’re producing high-fidelity chemical models at unprecedented scale. This is going to be a significant step forward for the field.\r\n— Sam Walton Norwood, CEO and founder at Mirror\r\nPowering the Diversity of AI Research\r\nTraining Cluster as a Service is a new collaboration between Hugging Face and NVIDIA to make AI compute more readily available to the global community of AI researchers.\r\nAccess to large-scale, high-performance compute is essential for building the next generation of AI models across every domain and language. Training Cluster as a Service will remove barriers for researchers and companies, unlocking the ability to train the most advanced models and push the boundaries of what’s possible in AI.\r\n— Clément Delangue, cofounder and CEO of Hugging Face\r\nIntegrating DGX Cloud Lepton with Hugging Face’s Training Cluster as a Service gives developers and researchers a seamless way to access high-performance NVIDIA GPUs across a broad network of cloud providers. This collaboration makes it easier for AI researchers and organizations to scale their AI training workloads while using familiar tools on Hugging Face.\r\n— Alexis Bjorlin, vice president of DGX Cloud at NVIDIA\r\nEnabling AI Builders with NVIDIA\r\nWe are excited to collaborate with NVIDIA to offer Training Cluster as a Service to Hugging Face organizations - you can get started today at hf.co/training-cluster\r\nToday at GTC Paris, NVIDIA announced many new contributions for Hugging Face users, from agents to robots!</li>\n<li>NVIDIA DGX Cloud Lepton connects Europe’s developers to global NVIDIA compute ecosystem</li>\n<li>NVIDIA AI customers can now deploy over 100k Hugging Face models with NIM</li>\n<li>Hugging Face users can build custom Physical AI models with NVIDIA Cosmos Predict-2</li>\n<li>NVIDIA Isaac GR00T N1.5 lands on Hugging Face to power humanoids robots</li>\n</ul>",{headings:472,localImagePaths:473,remoteImagePaths:474,frontmatter:475,imagePaths:478},[],[],[],{title:462,summary:15,pubDate:476,media:17,tags:477,link:465,thumbnail:15},"Wed, 11 Jun 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-introducing-training-cluster-as-a-service---a-new-collaboration-with-nvidia-c20249.md","2025-06-20-improving-hugging-face-model-access-for-kaggle-users-6bab7f",{id:480,data:482,body:487,filePath:488,digest:489,rendered:490,legacyId:500},{title:483,summary:15,pubDate:484,media:17,tags:485,link:486,thumbnail:15},"Improving Hugging Face Model Access for Kaggle Users",["Date","2025-05-14T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/kaggle-integration","Improving Hugging Face Model Access for Kaggle Users\r\nKaggle and Hugging Face users are part of one AI community. That’s why we’re excited to announce our plans to bring our platforms and communities closer to better serve AI developers everywhere.\r\nBeginning today, Kaggle is launching an integration that enhances visibility and discoverability for Hugging Face models directly on Kaggle.\r\nHow to get started\r\nYou can navigate from Hugging Face models to Kaggle and vice versa. Start by visiting a Hugging Face model page like Qwen/Qwen3-1.7B. To use it in a Kaggle Notebook, you can click on “Use this model” and select “Kaggle” to open up a Kaggle notebook with a pre-populated code snippet to load the model. You can do the same from a Hugging Face model page on Kaggle by clicking the “Code” button.\r\nWhen you run a notebook on Kaggle that references a model hosted on Hugging Face Hub, we will automatically generate a Hugging Face model page if one doesn’t exist already. You don’t need to make any special changes to your code. Additionally, when you make your notebook public, it will automatically show on the “Code” tab of the Kaggle model page.\r\nDiscover Hugging Face models and explore all the community examples in public notebooks in one place on Kaggle at https://www.kaggle.com/models. As more Hugging Face models are used on Kaggle, the number of models and associated code examples you can explore for inspiration will grow.\r\nWhen browsing Hugging Face models on Kaggle, we want to make it easy for you to navigate back to Hugging Face to explore additional details, metadata, community usage in Hugging Face Spaces, discussion, and more. Simply click “Open in Hugging Face” on the Kaggle model page.\r\nHow does this work with private and consent-gated Hugging Face models?\r\nIf you use a private Hugging Face model in your Kaggle notebook, authenticate via your Hugging Face account as normal (add your HF_TOKEN in the “Add-ons > Secrets” menu in the notebook editor). A Hugging Face model page won’t be generated on Kaggle.\r\nIf you want to access a consent-gated model in your Kaggle notebook, you’ll need to request access to it using a Hugging Face account and follow the prompts on your browser's Hugging Face model page as normal. Hugging Face has documentation to guide you through this process. Otherwise, the integration will work the same as for non-gated models.\r\nWhat’s next\r\nWe’re actively working on a solution to seamlessly use Hugging Face models in Kaggle competitions that require offline notebook submissions. While this will take a few more months to complete, we believe the wait will be worth it.\r\nYou can read Kaggle’s position on “AI Competitions as the gold standard for empirical rigor for GenAI evaluation” to understand why it’s so important for us to get this part of the integration right! But tl;dr – Kaggle is highly sensitive to data leakage and its impact on model contamination. Our goal is to design this integration to preserve the integrity of our competitions and their vital role in the industry, while enabling seamless access for Kaggle competitors to build with the best models from Hugging Face!\r\nWe’d love to hear your feedback in the meantime - share your thoughts and ideas here!\r\nHappy Kaggling!","src/content/posts/2025-06-20-improving-hugging-face-model-access-for-kaggle-users-6bab7f.md","e6cb602a37a7a8a5",{html:491,metadata:492},"<p>Improving Hugging Face Model Access for Kaggle Users\r\nKaggle and Hugging Face users are part of one AI community. That’s why we’re excited to announce our plans to bring our platforms and communities closer to better serve AI developers everywhere.\r\nBeginning today, Kaggle is launching an integration that enhances visibility and discoverability for Hugging Face models directly on Kaggle.\r\nHow to get started\r\nYou can navigate from Hugging Face models to Kaggle and vice versa. Start by visiting a Hugging Face model page like Qwen/Qwen3-1.7B. To use it in a Kaggle Notebook, you can click on “Use this model” and select “Kaggle” to open up a Kaggle notebook with a pre-populated code snippet to load the model. You can do the same from a Hugging Face model page on Kaggle by clicking the “Code” button.\r\nWhen you run a notebook on Kaggle that references a model hosted on Hugging Face Hub, we will automatically generate a Hugging Face model page if one doesn’t exist already. You don’t need to make any special changes to your code. Additionally, when you make your notebook public, it will automatically show on the “Code” tab of the Kaggle model page.\r\nDiscover Hugging Face models and explore all the community examples in public notebooks in one place on Kaggle at <a href=\"https://www.kaggle.com/models\">https://www.kaggle.com/models</a>. As more Hugging Face models are used on Kaggle, the number of models and associated code examples you can explore for inspiration will grow.\r\nWhen browsing Hugging Face models on Kaggle, we want to make it easy for you to navigate back to Hugging Face to explore additional details, metadata, community usage in Hugging Face Spaces, discussion, and more. Simply click “Open in Hugging Face” on the Kaggle model page.\r\nHow does this work with private and consent-gated Hugging Face models?\r\nIf you use a private Hugging Face model in your Kaggle notebook, authenticate via your Hugging Face account as normal (add your HF_TOKEN in the “Add-ons > Secrets” menu in the notebook editor). A Hugging Face model page won’t be generated on Kaggle.\r\nIf you want to access a consent-gated model in your Kaggle notebook, you’ll need to request access to it using a Hugging Face account and follow the prompts on your browser’s Hugging Face model page as normal. Hugging Face has documentation to guide you through this process. Otherwise, the integration will work the same as for non-gated models.\r\nWhat’s next\r\nWe’re actively working on a solution to seamlessly use Hugging Face models in Kaggle competitions that require offline notebook submissions. While this will take a few more months to complete, we believe the wait will be worth it.\r\nYou can read Kaggle’s position on “AI Competitions as the gold standard for empirical rigor for GenAI evaluation” to understand why it’s so important for us to get this part of the integration right! But tl;dr – Kaggle is highly sensitive to data leakage and its impact on model contamination. Our goal is to design this integration to preserve the integrity of our competitions and their vital role in the industry, while enabling seamless access for Kaggle competitors to build with the best models from Hugging Face!\r\nWe’d love to hear your feedback in the meantime - share your thoughts and ideas here!\r\nHappy Kaggling!</p>",{headings:493,localImagePaths:494,remoteImagePaths:495,frontmatter:496,imagePaths:499},[],[],[],{title:483,summary:15,pubDate:497,media:17,tags:498,link:486,thumbnail:15},"Wed, 14 May 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-improving-hugging-face-model-access-for-kaggle-users-6bab7f.md","2025-06-20-kv-cache-from-scratch-in-nanovlm-38726c",{id:501,data:503,body:508,filePath:509,digest:510,rendered:511,legacyId:545},{title:504,summary:15,pubDate:505,media:17,tags:506,link:507,thumbnail:15},"KV Cache from scratch in nanoVLM",["Date","2025-06-04T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/kv-cache","KV Cache from scratch in nanoVLM\r\nTL;DR\r\nWe have implemented KV Caching from scratch in our nanoVLM repository (a small codebase to train your own Vision Language Model with pure PyTorch). This gave us a 38% speedup in generation. In this blog post we cover KV Caching and all our experiences while implementing it. The lessons learnt are general and can be applied to all autoregressive language model generations. Implementing from scratch on a small codebase is a great learning experience, come along for the ride!\r\nIntroduction\r\nAutoregressive language models generate text by sampling one token at a time. During inference, the model processes a given input sequence, predicts the next token, appends it to the sequence, and repeats this process until some stopping criterion:\r\nThis step-by-step generation is inherently sequential:\r\n- To generate token , the model must consider the entire sequence from to . From the first instance in the above example would be\r\nthe\r\n, while all the previous tokens to would be[What, is, in]\r\n. - Although transformers are internally parallel, each new prediction requires a full forward pass through all transformer layers, which incurs a quadratic memory/compute in terms of the sequence length.\r\nThis repetition also leads to computational redundancy. In this post, we explore KV Caching, an optimisation technique that mitigates this inefficiency.\r\nTable of contents:\r\n- Revisiting the Transformer Architecture\r\n- Where Redundancy Creeps In\r\n- How KV Caching Fixes It\r\n- KV Caching in nanoVLM: From Theory to Practice\r\n- Summary: Why KV Caching Matters\r\nRevisiting the Transformer Architecture\r\nBefore diving into caching, let’s revisit how attention operates in transformer models. A Transformer language model consists of stacked layers, each composed of:\r\n- Multi-head self-attention\r\n- Feed-forward network (MLP)\r\n- Residual connections and layer normalisation\r\nTo understand where KV Caching helps, we focus on the self-attention mechanism, specifically within a single attention head.\r\nLet’s walk through a simple PyTorch implementation to visualise the key computations.\r\nimport torch\r\ninput_seq_length = 5\r\ndim_model = 10\r\ninput_ids_emb = torch.randn(input_seq_length, dim_model)\r\nW_q = torch.randn(dim_model, dim_model)\r\nW_k = torch.randn(dim_model, dim_model)\r\nW_v = torch.randn(dim_model, dim_model)\r\nQ = input_ids_emb @ W_q\r\nK = input_ids_emb @ W_k\r\nV = input_ids_emb @ W_v\r\nSelf-Attention Computation\r\nFor a sequence of input embeddings represented as , self-attention is computed as:\r\n- , with\r\n- , with\r\n- , with\r\n- Causal mask to prevent future token access\r\nThe final output is:\r\nHere’s a minimal PyTorch equivalent using a causal mask:\r\nimport torch.nn.functional as F\r\nimport math\r\nd_k = K.shape[-1]\r\nattention_scores = (Q @ K.T) / math.sqrt(d_k)\r\n# Lower triangular mask to prevent future token access\r\ncausal_mask = torch.tril(torch.ones(input_seq_length, input_seq_length))\r\nmasked_scores = attention_scores.masked_fill(causal_mask == 0, float('-inf'))\r\nattention_weights = F.softmax(masked_scores, dim=-1)\r\noutput = attention_weights @ V\r\nWhere Redundancy Creeps In\r\nIn autoregressive generation, the model generates one token at a time. With each step, it recomputes , , and for the entire sequence, even though the earlier tokens haven’t changed.\r\nnew_token_emb = torch.randn(1, dim_model)\r\nextended_input = torch.cat([input_ids_emb, new_token_emb], dim=0)\r\nQ_ext = extended_input @ W_q\r\nK_ext = extended_input @ W_k\r\nV_ext = extended_input @ W_v\r\n# (output_ext would be computed using Q_ext, K_ext, V_ext + masking)\r\nTo confirm the redundancy:\r\ntorch.testing.assert_close(K, K_ext[:input_seq_length]) # test pass\r\ntorch.testing.assert_close(V, V_ext[:input_seq_length]) # test pass\r\nThese checks show that for all but the newest token, and are identical to previously computed values.\r\nOriginal (5×5): Extended (6×6):\r\n■ ■ ■ ■ ■ ■ ■ ■ ■ ■ □\r\n■ ■ ■ ■ ■ ■ ■ ■ ■ ■ □\r\n■ ■ ■ ■ ■ → ■ ■ ■ ■ ■ □\r\n■ ■ ■ ■ ■ ■ ■ ■ ■ ■ □\r\n■ ■ ■ ■ ■ ■ ■ ■ ■ ■ □\r\n□ □ □ □ □ □\r\n- ■ = Already computed and reused\r\n- □ = Recomputed unnecessarily\r\nMost of the attention computation is repeated needlessly. This gets more expensive as sequences grow.\r\nHow KV Caching Fixes It\r\nTo eliminate this inefficiency, we use KV Caching:\r\n- After processing the initial prompt, we cache the computed keys ( ) and values ( ) for each layer.\r\n- During generation, we only compute and for the new token, and append them to the cache.\r\n- We compute for the current token and use it with the cached and to get the output.\r\nThis changes generation from full-sequence re-computation to a lightweight, incremental update.\r\n✅ In practice, this cache is a per-layer dictionary with keys \"key\" and \"value\", each of shape (\r\nbatch_size\r\n,num_heads\r\n,seq_len_cached\r\n,head_dim\r\n).\r\nThis is the foundation of how modern LLMs can generate long outputs efficiently.\r\nKV Caching in nanoVLM: From Theory to Practice\r\nNow that we understand the theory behind KV Caching, let’s see how it’s implemented in practice inside our nanoVLM repository. This is an ideal testbed, as it's a super concise and self-contained codebase.\r\nKV caching is enabled across three key components in our model:\r\n- The Attention block that uses and updates the KV cache\r\n- The Language model that tracks cache per layer\r\n- The Generation loop that separates prefill (the initial pass with the input prompt) and sequential decode phases\r\n1. Updating KV Cache in the Attention Block\r\nIn the LanguageModelGroupedAttention\r\nclass, we modify the forward\r\nfunction to accept and update a cache of keys and values (block_kv_cache\r\n).\r\nPreviously, the model recomputed and at every generation step. Now we only compute , for the current token, and append them to the cached values.\r\ndef forward(self, x, cos, sin, attention_mask=None, block_kv_cache=None):\r\nis_prefill = block_kv_cache is None\r\nB, T_curr, C = x.size()\r\n# Project inputs to Q, K, V\r\nq_curr, k_curr, v_curr = project_current_tokens(x)\r\nq, k_rotated = apply_rotary_pos_embd(q_curr, k_curr, cos, sin)\r\nif not is_prefill and block_kv_cache['key'] is not None:\r\n# Append new keys and values to the cache\r\nk = torch.cat([block_kv_cache['key'], k_rotated], dim=2)\r\nv = torch.cat([block_kv_cache['value'], v_curr], dim=2)\r\nelse:\r\n# First pass (prefill) — no cache\r\nk, v = k_rotated, v_curr\r\nblock_kv_cache = {'key': k, 'value': v}\r\nreturn attention_output, block_kv_cache\r\n2. Tracking Cache Across Layers\r\nIn the LanguageModel\r\nclass, we introduce layer-wise cache tracking. The start_pos\r\nargument helps the model compute correct rotary positional encodings for newly generated tokens.\r\ndef forward(self, x, kv_cache=None, start_pos=0):\r\nT_curr = x.size(1)\r\nposition_ids = torch.arange(start_pos, start_pos + T_curr, device=x.device)\r\ncos, sin = self.rotary_embd(position_ids)\r\nfor i, block in enumerate(self.blocks):\r\n# Pass per-layer KV cache\r\nx, kv_cache[i] = block(x, cos, sin, attention_mask, kv_cache[i])\r\nreturn x, kv_cache\r\nkv_cache\r\n: A list of dictionaries, one per transformer layer, holding previous keys and values.start_pos\r\n: Ensures that rotary embeddings are aligned with current generation index.\r\n3. Prefill vs Decode in the Generation Loop\r\nThe biggest architectural change is in the generate()\r\nmethod of the VisionLanguageModel\r\n.\r\nWe split generation into two stages:\r\n- PREFILL PHASE: Encode the full prompt and build the initial cache.\r\n- DECODE PHASE: Generate tokens one at a time using cached keys/values.\r\nPREFILL PHASE (cache construction)\r\n[Prompt: \"What is\"] → [Transformer] → [Cache: K, V for all layers]\r\nDECODE PHASE (token-by-token)\r\n[Token: \"the\"] → [Q(\"the\") + cached K/V] → [next token: \"?\"] → ...\r\nHere’s the corresponding code:\r\n# PREFILL: Process the input prompt, fill the cache\r\nprompt_output, kv_cache_list = self.forward(\r\ninputs,\r\nkv_cache=None,\r\nstart_pos=0\r\n)\r\n# DECODE: Generate one token at a time using cached K/V\r\nfor i in range(max_new_tokens):\r\nnext_token = sample_from(prompt_output)\r\ndecode_output, kv_cache_list = self.forward(\r\nnext_token,\r\nkv_cache=kv_cache_list,\r\nstart_pos=current_position # updated with each step\r\n)\r\nprompt_output = decode_output\r\nBy separating these phases, we avoid redundant computation and dramatically speed up inference, especially for long prompts.\r\nSummary of Changes\r\n| Module | Original Behaviour | New Behaviour |\r\n|---|---|---|\r\nLanguageModelGroupedAttention.forward |\r\nRecomputes , , on every step | Uses and updates KV cache |\r\nLanguageModel.forward |\r\nNo memory of previous state | Tracks per-layer KV cache, handles start_pos |\r\nVisionLanguageModel.generate |\r\nOne-phase generation loop | Split into prefill and decode phases |\r\nSummary: Why KV Caching Matters\r\n| Benefit | Explanation |\r\n|---|---|\r\n| Incremental growth | Cache grows by one row per new token |\r\n| Position-aware decoding | start_pos ensures correctness of position encoding calculations |\r\n| Efficiency | Reduces per-token inference to O(seq len ) instead of quadratic |\r\nKV caching eliminates unnecessary computation during autoregressive generation, enabling faster and more efficient inference, especially in long sequences and real-time applications. This is a trade-off between speed and memory, and its drawbacks can be more complex code and restricting fancier inference schemes, like beam-search, etc. KV caching is a popular method for speeding up LLM inference, making it possible to run them on consumer hardware, and now you know how it works too!","src/content/posts/2025-06-20-kv-cache-from-scratch-in-nanovlm-38726c.md","60851445ac47d5de",{html:512,metadata:513},"<p>KV Cache from scratch in nanoVLM\r\nTL;DR\r\nWe have implemented KV Caching from scratch in our nanoVLM repository (a small codebase to train your own Vision Language Model with pure PyTorch). This gave us a 38% speedup in generation. In this blog post we cover KV Caching and all our experiences while implementing it. The lessons learnt are general and can be applied to all autoregressive language model generations. Implementing from scratch on a small codebase is a great learning experience, come along for the ride!\r\nIntroduction\r\nAutoregressive language models generate text by sampling one token at a time. During inference, the model processes a given input sequence, predicts the next token, appends it to the sequence, and repeats this process until some stopping criterion:\r\nThis step-by-step generation is inherently sequential:</p>\n<ul>\n<li>To generate token , the model must consider the entire sequence from to . From the first instance in the above example would be\r\nthe\r\n, while all the previous tokens to would be[What, is, in]\r\n. - Although transformers are internally parallel, each new prediction requires a full forward pass through all transformer layers, which incurs a quadratic memory/compute in terms of the sequence length.\r\nThis repetition also leads to computational redundancy. In this post, we explore KV Caching, an optimisation technique that mitigates this inefficiency.\r\nTable of contents:</li>\n<li>Revisiting the Transformer Architecture</li>\n<li>Where Redundancy Creeps In</li>\n<li>How KV Caching Fixes It</li>\n<li>KV Caching in nanoVLM: From Theory to Practice</li>\n<li>Summary: Why KV Caching Matters\r\nRevisiting the Transformer Architecture\r\nBefore diving into caching, let’s revisit how attention operates in transformer models. A Transformer language model consists of stacked layers, each composed of:</li>\n<li>Multi-head self-attention</li>\n<li>Feed-forward network (MLP)</li>\n<li>Residual connections and layer normalisation\r\nTo understand where KV Caching helps, we focus on the self-attention mechanism, specifically within a single attention head.\r\nLet’s walk through a simple PyTorch implementation to visualise the key computations.\r\nimport torch\r\ninput_seq_length = 5\r\ndim_model = 10\r\ninput_ids_emb = torch.randn(input_seq_length, dim_model)\r\nW_q = torch.randn(dim_model, dim_model)\r\nW_k = torch.randn(dim_model, dim_model)\r\nW_v = torch.randn(dim_model, dim_model)\r\nQ = input_ids_emb @ W_q\r\nK = input_ids_emb @ W_k\r\nV = input_ids_emb @ W_v\r\nSelf-Attention Computation\r\nFor a sequence of input embeddings represented as , self-attention is computed as:</li>\n<li>, with</li>\n<li>, with</li>\n<li>, with</li>\n<li>Causal mask to prevent future token access\r\nThe final output is:\r\nHere’s a minimal PyTorch equivalent using a causal mask:\r\nimport torch.nn.functional as F\r\nimport math\r\nd_k = K.shape[-1]\r\nattention_scores = (Q @ K.T) / math.sqrt(d_k)</li>\n</ul>\n<h1 id=\"lower-triangular-mask-to-prevent-future-token-access\">Lower triangular mask to prevent future token access</h1>\n<p>causal_mask = torch.tril(torch.ones(input_seq_length, input_seq_length))\r\nmasked_scores = attention_scores.masked_fill(causal_mask == 0, float(‘-inf’))\r\nattention_weights = F.softmax(masked_scores, dim=-1)\r\noutput = attention_weights @ V\r\nWhere Redundancy Creeps In\r\nIn autoregressive generation, the model generates one token at a time. With each step, it recomputes , , and for the entire sequence, even though the earlier tokens haven’t changed.\r\nnew_token_emb = torch.randn(1, dim_model)\r\nextended_input = torch.cat([input_ids_emb, new_token_emb], dim=0)\r\nQ_ext = extended_input @ W_q\r\nK_ext = extended_input @ W_k\r\nV_ext = extended_input @ W_v</p>\n<h1 id=\"output_ext-would-be-computed-using-q_ext-k_ext-v_ext--masking\">(output_ext would be computed using Q_ext, K_ext, V_ext + masking)</h1>\n<p>To confirm the redundancy:\r\ntorch.testing.assert_close(K, K_ext[:input_seq_length]) # test pass\r\ntorch.testing.assert_close(V, V_ext[:input_seq_length]) # test pass\r\nThese checks show that for all but the newest token, and are identical to previously computed values.\r\nOriginal (5×5): Extended (6×6):\r\n■ ■ ■ ■ ■ ■ ■ ■ ■ ■ □\r\n■ ■ ■ ■ ■ ■ ■ ■ ■ ■ □\r\n■ ■ ■ ■ ■ → ■ ■ ■ ■ ■ □\r\n■ ■ ■ ■ ■ ■ ■ ■ ■ ■ □\r\n■ ■ ■ ■ ■ ■ ■ ■ ■ ■ □\r\n□ □ □ □ □ □</p>\n<ul>\n<li>■ = Already computed and reused</li>\n<li>□ = Recomputed unnecessarily\r\nMost of the attention computation is repeated needlessly. This gets more expensive as sequences grow.\r\nHow KV Caching Fixes It\r\nTo eliminate this inefficiency, we use KV Caching:</li>\n<li>After processing the initial prompt, we cache the computed keys ( ) and values ( ) for each layer.</li>\n<li>During generation, we only compute and for the new token, and append them to the cache.</li>\n<li>We compute for the current token and use it with the cached and to get the output.\r\nThis changes generation from full-sequence re-computation to a lightweight, incremental update.\r\n✅ In practice, this cache is a per-layer dictionary with keys “key” and “value”, each of shape (\r\nbatch_size\r\n,num_heads\r\n,seq_len_cached\r\n,head_dim\r\n).\r\nThis is the foundation of how modern LLMs can generate long outputs efficiently.\r\nKV Caching in nanoVLM: From Theory to Practice\r\nNow that we understand the theory behind KV Caching, let’s see how it’s implemented in practice inside our nanoVLM repository. This is an ideal testbed, as it’s a super concise and self-contained codebase.\r\nKV caching is enabled across three key components in our model:</li>\n<li>The Attention block that uses and updates the KV cache</li>\n<li>The Language model that tracks cache per layer</li>\n<li>The Generation loop that separates prefill (the initial pass with the input prompt) and sequential decode phases</li>\n</ul>\n<ol>\n<li>Updating KV Cache in the Attention Block\r\nIn the LanguageModelGroupedAttention\r\nclass, we modify the forward\r\nfunction to accept and update a cache of keys and values (block_kv_cache\r\n).\r\nPreviously, the model recomputed and at every generation step. Now we only compute , for the current token, and append them to the cached values.\r\ndef forward(self, x, cos, sin, attention_mask=None, block_kv_cache=None):\r\nis_prefill = block_kv_cache is None\r\nB, T_curr, C = x.size()</li>\n</ol>\n<h1 id=\"project-inputs-to-q-k-v\">Project inputs to Q, K, V</h1>\n<p>q_curr, k_curr, v_curr = project_current_tokens(x)\r\nq, k_rotated = apply_rotary_pos_embd(q_curr, k_curr, cos, sin)\r\nif not is_prefill and block_kv_cache[‘key’] is not None:</p>\n<h1 id=\"append-new-keys-and-values-to-the-cache\">Append new keys and values to the cache</h1>\n<p>k = torch.cat([block_kv_cache[‘key’], k_rotated], dim=2)\r\nv = torch.cat([block_kv_cache[‘value’], v_curr], dim=2)\r\nelse:</p>\n<h1 id=\"first-pass-prefill--no-cache\">First pass (prefill) — no cache</h1>\n<p>k, v = k_rotated, v_curr\r\nblock_kv_cache = {‘key’: k, ‘value’: v}\r\nreturn attention_output, block_kv_cache\r\n2. Tracking Cache Across Layers\r\nIn the LanguageModel\r\nclass, we introduce layer-wise cache tracking. The start_pos\r\nargument helps the model compute correct rotary positional encodings for newly generated tokens.\r\ndef forward(self, x, kv_cache=None, start_pos=0):\r\nT_curr = x.size(1)\r\nposition_ids = torch.arange(start_pos, start_pos + T_curr, device=x.device)\r\ncos, sin = self.rotary_embd(position_ids)\r\nfor i, block in enumerate(self.blocks):</p>\n<h1 id=\"pass-per-layer-kv-cache\">Pass per-layer KV cache</h1>\n<p>x, kv_cache[i] = block(x, cos, sin, attention_mask, kv_cache[i])\r\nreturn x, kv_cache\r\nkv_cache\r\n: A list of dictionaries, one per transformer layer, holding previous keys and values.start_pos\r\n: Ensures that rotary embeddings are aligned with current generation index.\r\n3. Prefill vs Decode in the Generation Loop\r\nThe biggest architectural change is in the generate()\r\nmethod of the VisionLanguageModel\r\n.\r\nWe split generation into two stages:</p>\n<ul>\n<li>PREFILL PHASE: Encode the full prompt and build the initial cache.</li>\n<li>DECODE PHASE: Generate tokens one at a time using cached keys/values.\r\nPREFILL PHASE (cache construction)\r\n[Prompt: “What is”] → [Transformer] → [Cache: K, V for all layers]\r\nDECODE PHASE (token-by-token)\r\n[Token: “the”] → [Q(“the”) + cached K/V] → [next token: ”?”] → …\r\nHere’s the corresponding code:</li>\n</ul>\n<h1 id=\"prefill-process-the-input-prompt-fill-the-cache\">PREFILL: Process the input prompt, fill the cache</h1>\n<p>prompt_output, kv_cache_list = self.forward(\r\ninputs,\r\nkv_cache=None,\r\nstart_pos=0\r\n)</p>\n<h1 id=\"decode-generate-one-token-at-a-time-using-cached-kv\">DECODE: Generate one token at a time using cached K/V</h1>\n<p>for i in range(max_new_tokens):\r\nnext_token = sample_from(prompt_output)\r\ndecode_output, kv_cache_list = self.forward(\r\nnext_token,\r\nkv_cache=kv_cache_list,\r\nstart_pos=current_position # updated with each step\r\n)\r\nprompt_output = decode_output\r\nBy separating these phases, we avoid redundant computation and dramatically speed up inference, especially for long prompts.\r\nSummary of Changes</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Module</th><th>Original Behaviour</th><th>New Behaviour</th></tr></thead><tbody><tr><td>LanguageModelGroupedAttention.forward</td><td></td><td></td></tr><tr><td>Recomputes , , on every step</td><td>Uses and updates KV cache</td><td></td></tr><tr><td>LanguageModel.forward</td><td></td><td></td></tr><tr><td>No memory of previous state</td><td>Tracks per-layer KV cache, handles start_pos</td><td></td></tr><tr><td>VisionLanguageModel.generate</td><td></td><td></td></tr><tr><td>One-phase generation loop</td><td>Split into prefill and decode phases</td><td></td></tr><tr><td>Summary: Why KV Caching Matters</td><td></td><td></td></tr><tr><td>Benefit</td><td>Explanation</td><td></td></tr><tr><td>---</td><td>---</td><td></td></tr><tr><td>Incremental growth</td><td>Cache grows by one row per new token</td><td></td></tr><tr><td>Position-aware decoding</td><td>start_pos ensures correctness of position encoding calculations</td><td></td></tr><tr><td>Efficiency</td><td>Reduces per-token inference to O(seq len ) instead of quadratic</td><td></td></tr><tr><td>KV caching eliminates unnecessary computation during autoregressive generation, enabling faster and more efficient inference, especially in long sequences and real-time applications. This is a trade-off between speed and memory, and its drawbacks can be more complex code and restricting fancier inference schemes, like beam-search, etc. KV caching is a popular method for speeding up LLM inference, making it possible to run them on consumer hardware, and now you know how it works too!</td><td></td><td></td></tr></tbody></table>",{headings:514,localImagePaths:539,remoteImagePaths:540,frontmatter:541,imagePaths:544},[515,518,521,524,527,530,533,536],{depth:31,slug:516,text:517},"lower-triangular-mask-to-prevent-future-token-access","Lower triangular mask to prevent future token access",{depth:31,slug:519,text:520},"output_ext-would-be-computed-using-q_ext-k_ext-v_ext--masking","(output_ext would be computed using Q_ext, K_ext, V_ext + masking)",{depth:31,slug:522,text:523},"project-inputs-to-q-k-v","Project inputs to Q, K, V",{depth:31,slug:525,text:526},"append-new-keys-and-values-to-the-cache","Append new keys and values to the cache",{depth:31,slug:528,text:529},"first-pass-prefill--no-cache","First pass (prefill) — no cache",{depth:31,slug:531,text:532},"pass-per-layer-kv-cache","Pass per-layer KV cache",{depth:31,slug:534,text:535},"prefill-process-the-input-prompt-fill-the-cache","PREFILL: Process the input prompt, fill the cache",{depth:31,slug:537,text:538},"decode-generate-one-token-at-a-time-using-cached-kv","DECODE: Generate one token at a time using cached K/V",[],[],{title:504,summary:15,pubDate:542,media:17,tags:543,link:507,thumbnail:15},"Wed, 04 Jun 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-kv-cache-from-scratch-in-nanovlm-38726c.md","2025-06-20-microsoft-and-hugging-face-expand-collaboration-576333",{id:546,data:548,body:553,filePath:554,digest:555,rendered:556,legacyId:566},{title:549,summary:15,pubDate:550,media:17,tags:551,link:552,thumbnail:15},"Microsoft and Hugging Face expand collaboration",["Date","2025-05-19T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/azure-ai-foundry","Microsoft and Hugging Face expand collaboration to make open models easy to use on Azure\r\nToday at the Microsoft Build conference, Satya Nadella announced an expanded collaboration with Hugging Face, to make its wide diversity of open models easy to deploy on Azure secure infrastructure.\r\nIf you head over to Azure AI Foundry today, you will find a vastly expanded collection of 10,000+ Hugging Face models you can deploy in a couple clicks to power AI applications working with text, audio and images. And we’re just getting started!\r\nIt’s time to build - an expanded collaboration\r\n2 years ago, Microsoft and Hugging Face started a collaboration to make open models more easily accessible on Azure - back then the Hub was home to 200,000 open models.\r\nWith now close to 2 million open models on Hugging Face, covering a wide diversity of tasks, modalities, domains and languages, it was time to take our collaboration to the next level. The new collaboration announced today creates a framework for mutual success to vastly expand how Azure customers can benefit from Hugging Face.\r\n\"Open source is how AI moves faster—with transparency, choice, and community at its core. This collaboration represents our commitment to that momentum. By combining Hugging Face’s vibrant model ecosystem with Azure’s secure, enterprise-grade infrastructure, we’re giving developers the freedom to pick the best model for the job—and helping organizations innovate safely and at scale.”\r\n-- Asha Sharma, Corporate Vice President at Microsoft\r\nMaking more open models easily accessible to Azure customers, for secure deployment alongside company private data, will enable enterprises to build AI applications and agents while being fully in control of their technology and data.\r\nWe’re enabling companies to take control of their AI destiny, deploying the best open models securely within their Azure account, to build AI applications they can trust and verify.\r\n-- Clement Delangue, CEO and cofounder at Hugging Face\r\nHow to use Hugging Face in Azure AI Foundry\r\nLet’s head over to Azure AI Foundry, and select the Model Catalog. Here you can now find over 10,000 models under the Hugging Face Collection.\r\nModels in the Collection include the most popular, trending models on Hugging Face for a wide range of tasks to work with text, audio and images - including text generation, feature extraction, fill-mask, translation, identifying sentence similarity, image classification, image segmentation, text to image generation, image to text conversion, automatic speech recognition and audio-classification.\r\nTo make the Hugging Face Collection on Azure AI Foundry enterprise-ready, we are only featuring models:\r\n- having successfully passed Hugging Face security tests to screen for any vulnerability, including with ProtectAI Guardian and JFrog security scanner\r\n- with model weights stored in safetensors format, avoiding potential Pickle vulnerabilities\r\n- without remote code, to avoid any arbitrary code insertion at runtime.\r\nIn addition, Microsoft and Hugging Face will continuously test inference containers for vulnerabilities to maintain and patch them as needed.\r\nNow let’s say you want to deploy for instance the popular Microsoft Phi-4 Reasoning Plus open model.\r\nFirst, let’s select the model in the Hugging Face Collection in Azure AI Foundry, and click the “Deploy button”. The form allows you to select a VM, instance count and deployment parameters, and start the deployment process with just another click!\r\nNow, if you prefer browsing models on the Hub, you can also start from the model page - the “Deploy on Azure ML” option will take you to the same deployment option within Azure AI Machine Learning Studio.\r\nMore Hugging Face to come in Azure AI Foundry\r\nWe are really excited about all the new Hugging Face models and modalities now directly available within Azure AI Foundry, but we’re not going to stop there!\r\nIn the weeks and months to come, you can expect a rolling thunder of updates:\r\n- Day-0 releases - Hugging Face will collaborate with Microsoft to make new models from top model providers available in Azure AI Foundry the same day they land on Hugging Face\r\n- Trending models updates - Hugging Face will continuously monitor trending models to enable them on Azure AI Foundry on a daily basis\r\n- New modalities - Hugging Face and Microsoft will work together to enable more modalities and domain-specific tasks, including video, 3D, time series, protein and more.\r\n- Agents and tools - Small, efficient, specialized open models make them ideal to build powerful but secure AI agents and applications\r\nIf you’re on Azure, it’s time to build with open models!","src/content/posts/2025-06-20-microsoft-and-hugging-face-expand-collaboration-576333.md","88afcea849bca6c0",{html:557,metadata:558},"<p>Microsoft and Hugging Face expand collaboration to make open models easy to use on Azure\r\nToday at the Microsoft Build conference, Satya Nadella announced an expanded collaboration with Hugging Face, to make its wide diversity of open models easy to deploy on Azure secure infrastructure.\r\nIf you head over to Azure AI Foundry today, you will find a vastly expanded collection of 10,000+ Hugging Face models you can deploy in a couple clicks to power AI applications working with text, audio and images. And we’re just getting started!\r\nIt’s time to build - an expanded collaboration\r\n2 years ago, Microsoft and Hugging Face started a collaboration to make open models more easily accessible on Azure - back then the Hub was home to 200,000 open models.\r\nWith now close to 2 million open models on Hugging Face, covering a wide diversity of tasks, modalities, domains and languages, it was time to take our collaboration to the next level. The new collaboration announced today creates a framework for mutual success to vastly expand how Azure customers can benefit from Hugging Face.\r\n“Open source is how AI moves faster—with transparency, choice, and community at its core. This collaboration represents our commitment to that momentum. By combining Hugging Face’s vibrant model ecosystem with Azure’s secure, enterprise-grade infrastructure, we’re giving developers the freedom to pick the best model for the job—and helping organizations innovate safely and at scale.”\r\n— Asha Sharma, Corporate Vice President at Microsoft\r\nMaking more open models easily accessible to Azure customers, for secure deployment alongside company private data, will enable enterprises to build AI applications and agents while being fully in control of their technology and data.\r\nWe’re enabling companies to take control of their AI destiny, deploying the best open models securely within their Azure account, to build AI applications they can trust and verify.\r\n— Clement Delangue, CEO and cofounder at Hugging Face\r\nHow to use Hugging Face in Azure AI Foundry\r\nLet’s head over to Azure AI Foundry, and select the Model Catalog. Here you can now find over 10,000 models under the Hugging Face Collection.\r\nModels in the Collection include the most popular, trending models on Hugging Face for a wide range of tasks to work with text, audio and images - including text generation, feature extraction, fill-mask, translation, identifying sentence similarity, image classification, image segmentation, text to image generation, image to text conversion, automatic speech recognition and audio-classification.\r\nTo make the Hugging Face Collection on Azure AI Foundry enterprise-ready, we are only featuring models:</p>\n<ul>\n<li>having successfully passed Hugging Face security tests to screen for any vulnerability, including with ProtectAI Guardian and JFrog security scanner</li>\n<li>with model weights stored in safetensors format, avoiding potential Pickle vulnerabilities</li>\n<li>without remote code, to avoid any arbitrary code insertion at runtime.\r\nIn addition, Microsoft and Hugging Face will continuously test inference containers for vulnerabilities to maintain and patch them as needed.\r\nNow let’s say you want to deploy for instance the popular Microsoft Phi-4 Reasoning Plus open model.\r\nFirst, let’s select the model in the Hugging Face Collection in Azure AI Foundry, and click the “Deploy button”. The form allows you to select a VM, instance count and deployment parameters, and start the deployment process with just another click!\r\nNow, if you prefer browsing models on the Hub, you can also start from the model page - the “Deploy on Azure ML” option will take you to the same deployment option within Azure AI Machine Learning Studio.\r\nMore Hugging Face to come in Azure AI Foundry\r\nWe are really excited about all the new Hugging Face models and modalities now directly available within Azure AI Foundry, but we’re not going to stop there!\r\nIn the weeks and months to come, you can expect a rolling thunder of updates:</li>\n<li>Day-0 releases - Hugging Face will collaborate with Microsoft to make new models from top model providers available in Azure AI Foundry the same day they land on Hugging Face</li>\n<li>Trending models updates - Hugging Face will continuously monitor trending models to enable them on Azure AI Foundry on a daily basis</li>\n<li>New modalities - Hugging Face and Microsoft will work together to enable more modalities and domain-specific tasks, including video, 3D, time series, protein and more.</li>\n<li>Agents and tools - Small, efficient, specialized open models make them ideal to build powerful but secure AI agents and applications\r\nIf you’re on Azure, it’s time to build with open models!</li>\n</ul>",{headings:559,localImagePaths:560,remoteImagePaths:561,frontmatter:562,imagePaths:565},[],[],[],{title:549,summary:15,pubDate:563,media:17,tags:564,link:552,thumbnail:15},"Mon, 19 May 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-microsoft-and-hugging-face-expand-collaboration-576333.md","2025-06-20-no-gpu-left-behind-unlocking-efficiency-with-co-located-vllm-in-trl-885911",{id:567,data:569,body:574,filePath:575,digest:576,rendered:577,legacyId:605},{title:570,summary:15,pubDate:571,media:17,tags:572,link:573,thumbnail:15},"No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL",["Date","2025-06-03T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/vllm-colocate","No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL\r\n🚀 Introduction\r\nTRL supports training LLMs using GRPO, an online learning algorithm recently introduced in the DeepSeekMath paper. In GRPO, the model learns from its own outputs: it generates responses during training, receives feedback, and uses that feedback to improve itself over time.\r\nThis makes generation a critical step in the training loop — and also a major bottleneck. To speed up generation, TRL integrates with vLLM. This combination lets you train powerful models more efficiently in GRPO setup. However, there’s a catch.\r\n🧨 The Problem\r\nBefore TRL v0.18.0, vLLM was only supported in server mode, running as a separate process on different GPUs from the training job. It communicated with the training script over HTTP, which made the setup modular and easy to use — but also introduced GPU inefficiencies.\r\nHere’s what happens:\r\n- During training, the model needs to generate completions frequently.\r\n- The trainer sends a request to the vLLM server, which runs on its own GPUs.\r\n- While vLLM generates, the training GPUs sit idle and wait.\r\n- Once generation is done, vLLM GPUs become idle, and training resumes.\r\nThis “ping-pong” between training and generation causes:\r\n- Wasted GPU time on both sides\r\n- Increased demand for extra GPUs just to run inference\r\n- Reduced overall throughput and higher cost\r\nIn online learning methods like GRPO — where generation happens constantly — this inefficiency becomes even more painful. You spend more on hardware, but don't get the performance you'd expect.\r\nSo, the key question becomes: Can we share the same GPUs for both training and generation, instead of separating them?\r\n💡 The Opportunity\r\nThe main issue was that training and inference ran on separate GPUs, leading to idle time and underutilization. The natural solution? Run both on the same GPUs. Instead of having vLLM operate as a standalone server in its own process and devices, what if vLLM could run alongside the training code, within the same distributed process group? This would let us launch a single distributed job where training and inference share the same devices, switching between tasks efficiently without wasting resources.\r\nThis approach is what we refer to as colocation. Training and inference are co-located on the same GPUs and coordinated via the same process group, allowing them to take turns smoothly — no extra hardware needed.\r\nPreviously, this wasn’t possible in TRL, which relied on vLLM as an external HTTP server. That changed with our PR #3394, which added support for vLLM’s external launcher and true integration into the training process.\r\nWhat It Enables\r\nUnified Execution: By embedding vLLM in the same process group, both training and inference tasks can share the same GPUs, taking turns instead of waiting on each other. This reduces idle time and boosts overall efficiency.\r\nSkip HTTP Communication: No need for REST API calls or networking — vLLM runs inline with the training loop, avoiding overhead and latency.\r\nTorchrun Compatibility: Works seamlessly with\r\ntorchrun\r\n, so it's easy to scale across nodes with minimal config changes.TP and DP Support: Compatible with Tensor Parallelism and Data Parallelism, making it suitable for large-scale training runs.\r\nSPMD Execution Pattern: Uses a Single Program, Multiple Data (SPMD) model, where each GPU runs its own instance of the engine in sync. Ideal for distributed multi-GPU, multi-node setups.\r\nSimplified Deployment: You no longer need to maintain a separate server script — vLLM is launched and controlled directly inside your training job.\r\nEnhanced Throughput: By avoiding idle GPUs and eliminating inter-process communication, the system delivers faster training and generation, especially important in online learning setups like GRPO.\r\nRobust Inter-process Communication: This is more robust because it avoids the complexity of setting up distributed process groups between independent processes, as required in server mode.\r\nThanks to this feature, co-located training and inference is no longer a hack — it’s now first-class, scalable, and production-ready.\r\n🧩 Design: From Separate Servers to Shared GPUs\r\nThe shift from server TRL to co-located TRL is all about smarter GPU usage. The diagram below shows the difference:\r\nServer TRL Setup (Top Row)\r\nIn the server TRL setup, training and inference run on separate GPUs. For example:\r\n- GPUs 0 through 2 are used for training.\r\n- GPU 3 is fully dedicated to running vLLM as a separate server.\r\nDuring training steps, GPU 3 sits idle. During generation steps (inference), GPUs 0–2 are idle while GPU 3 generates outputs.\r\nThis leads to:\r\n- Inefficient GPU usage, with devices frequently waiting on each other\r\n- Extra GPUs provisioned solely for inference\r\n- Increased cost and complexity\r\nCo-located TRL Setup (Bottom Row)\r\nIn contrast, the co-located TRL setup runs both training and vLLM on the same GPUs. Each GPU:\r\n- Runs the training loop\r\n- Launches a vLLM engine within the same process\r\nTraining and inference take turns using the GPU’s resources — no need for dedicated devices or separate processes.\r\nThis design:\r\n- Reduces idle time\r\n- Minimizes inter-process and HTTP communication\r\n- Fully utilizes available GPU memory and compute\r\n- Delivers faster throughput without increasing hardware requirements\r\n🛠️ Implementation Notes\r\nInstead of launching vLLM as a server, the trainer now launches vLLM in-process using the external launcher, as shown below:\r\nself.llm = LLM(\r\nmodel=model.name_or_path,\r\ntensor_parallel_size=args.vllm_tensor_parallel_size,\r\ngpu_memory_utilization=self.vllm_gpu_memory_utilization,\r\nmax_num_seqs=self.args.per_device_train_batch_size\r\n* self.vllm_tensor_parallel_size\r\n* self.args.gradient_accumulation_steps,\r\nmax_model_len=self.max_prompt_length + self.max_completion_length,\r\ndistributed_executor_backend=\"external_launcher\",\r\n# Feed identical seed for tp groups to ensure sampling results are the same across workers\r\nseed=self.accelerator.process_index // self.vllm_tensor_parallel_size,\r\n)\r\nCo-located vLLM respects the torch.distributed process group and rank structure. This allows vLLM to be initialized alongside training without conflict and makes TP/DP setups work seamlessly:\r\nif self.vllm_tensor_parallel_size > 1:\r\n# Create subgroups of ranks for TP, each group with `vllm_tensor_parallel_size` ranks.\r\nself.tp_group, _ = torch.distributed.new_subgroups_by_enumeration(\r\n[\r\nlist(range(i * self.vllm_tensor_parallel_size, (i + 1) * self.vllm_tensor_parallel_size))\r\nfor i in range(self.accelerator.num_processes // self.vllm_tensor_parallel_size)\r\n]\r\n)\r\nCo-located vLLM no longer relies on REST APIs — it runs directly in memory and communicates via native Python calls:\r\nif self.vllm_tensor_parallel_size > 1:\r\norig_size = len(prompts_text)\r\ngathered_prompts = [None for _ in range(self.vllm_tensor_parallel_size)]\r\ntorch.distributed.all_gather_object(gathered_prompts, prompts_text, group=self.tp_group)\r\nall_prompts_text = [p for sublist in gathered_prompts for p in sublist]\r\nelse:\r\nall_prompts_text = prompts_text\r\nwith profiling_context(self, \"vLLM.generate\"):\r\nall_outputs = self.llm.generate(all_prompts_text, sampling_params=sampling_params, use_tqdm=False)\r\ncompletion_ids = [output.token_ids for outputs in all_outputs for output in outputs.outputs]\r\nif self.vllm_tensor_parallel_size > 1:\r\nlocal_rank_in_group = torch.distributed.get_rank(group=self.tp_group)\r\ntp_slice = slice(local_rank_in_group * orig_size, (local_rank_in_group + 1) * orig_size)\r\ncompletion_ids = completion_ids[tp_slice]\r\nTo use this setup, simply set vllm_mode=\"colocate\" in your GRPO configuration:\r\ntraining_args = GRPOConfig(\r\n...,\r\nuse_vllm=True,\r\nvllm_mode=\"colocate\",\r\n)\r\nNote: Depending on the model size and the overall GPU memory requirements for training, you may need to adjust the vllm_gpu_memory_utilization parameter in\r\nGRPOConfig\r\nto avoid underutilization or out-of-memory errors.\r\n📊 Showcase: Co-located vs. Plain TRL Performance\r\nTo measure the impact of colocation, we ran a series of experiments comparing the traditional server mode (where vLLM runs on a separate GPU as a standalone server) with the new co-locate mode (where training and inference share the same GPUs).\r\nIn server mode, only 7 GPUs are used for training because 1 GPU is fully dedicated to the vLLM inference server.\r\nIn co-locate mode, all 8 GPUs are used for training — increasing the effective batch size by default.\r\nTo ensure a fair comparison, we normalized throughput in server mode by a factor of 8/7. This adjustment accounts for the greater training capacity in co-locate mode and allows us to compare the two setups under equal training conditions.\r\nExperiment 1: 1.5B Model — Varying Batch Sizes\r\n- As the batch size increases, throughput improves in both setups.\r\n- Co-located setup reaches up to 1.43× speedup at the largest batch size.\r\n- Larger batches make better use of shared GPU memory in co-located mode.\r\nExperiment 2: 1.5B Model — Varying Tensor Parallelism (TP)\r\n- In the co-located setup, increasing TP reduces performance.\r\n- More sharding introduces more communication overhead — which is not ideal for smaller models.\r\n- Takeaway: For small models, avoid over-sharding in co-located mode.\r\nExperiment 3: 7B Model — Varying Batch Sizes\r\n- Again, co-located mode scales better with batch size.\r\n- Gains reach 1.35× speedup at the largest batch tested.\r\nExperiment 4: 7B Model — Varying Tensor Parallelism (TP)\r\n- Opposite trend from the 1.5B model.\r\n- With 7B, more TP improves throughput, reaching up to 1.73× speedup.\r\n- Larger models benefit from sharding in co-located setups.\r\n📊 Scaling to 72B Model\r\nWhen training large models like Qwen2.5-Math-72B, it's important to use the right strategies to make training efficient, scalable, and stable across many GPUs and nodes. In our setup, we combined co-located vLLM with several key optimizations to make this work efficiently.\r\nSleep Mode in vLLM\r\nWhen using co-located training, managing GPU memory is crucial so that both training and inference can run smoothly on the same devices. To support this, we added vLLM’s sleep()\r\nAPI into the GRPO training loop.\r\nThe sleep()\r\nfunction temporarily pauses the vLLM engine and frees up GPU memory. It supports two levels:\r\nLevel 1: Unloads model weights from GPU (keeps them in CPU memory) and clears the KV cache. Useful when the same model will be reused soon.\r\nLevel 2: Unloads both model weights and KV cache entirely. Best for scenarios where the model will change or won’t be reused right away.\r\nIn GRPO, the model is updated after every step — so we use Level 2 sleep.\r\nBenefits of Level 2 sleep:\r\n- Maximizes free GPU memory for training\r\n- Avoids memory contention between training and generation\r\n- Keeps colocation efficient, even for large models like Qwen2.5-72B\r\nThis small addition makes a big difference in enabling smooth and scalable co-located training.\r\nDeepSpeed Optimizations\r\nTo train large models like Qwen2.5-72B, we rely on DeepSpeed ZeRO Stage 3, the same setup used in plain TRL.\r\nZeRO helps scale large models by distributing memory across GPUs. Stage 3 goes further by partitioning:\r\n- Model weights\r\n- Gradients\r\n- Optimizer states\r\nThis is essential for models that can’t fit on a single GPU. With ZeRO Stage 3, each GPU handles only a portion of the model.\r\nAdditional options we enable:\r\n\"offload_optimizer\": {\"device\": \"cpu\"}\r\nMoves optimizer states to CPU to free GPU memory — critical in co-located setups.\"overlap_comm\": true\r\nEnables communication overlap with computation, speeding up training.\"contiguous_gradients\": true\r\nAllocates gradients in a single memory block, improving memory access and reducing fragmentation.\r\nThese optimizations help train 72B models efficiently, and ensure colocation remains stable under tight memory constraints.\r\nAccelerate Integration\r\nAs recommended in TRL, we use Accelerate, a lightweight library that simplifies distributed training. It handles:\r\n- Multi-GPU and multi-node job launching\r\n- Data parallelism\r\n- Gradient accumulation\r\n- Distributed data loading\r\nThis makes the setup clean, scalable, and easy to maintain.\r\nExperiment 5: Qwen2.5-Math-72B — Throughput, Accuracy, and Benchmark Results\r\nThroughput\r\nEven with 4 fewer GPUs, the co-locate setup is ~1.26× faster than plain TRL.\r\nThis highlights the effectiveness of smarter GPU sharing and memory cleanup using sleep()\r\n.\r\nReward Curve\r\nTraining reward plots for co-locate and plain setups are nearly identical, demonstrating that:\r\nMath500 Benchmark\r\nWe evaluated three models: Base model, Co-locate-trained model, Plain-trained model on the Math500 benchmark. Both trained models outperform the base, and the co-locate model performs on par with the plain-trained model — confirming that colocation does not compromise downstream performance.\r\n🎓 Challenges & Lessons Learned & next steps\r\nThrough our work on scaling GRPO training with co-located vLLM, we've faced several critical challenges and learned important lessons about efficiency, flexibility, and system design when training large models.\r\nChallenges\r\nTensor Parallelism Bug in vLLM ≥ 0.8.0. Tensor Parallelism (TP) with external_launcher stopped working in vLLM version 0.8.0 and above. This was tracked under Issue #15895. To identify the breaking point, we followed the approach described in this vLLM developer blog post, which provides wheels for every commit. After bisecting, we identified the breaking commit as cc10281. The root cause was determinism — the newer versions required explicitly setting the random seed. Once the seed was set, the issue went away.\r\nLevel 2 Sleep Buffer Bug. Initially, level 2 sleep didn’t work correctly when we tried to reload weights using load_weights. This issue was tracked in Issue #16564. The problem was that model buffers (like running mean/var in BatchNorm) weren’t restored after waking up from sleep. The fix came with PR #16889, which added logic to explicitly restore buffers when waking up from level 2 sleep. We now keep a copy of the original buffers and manually reapply them after loading new weights.\r\nSegmentation Fault on Exit. There’s still an open issue with vLLM sleep causing a segmentation fault at the end of training when closing processes. This was reported in Issue #16993. This crash happens during shutdown but does not break training itself, so we were able to complete all demos and experiments shared in this blog. However, we’re waiting for an official fix before integrating sleep() fully into TRL upstream.\r\nThese challenges were not blockers, but they required careful debugging, version control, and a deeper understanding of how vLLM manages memory and parallelism under the hood.\r\nLessons Learned\r\nCo-located inference dramatically improves GPU utilization. By allowing training and generation to share the same GPUs, we eliminate idle time and reduce hardware requirements — achieving higher throughput even with fewer GPUs.\r\nvLLM's sleep() feature is essential for large-scale colocation. It enables fine-grained control over memory usage, allowing training to fully reclaim GPU memory between generation steps — a key enabler for models like Qwen2.5-72B.\r\nDeepSpeed ZeRO Stage 3 is essential for training large models. It allows extremely large networks to fit into memory by distributing model weights, gradients, and optimizer states across multiple GPUs. In our experience, enabling contiguous_gradients helped reduce memory fragmentation, while offloading the optimizer to the CPU freed up critical GPU memory — both of which were especially helpful in colocated setups.\r\nColocation is powerful but comes with trade-offs. It works best when GPU memory is carefully managed, often requiring manual tuning of memory usage parameters like vllm_gpu_memory_utilization. While it offers clear throughput benefits and reduces idle GPU time, colocation may not be ideal for models with tight memory budgets or when memory fragmentation is not well controlled. When done right, though, it unlocks significant efficiency gains.\r\nTP/DP compatibility, Accelerate, and torchrun support make deployment seamless. Despite the complexity of the underlying architecture, the entire system can be launched and scaled with standard distributed tools.\r\nCo-located training maintains model quality. Across multiple benchmarks (Math500, AIME24), co-located and plain setups produced comparable results, validating that performance isn’t sacrificed for efficiency.\r\n✅ Conclusion\r\nThis blog post explored how co-locating vLLM with GRPO training unlocks significant efficiency gains when training large language models — including models as large as Qwen2.5-72B.\r\nTraditionally, TRL only supported vLLM in server mode, which required separate processes and GPUs for inference, leading to wasted compute and idle time. With the introduction of vLLM’s external launcher and the colocation PR in TRL PR #3394, we can now run training and inference within the same distributed process group, on the same GPUs, with full support for TP, DP, and Accelerate.\r\nWhile challenges remain — such as version-specific vLLM bugs and edge cases such as with sleep() — the overall results show that co-located GRPO is a practical, scalable solution for training large models efficiently. We’re excited to continue refining this setup, integrating features like FSDP, and pushing the limits of large model training — making it faster, cheaper, and more accessible for everyone building the next generation of LLMs.\r\n✅ Give It a Try!\r\nBelow is an example to try out GRPO training with co-located vLLM.\r\n📄 train_grpo_colocate.py\r\nfrom datasets import load_dataset\r\nfrom trl import GRPOConfig, GRPOTrainer\r\n# Load dataset\r\ndataset = load_dataset(\"trl-lib/tldr\", split=\"train\")\r\n# Define the reward function\r\ndef reward_len(completions, **kwargs):\r\nreturn [-abs(20 - len(completion)) for completion in completions]\r\n# Define training arguments\r\ntraining_args = GRPOConfig(\r\noutput_dir=\"Qwen2-0.5B-GRPO\",\r\nlogging_steps=1,\r\nuse_vllm=True,\r\nvllm_mode=\"colocate\",\r\nvllm_tensor_parallel_size=1,\r\nvllm_gpu_memory_utilization=0.3,\r\nmax_prompt_length=512,\r\nmax_completion_length=1024,\r\nmax_steps=2,\r\nnum_generations=4,\r\nnum_train_epochs=1,\r\nper_device_train_batch_size=4,\r\npush_to_hub=False,\r\nreport_to=None\r\n)\r\n# Create and run the trainer\r\ntrainer = GRPOTrainer(\r\nmodel=\"Qwen/Qwen2-0.5B-Instruct\",\r\nreward_funcs=reward_len,\r\nargs=training_args,\r\ntrain_dataset=dataset,\r\n)\r\ntrainer.train()","src/content/posts/2025-06-20-no-gpu-left-behind-unlocking-efficiency-with-co-located-vllm-in-trl-885911.md","caf813f7a77df91c",{html:578,metadata:579},"<p>No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL\r\n🚀 Introduction\r\nTRL supports training LLMs using GRPO, an online learning algorithm recently introduced in the DeepSeekMath paper. In GRPO, the model learns from its own outputs: it generates responses during training, receives feedback, and uses that feedback to improve itself over time.\r\nThis makes generation a critical step in the training loop — and also a major bottleneck. To speed up generation, TRL integrates with vLLM. This combination lets you train powerful models more efficiently in GRPO setup. However, there’s a catch.\r\n🧨 The Problem\r\nBefore TRL v0.18.0, vLLM was only supported in server mode, running as a separate process on different GPUs from the training job. It communicated with the training script over HTTP, which made the setup modular and easy to use — but also introduced GPU inefficiencies.\r\nHere’s what happens:</p>\n<ul>\n<li>During training, the model needs to generate completions frequently.</li>\n<li>The trainer sends a request to the vLLM server, which runs on its own GPUs.</li>\n<li>While vLLM generates, the training GPUs sit idle and wait.</li>\n<li>Once generation is done, vLLM GPUs become idle, and training resumes.\r\nThis “ping-pong” between training and generation causes:</li>\n<li>Wasted GPU time on both sides</li>\n<li>Increased demand for extra GPUs just to run inference</li>\n<li>Reduced overall throughput and higher cost\r\nIn online learning methods like GRPO — where generation happens constantly — this inefficiency becomes even more painful. You spend more on hardware, but don’t get the performance you’d expect.\r\nSo, the key question becomes: Can we share the same GPUs for both training and generation, instead of separating them?\r\n💡 The Opportunity\r\nThe main issue was that training and inference ran on separate GPUs, leading to idle time and underutilization. The natural solution? Run both on the same GPUs. Instead of having vLLM operate as a standalone server in its own process and devices, what if vLLM could run alongside the training code, within the same distributed process group? This would let us launch a single distributed job where training and inference share the same devices, switching between tasks efficiently without wasting resources.\r\nThis approach is what we refer to as colocation. Training and inference are co-located on the same GPUs and coordinated via the same process group, allowing them to take turns smoothly — no extra hardware needed.\r\nPreviously, this wasn’t possible in TRL, which relied on vLLM as an external HTTP server. That changed with our PR #3394, which added support for vLLM’s external launcher and true integration into the training process.\r\nWhat It Enables\r\nUnified Execution: By embedding vLLM in the same process group, both training and inference tasks can share the same GPUs, taking turns instead of waiting on each other. This reduces idle time and boosts overall efficiency.\r\nSkip HTTP Communication: No need for REST API calls or networking — vLLM runs inline with the training loop, avoiding overhead and latency.\r\nTorchrun Compatibility: Works seamlessly with\r\ntorchrun\r\n, so it’s easy to scale across nodes with minimal config changes.TP and DP Support: Compatible with Tensor Parallelism and Data Parallelism, making it suitable for large-scale training runs.\r\nSPMD Execution Pattern: Uses a Single Program, Multiple Data (SPMD) model, where each GPU runs its own instance of the engine in sync. Ideal for distributed multi-GPU, multi-node setups.\r\nSimplified Deployment: You no longer need to maintain a separate server script — vLLM is launched and controlled directly inside your training job.\r\nEnhanced Throughput: By avoiding idle GPUs and eliminating inter-process communication, the system delivers faster training and generation, especially important in online learning setups like GRPO.\r\nRobust Inter-process Communication: This is more robust because it avoids the complexity of setting up distributed process groups between independent processes, as required in server mode.\r\nThanks to this feature, co-located training and inference is no longer a hack — it’s now first-class, scalable, and production-ready.\r\n🧩 Design: From Separate Servers to Shared GPUs\r\nThe shift from server TRL to co-located TRL is all about smarter GPU usage. The diagram below shows the difference:\r\nServer TRL Setup (Top Row)\r\nIn the server TRL setup, training and inference run on separate GPUs. For example:</li>\n<li>GPUs 0 through 2 are used for training.</li>\n<li>GPU 3 is fully dedicated to running vLLM as a separate server.\r\nDuring training steps, GPU 3 sits idle. During generation steps (inference), GPUs 0–2 are idle while GPU 3 generates outputs.\r\nThis leads to:</li>\n<li>Inefficient GPU usage, with devices frequently waiting on each other</li>\n<li>Extra GPUs provisioned solely for inference</li>\n<li>Increased cost and complexity\r\nCo-located TRL Setup (Bottom Row)\r\nIn contrast, the co-located TRL setup runs both training and vLLM on the same GPUs. Each GPU:</li>\n<li>Runs the training loop</li>\n<li>Launches a vLLM engine within the same process\r\nTraining and inference take turns using the GPU’s resources — no need for dedicated devices or separate processes.\r\nThis design:</li>\n<li>Reduces idle time</li>\n<li>Minimizes inter-process and HTTP communication</li>\n<li>Fully utilizes available GPU memory and compute</li>\n<li>Delivers faster throughput without increasing hardware requirements\r\n🛠️ Implementation Notes\r\nInstead of launching vLLM as a server, the trainer now launches vLLM in-process using the external launcher, as shown below:\r\nself.llm = LLM(\r\nmodel=model.name_or_path,\r\ntensor_parallel_size=args.vllm_tensor_parallel_size,\r\ngpu_memory_utilization=self.vllm_gpu_memory_utilization,\r\nmax_num_seqs=self.args.per_device_train_batch_size</li>\n</ul>\n<ul>\n<li>self.vllm_tensor_parallel_size</li>\n<li>self.args.gradient_accumulation_steps,\r\nmax_model_len=self.max_prompt_length + self.max_completion_length,\r\ndistributed_executor_backend=“external_launcher”,</li>\n</ul>\n<h1 id=\"feed-identical-seed-for-tp-groups-to-ensure-sampling-results-are-the-same-across-workers\">Feed identical seed for tp groups to ensure sampling results are the same across workers</h1>\n<p>seed=self.accelerator.process_index // self.vllm_tensor_parallel_size,\r\n)\r\nCo-located vLLM respects the torch.distributed process group and rank structure. This allows vLLM to be initialized alongside training without conflict and makes TP/DP setups work seamlessly:\r\nif self.vllm_tensor_parallel_size > 1:</p>\n<h1 id=\"create-subgroups-of-ranks-for-tp-each-group-with-vllm_tensor_parallel_size-ranks\">Create subgroups of ranks for TP, each group with <code>vllm_tensor_parallel_size</code> ranks.</h1>\n<p>self.tp_group, _ = torch.distributed.new_subgroups_by_enumeration(\r\n[\r\nlist(range(i * self.vllm_tensor_parallel_size, (i + 1) * self.vllm_tensor_parallel_size))\r\nfor i in range(self.accelerator.num_processes // self.vllm_tensor_parallel_size)\r\n]\r\n)\r\nCo-located vLLM no longer relies on REST APIs — it runs directly in memory and communicates via native Python calls:\r\nif self.vllm_tensor_parallel_size > 1:\r\norig_size = len(prompts_text)\r\ngathered_prompts = [None for _ in range(self.vllm_tensor_parallel_size)]\r\ntorch.distributed.all_gather_object(gathered_prompts, prompts_text, group=self.tp_group)\r\nall_prompts_text = [p for sublist in gathered_prompts for p in sublist]\r\nelse:\r\nall_prompts_text = prompts_text\r\nwith profiling_context(self, “vLLM.generate”):\r\nall_outputs = self.llm.generate(all_prompts_text, sampling_params=sampling_params, use_tqdm=False)\r\ncompletion_ids = [output.token_ids for outputs in all_outputs for output in outputs.outputs]\r\nif self.vllm_tensor_parallel_size > 1:\r\nlocal_rank_in_group = torch.distributed.get_rank(group=self.tp_group)\r\ntp_slice = slice(local_rank_in_group * orig_size, (local_rank_in_group + 1) * orig_size)\r\ncompletion_ids = completion_ids[tp_slice]\r\nTo use this setup, simply set vllm_mode=“colocate” in your GRPO configuration:\r\ntraining_args = GRPOConfig(\r\n…,\r\nuse_vllm=True,\r\nvllm_mode=“colocate”,\r\n)\r\nNote: Depending on the model size and the overall GPU memory requirements for training, you may need to adjust the vllm_gpu_memory_utilization parameter in\r\nGRPOConfig\r\nto avoid underutilization or out-of-memory errors.\r\n📊 Showcase: Co-located vs. Plain TRL Performance\r\nTo measure the impact of colocation, we ran a series of experiments comparing the traditional server mode (where vLLM runs on a separate GPU as a standalone server) with the new co-locate mode (where training and inference share the same GPUs).\r\nIn server mode, only 7 GPUs are used for training because 1 GPU is fully dedicated to the vLLM inference server.\r\nIn co-locate mode, all 8 GPUs are used for training — increasing the effective batch size by default.\r\nTo ensure a fair comparison, we normalized throughput in server mode by a factor of 8/7. This adjustment accounts for the greater training capacity in co-locate mode and allows us to compare the two setups under equal training conditions.\r\nExperiment 1: 1.5B Model — Varying Batch Sizes</p>\n<ul>\n<li>As the batch size increases, throughput improves in both setups.</li>\n<li>Co-located setup reaches up to 1.43× speedup at the largest batch size.</li>\n<li>Larger batches make better use of shared GPU memory in co-located mode.\r\nExperiment 2: 1.5B Model — Varying Tensor Parallelism (TP)</li>\n<li>In the co-located setup, increasing TP reduces performance.</li>\n<li>More sharding introduces more communication overhead — which is not ideal for smaller models.</li>\n<li>Takeaway: For small models, avoid over-sharding in co-located mode.\r\nExperiment 3: 7B Model — Varying Batch Sizes</li>\n<li>Again, co-located mode scales better with batch size.</li>\n<li>Gains reach 1.35× speedup at the largest batch tested.\r\nExperiment 4: 7B Model — Varying Tensor Parallelism (TP)</li>\n<li>Opposite trend from the 1.5B model.</li>\n<li>With 7B, more TP improves throughput, reaching up to 1.73× speedup.</li>\n<li>Larger models benefit from sharding in co-located setups.\r\n📊 Scaling to 72B Model\r\nWhen training large models like Qwen2.5-Math-72B, it’s important to use the right strategies to make training efficient, scalable, and stable across many GPUs and nodes. In our setup, we combined co-located vLLM with several key optimizations to make this work efficiently.\r\nSleep Mode in vLLM\r\nWhen using co-located training, managing GPU memory is crucial so that both training and inference can run smoothly on the same devices. To support this, we added vLLM’s sleep()\r\nAPI into the GRPO training loop.\r\nThe sleep()\r\nfunction temporarily pauses the vLLM engine and frees up GPU memory. It supports two levels:\r\nLevel 1: Unloads model weights from GPU (keeps them in CPU memory) and clears the KV cache. Useful when the same model will be reused soon.\r\nLevel 2: Unloads both model weights and KV cache entirely. Best for scenarios where the model will change or won’t be reused right away.\r\nIn GRPO, the model is updated after every step — so we use Level 2 sleep.\r\nBenefits of Level 2 sleep:</li>\n<li>Maximizes free GPU memory for training</li>\n<li>Avoids memory contention between training and generation</li>\n<li>Keeps colocation efficient, even for large models like Qwen2.5-72B\r\nThis small addition makes a big difference in enabling smooth and scalable co-located training.\r\nDeepSpeed Optimizations\r\nTo train large models like Qwen2.5-72B, we rely on DeepSpeed ZeRO Stage 3, the same setup used in plain TRL.\r\nZeRO helps scale large models by distributing memory across GPUs. Stage 3 goes further by partitioning:</li>\n<li>Model weights</li>\n<li>Gradients</li>\n<li>Optimizer states\r\nThis is essential for models that can’t fit on a single GPU. With ZeRO Stage 3, each GPU handles only a portion of the model.\r\nAdditional options we enable:\r\n“offload_optimizer”: {“device”: “cpu”}\r\nMoves optimizer states to CPU to free GPU memory — critical in co-located setups.”overlap_comm”: true\r\nEnables communication overlap with computation, speeding up training.”contiguous_gradients”: true\r\nAllocates gradients in a single memory block, improving memory access and reducing fragmentation.\r\nThese optimizations help train 72B models efficiently, and ensure colocation remains stable under tight memory constraints.\r\nAccelerate Integration\r\nAs recommended in TRL, we use Accelerate, a lightweight library that simplifies distributed training. It handles:</li>\n<li>Multi-GPU and multi-node job launching</li>\n<li>Data parallelism</li>\n<li>Gradient accumulation</li>\n<li>Distributed data loading\r\nThis makes the setup clean, scalable, and easy to maintain.\r\nExperiment 5: Qwen2.5-Math-72B — Throughput, Accuracy, and Benchmark Results\r\nThroughput\r\nEven with 4 fewer GPUs, the co-locate setup is ~1.26× faster than plain TRL.\r\nThis highlights the effectiveness of smarter GPU sharing and memory cleanup using sleep()\r\n.\r\nReward Curve\r\nTraining reward plots for co-locate and plain setups are nearly identical, demonstrating that:\r\nMath500 Benchmark\r\nWe evaluated three models: Base model, Co-locate-trained model, Plain-trained model on the Math500 benchmark. Both trained models outperform the base, and the co-locate model performs on par with the plain-trained model — confirming that colocation does not compromise downstream performance.\r\n🎓 Challenges &#x26; Lessons Learned &#x26; next steps\r\nThrough our work on scaling GRPO training with co-located vLLM, we’ve faced several critical challenges and learned important lessons about efficiency, flexibility, and system design when training large models.\r\nChallenges\r\nTensor Parallelism Bug in vLLM ≥ 0.8.0. Tensor Parallelism (TP) with external_launcher stopped working in vLLM version 0.8.0 and above. This was tracked under Issue #15895. To identify the breaking point, we followed the approach described in this vLLM developer blog post, which provides wheels for every commit. After bisecting, we identified the breaking commit as cc10281. The root cause was determinism — the newer versions required explicitly setting the random seed. Once the seed was set, the issue went away.\r\nLevel 2 Sleep Buffer Bug. Initially, level 2 sleep didn’t work correctly when we tried to reload weights using load_weights. This issue was tracked in Issue #16564. The problem was that model buffers (like running mean/var in BatchNorm) weren’t restored after waking up from sleep. The fix came with PR #16889, which added logic to explicitly restore buffers when waking up from level 2 sleep. We now keep a copy of the original buffers and manually reapply them after loading new weights.\r\nSegmentation Fault on Exit. There’s still an open issue with vLLM sleep causing a segmentation fault at the end of training when closing processes. This was reported in Issue #16993. This crash happens during shutdown but does not break training itself, so we were able to complete all demos and experiments shared in this blog. However, we’re waiting for an official fix before integrating sleep() fully into TRL upstream.\r\nThese challenges were not blockers, but they required careful debugging, version control, and a deeper understanding of how vLLM manages memory and parallelism under the hood.\r\nLessons Learned\r\nCo-located inference dramatically improves GPU utilization. By allowing training and generation to share the same GPUs, we eliminate idle time and reduce hardware requirements — achieving higher throughput even with fewer GPUs.\r\nvLLM’s sleep() feature is essential for large-scale colocation. It enables fine-grained control over memory usage, allowing training to fully reclaim GPU memory between generation steps — a key enabler for models like Qwen2.5-72B.\r\nDeepSpeed ZeRO Stage 3 is essential for training large models. It allows extremely large networks to fit into memory by distributing model weights, gradients, and optimizer states across multiple GPUs. In our experience, enabling contiguous_gradients helped reduce memory fragmentation, while offloading the optimizer to the CPU freed up critical GPU memory — both of which were especially helpful in colocated setups.\r\nColocation is powerful but comes with trade-offs. It works best when GPU memory is carefully managed, often requiring manual tuning of memory usage parameters like vllm_gpu_memory_utilization. While it offers clear throughput benefits and reduces idle GPU time, colocation may not be ideal for models with tight memory budgets or when memory fragmentation is not well controlled. When done right, though, it unlocks significant efficiency gains.\r\nTP/DP compatibility, Accelerate, and torchrun support make deployment seamless. Despite the complexity of the underlying architecture, the entire system can be launched and scaled with standard distributed tools.\r\nCo-located training maintains model quality. Across multiple benchmarks (Math500, AIME24), co-located and plain setups produced comparable results, validating that performance isn’t sacrificed for efficiency.\r\n✅ Conclusion\r\nThis blog post explored how co-locating vLLM with GRPO training unlocks significant efficiency gains when training large language models — including models as large as Qwen2.5-72B.\r\nTraditionally, TRL only supported vLLM in server mode, which required separate processes and GPUs for inference, leading to wasted compute and idle time. With the introduction of vLLM’s external launcher and the colocation PR in TRL PR #3394, we can now run training and inference within the same distributed process group, on the same GPUs, with full support for TP, DP, and Accelerate.\r\nWhile challenges remain — such as version-specific vLLM bugs and edge cases such as with sleep() — the overall results show that co-located GRPO is a practical, scalable solution for training large models efficiently. We’re excited to continue refining this setup, integrating features like FSDP, and pushing the limits of large model training — making it faster, cheaper, and more accessible for everyone building the next generation of LLMs.\r\n✅ Give It a Try!\r\nBelow is an example to try out GRPO training with co-located vLLM.\r\n📄 train_grpo_colocate.py\r\nfrom datasets import load_dataset\r\nfrom trl import GRPOConfig, GRPOTrainer</li>\n</ul>\n<h1 id=\"load-dataset\">Load dataset</h1>\n<p>dataset = load_dataset(“trl-lib/tldr”, split=“train”)</p>\n<h1 id=\"define-the-reward-function\">Define the reward function</h1>\n<p>def reward_len(completions, **kwargs):\r\nreturn [-abs(20 - len(completion)) for completion in completions]</p>\n<h1 id=\"define-training-arguments\">Define training arguments</h1>\n<p>training_args = GRPOConfig(\r\noutput_dir=“Qwen2-0.5B-GRPO”,\r\nlogging_steps=1,\r\nuse_vllm=True,\r\nvllm_mode=“colocate”,\r\nvllm_tensor_parallel_size=1,\r\nvllm_gpu_memory_utilization=0.3,\r\nmax_prompt_length=512,\r\nmax_completion_length=1024,\r\nmax_steps=2,\r\nnum_generations=4,\r\nnum_train_epochs=1,\r\nper_device_train_batch_size=4,\r\npush_to_hub=False,\r\nreport_to=None\r\n)</p>\n<h1 id=\"create-and-run-the-trainer\">Create and run the trainer</h1>\n<p>trainer = GRPOTrainer(\r\nmodel=“Qwen/Qwen2-0.5B-Instruct”,\r\nreward_funcs=reward_len,\r\nargs=training_args,\r\ntrain_dataset=dataset,\r\n)\r\ntrainer.train()</p>",{headings:580,localImagePaths:599,remoteImagePaths:600,frontmatter:601,imagePaths:604},[581,584,587,590,593,596],{depth:31,slug:582,text:583},"feed-identical-seed-for-tp-groups-to-ensure-sampling-results-are-the-same-across-workers","Feed identical seed for tp groups to ensure sampling results are the same across workers",{depth:31,slug:585,text:586},"create-subgroups-of-ranks-for-tp-each-group-with-vllm_tensor_parallel_size-ranks","Create subgroups of ranks for TP, each group with vllm_tensor_parallel_size ranks.",{depth:31,slug:588,text:589},"load-dataset","Load dataset",{depth:31,slug:591,text:592},"define-the-reward-function","Define the reward function",{depth:31,slug:594,text:595},"define-training-arguments","Define training arguments",{depth:31,slug:597,text:598},"create-and-run-the-trainer","Create and run the trainer",[],[],{title:570,summary:15,pubDate:602,media:17,tags:603,link:573,thumbnail:15},"Tue, 03 Jun 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-no-gpu-left-behind-unlocking-efficiency-with-co-located-vllm-in-trl-885911.md","2025-06-20-screensuite---the-most-comprehensive-evaluation-suite-for-gui-agents-8f58a7",{id:606,data:608,body:613,filePath:614,digest:615,rendered:616,legacyId:626},{title:609,summary:15,pubDate:610,media:17,tags:611,link:612,thumbnail:15},"ScreenSuite - The most comprehensive evaluation suite for GUI Agents!",["Date","2025-06-06T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/screensuite","ScreenSuite - The most comprehensive evaluation suite for GUI Agents!\r\nReleasing ScreenSuite, the most comprehensive evaluation suite for GUI Agents!\r\nTL;DR\r\nOver the past few weeks, we’ve been working tirelessly on making GUI agents more open, accessible and easy to integrate. Along the way, we created the largest benchmarking suite for GUI agents performances 👉 let us introduce ScreenSuite.\r\nWe are very excited to share it with you today: ScreenSuite is the most comprehensive and easiest way to evaluate Vision Language Models (VLMs)across many agentic capabilities!\r\nWTF is a GUI Agent?\r\nGUI Agents in action - courtesy of OSWorld\r\nIn short, an AI Agent is a robot that acts in the virtual world. (more thorough definition here)\r\nIn particular, a “GUI Agent” is an agent that lives in a GUI. Think “an agent that can do clicks and navigate on my desktop or my phone”, à la Claude Computer Use.\r\nThis means in essence that the AI model powering the agent will be given a task like “Fill the rest of this Excel column”, along with screen captures of the GUI. Using this information, it will then decide to take action on the system : click(x=130, y=540)\r\nto open a web browser, type(”Value for XYZ in 2025\")\r\n, scroll(down=2)\r\nto read further… To see a GUI agent in action, you can try our Open Computer Agent, powered by Qwen2.5-VL-72B.\r\nA good GUI agent will be able to navigate a computer just like we would, thus unlocking all computer tasks : scrolling through Google Maps, editing a file, buying an item online. This involves a variety of capabilities that can be hard to evaluate.\r\nIntroducing ScreenSuite 🥳\r\nThe literature, for instance Xu et al. (2025) or Qin et al. (2025), generally splits GUI agent abilities amongst several categories:\r\n- Perception: correctly perceiving the informati displayed on screen\r\n- Grounding: understanding the positioning of elements - this is paramount to click the correct place\r\n- Single step actions: solving instructions correctly over one action\r\n- Multi-step agents: solving a higher-level goal through several actions in a GUI environment.\r\nSo our first contribution is to gather and unify a comprehensive suite of 13 benchmarks spanning the full range of these GUI agent capabilities.\r\nIf you look at the last category listed above, evaluating Multi-step agentic capabilities is especially challenging as it requires virtual machines to run the agent’s environment, be it Windows, Android, Ubuntu... To address this, we provide support both for E2B desktop remote sandboxes, and we created from scratch a new option to easily launch Ubuntu or Android virtual machines in Docker!\r\n| Category | Benchmark | Environment | Sample count |\r\n|---|---|---|---|\r\n| Perception / Grounding 👁️ | ScreenQA-Short | Mobile | 8.4k |\r\n| ScreenQA-Complex | Mobile | 11.8k | |\r\n| ScreenSpot-v2 | Desktop | 1.3k | |\r\n| ScreenSpot-Pro | Desktop | 1.6k | |\r\n| WebSRC | Web | 52k | |\r\n| VisualWebBench | Web | 1.5k | |\r\n| Single-Step Actions 🎯 | Showdown-clicks | Web | 0.6k |\r\n| AndroidControl | Mobile | 3k | |\r\n| Multimodal-Mind2web | Web | 6.4k | |\r\n| Multi-Step Agents 🐾 | AndroidWorld (incl MobileMiniWob) | Mobile | 116 tasks, infinite |\r\n| OSWorld | Desktop | 369 | |\r\n| BrowseComp | Web | 1.27k | |\r\n| GAIA-Web | Web | 132 | |\r\n| Mind2Web-Live | Web | 208 |\r\nImplementation details\r\nWe’ve carefully designed our benchmark suite with modularity and consistency in mind, ensuring strong alignment across tasks and environments. When required, especially for online benchmarks, we leverage smolagents as framework layer to streamline agent execution and orchestration.\r\nTo support reproducibility and ease of use, we’ve built custom Dockerized containers that allow local deployment of full Ubuntu Desktop or Android environments.\r\nUnlike many existing GUI benchmarks that rely on accessibility trees or other metadata alongside visual input, our stack is intentionally vision-only. While this can result in different scores on some established leaderboards, we deem that it creates a more realistic and challenging setup, one that better reflects how humans perceive and interact with graphical interfaces.\r\n– All agentic frameworks (Android World, OSWorld, GAIAWeb, Mind2Web) use smolagents and rely solely on vision, without any accessibility tree or DOM added (in contrast with evaluation settings reported in other sources). – Mind2Web (Multimodal) originally used element-name-based multi-choice selection based on the accessibility tree and screenshots, but was later adapted to click precision within bounding boxes using vision only, which significantly increases task difficulty.\r\nRanking leading VLMs on ScreenSuite 📊\r\nWe’ve evaluated leading VLMs on the benchmark\r\n- The Qwen-2.5-VL series of models from 3B to 72B. These models are known for their amazing localization capabilities, in other words they know the coordinates of any element in an image which makes them suited for GUI agents that need to click precisely.\r\n- UI-Tars-1.5-7B, the all-rounder by ByteDance.\r\n- Holo1-7B, the latest model by H company, showing extremely performant localization for its size.\r\n- GPT-4o\r\nOur scores are in general agreement with the scores reported in various sources! With the caveat that we evaluate on vision only, causing some differences, see implementation details above.\r\n💡 Note that ScreenSuite does not intend to exactly reproduce benchmarks published in the industry: we evaluate models on GUI agentic capabilities based on vision. As a result, on benchmarks like Mind2Web where other benchmarks gave the agent a view of information rich context like DOM or accessibility tree, our evaluation setting is much harder, thus ScreenSuite does not match other sources.\r\nStart your custom evaluation in 30s ⚡️\r\nHead to the repository.\r\n- Clone the repository with submodules:\r\ngit clone --recurse-submodules git@github.com:huggingface/screensuite.git\r\n- Install the package:\r\nuv sync --extra submodules --python 3.11\r\n- Run\r\npython run.py\r\n- Alternatively, run\r\npython examples/run_benchmarks.py\r\nfor more fine-grained control, like running evaluations for several models in parallel.\r\n- Alternatively, run\r\nThe multistep benchmarks requires a bare-metal machine to run and deploy desktop/mobile* environment *emulators (see README.md)\r\nNext steps 🚀\r\nRunning consistent and meaningful evaluations easily allows the community to quickly iterate and make progress in this field, as we’ve seen with Eleuther LM evaluation harness, the Open LLM Leaderboard and the Chatbot Arena.\r\nWe hope to see much more capable open models in the coming month that can run a wide range of tasks reliably and even run locally!\r\nTo support this effort:\r\n- ⭐️ Go star the ScreenSuite repo and give us feedback in issues/PRs!\r\n- 👉 Follow the smolagents org to stay up-to-date.","src/content/posts/2025-06-20-screensuite---the-most-comprehensive-evaluation-suite-for-gui-agents!-8f58a7.md","a385b1f7e87f3c63",{html:617,metadata:618},"<p>ScreenSuite - The most comprehensive evaluation suite for GUI Agents!\r\nReleasing ScreenSuite, the most comprehensive evaluation suite for GUI Agents!\r\nTL;DR\r\nOver the past few weeks, we’ve been working tirelessly on making GUI agents more open, accessible and easy to integrate. Along the way, we created the largest benchmarking suite for GUI agents performances 👉 let us introduce ScreenSuite.\r\nWe are very excited to share it with you today: ScreenSuite is the most comprehensive and easiest way to evaluate Vision Language Models (VLMs)across many agentic capabilities!\r\nWTF is a GUI Agent?\r\nGUI Agents in action - courtesy of OSWorld\r\nIn short, an AI Agent is a robot that acts in the virtual world. (more thorough definition here)\r\nIn particular, a “GUI Agent” is an agent that lives in a GUI. Think “an agent that can do clicks and navigate on my desktop or my phone”, à la Claude Computer Use.\r\nThis means in essence that the AI model powering the agent will be given a task like “Fill the rest of this Excel column”, along with screen captures of the GUI. Using this information, it will then decide to take action on the system : click(x=130, y=540)\r\nto open a web browser, type(”Value for XYZ in 2025”)\r\n, scroll(down=2)\r\nto read further… To see a GUI agent in action, you can try our Open Computer Agent, powered by Qwen2.5-VL-72B.\r\nA good GUI agent will be able to navigate a computer just like we would, thus unlocking all computer tasks : scrolling through Google Maps, editing a file, buying an item online. This involves a variety of capabilities that can be hard to evaluate.\r\nIntroducing ScreenSuite 🥳\r\nThe literature, for instance Xu et al. (2025) or Qin et al. (2025), generally splits GUI agent abilities amongst several categories:</p>\n<ul>\n<li>Perception: correctly perceiving the informati displayed on screen</li>\n<li>Grounding: understanding the positioning of elements - this is paramount to click the correct place</li>\n<li>Single step actions: solving instructions correctly over one action</li>\n<li>Multi-step agents: solving a higher-level goal through several actions in a GUI environment.\r\nSo our first contribution is to gather and unify a comprehensive suite of 13 benchmarks spanning the full range of these GUI agent capabilities.\r\nIf you look at the last category listed above, evaluating Multi-step agentic capabilities is especially challenging as it requires virtual machines to run the agent’s environment, be it Windows, Android, Ubuntu… To address this, we provide support both for E2B desktop remote sandboxes, and we created from scratch a new option to easily launch Ubuntu or Android virtual machines in Docker!\r\n| Category | Benchmark | Environment | Sample count |\r\n|---|---|---|---|\r\n| Perception / Grounding 👁️ | ScreenQA-Short | Mobile | 8.4k |\r\n| ScreenQA-Complex | Mobile | 11.8k | |\r\n| ScreenSpot-v2 | Desktop | 1.3k | |\r\n| ScreenSpot-Pro | Desktop | 1.6k | |\r\n| WebSRC | Web | 52k | |\r\n| VisualWebBench | Web | 1.5k | |\r\n| Single-Step Actions 🎯 | Showdown-clicks | Web | 0.6k |\r\n| AndroidControl | Mobile | 3k | |\r\n| Multimodal-Mind2web | Web | 6.4k | |\r\n| Multi-Step Agents 🐾 | AndroidWorld (incl MobileMiniWob) | Mobile | 116 tasks, infinite |\r\n| OSWorld | Desktop | 369 | |\r\n| BrowseComp | Web | 1.27k | |\r\n| GAIA-Web | Web | 132 | |\r\n| Mind2Web-Live | Web | 208 |\r\nImplementation details\r\nWe’ve carefully designed our benchmark suite with modularity and consistency in mind, ensuring strong alignment across tasks and environments. When required, especially for online benchmarks, we leverage smolagents as framework layer to streamline agent execution and orchestration.\r\nTo support reproducibility and ease of use, we’ve built custom Dockerized containers that allow local deployment of full Ubuntu Desktop or Android environments.\r\nUnlike many existing GUI benchmarks that rely on accessibility trees or other metadata alongside visual input, our stack is intentionally vision-only. While this can result in different scores on some established leaderboards, we deem that it creates a more realistic and challenging setup, one that better reflects how humans perceive and interact with graphical interfaces.\r\n– All agentic frameworks (Android World, OSWorld, GAIAWeb, Mind2Web) use smolagents and rely solely on vision, without any accessibility tree or DOM added (in contrast with evaluation settings reported in other sources). – Mind2Web (Multimodal) originally used element-name-based multi-choice selection based on the accessibility tree and screenshots, but was later adapted to click precision within bounding boxes using vision only, which significantly increases task difficulty.\r\nRanking leading VLMs on ScreenSuite 📊\r\nWe’ve evaluated leading VLMs on the benchmark</li>\n<li>The Qwen-2.5-VL series of models from 3B to 72B. These models are known for their amazing localization capabilities, in other words they know the coordinates of any element in an image which makes them suited for GUI agents that need to click precisely.</li>\n<li>UI-Tars-1.5-7B, the all-rounder by ByteDance.</li>\n<li>Holo1-7B, the latest model by H company, showing extremely performant localization for its size.</li>\n<li>GPT-4o\r\nOur scores are in general agreement with the scores reported in various sources! With the caveat that we evaluate on vision only, causing some differences, see implementation details above.\r\n💡 Note that ScreenSuite does not intend to exactly reproduce benchmarks published in the industry: we evaluate models on GUI agentic capabilities based on vision. As a result, on benchmarks like Mind2Web where other benchmarks gave the agent a view of information rich context like DOM or accessibility tree, our evaluation setting is much harder, thus ScreenSuite does not match other sources.\r\nStart your custom evaluation in 30s ⚡️\r\nHead to the repository.</li>\n<li>Clone the repository with submodules:\r\ngit clone —recurse-submodules <a href=\"mailto:git@github.com\">git@github.com</a>:huggingface/screensuite.git</li>\n<li>Install the package:\r\nuv sync —extra submodules —python 3.11</li>\n<li>Run\r\npython run.py</li>\n<li>Alternatively, run\r\npython examples/run_benchmarks.py\r\nfor more fine-grained control, like running evaluations for several models in parallel.</li>\n<li>Alternatively, run\r\nThe multistep benchmarks requires a bare-metal machine to run and deploy desktop/mobile* environment *emulators (see README.md)\r\nNext steps 🚀\r\nRunning consistent and meaningful evaluations easily allows the community to quickly iterate and make progress in this field, as we’ve seen with Eleuther LM evaluation harness, the Open LLM Leaderboard and the Chatbot Arena.\r\nWe hope to see much more capable open models in the coming month that can run a wide range of tasks reliably and even run locally!\r\nTo support this effort:</li>\n<li>⭐️ Go star the ScreenSuite repo and give us feedback in issues/PRs!</li>\n<li>👉 Follow the smolagents org to stay up-to-date.</li>\n</ul>",{headings:619,localImagePaths:620,remoteImagePaths:621,frontmatter:622,imagePaths:625},[],[],[],{title:609,summary:15,pubDate:623,media:17,tags:624,link:612,thumbnail:15},"Fri, 06 Jun 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-screensuite---the-most-comprehensive-evaluation-suite-for-gui-agents!-8f58a7.md","2025-06-20-nanovlm-the-simplest-repository-to-train-your-vlm-in-pure-pytorch-46c426",{id:627,data:629,body:634,filePath:635,digest:636,rendered:637,legacyId:652},{title:630,summary:15,pubDate:631,media:17,tags:632,link:633,thumbnail:15},"nanoVLM: The simplest repository to train your VLM in pure PyTorch",["Date","2025-05-21T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/nanovlm","nanoVLM: The simplest repository to train your VLM in pure PyTorch\r\nnanoVLM is the simplest way to get started with training your very own Vision Language Model (VLM) using pure PyTorch. It is lightweight toolkit which allows you to launch a VLM training on a free tier colab notebook.\r\nWe were inspired by Andrej Karpathy’s nanoGPT, and provide a similar project for the vision domain.\r\nAt its heart, nanoVLM is a toolkit that helps you build and train a model that can understand both images and text, and then generate text based on that. The beauty of nanoVLM lies in its simplicity. The entire codebase is intentionally kept minimal and readable, making it perfect for beginners or anyone who wants to peek under the hood of VLMs without getting overwhelmed.\r\nIn this blog post, we cover the core ideas behind the project and provide a simple way to interact with the repository. We not only go into the details of the project but also encapsulate all of it so that you can quickly get started.\r\nTable of contents:\r\n- What is a Vision Language Model?\r\n- Working with the repository\r\n- Architecture\r\n- Train your own VLM\r\n- Run inference on a pre-trained model\r\n- Conclusion\r\n- References\r\nTL;DR\r\nYou can start training a Vision Language Model using our nanoVLM toolkit by following these steps:\r\n# Clone the repo\r\ngit clone https://github.com/huggingface/nanoVLM.git\r\n# Execute the training script\r\npython train.py\r\nHere is a Colab notebook that will help you launch a training run with no local setup required!\r\nWhat is a Vision Language Model?\r\nAs the name suggests, a Vision Language Model (VLM) is a multi-modal model that processes two modalities: vision and text. These models typically take images and/or text as input and generate text as output.\r\nGenerating text (output) conditioned on the understanding of images and texts (inputs) is a powerful paradigm. It enables a wide range of applications, from image captioning and object detection to answering questions about visual content (as shown in the table below). One thing to note is that nanoVLM focuses only on Visual Question Answering as the training objective.\r\n| Caption the image | Two cats lying down on a bed with remotes near them | Captioning | |\r\n| Detect the objects in the image | <locxx><locxx><locxx><locxx> |\r\nObject Detection | |\r\n| Segment the objects in the image | <segxx><segxx><segxx> |\r\nSemantic Segmentation | |\r\n| How many cats are in the image? | 2 | Visual Question Answering |\r\nIf you are interested in learning more about VLMs, we strongly recommend reading our latest blog on the topic: Vision Language Models (Better, Faster, Stronger)\r\nWorking with the repository\r\n\"Talk is cheap, show me the code\" - Linus Torvalds\r\nIn this section, we’ll guide you through the codebase. It’s helpful to keep a tab open for reference as you follow along.\r\nBelow is the folder structure of our repository. We have removed helper files for brevity.\r\n.\r\n├── data\r\n│ ├── collators.py\r\n│ ├── datasets.py\r\n│ └── processors.py\r\n├── generate.py\r\n├── models\r\n│ ├── config.py\r\n│ ├── language_model.py\r\n│ ├── modality_projector.py\r\n│ ├── utils.py\r\n│ ├── vision_language_model.py\r\n│ └── vision_transformer.py\r\n└── train.py\r\nArchitecture\r\n.\r\n├── data\r\n│ └── ...\r\n├── models # 👈 You are here\r\n│ └── ...\r\n└── train.py\r\nWe model nanoVLM after two well known and widely used architectures. Our vision backbone\r\n(models/vision_transformer.py\r\n) is the standard vision transformer, more specifically Google’s\r\nSigLIP vision encoder. Our language\r\nbackbone follows the Llama 3 architecture.\r\nThe vision and text modalities are aligned using a Modality Projection module. This module takes the image embeddings produced by the vision backbone as input, and transforms them into embeddings compatible with the text embeddings from the embedding layer of the language model. These embeddings are then concatenated and fed into the language decoder. The Modality Projection module consists of a pixel shuffle operation followed by a linear layer.\r\nPixel shuffle reduces the number of image tokens, which helps reduce computational cost and speeds up training, especially for transformer-based language decoders which are sensitive to input length. The figure below demonstrates the concept.\r\nAll the files are very lightweight and well documented. We highly encourage you to check them out\r\nindividually to get a better understanding of the implementation details (models/xxx.py\r\n)\r\nWhile training, we use the following pre-trained backbone weights:\r\n- Vision backbone:\r\ngoogle/siglip-base-patch16-224\r\n- Language backbone:\r\nHuggingFaceTB/SmolLM2-135M\r\nOne could also swap out the backbones with other variants of SigLIP/SigLIP 2 (for the vision backbone) and SmolLM2 (for the language backbone).\r\nTrain your own VLM\r\nNow that we are familiar with the architecture, let's shift gears and talk about how to train your own Vision Language Model using train.py\r\n.\r\n.\r\n├── data\r\n│ └── ...\r\n├── models\r\n│ └── ...\r\n└── train.py # 👈 You are here\r\nYou can kick off training with:\r\npython train.py\r\nThis script is your one-stop shop for the entire training pipeline, including:\r\n- Dataset loading and preprocessing\r\n- Model initialization\r\n- Optimization and logging\r\nConfiguration\r\nBefore anything else, the script loads two configuration classes from models/config.py\r\n:\r\nTrainConfig\r\n: Configuration parameters useful for training, like learning rates, checkpoint paths, etc.VLMConfig\r\n: The configuration parameters used to initialize the VLM, like hidden dimensions, number of attention heads, etc.\r\nData Loading\r\nAt the heart of the data pipeline is the get_dataloaders\r\nfunction. It:\r\n- Loads datasets via Hugging Face’s\r\nload_dataset\r\nAPI. - Combines and shuffles multiple datasets (if provided).\r\n- Applies a train/val split via indexing.\r\n- Wraps them in custom datasets (\r\nVQADataset\r\n,MMStarDataset\r\n) and collators (VQACollator\r\n,MMStarCollator\r\n).\r\nA helpful flag here is\r\ndata_cutoff_idx\r\n, useful for debugging on small subsets.\r\nModel Initialization\r\nThe model is built via the VisionLanguageModel\r\nclass. If you're resuming from a checkpoint, it’s as easy as:\r\nfrom models.vision_language_model import VisionLanguageModel\r\nmodel = VisionLanguageModel.from_pretrained(model_path)\r\nOtherwise, you get a freshly initialized model with optionally preloaded backbones for both vision and language.\r\nOptimizer Setup: Two LRs\r\nBecause the modality projector (MP\r\n) is freshly initialized while the backbones are pre-trained, the\r\noptimizer is split into two parameter groups, each with its own learning rate:\r\n- A higher LR for the MP\r\n- A smaller LR for the encoder/decoder stack\r\nThis balance ensures the MP learns quickly while preserving knowledge in the vision and language backbones.\r\nTraining Loop\r\nThis part is fairly standard but thoughtfully structured:\r\n- Mixed precision is used with\r\ntorch.autocast\r\nto improve performance. - A cosine learning rate schedule with linear warmup is implemented via\r\nget_lr\r\n. - Token throughput (tokens/sec) is logged per batch for performance monitoring.\r\nEvery 250 steps (configurable), the model is evaluated on the validation and MMStar\r\ntest datasets. If accuracy improves, the model is checkpointed.\r\nLogging & Monitoring\r\nIf log_wandb\r\nis enabled, training stats like batch_loss\r\n, val_loss\r\n, accuracy\r\n, and tokens_per_second\r\nare logged to Weights & Biases for real-time tracking.\r\nRuns are auto-named using metadata like sample size, batch size, epoch count, learning rates, and the date,\r\nall handled by the helper get_run_name\r\n.\r\nPush to Hub\r\nUse the following to push the trained model to the Hub for others to find and test:\r\nmodel.save_pretrained(save_path)\r\nYou can easily push them using:\r\nmodel.push_to_hub(\"hub/id\")\r\nRun inference on a pre-trained model\r\nUsing nanoVLM as the toolkit, we have trained a model and published it to Hub.\r\nWe have used the google/siglip-base-patch16-224\r\nand HuggingFaceTB/SmolLM2-135M\r\nas backbones. The model was\r\ntrained this for ~6h on a single H100 GPU on ~1.7M samples of the cauldron.\r\nThis model isn't intended to compete with SoTA models, but rather to demystify the components and training process of VLMs.\r\n.\r\n├── data\r\n│ └── ...\r\n├── generate.py # 👈 You are here\r\n├── models\r\n│ └── ...\r\n└── ...\r\nLet’s run inference on the trained model using the generate.py\r\nscript. You can run the generation script using the following command:\r\npython generate.py\r\nThis will use the default arguments and run the query “What is this?” on the image assets/image.png\r\n.\r\nYou can use this script on your own images and prompts like so:\r\npython generate.py --image path/to/image.png --prompt \"You prompt here\"\r\nIf you want to visualize the heart of the script, it is just these lines:\r\nmodel = VisionLanguageModel.from_pretrained(source).to(device)\r\nmodel.eval()\r\ntokenizer = get_tokenizer(model.cfg.lm_tokenizer)\r\nimage_processor = get_image_processor(model.cfg.vit_img_size)\r\ntemplate = f\"Question: {args.prompt} Answer:\"\r\nencoded = tokenizer.batch_encode_plus([template], return_tensors=\"pt\")\r\ntokens = encoded[\"input_ids\"].to(device)\r\nimg = Image.open(args.image).convert(\"RGB\")\r\nimg_t = image_processor(img).unsqueeze(0).to(device)\r\nprint(\"\\nInput:\\n \", args.prompt, \"\\n\\nOutputs:\")\r\nfor i in range(args.generations):\r\ngen = model.generate(tokens, img_t, max_new_tokens=args.max_new_tokens)\r\nout = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\r\nprint(f\" >> Generation {i+1}: {out}\")\r\nWe create the model and set it to eval\r\n. Initialize the tokenizer, which tokenizes the text prompt,\r\nand the image processor, which is used to process the images. The next step is to process the inputs\r\nand run model.generate\r\nto generate the output text. Finally, decode the output using batch_decode\r\n.\r\n| Image | Prompt | Generation |\r\n|---|---|---|\r\n| What is this? | In the picture I can see the pink color bed sheet. I can see two cats lying on the bed sheet. | |\r\n| What is the woman doing? | Here in the middle she is performing yoga |\r\nIf you want to run inference on the trained model in a UI interface, here is the Hugging Face Space for you to interact with the model.\r\nConclusion\r\nIn this blog post, we walked through what VLMs are, explored the architecture choices that power nanoVLM, and unpacked the training and inference workflows in detail.\r\nBy keeping the codebase lightweight and readable, nanoVLM aims to serve as both a learning tool and a foundation you can build upon. Whether you’re looking to understand how multi-modal inputs are aligned, or you want to train a VLM on your own dataset, this repository gives you a head start.\r\nIf you try it out, build on top of it, or just have questions we’d love to hear from you. Happy tinkering!","src/content/posts/2025-06-20-nanovlm-the-simplest-repository-to-train-your-vlm-in-pure-pytorch-46c426.md","12e3cc01d260bead",{html:638,metadata:639},"<p>nanoVLM: The simplest repository to train your VLM in pure PyTorch\r\nnanoVLM is the simplest way to get started with training your very own Vision Language Model (VLM) using pure PyTorch. It is lightweight toolkit which allows you to launch a VLM training on a free tier colab notebook.\r\nWe were inspired by Andrej Karpathy’s nanoGPT, and provide a similar project for the vision domain.\r\nAt its heart, nanoVLM is a toolkit that helps you build and train a model that can understand both images and text, and then generate text based on that. The beauty of nanoVLM lies in its simplicity. The entire codebase is intentionally kept minimal and readable, making it perfect for beginners or anyone who wants to peek under the hood of VLMs without getting overwhelmed.\r\nIn this blog post, we cover the core ideas behind the project and provide a simple way to interact with the repository. We not only go into the details of the project but also encapsulate all of it so that you can quickly get started.\r\nTable of contents:</p>\n<ul>\n<li>What is a Vision Language Model?</li>\n<li>Working with the repository</li>\n<li>Architecture</li>\n<li>Train your own VLM</li>\n<li>Run inference on a pre-trained model</li>\n<li>Conclusion</li>\n<li>References\r\nTL;DR\r\nYou can start training a Vision Language Model using our nanoVLM toolkit by following these steps:</li>\n</ul>\n<h1 id=\"clone-the-repo\">Clone the repo</h1>\n<p>git clone <a href=\"https://github.com/huggingface/nanoVLM.git\">https://github.com/huggingface/nanoVLM.git</a></p>\n<h1 id=\"execute-the-training-script\">Execute the training script</h1>\n<p>python train.py\r\nHere is a Colab notebook that will help you launch a training run with no local setup required!\r\nWhat is a Vision Language Model?\r\nAs the name suggests, a Vision Language Model (VLM) is a multi-modal model that processes two modalities: vision and text. These models typically take images and/or text as input and generate text as output.\r\nGenerating text (output) conditioned on the understanding of images and texts (inputs) is a powerful paradigm. It enables a wide range of applications, from image captioning and object detection to answering questions about visual content (as shown in the table below). One thing to note is that nanoVLM focuses only on Visual Question Answering as the training objective.\r\n| Caption the image | Two cats lying down on a bed with remotes near them | Captioning | |\r\n| Detect the objects in the image | <locxx><locxx><locxx><locxx> |\r\nObject Detection | |\r\n| Segment the objects in the image | <segxx><segxx><segxx> |\r\nSemantic Segmentation | |\r\n| How many cats are in the image? | 2 | Visual Question Answering |\r\nIf you are interested in learning more about VLMs, we strongly recommend reading our latest blog on the topic: Vision Language Models (Better, Faster, Stronger)\r\nWorking with the repository\r\n“Talk is cheap, show me the code” - Linus Torvalds\r\nIn this section, we’ll guide you through the codebase. It’s helpful to keep a tab open for reference as you follow along.\r\nBelow is the folder structure of our repository. We have removed helper files for brevity.\r\n.\r\n├── data\r\n│ ├── collators.py\r\n│ ├── datasets.py\r\n│ └── processors.py\r\n├── generate.py\r\n├── models\r\n│ ├── config.py\r\n│ ├── language_model.py\r\n│ ├── modality_projector.py\r\n│ ├── utils.py\r\n│ ├── vision_language_model.py\r\n│ └── vision_transformer.py\r\n└── train.py\r\nArchitecture\r\n.\r\n├── data\r\n│ └── …\r\n├── models # 👈 You are here\r\n│ └── …\r\n└── train.py\r\nWe model nanoVLM after two well known and widely used architectures. Our vision backbone\r\n(models/vision_transformer.py\r\n) is the standard vision transformer, more specifically Google’s\r\nSigLIP vision encoder. Our language\r\nbackbone follows the Llama 3 architecture.\r\nThe vision and text modalities are aligned using a Modality Projection module. This module takes the image embeddings produced by the vision backbone as input, and transforms them into embeddings compatible with the text embeddings from the embedding layer of the language model. These embeddings are then concatenated and fed into the language decoder. The Modality Projection module consists of a pixel shuffle operation followed by a linear layer.\r\nPixel shuffle reduces the number of image tokens, which helps reduce computational cost and speeds up training, especially for transformer-based language decoders which are sensitive to input length. The figure below demonstrates the concept.\r\nAll the files are very lightweight and well documented. We highly encourage you to check them out\r\nindividually to get a better understanding of the implementation details (models/xxx.py\r\n)\r\nWhile training, we use the following pre-trained backbone weights:</segxx></segxx></segxx></locxx></locxx></locxx></locxx></p>\n<ul>\n<li>Vision backbone:\r\ngoogle/siglip-base-patch16-224</li>\n<li>Language backbone:\r\nHuggingFaceTB/SmolLM2-135M\r\nOne could also swap out the backbones with other variants of SigLIP/SigLIP 2 (for the vision backbone) and SmolLM2 (for the language backbone).\r\nTrain your own VLM\r\nNow that we are familiar with the architecture, let’s shift gears and talk about how to train your own Vision Language Model using train.py\r\n.\r\n.\r\n├── data\r\n│ └── …\r\n├── models\r\n│ └── …\r\n└── train.py # 👈 You are here\r\nYou can kick off training with:\r\npython train.py\r\nThis script is your one-stop shop for the entire training pipeline, including:</li>\n<li>Dataset loading and preprocessing</li>\n<li>Model initialization</li>\n<li>Optimization and logging\r\nConfiguration\r\nBefore anything else, the script loads two configuration classes from models/config.py\r\n:\r\nTrainConfig\r\n: Configuration parameters useful for training, like learning rates, checkpoint paths, etc.VLMConfig\r\n: The configuration parameters used to initialize the VLM, like hidden dimensions, number of attention heads, etc.\r\nData Loading\r\nAt the heart of the data pipeline is the get_dataloaders\r\nfunction. It:</li>\n<li>Loads datasets via Hugging Face’s\r\nload_dataset\r\nAPI. - Combines and shuffles multiple datasets (if provided).</li>\n<li>Applies a train/val split via indexing.</li>\n<li>Wraps them in custom datasets (\r\nVQADataset\r\n,MMStarDataset\r\n) and collators (VQACollator\r\n,MMStarCollator\r\n).\r\nA helpful flag here is\r\ndata_cutoff_idx\r\n, useful for debugging on small subsets.\r\nModel Initialization\r\nThe model is built via the VisionLanguageModel\r\nclass. If you’re resuming from a checkpoint, it’s as easy as:\r\nfrom models.vision_language_model import VisionLanguageModel\r\nmodel = VisionLanguageModel.from_pretrained(model_path)\r\nOtherwise, you get a freshly initialized model with optionally preloaded backbones for both vision and language.\r\nOptimizer Setup: Two LRs\r\nBecause the modality projector (MP\r\n) is freshly initialized while the backbones are pre-trained, the\r\noptimizer is split into two parameter groups, each with its own learning rate:</li>\n<li>A higher LR for the MP</li>\n<li>A smaller LR for the encoder/decoder stack\r\nThis balance ensures the MP learns quickly while preserving knowledge in the vision and language backbones.\r\nTraining Loop\r\nThis part is fairly standard but thoughtfully structured:</li>\n<li>Mixed precision is used with\r\ntorch.autocast\r\nto improve performance. - A cosine learning rate schedule with linear warmup is implemented via\r\nget_lr\r\n. - Token throughput (tokens/sec) is logged per batch for performance monitoring.\r\nEvery 250 steps (configurable), the model is evaluated on the validation and MMStar\r\ntest datasets. If accuracy improves, the model is checkpointed.\r\nLogging &#x26; Monitoring\r\nIf log_wandb\r\nis enabled, training stats like batch_loss\r\n, val_loss\r\n, accuracy\r\n, and tokens_per_second\r\nare logged to Weights &#x26; Biases for real-time tracking.\r\nRuns are auto-named using metadata like sample size, batch size, epoch count, learning rates, and the date,\r\nall handled by the helper get_run_name\r\n.\r\nPush to Hub\r\nUse the following to push the trained model to the Hub for others to find and test:\r\nmodel.save_pretrained(save_path)\r\nYou can easily push them using:\r\nmodel.push_to_hub(“hub/id”)\r\nRun inference on a pre-trained model\r\nUsing nanoVLM as the toolkit, we have trained a model and published it to Hub.\r\nWe have used the google/siglip-base-patch16-224\r\nand HuggingFaceTB/SmolLM2-135M\r\nas backbones. The model was\r\ntrained this for ~6h on a single H100 GPU on ~1.7M samples of the cauldron.\r\nThis model isn’t intended to compete with SoTA models, but rather to demystify the components and training process of VLMs.\r\n.\r\n├── data\r\n│ └── …\r\n├── generate.py # 👈 You are here\r\n├── models\r\n│ └── …\r\n└── …\r\nLet’s run inference on the trained model using the generate.py\r\nscript. You can run the generation script using the following command:\r\npython generate.py\r\nThis will use the default arguments and run the query “What is this?” on the image assets/image.png\r\n.\r\nYou can use this script on your own images and prompts like so:\r\npython generate.py —image path/to/image.png —prompt “You prompt here”\r\nIf you want to visualize the heart of the script, it is just these lines:\r\nmodel = VisionLanguageModel.from_pretrained(source).to(device)\r\nmodel.eval()\r\ntokenizer = get_tokenizer(model.cfg.lm_tokenizer)\r\nimage_processor = get_image_processor(model.cfg.vit_img_size)\r\ntemplate = f”Question: {args.prompt} Answer:”\r\nencoded = tokenizer.batch_encode_plus([template], return_tensors=“pt”)\r\ntokens = encoded[“input_ids”].to(device)\r\nimg = Image.open(args.image).convert(“RGB”)\r\nimg_t = image_processor(img).unsqueeze(0).to(device)\r\nprint(“\\nInput:\\n ”, args.prompt, “\\n\\nOutputs:”)\r\nfor i in range(args.generations):\r\ngen = model.generate(tokens, img_t, max_new_tokens=args.max_new_tokens)\r\nout = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\r\nprint(f” >> Generation {i+1}: {out}”)\r\nWe create the model and set it to eval\r\n. Initialize the tokenizer, which tokenizes the text prompt,\r\nand the image processor, which is used to process the images. The next step is to process the inputs\r\nand run model.generate\r\nto generate the output text. Finally, decode the output using batch_decode\r\n.\r\n| Image | Prompt | Generation |\r\n|---|---|---|\r\n| What is this? | In the picture I can see the pink color bed sheet. I can see two cats lying on the bed sheet. | |\r\n| What is the woman doing? | Here in the middle she is performing yoga |\r\nIf you want to run inference on the trained model in a UI interface, here is the Hugging Face Space for you to interact with the model.\r\nConclusion\r\nIn this blog post, we walked through what VLMs are, explored the architecture choices that power nanoVLM, and unpacked the training and inference workflows in detail.\r\nBy keeping the codebase lightweight and readable, nanoVLM aims to serve as both a learning tool and a foundation you can build upon. Whether you’re looking to understand how multi-modal inputs are aligned, or you want to train a VLM on your own dataset, this repository gives you a head start.\r\nIf you try it out, build on top of it, or just have questions we’d love to hear from you. Happy tinkering!</li>\n</ul>",{headings:640,localImagePaths:647,remoteImagePaths:648,frontmatter:649,imagePaths:651},[641,644],{depth:31,slug:642,text:643},"clone-the-repo","Clone the repo",{depth:31,slug:645,text:646},"execute-the-training-script","Execute the training script",[],[],{title:630,summary:15,pubDate:181,media:17,tags:650,link:633,thumbnail:15},[19,20,21],[],"2025-06-20-nanovlm-the-simplest-repository-to-train-your-vlm-in-pure-pytorch-46c426.md","2025-06-20-smolvla-efficient-vision-language-action-model-trained-on-lerobot-community-data-04073c",{id:653,data:655,body:660,filePath:661,digest:662,rendered:663,legacyId:672},{title:656,summary:15,pubDate:657,media:17,tags:658,link:659,thumbnail:15},"SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data",["Date","2025-06-03T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/smolvla","SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data\r\n🧭TL;DR\r\nToday, we introduce SmolVLA, a compact (450M), open-source Vision-Language-Action model for robotics that runs on consumer hardware.\r\n- Pretrained only on compatibly licensed, open-source community-shared datasets under the lerobot tag.\r\n- SmolVLA-450M outperforms much larger VLAs and strong baselines such as ACT on simulation (LIBERO, Meta-World) and real-world tasks (SO100, SO101).\r\n- Supports asynchronous inference for 30% faster response and 2× task throughput.\r\nUseful links:\r\n- Hardware used to train and evaluate SO-100/101: https://github.com/TheRobotStudio/SO-ARM100\r\n- Base model https://huggingface.co/lerobot/smolvla_base\r\n- Paper: https://huggingface.co/papers/2506.01844\r\n📚 Table of Contents\r\n- 🧭 TL;DR\r\n- 📖 Introduction\r\n- 🤖 Meet SmolVLA\r\n- 🚀 How to Use SmolVLA?\r\n- 🧠 Method\r\n- 📦 Community Datasets\r\n- 📊 Results\r\n- ✅ Conclusion\r\n- 📣 Call to Action\r\nIntroduction\r\nOver the past few years, Transformers have driven remarkable progress in AI, from language models capable of human-like reasoning to multimodal systems that understand both images and text. However, in real-world robotics, advancements have been much slower. Robots still struggle to generalize across diverse objects, environments, and tasks. This limited progress stems from a lack of high-quality, diverse data and the absence of models that can reason and act like humans in the physical world.\r\nIn response to these challenges, the field has recently turned to vision-language-action (VLA) models, which aim to unify perception, language understanding, and action prediction within a single architecture. VLAs typically take as input raw visual observations and natural language instructions, and output corresponding robot actions. While promising, much of the recent progress in VLAs remains locked behind proprietary models trained on large-scale private datasets, often requiring costly hardware setups and extensive engineering resources. As a result, the broader robotics research community faces significant barriers in reproducing and building upon these models.\r\nSmolVLA addresses this gap by offering an open-source, compact, and efficient VLA model that can be trained on consumer-grade hardware using only publicly available datasets. By releasing not only model weights but also using very affordable open-source hardware, SmolVLA aims to democratize access to vision-language-action models and accelerate research toward generalist robotic agents.\r\nFigure 1: Comparison of SmolVLA across task variations. From left to right: (1) asynchronous pick-place cube counting, (2) synchronous pick-place cube counting, (3) pick-place cube counting under perturbations, and (4) generalization on pick-and-place of the lego block with real-world SO101.\r\nMeet SmolVLA!\r\nSmolVLA-450M is our open-source, compact yet capable VLA model. It is:\r\n- Small enough to run on CPU, train on a single consumer GPU, or even a MacBook!\r\n- Trained on public, community-shared robotics data\r\n- Released with full training and inference recipes\r\n- Can be tested and deployed on very affordable hardware (SO-100, SO-101, LeKiwi, etc.)\r\nInspired by the training paradigms of Large Language Models (LLMs), SmolVLA goes through a pretraining phase on general manipulation data, followed by task-specific post-training. Architecturally, it combines Transformers with flow-matching decoders, and is optimized for speed and low-latency inference with the following design choices:\r\n- Skipping half of the layers of the vision model for faster inference and smaller size\r\n- Interleaving self-attention and cross-attention blocks\r\n- Using fewer visual tokens\r\n- Leveraging smaller pretrained VLMs\r\nDespite using fewer than 30k training episodes—an order of magnitude less than other VLAs—SmolVLA matches or exceeds the performance of much larger models, both in simulation and the real world.\r\nTo make real-time robotics easier to use, we introduce an asynchronous inference stack. This technology separates how robots perform actions from how they understand what they see and hear. Because of this separation, robots can respond more quickly in fast-changing environments.\r\nFigure 2. SmolVLA takes as input a sequence of RGB images from multiple cameras, the robot’s current sensorimotor state, and a natural language instruction. The VLM encodes these into contextual features, which condition the action expert to generate a continuous sequence of actions.\r\n🚀 How to Use SmolVLA?\r\nSmolVLA is designed to be easy to use and integrate—whether you're finetuning on your own data or plugging it into an existing robotics stack.\r\nInstall\r\nFirst, install the required dependencies:\r\ngit clone https://github.com/huggingface/lerobot.git\r\ncd lerobot\r\npip install -e \".[smolvla]\"\r\nFinetune the pretrained model\r\nUse smolvla_base\r\n, our pretrained 450M model, with the lerobot training framework:\r\npython lerobot/scripts/train.py \\\r\n--policy.path=lerobot/smolvla_base \\\r\n--dataset.repo_id=lerobot/svla_so100_stacking \\\r\n--batch_size=64 \\\r\n--steps=20000 # 10% of training budget\r\nTrain from scratch\r\nIf you'd like to build from the architecture (pretrained VLM + action expert) rather than a pretrained checkpoint:\r\npython lerobot/scripts/train.py \\\r\n--policy.type=smolvla \\\r\n--dataset.repo_id=lerobot/svla_so100_stacking \\\r\n--batch_size=64 \\\r\n--steps=200000\r\nYou can also load SmolVLAPolicy\r\ndirectly:\r\nfrom lerobot.common.policies.smolvla.modeling_smolvla import SmolVLAPolicy\r\npolicy = SmolVLAPolicy.from_pretrained(\"lerobot/smolvla_base\")\r\nMethod\r\nSmolVLA is not only a lightweight yet capable model, but also a method for training and evaluating generalist robotics policies. In this section, we introduce the model architecture behind SmolVLA and the asynchronous inference setup used for evaluation, which has proven to be more adaptable and capable of faster recovery.\r\nSmolVLA consists of two core components: a Vision-Language Model (VLM) that processes multimodal inputs and an action expert that outputs robot control commands. Below, we share the details of the main components of SmolVLA architecture and the Asynchronous Inference. More details can be found in our technical report.\r\nMain Architecture\r\nVision-Language Model (VLM)\r\nWe use SmolVLM2 as our VLM backbone. It’s optimized for multi-image inputs and consists of a SigLIP vision encoder and a SmolLM2 language decoder.\r\n- Image tokens are extracted via the vision encoder\r\n- Language instructions are tokenized and fed directly into the decoder.\r\n- Sensorimotor states are projected into a single token using a linear layer to align with the token dimension of the language model.\r\nThe decoder layers process concatenated image, language, and state tokens. The resulting features are then passed to the action expert.\r\nAction Expert: Flow Matching Transformer\r\nSmolVLA’s action expert is a compact transformer (~100M parameters) that generates action chunks, i.e. sequences of future robot actions, conditioned on the VLM’s outputs. It is trained using a flow matching objective, which teaches the model to guide noisy samples back to the ground truth. In contrast, while discrete action representations (e.g., via tokenization) are powerful, they often require autoregressive decoding, which is slow and inefficient at inference time. While flow matching allows direct, non-autoregressive prediction of continuous actions, enabling real-time control with high precision.\r\nMore intuitively, during training, we add random noise to the robot’s real action sequences and ask the model to predict the “correction vector” that brings them back to the correct trajectory. This forms a smooth vector field over the action space, helping the model learn accurate and stable control policies.\r\nWe implement this using a transformer architecture with interleaved attention blocks (see the figure 2), and reduce its hidden size to 75% of the VLM’s, keeping the model lightweight for deployment.\r\nDesign Choices for Efficiency and Robustness\r\nWhile combining a vision-language model with an action prediction module is a common design pattern in recent VLA systems—such as Pi0, GR00T, Diffusion Policy — we identified several architectural choices that significantly enhance the robustness and performance. In SmolVLA, we apply three key techniques: reducing the number of visual tokens, skipping upper layers in the VLM, and interleaving cross- and self-attention layers in the action expert.\r\nVisual Token Reduction\r\nHigh-resolution images improve perception but can significantly slow down inference. To strike a balance, SmolVLA limits the number of visual tokens to 64 per frame during both training and inference. For example, a 512×512 image is compressed into just 64 tokens, instead of 1024, using PixelShuffle as an efficient shuffling technique. While the underlying Vision-Language Model (VLM) was originally pretrained using image tiling for broader coverage, SmolVLA uses only the global image at runtime to keep inference lightweight and fast.\r\nFaster Inference via Layer Skipping\r\nRather than always relying on the final layer of the VLM—which can be expensive and sometimes suboptimal—we use features from intermediate layers. Prior work has shown that early layers often provide better representations for downstream tasks. In SmolVLA, the action expert only attends to VLM features up to a configurable layer NN during training, set to half the total layers. This halves the compute cost of both the VLM and the action expert, significantly speeding up inference with minimal performance loss.\r\nInterleaved Cross and Self-Attention\r\nInside the action expert, attention layers alternate between:\r\n- Cross-attention (CA), where action tokens attend to the VLM’s features\r\n- Self-attention (SA), where action tokens attend to each other (causally—only to the past)\r\nWe found that this interleaved design is both lighter and more effective than using full attention blocks. Models that rely only on CA or only on SA tend to sacrifice either smoothness or grounding.\r\nIn SmolVLA, CA ensures that actions are well-conditioned on perception and instructions, while SA improves temporal smoothness—especially critical for real-world control, where jittery predictions can result in unsafe or unstable behavior.\r\nAsynchronous Inference\r\nFigure 3. Asynchronous inference. Illustration of the asynchronous inference stack. Note that the policy can be run on a remote server, possibly with GPUs.\r\nModern visuomotor policies output action chunks—sequences of actions to execute. There are two ways to manage them:\r\n- Synchronous (sync): The robot executes a chunk, then pauses while the next one is computed. Simple, but causes a delay where the robot can't react to new inputs.\r\n- Asynchronous (async): While executing the current chunk, the robot already sends the latest observation to a Policy Server (possibly hosted on GPU) for the next chunk. This avoids idle time and improves reactivity.\r\nOur async stack decouples action execution from chunk prediction, resulting in higher adaptability, and the complete lack of execution lags at runtime. It relies on the following key mechanisms:\r\n- 1. Early trigger: When the queue length falls below a threshold (e.g., 70%), we send an observation to a Policy Server, calling for a new action chunk.\r\n- 2. Decoupled threads: Control loop keeps executing → inference happens in parallel (non-blocking).\r\n- 3. Chunk fusion: Overlapping actions from successive chunks are stitched with a simple merge rule to avoid jitter.\r\nWe are really excited about releasing asynchronous inference because it guarantees greater adaptability and improved performance without changing the model. In short, async inference keeps the robot responsive by overlapping execution and remote prediction.\r\nCommunity Datasets\r\nWhile vision and language models thrive on web-scale datasets like LAION, ImageNet, and Common Crawl, robotics lacks a comparable resource. There’s no “Internet of robots.” Instead, data is fragmented across robot types, sensors, control schemes, and formats—forming disconnected \"data islands\". In our previous post, we explored how this fragmentation could be resolved through open, collaborative efforts. Just as ImageNet catalyzed breakthroughs in computer vision by providing a large, diverse benchmark, we believe that community-driven robotics datasets can play the same foundational role for generalist robot policies.\r\nSmolVLA is our first step toward that vision: It is pretrained on a curated mix of publicly available, community-contributed datasets designed to reflect real-world variation. Rather than optimizing for dataset size alone, we focus on diversity: a range of behaviors, camera viewpoints, and embodiments that promote transfer and generalization.\r\nAll training data used in SmolVLA comes from LeRobot Community Datasets , robotics datasets shared on the Hugging Face Hub under the lerobot\r\ntag. Collected in diverse settings, from labs to living rooms, these datasets represent an open, decentralized effort to scale real-world robot data.\r\nFigure 4. A glimpse of the community dataset. Special thanks to Ville Kuosmanen for creating the visualization.\r\nUnlike academic benchmarks, community datasets naturally capture messy, realistic interactions: varied lighting, suboptimal demonstrations, unconventional objects, and heterogeneous control schemes. This kind of diversity will be very useful for learning robust, general-purpose representations.\r\nWe used a customfiltering tool created by Alexandre Chapin and Ville Kuosmanen to select datasets based on frame count, visual quality, and task coverage. After a meticulous manual review (special thanks to Marina Barannikov), we curated a collection of 487 high-quality datasets focused on the SO100 robotic arm, standardized at 30 FPS. This yielded around 10 million frames—at least one order of magnitude smaller than other popular benchmark datasets, yet significantly more diverse.\r\nImproving Task Annotations\r\nA common issue across community datasets was noisy or missing task descriptions. Many episodes lacked annotations or included vague labels like “task desc” or “Move”, “Pick”. To improve quality and standardize the textual input across datasets, we used Qwen2.5-VL-3B-Instruct to generate concise, action-oriented descriptions.\r\nGiven sample frames and the original label, the model was prompted to rewrite the instruction in under 30 characters, starting with an action verb (e.g., “Pick,” “Place,” “Open”).\r\nThe prompt used is:\r\nHere is a current task description: {current_task}. Generate a very short, clear, and complete one-sentence describing the action performed by the robot arm (max 30 characters). Do not include unnecessary words.\r\nBe concise.\r\nHere are some examples: Pick up the cube and place it in the box, open the drawer and so on.\r\nStart directly with an action verb like “Pick”, “Place”, “Open”, etc.\r\nSimilar to the provided examples, what is the main action done by the robot arm?\r\nStandardizing Camera Views\r\nAnother challenge was inconsistent camera naming. Some datasets used clear names like top or wrist.right\r\n, while others used ambiguous labels like images.laptop\r\n, which varied in meaning.\r\nTo fix this, we manually went through the datasets and mapped each camera view to a standardized scheme:\r\nOBS_IMAGE_1\r\n: Top-down view\r\nOBS_IMAGE_2\r\n: Wrist-mounted view\r\nOBS_IMAGE_3+\r\n: Additional viewpoints\r\nWe further isolate the contributions of community dataset pretraining and multitask finetuning. Without pretraining on the LeRobot community datasets, SmolVLA initially achieves 51.7% success on SO100. After pretraining on community-collected data, performance jumps to 78.3%, a +26.6% absolute improvement. Multitask finetuning further boosts performance, showing strong task transfer capabilities even in low-data regimes.\r\nTable 1. Impact of Pretraining on Community Datasets and Multitask Finetuning.\r\nResults\r\nWe evaluate SmolVLA across simulation and real-world benchmarks to test its generalization, efficiency, and robustness. Despite being compact, It consistently outperforms or matches the performance of significantly larger models and policies pretrained on higher-scale robotics data.\r\nTable 2. SmolVLA Performance on Simulation Benchmarks.\r\nTable 3. SmolVLA vs Baselines on Real-World Tasks (SO100).\r\nIn real-world settings, SmolVLA is evaluated on two diverse suites: SO100 and SO101. These tasks include pick-place, stacking, and sorting, with both in-distribution and out-of-distribution object configurations. On SO101, SmolVLA also excels in generalization:\r\nTable 4. Generalization of SmolVLA to New Embodiment (SO101) vs ACT..\r\nFinally, we evaluate SmolVLA under synchronous and asynchronous inference modes. Async inference decouples action execution from model inference, allowing the policy to react while the robot is moving.\r\n- Both modes achieve similar task success (≈78%), but async inference:\r\n- Completes tasks ~30% faster (9.7s vs. 13.75s)\r\n- Enables 2× more completions in fixed-time settings (19 vs. 9 cubes)\r\nThis results in more responsive and robust real-world performance, especially in dynamic environments with shifting objects or external disturbances.\r\nFigure 5. Asynchronous vs. Synchronous Inference in Real-World Tasks. (a) Task success rates (%), (b) average completion time(s), and (c) number of tasks completed within a fixed time window.\r\nConclusion\r\nSmolVLA is our contribution to building robotics foundation models that are open, efficient, and reproducible. Despite its small size, it matches or outperforms larger, proprietary models across a range of real-world and simulated tasks. By relying solely on community-contributed datasets and affordable hardware, SmolVLA lowers the barrier to entry for researchers, educators, and hobbyists alike. But this is just the beginning. SmolVLA is more than just a model — it's part of a growing open-source movement toward scalable, collaborative robotics.\r\nCall to Action:\r\n- 🔧 Try it out! Finetune SmolVLA on your own data, deploy it on affordable hardware, or benchmark it against your current stack and share it on twitter/linkedin.\r\n- 🤖 Upload the dataset! Got a robot? Collect and share your data using the lerobot format. Help expand the community dataset that powers SmolVLA.\r\n- 💬 Join the blog discussion. Drop your questions, ideas, or feedback in the discussion below. We’re happy to help with integration, training, or deployment.\r\n- 📊 Contribute. Improve datasets, report issues, suggest new ideas. Every contribution helps.\r\n- 🌍 Spread the word. Share SmolVLA with fellow researchers, developers, or educators interested in efficient, real-time robotic policies.\r\n- 📫 Stay in touch: Follow the LeRobot organization and Discord server for updates, tutorials, and new releases.\r\nTogether, we can make real-world robotics more capable, more affordable, and more open. ✨","src/content/posts/2025-06-20-smolvla-efficient-vision-language-action-model-trained-on-lerobot-community-data-04073c.md","bb3923da6beacfbb",{html:664,metadata:665},"<p>SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data\r\n🧭TL;DR\r\nToday, we introduce SmolVLA, a compact (450M), open-source Vision-Language-Action model for robotics that runs on consumer hardware.</p>\n<ul>\n<li>Pretrained only on compatibly licensed, open-source community-shared datasets under the lerobot tag.</li>\n<li>SmolVLA-450M outperforms much larger VLAs and strong baselines such as ACT on simulation (LIBERO, Meta-World) and real-world tasks (SO100, SO101).</li>\n<li>Supports asynchronous inference for 30% faster response and 2× task throughput.\r\nUseful links:</li>\n<li>Hardware used to train and evaluate SO-100/101: <a href=\"https://github.com/TheRobotStudio/SO-ARM100\">https://github.com/TheRobotStudio/SO-ARM100</a></li>\n<li>Base model <a href=\"https://huggingface.co/lerobot/smolvla_base\">https://huggingface.co/lerobot/smolvla_base</a></li>\n<li>Paper: <a href=\"https://huggingface.co/papers/2506.01844\">https://huggingface.co/papers/2506.01844</a>\r\n📚 Table of Contents</li>\n<li>🧭 TL;DR</li>\n<li>📖 Introduction</li>\n<li>🤖 Meet SmolVLA</li>\n<li>🚀 How to Use SmolVLA?</li>\n<li>🧠 Method</li>\n<li>📦 Community Datasets</li>\n<li>📊 Results</li>\n<li>✅ Conclusion</li>\n<li>📣 Call to Action\r\nIntroduction\r\nOver the past few years, Transformers have driven remarkable progress in AI, from language models capable of human-like reasoning to multimodal systems that understand both images and text. However, in real-world robotics, advancements have been much slower. Robots still struggle to generalize across diverse objects, environments, and tasks. This limited progress stems from a lack of high-quality, diverse data and the absence of models that can reason and act like humans in the physical world.\r\nIn response to these challenges, the field has recently turned to vision-language-action (VLA) models, which aim to unify perception, language understanding, and action prediction within a single architecture. VLAs typically take as input raw visual observations and natural language instructions, and output corresponding robot actions. While promising, much of the recent progress in VLAs remains locked behind proprietary models trained on large-scale private datasets, often requiring costly hardware setups and extensive engineering resources. As a result, the broader robotics research community faces significant barriers in reproducing and building upon these models.\r\nSmolVLA addresses this gap by offering an open-source, compact, and efficient VLA model that can be trained on consumer-grade hardware using only publicly available datasets. By releasing not only model weights but also using very affordable open-source hardware, SmolVLA aims to democratize access to vision-language-action models and accelerate research toward generalist robotic agents.\r\nFigure 1: Comparison of SmolVLA across task variations. From left to right: (1) asynchronous pick-place cube counting, (2) synchronous pick-place cube counting, (3) pick-place cube counting under perturbations, and (4) generalization on pick-and-place of the lego block with real-world SO101.\r\nMeet SmolVLA!\r\nSmolVLA-450M is our open-source, compact yet capable VLA model. It is:</li>\n<li>Small enough to run on CPU, train on a single consumer GPU, or even a MacBook!</li>\n<li>Trained on public, community-shared robotics data</li>\n<li>Released with full training and inference recipes</li>\n<li>Can be tested and deployed on very affordable hardware (SO-100, SO-101, LeKiwi, etc.)\r\nInspired by the training paradigms of Large Language Models (LLMs), SmolVLA goes through a pretraining phase on general manipulation data, followed by task-specific post-training. Architecturally, it combines Transformers with flow-matching decoders, and is optimized for speed and low-latency inference with the following design choices:</li>\n<li>Skipping half of the layers of the vision model for faster inference and smaller size</li>\n<li>Interleaving self-attention and cross-attention blocks</li>\n<li>Using fewer visual tokens</li>\n<li>Leveraging smaller pretrained VLMs\r\nDespite using fewer than 30k training episodes—an order of magnitude less than other VLAs—SmolVLA matches or exceeds the performance of much larger models, both in simulation and the real world.\r\nTo make real-time robotics easier to use, we introduce an asynchronous inference stack. This technology separates how robots perform actions from how they understand what they see and hear. Because of this separation, robots can respond more quickly in fast-changing environments.\r\nFigure 2. SmolVLA takes as input a sequence of RGB images from multiple cameras, the robot’s current sensorimotor state, and a natural language instruction. The VLM encodes these into contextual features, which condition the action expert to generate a continuous sequence of actions.\r\n🚀 How to Use SmolVLA?\r\nSmolVLA is designed to be easy to use and integrate—whether you’re finetuning on your own data or plugging it into an existing robotics stack.\r\nInstall\r\nFirst, install the required dependencies:\r\ngit clone <a href=\"https://github.com/huggingface/lerobot.git\">https://github.com/huggingface/lerobot.git</a>\r\ncd lerobot\r\npip install -e ”.[smolvla]”\r\nFinetune the pretrained model\r\nUse smolvla_base\r\n, our pretrained 450M model, with the lerobot training framework:\r\npython lerobot/scripts/train.py <br>\n—policy.path=lerobot/smolvla_base <br>\n—dataset.repo_id=lerobot/svla_so100_stacking <br>\n—batch_size=64 <br>\n—steps=20000 # 10% of training budget\r\nTrain from scratch\r\nIf you’d like to build from the architecture (pretrained VLM + action expert) rather than a pretrained checkpoint:\r\npython lerobot/scripts/train.py <br>\n—policy.type=smolvla <br>\n—dataset.repo_id=lerobot/svla_so100_stacking <br>\n—batch_size=64 <br>\n—steps=200000\r\nYou can also load SmolVLAPolicy\r\ndirectly:\r\nfrom lerobot.common.policies.smolvla.modeling_smolvla import SmolVLAPolicy\r\npolicy = SmolVLAPolicy.from_pretrained(“lerobot/smolvla_base”)\r\nMethod\r\nSmolVLA is not only a lightweight yet capable model, but also a method for training and evaluating generalist robotics policies. In this section, we introduce the model architecture behind SmolVLA and the asynchronous inference setup used for evaluation, which has proven to be more adaptable and capable of faster recovery.\r\nSmolVLA consists of two core components: a Vision-Language Model (VLM) that processes multimodal inputs and an action expert that outputs robot control commands. Below, we share the details of the main components of SmolVLA architecture and the Asynchronous Inference. More details can be found in our technical report.\r\nMain Architecture\r\nVision-Language Model (VLM)\r\nWe use SmolVLM2 as our VLM backbone. It’s optimized for multi-image inputs and consists of a SigLIP vision encoder and a SmolLM2 language decoder.</li>\n<li>Image tokens are extracted via the vision encoder</li>\n<li>Language instructions are tokenized and fed directly into the decoder.</li>\n<li>Sensorimotor states are projected into a single token using a linear layer to align with the token dimension of the language model.\r\nThe decoder layers process concatenated image, language, and state tokens. The resulting features are then passed to the action expert.\r\nAction Expert: Flow Matching Transformer\r\nSmolVLA’s action expert is a compact transformer (~100M parameters) that generates action chunks, i.e. sequences of future robot actions, conditioned on the VLM’s outputs. It is trained using a flow matching objective, which teaches the model to guide noisy samples back to the ground truth. In contrast, while discrete action representations (e.g., via tokenization) are powerful, they often require autoregressive decoding, which is slow and inefficient at inference time. While flow matching allows direct, non-autoregressive prediction of continuous actions, enabling real-time control with high precision.\r\nMore intuitively, during training, we add random noise to the robot’s real action sequences and ask the model to predict the “correction vector” that brings them back to the correct trajectory. This forms a smooth vector field over the action space, helping the model learn accurate and stable control policies.\r\nWe implement this using a transformer architecture with interleaved attention blocks (see the figure 2), and reduce its hidden size to 75% of the VLM’s, keeping the model lightweight for deployment.\r\nDesign Choices for Efficiency and Robustness\r\nWhile combining a vision-language model with an action prediction module is a common design pattern in recent VLA systems—such as Pi0, GR00T, Diffusion Policy — we identified several architectural choices that significantly enhance the robustness and performance. In SmolVLA, we apply three key techniques: reducing the number of visual tokens, skipping upper layers in the VLM, and interleaving cross- and self-attention layers in the action expert.\r\nVisual Token Reduction\r\nHigh-resolution images improve perception but can significantly slow down inference. To strike a balance, SmolVLA limits the number of visual tokens to 64 per frame during both training and inference. For example, a 512×512 image is compressed into just 64 tokens, instead of 1024, using PixelShuffle as an efficient shuffling technique. While the underlying Vision-Language Model (VLM) was originally pretrained using image tiling for broader coverage, SmolVLA uses only the global image at runtime to keep inference lightweight and fast.\r\nFaster Inference via Layer Skipping\r\nRather than always relying on the final layer of the VLM—which can be expensive and sometimes suboptimal—we use features from intermediate layers. Prior work has shown that early layers often provide better representations for downstream tasks. In SmolVLA, the action expert only attends to VLM features up to a configurable layer NN during training, set to half the total layers. This halves the compute cost of both the VLM and the action expert, significantly speeding up inference with minimal performance loss.\r\nInterleaved Cross and Self-Attention\r\nInside the action expert, attention layers alternate between:</li>\n<li>Cross-attention (CA), where action tokens attend to the VLM’s features</li>\n<li>Self-attention (SA), where action tokens attend to each other (causally—only to the past)\r\nWe found that this interleaved design is both lighter and more effective than using full attention blocks. Models that rely only on CA or only on SA tend to sacrifice either smoothness or grounding.\r\nIn SmolVLA, CA ensures that actions are well-conditioned on perception and instructions, while SA improves temporal smoothness—especially critical for real-world control, where jittery predictions can result in unsafe or unstable behavior.\r\nAsynchronous Inference\r\nFigure 3. Asynchronous inference. Illustration of the asynchronous inference stack. Note that the policy can be run on a remote server, possibly with GPUs.\r\nModern visuomotor policies output action chunks—sequences of actions to execute. There are two ways to manage them:</li>\n<li>Synchronous (sync): The robot executes a chunk, then pauses while the next one is computed. Simple, but causes a delay where the robot can’t react to new inputs.</li>\n<li>Asynchronous (async): While executing the current chunk, the robot already sends the latest observation to a Policy Server (possibly hosted on GPU) for the next chunk. This avoids idle time and improves reactivity.\r\nOur async stack decouples action execution from chunk prediction, resulting in higher adaptability, and the complete lack of execution lags at runtime. It relies on the following key mechanisms:</li>\n<li>\n<ol>\n<li>Early trigger: When the queue length falls below a threshold (e.g., 70%), we send an observation to a Policy Server, calling for a new action chunk.</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>Decoupled threads: Control loop keeps executing → inference happens in parallel (non-blocking).</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>Chunk fusion: Overlapping actions from successive chunks are stitched with a simple merge rule to avoid jitter.\r\nWe are really excited about releasing asynchronous inference because it guarantees greater adaptability and improved performance without changing the model. In short, async inference keeps the robot responsive by overlapping execution and remote prediction.\r\nCommunity Datasets\r\nWhile vision and language models thrive on web-scale datasets like LAION, ImageNet, and Common Crawl, robotics lacks a comparable resource. There’s no “Internet of robots.” Instead, data is fragmented across robot types, sensors, control schemes, and formats—forming disconnected “data islands”. In our previous post, we explored how this fragmentation could be resolved through open, collaborative efforts. Just as ImageNet catalyzed breakthroughs in computer vision by providing a large, diverse benchmark, we believe that community-driven robotics datasets can play the same foundational role for generalist robot policies.\r\nSmolVLA is our first step toward that vision: It is pretrained on a curated mix of publicly available, community-contributed datasets designed to reflect real-world variation. Rather than optimizing for dataset size alone, we focus on diversity: a range of behaviors, camera viewpoints, and embodiments that promote transfer and generalization.\r\nAll training data used in SmolVLA comes from LeRobot Community Datasets , robotics datasets shared on the Hugging Face Hub under the lerobot\r\ntag. Collected in diverse settings, from labs to living rooms, these datasets represent an open, decentralized effort to scale real-world robot data.\r\nFigure 4. A glimpse of the community dataset. Special thanks to Ville Kuosmanen for creating the visualization.\r\nUnlike academic benchmarks, community datasets naturally capture messy, realistic interactions: varied lighting, suboptimal demonstrations, unconventional objects, and heterogeneous control schemes. This kind of diversity will be very useful for learning robust, general-purpose representations.\r\nWe used a customfiltering tool created by Alexandre Chapin and Ville Kuosmanen to select datasets based on frame count, visual quality, and task coverage. After a meticulous manual review (special thanks to Marina Barannikov), we curated a collection of 487 high-quality datasets focused on the SO100 robotic arm, standardized at 30 FPS. This yielded around 10 million frames—at least one order of magnitude smaller than other popular benchmark datasets, yet significantly more diverse.\r\nImproving Task Annotations\r\nA common issue across community datasets was noisy or missing task descriptions. Many episodes lacked annotations or included vague labels like “task desc” or “Move”, “Pick”. To improve quality and standardize the textual input across datasets, we used Qwen2.5-VL-3B-Instruct to generate concise, action-oriented descriptions.\r\nGiven sample frames and the original label, the model was prompted to rewrite the instruction in under 30 characters, starting with an action verb (e.g., “Pick,” “Place,” “Open”).\r\nThe prompt used is:\r\nHere is a current task description: {current_task}. Generate a very short, clear, and complete one-sentence describing the action performed by the robot arm (max 30 characters). Do not include unnecessary words.\r\nBe concise.\r\nHere are some examples: Pick up the cube and place it in the box, open the drawer and so on.\r\nStart directly with an action verb like “Pick”, “Place”, “Open”, etc.\r\nSimilar to the provided examples, what is the main action done by the robot arm?\r\nStandardizing Camera Views\r\nAnother challenge was inconsistent camera naming. Some datasets used clear names like top or wrist.right\r\n, while others used ambiguous labels like images.laptop\r\n, which varied in meaning.\r\nTo fix this, we manually went through the datasets and mapped each camera view to a standardized scheme:\r\nOBS_IMAGE_1\r\n: Top-down view\r\nOBS_IMAGE_2\r\n: Wrist-mounted view\r\nOBS_IMAGE_3+\r\n: Additional viewpoints\r\nWe further isolate the contributions of community dataset pretraining and multitask finetuning. Without pretraining on the LeRobot community datasets, SmolVLA initially achieves 51.7% success on SO100. After pretraining on community-collected data, performance jumps to 78.3%, a +26.6% absolute improvement. Multitask finetuning further boosts performance, showing strong task transfer capabilities even in low-data regimes.\r\nTable 1. Impact of Pretraining on Community Datasets and Multitask Finetuning.\r\nResults\r\nWe evaluate SmolVLA across simulation and real-world benchmarks to test its generalization, efficiency, and robustness. Despite being compact, It consistently outperforms or matches the performance of significantly larger models and policies pretrained on higher-scale robotics data.\r\nTable 2. SmolVLA Performance on Simulation Benchmarks.\r\nTable 3. SmolVLA vs Baselines on Real-World Tasks (SO100).\r\nIn real-world settings, SmolVLA is evaluated on two diverse suites: SO100 and SO101. These tasks include pick-place, stacking, and sorting, with both in-distribution and out-of-distribution object configurations. On SO101, SmolVLA also excels in generalization:\r\nTable 4. Generalization of SmolVLA to New Embodiment (SO101) vs ACT..\r\nFinally, we evaluate SmolVLA under synchronous and asynchronous inference modes. Async inference decouples action execution from model inference, allowing the policy to react while the robot is moving.</li>\n</ol>\n</li>\n<li>Both modes achieve similar task success (≈78%), but async inference:</li>\n<li>Completes tasks ~30% faster (9.7s vs. 13.75s)</li>\n<li>Enables 2× more completions in fixed-time settings (19 vs. 9 cubes)\r\nThis results in more responsive and robust real-world performance, especially in dynamic environments with shifting objects or external disturbances.\r\nFigure 5. Asynchronous vs. Synchronous Inference in Real-World Tasks. (a) Task success rates (%), (b) average completion time(s), and (c) number of tasks completed within a fixed time window.\r\nConclusion\r\nSmolVLA is our contribution to building robotics foundation models that are open, efficient, and reproducible. Despite its small size, it matches or outperforms larger, proprietary models across a range of real-world and simulated tasks. By relying solely on community-contributed datasets and affordable hardware, SmolVLA lowers the barrier to entry for researchers, educators, and hobbyists alike. But this is just the beginning. SmolVLA is more than just a model — it’s part of a growing open-source movement toward scalable, collaborative robotics.\r\nCall to Action:</li>\n<li>🔧 Try it out! Finetune SmolVLA on your own data, deploy it on affordable hardware, or benchmark it against your current stack and share it on twitter/linkedin.</li>\n<li>🤖 Upload the dataset! Got a robot? Collect and share your data using the lerobot format. Help expand the community dataset that powers SmolVLA.</li>\n<li>💬 Join the blog discussion. Drop your questions, ideas, or feedback in the discussion below. We’re happy to help with integration, training, or deployment.</li>\n<li>📊 Contribute. Improve datasets, report issues, suggest new ideas. Every contribution helps.</li>\n<li>🌍 Spread the word. Share SmolVLA with fellow researchers, developers, or educators interested in efficient, real-time robotic policies.</li>\n<li>📫 Stay in touch: Follow the LeRobot organization and Discord server for updates, tutorials, and new releases.\r\nTogether, we can make real-world robotics more capable, more affordable, and more open. ✨</li>\n</ul>",{headings:666,localImagePaths:667,remoteImagePaths:668,frontmatter:669,imagePaths:671},[],[],[],{title:656,summary:15,pubDate:602,media:17,tags:670,link:659,thumbnail:15},[19,20,21],[],"2025-06-20-smolvla-efficient-vision-language-action-model-trained-on-lerobot-community-data-04073c.md","2025-06-20-the-transformers-library-standardizing-model-definitions-a2ac46",{id:673,data:675,body:680,filePath:681,digest:682,rendered:683,legacyId:693},{title:676,summary:15,pubDate:677,media:17,tags:678,link:679,thumbnail:15},"The Transformers Library: standardizing model definitions",["Date","2025-05-15T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/transformers-model-definition","The Transformers Library: standardizing model definitions\r\nTLDR: Going forward, we're aiming for Transformers to be the pivot across frameworks: if a model architecture is supported by transformers, you can expect it to be supported in the rest of the ecosystem.\r\nTransformers was created in 2019, shortly following the release of the BERT Transformer model. Since then, we've continuously aimed to add state-of-the-art architectures, initially focused on NLP, then growing to Audio and computer vision. Today, transformers is the default library for LLMs and VLMs in the Python ecosystem.\r\nTransformers now supports 300+ model architectures, with an average of ~3 new architectures added every week. We have aimed for these architectures to be released in a timely manner; having day-0 support for the most sought-after architectures (Llamas, Qwens, GLMs, etc.).\r\nA model-definition library\r\nOver time, Transformers has become a central component in the ML ecosystem, becoming one of the most complete toolkits in terms of model diversity; it's integrated in all popular training frameworks such as Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, TRL, Nanotron, etc.\r\nRecently, we've been working hand in hand with the most popular inference engines (vLLM, SGLang, TGI, ...) for them\r\nto use transformers\r\nas a backend. The value added is significant: as soon as a model is added to transformers\r\n,\r\nit becomes available in these inference engines, while taking advantage of the strengths each engine provides: inference optimizations, specialized kernels, dynamic batching, etc.\r\nAs an example, here is how you would work with the transformers\r\nbackend in vLLM:\r\nfrom vllm import LLM\r\nllm = LLM(model=\"new-transformers-model\", model_impl=\"transformers\")\r\nThat's all it takes for a new model to enjoy super-fast and production-grade serving with vLLM!\r\nRead more about it in the vLLM documentation.\r\nWe've also been working very closely with llama.cpp and\r\nMLX so that the implementations between transformers\r\nand these modeling libraries have great interoperability. For example, thanks to a significant community effort,\r\nit's now very easy to load GGUF files in transformers\r\nfor\r\nfurther fine-tuning. Conversely, transformers models can be easily\r\nconverted to GGUF files for use with\r\nllama.cpp.\r\nThe same is true for MLX, where the transformers' safetensors files are directly compatible with MLX's models.\r\nWe are super proud that the transformers\r\nformat is being adopted by the community, bringing a lot of interoperability\r\nwe all benefit from. Train a model with Unsloth, deploy it with SGLang, and export it to llama.cpp to run locally! We\r\naim to keep supporting the community going forward.\r\nStriving for even simpler model contributions\r\nTo make it easier for the community to use transformers as a reference for model definitions, we strive to significantly reduce the barrier to model contributions. We have been doing this effort for a few years, but we'll accelerate significantly over the next few weeks:\r\n- The modeling code of each model will be further simplified; with clear, concise APIs for the most important components (KV cache, different Attention functions, kernel optimization)\r\n- We'll deprecate redundant components in favor of having a simple, single way to use our APIs: encouraging efficient tokenization by deprecating slow tokenizers, and similarly using the fast vectorized vision processors.\r\n- We'll continue to reinforce the work around modular model definitions, with the goal for new models to require absolute minimal code changes. 6000 line contributions, 20 files changes for new models are a thing of the past.\r\nHow does this affect you?\r\nWhat this means for you, as a model user\r\nAs a model user, in the future you should see even more interoperability in the tools that you use.\r\nThis does not mean that we intend to lock you in using transformers\r\nin your experiments; rather, it means that\r\nthanks to this modeling standardization, you can expect the tools that you use for training, for inference, and for\r\nproduction, to efficiently work together.\r\nWhat this means for you, as a model creator\r\nAs a model creator, this means that a single contribution will get your model available in all downstream libraries that have integrated that modeling implementation. We have seen this many times over the years: releasing a model is stressful and integrating in all important libraries is often a significant time-sink.\r\nBy standardizing the model implementation in a community-driven manner, we hope to lower the barrier of contributions to the field across libraries.\r\nWe firmly believe this renewed direction will help standardize an ecosystem which is often at risk of fragmentation. We'd love to hear your feedback on the direction the team has decided to take; and of changes we could do to get there. Please come and see us over at the transformers-community support tab on the Hub!","src/content/posts/2025-06-20-the-transformers-library-standardizing-model-definitions-a2ac46.md","c7de8ee6b2bc1ebc",{html:684,metadata:685},"<p>The Transformers Library: standardizing model definitions\r\nTLDR: Going forward, we’re aiming for Transformers to be the pivot across frameworks: if a model architecture is supported by transformers, you can expect it to be supported in the rest of the ecosystem.\r\nTransformers was created in 2019, shortly following the release of the BERT Transformer model. Since then, we’ve continuously aimed to add state-of-the-art architectures, initially focused on NLP, then growing to Audio and computer vision. Today, transformers is the default library for LLMs and VLMs in the Python ecosystem.\r\nTransformers now supports 300+ model architectures, with an average of ~3 new architectures added every week. We have aimed for these architectures to be released in a timely manner; having day-0 support for the most sought-after architectures (Llamas, Qwens, GLMs, etc.).\r\nA model-definition library\r\nOver time, Transformers has become a central component in the ML ecosystem, becoming one of the most complete toolkits in terms of model diversity; it’s integrated in all popular training frameworks such as Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, TRL, Nanotron, etc.\r\nRecently, we’ve been working hand in hand with the most popular inference engines (vLLM, SGLang, TGI, …) for them\r\nto use transformers\r\nas a backend. The value added is significant: as soon as a model is added to transformers\r\n,\r\nit becomes available in these inference engines, while taking advantage of the strengths each engine provides: inference optimizations, specialized kernels, dynamic batching, etc.\r\nAs an example, here is how you would work with the transformers\r\nbackend in vLLM:\r\nfrom vllm import LLM\r\nllm = LLM(model=“new-transformers-model”, model_impl=“transformers”)\r\nThat’s all it takes for a new model to enjoy super-fast and production-grade serving with vLLM!\r\nRead more about it in the vLLM documentation.\r\nWe’ve also been working very closely with llama.cpp and\r\nMLX so that the implementations between transformers\r\nand these modeling libraries have great interoperability. For example, thanks to a significant community effort,\r\nit’s now very easy to load GGUF files in transformers\r\nfor\r\nfurther fine-tuning. Conversely, transformers models can be easily\r\nconverted to GGUF files for use with\r\nllama.cpp.\r\nThe same is true for MLX, where the transformers’ safetensors files are directly compatible with MLX’s models.\r\nWe are super proud that the transformers\r\nformat is being adopted by the community, bringing a lot of interoperability\r\nwe all benefit from. Train a model with Unsloth, deploy it with SGLang, and export it to llama.cpp to run locally! We\r\naim to keep supporting the community going forward.\r\nStriving for even simpler model contributions\r\nTo make it easier for the community to use transformers as a reference for model definitions, we strive to significantly reduce the barrier to model contributions. We have been doing this effort for a few years, but we’ll accelerate significantly over the next few weeks:</p>\n<ul>\n<li>The modeling code of each model will be further simplified; with clear, concise APIs for the most important components (KV cache, different Attention functions, kernel optimization)</li>\n<li>We’ll deprecate redundant components in favor of having a simple, single way to use our APIs: encouraging efficient tokenization by deprecating slow tokenizers, and similarly using the fast vectorized vision processors.</li>\n<li>We’ll continue to reinforce the work around modular model definitions, with the goal for new models to require absolute minimal code changes. 6000 line contributions, 20 files changes for new models are a thing of the past.\r\nHow does this affect you?\r\nWhat this means for you, as a model user\r\nAs a model user, in the future you should see even more interoperability in the tools that you use.\r\nThis does not mean that we intend to lock you in using transformers\r\nin your experiments; rather, it means that\r\nthanks to this modeling standardization, you can expect the tools that you use for training, for inference, and for\r\nproduction, to efficiently work together.\r\nWhat this means for you, as a model creator\r\nAs a model creator, this means that a single contribution will get your model available in all downstream libraries that have integrated that modeling implementation. We have seen this many times over the years: releasing a model is stressful and integrating in all important libraries is often a significant time-sink.\r\nBy standardizing the model implementation in a community-driven manner, we hope to lower the barrier of contributions to the field across libraries.\r\nWe firmly believe this renewed direction will help standardize an ecosystem which is often at risk of fragmentation. We’d love to hear your feedback on the direction the team has decided to take; and of changes we could do to get there. Please come and see us over at the transformers-community support tab on the Hub!</li>\n</ul>",{headings:686,localImagePaths:687,remoteImagePaths:688,frontmatter:689,imagePaths:692},[],[],[],{title:676,summary:15,pubDate:690,media:17,tags:691,link:679,thumbnail:15},"Thu, 15 May 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-the-transformers-library-standardizing-model-definitions-a2ac46.md","2025-06-20-tiny-agents-in-python-a-mcp-powered-agent-in-70-lines-of-code-e33440",{id:694,data:696,body:701,filePath:702,digest:703,rendered:704,legacyId:836},{title:697,summary:15,pubDate:698,media:17,tags:699,link:700,thumbnail:15},"Tiny Agents in Python: a MCP-powered agent in ~70 lines of code",["Date","2025-05-23T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/python-tiny-agents","Tiny Agents in Python: an MCP-powered agent in ~70 lines of code\r\nInspired by Tiny Agents in JS, we ported the idea to Python 🐍 and extended the\r\nhuggingface_hub\r\nclient SDK to act as a MCP Client so it can pull tools from MCP servers and pass them to the LLM during inference.\r\nMCP (Model Context Protocol) is an open protocol that standardizes how Large Language Models (LLMs) interact with external tools and APIs. Essentially, it removed the need to write custom integrations for each tool, making it simpler to plug new capabilities into your LLMs.\r\nIn this blog post, we'll show you how to get started with a tiny Agent in Python connected to MCP servers to unlock powerful tool capabilities. You'll see just how easy it is to spin up your own Agent and start building!\r\nSpoiler : An Agent is essentially a while loop built right on top of an MCP Client!\r\nHow to Run the Demo\r\nThis section walks you through how to use existing Tiny Agents. We'll cover the setup and the commands to get an agent running.\r\nFirst, you need to install the latest version of huggingface_hub\r\nwith the mcp\r\nextra to get all the necessary components.\r\npip install \"huggingface_hub[mcp]>=0.32.0\"\r\nNow, let's run an agent using the CLI!\r\nThe coolest part is that you can load agents directly from the Hugging Face Hub tiny-agents Dataset, or specify a path to your own local agent configuration!\r\n> tiny-agents run --help\r\nUsage: tiny-agents run [OPTIONS] [PATH] COMMAND [ARGS]...\r\nRun the Agent in the CLI\r\n╭─ Arguments ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\r\n│ path [PATH] Path to a local folder containing an agent.json file or a built-in agent stored in the 'tiny-agents/tiny-agents' Hugging Face dataset │\r\n│ (https://huggingface.co/datasets/tiny-agents/tiny-agents) │\r\n╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n╭─ Options ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\r\n│ --help Show this message and exit. │\r\n╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\nIf you don't provide a path to a specific agent configuration, our Tiny Agent will connect by default to the following two MCP servers:\r\n- the \"canonical\" file system server, which gets access to your Desktop,\r\n- and the Playwright MCP server, which knows how to use a sandboxed Chromium browser for you.\r\nThe following example shows a web-browsing agent configured to use the Qwen/Qwen2.5-72B-Instruct model via Nebius inference provider, and it comes equipped with a playwright MCP server, which lets it use a web browser! The agent config is loaded specifying its path in the tiny-agents/tiny-agents\r\nHugging Face dataset.\r\nWhen you run the agent, you'll see it load, listing the tools it has discovered from its connected MCP servers. Then, it's ready for your prompts!\r\nPrompt used in this demo:\r\ndo a Web Search for HF inference providers on Brave Search and open the first result and then give me the list of the inference providers supported on Hugging Face\r\nYou can also use Gradio Spaces as MCP servers! The following example uses Qwen/Qwen2.5-72B-Instruct model via Nebius inference provider, and connects to a FLUX.1 [schnell]\r\nimage generation HF Space as an MCP server. The agent is loaded from its configuration in the tiny-agents/tiny-agents dataset on the Hugging Face Hub.\r\nPrompt used in this demo:\r\nGenerate a 1024x1024 image of a tiny astronaut hatching from an egg on the surface of the moon.\r\nNow that you've seen how to run existing Tiny Agents, the following sections will dive deeper into how they work and how to build your own.\r\nAgent Configuration\r\nEach agent's behavior (its default model, inference provider, which MCP servers to connect to, and its initial system prompt) is defined by an agent.json\r\nfile. You can also provide a custom PROMPT.md\r\nin the same directory for a more detailed system prompt. Here is an example:\r\nagent.json\r\nThe model\r\nand provider\r\nfields specify the LLM and inference provider used by the agent.\r\nThe servers\r\narray defines the MCP servers the agent will connect to.\r\nIn this example, a \"stdio\" MCP server is configured. This type of server runs as a local process. The Agent starts it using the specified command\r\nand args\r\n, and then communicates with it via stdin/stdout to discover and execute available tools.\r\n{\r\n\"model\": \"Qwen/Qwen2.5-72B-Instruct\",\r\n\"provider\": \"nebius\",\r\n\"servers\": [\r\n{\r\n\"type\": \"stdio\",\r\n\"config\": {\r\n\"command\": \"npx\",\r\n\"args\": [\"@playwright/mcp@latest\"]\r\n}\r\n}\r\n]\r\n}\r\nPROMPT.md\r\nYou are an agent - please keep going until the user’s query is completely resolved [...]\r\nYou can find more details about Hugging Face Inference Providers here.\r\nLLMs Can Use Tools\r\nModern LLMs are built for function calling (or tool use), which enables users to easily build applications tailored to specific use cases and real-world tasks.\r\nA function is defined by its schema, which informs the LLM what it does and what input arguments it expects. The LLM decides when to use a tool, the Agent then orchestrates running the tool and feeding the result back.\r\ntools = [\r\n{\r\n\"type\": \"function\",\r\n\"function\": {\r\n\"name\": \"get_weather\",\r\n\"description\": \"Get current temperature for a given location.\",\r\n\"parameters\": {\r\n\"type\": \"object\",\r\n\"properties\": {\r\n\"location\": {\r\n\"type\": \"string\",\r\n\"description\": \"City and country e.g. Paris, France\"\r\n}\r\n},\r\n\"required\": [\"location\"],\r\n},\r\n}\r\n}\r\n]\r\nInferenceClient\r\nimplements the same tool calling interface as the OpenAI Chat Completions API, which is the established standard for inference providers and the community.\r\nBuilding our Python MCP Client\r\nThe MCPClient\r\nis the heart of our tool-use functionality. It's now part of huggingface_hub\r\nand uses the AsyncInferenceClient\r\nto communicate with LLMs.\r\nThe full\r\nMCPClient\r\ncode is in here if you want to follow along using the actual code 🤓\r\nKey responsibilities of the MCPClient\r\n:\r\n- Manage async connections to one or more MCP servers.\r\n- Discover tools from these servers.\r\n- Format these tools for the LLM.\r\n- Execute tool calls via the correct MCP server.\r\nHere’s a glimpse of how it connects to an MCP server (the add_mcp_server\r\nmethod):\r\n# Lines 111-219 of `MCPClient.add_mcp_server`\r\n# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L111:L219\r\nclass MCPClient:\r\n...\r\nasync def add_mcp_server(self, type: ServerType, **params: Any):\r\n# 'type' can be \"stdio\", \"sse\", or \"http\"\r\n# 'params' are specific to the server type, e.g.:\r\n# for \"stdio\": {\"command\": \"my_tool_server_cmd\", \"args\": [\"--port\", \"1234\"]}\r\n# for \"http\": {\"url\": \"http://my.tool.server/mcp\"}\r\n# 1. Establish connection based on type (stdio, sse, http)\r\n# (Uses mcp.client.stdio_client, sse_client, or streamablehttp_client)\r\nread, write = await self.exit_stack.enter_async_context(...)\r\n# 2. Create an MCP ClientSession\r\nsession = await self.exit_stack.enter_async_context(\r\nClientSession(read_stream=read, write_stream=write, ...)\r\n)\r\nawait session.initialize()\r\n# 3. List tools from the server\r\nresponse = await session.list_tools()\r\nfor tool in response.tools:\r\n# Store session for this tool\r\nself.sessions[tool.name] = session\r\n# Add tool to the list of available tools and Format for LLM\r\nself.available_tools.append({\r\n\"type\": \"function\",\r\n\"function\": {\r\n\"name\": tool.name,\r\n\"description\": tool.description,\r\n\"parameters\": tool.input_schema,\r\n},\r\n})\r\nIt supports stdio\r\nservers for local tools (like accessing your file system), and http\r\nservers for remote tools! It's also compatible with sse\r\n, which is the previous standard for remote tools.\r\nUsing the Tools: Streaming and Processing\r\nThe MCPClient\r\n's process_single_turn_with_tools\r\nmethod is where the LLM interaction happens. It sends the conversation history and available tools to the LLM via AsyncInferenceClient.chat.completions.create(..., stream=True)\r\n.\r\n1. Prepare tools and calling the LLM\r\nFirst, the method determines all tools the LLM should be aware of for the current turn – this includes tools from MCP servers and any special \"exit loop\" tools for agent control; then, it makes a streaming call to the LLM:\r\n# Lines 241-251 of `MCPClient.process_single_turn_with_tools`\r\n# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L241:L251\r\n# Prepare tools list based on options\r\ntools = self.available_tools\r\nif exit_loop_tools is not None:\r\ntools = [*exit_loop_tools, *self.available_tools]\r\n# Create the streaming request to the LLM\r\nresponse = await self.client.chat.completions.create(\r\nmessages=messages,\r\ntools=tools,\r\ntool_choice=\"auto\", # LLM decides if it needs a tool\r\nstream=True,\r\n)\r\nAs chunks arrive from the LLM, the method iterates through them. Each chunk is immediately yielded, then we reconstruct the complete text response and any tool calls.\r\n# Lines 258-290 of `MCPClient.process_single_turn_with_tools`\r\n# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L258:L290\r\n# Read from stream\r\nasync for chunk in response:\r\n# Yield each chunk to caller\r\nyield chunk\r\n# Aggregate LLM's text response and parts of tool calls\r\n…\r\n2. Executing tools\r\nOnce the stream is complete, if the LLM requested any tool calls (now fully reconstructed in final_tool_calls\r\n), the method processes each one:\r\n# Lines 293-313 of `MCPClient.process_single_turn_with_tools`\r\n# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L293:L313\r\nfor tool_call in final_tool_calls.values():\r\nfunction_name = tool_call.function.name\r\nfunction_args = json.loads(tool_call.function.arguments or \"{}\")\r\n# Prepare a message to store the tool's result\r\ntool_message = {\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": \"\", \"name\": function_name}\r\n# a. Is this a special \"exit loop\" tool?\r\nif exit_loop_tools and function_name in [t.function.name for t in exit_loop_tools]:\r\n# If so, yield a message and terminate this turn's processing\r\nmessages.append(ChatCompletionInputMessage.parse_obj_as_instance(tool_message))\r\nyield ChatCompletionInputMessage.parse_obj_as_instance(tool_message)\r\nreturn # The Agent's main loop will handle this signal\r\n# b. It's a regular tool: find the MCP session and execute it\r\nsession = self.sessions.get(function_name) # self.sessions maps tool names to MCP connections\r\nif session is not None:\r\nresult = await session.call_tool(function_name, function_args)\r\ntool_message[\"content\"] = format_result(result) # format_result processes tool output\r\nelse:\r\ntool_message[\"content\"] = f\"Error: No session found for tool: {function_name}\"\r\ntool_message[\"content\"] = error_msg\r\n# Add tool result to history and yield it\r\n...\r\nIt first checks if the tool called exits the loop (exit_loop_tool\r\n). If not, it finds the correct MCP session responsible for that tool and calls session.call_tool()\r\n. The result (or error response) is then formatted, added to the conversation history, and yielded so the Agent is aware of the tool's output.\r\nOur Tiny Python Agent: It's (Almost) Just a Loop!\r\nWith the MCPClient\r\ndoing all the job for tool interactions, our Agent\r\nclass becomes wonderfully simple. It inherits from MCPClient\r\nand adds the conversation management logic.\r\nThe Agent class is tiny and focuses on the conversational loop, the code can be found here.\r\n1. Initializing the Agent\r\nWhen an Agent is created, it takes an agent config (model, provider, which MCP servers to use, system prompt) and initializes the conversation history with the system prompt. The load_tools()\r\nmethod then iterates through the server configurations (defined in agent.json) and calls add_mcp_server\r\n(from the parent MCPClient\r\n) for each one, populating the agent's toolbox.\r\n# Lines 12-54 of `Agent`\r\n# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/agent.py#L12:L54\r\nclass Agent(MCPClient):\r\ndef __init__(\r\nself,\r\n*,\r\nmodel: str,\r\nservers: Iterable[Dict], # Configuration for MCP servers\r\nprovider: Optional[PROVIDER_OR_POLICY_T] = None,\r\napi_key: Optional[str] = None,\r\nprompt: Optional[str] = None, # The system prompt\r\n):\r\n# Initialize the underlying MCPClient with model, provider, etc.\r\nsuper().__init__(model=model, provider=provider, api_key=api_key)\r\n# Store server configurations to be loaded\r\nself._servers_cfg = list(servers)\r\n# Start the conversation with a system message\r\nself.messages: List[Union[Dict, ChatCompletionInputMessage]] = [\r\n{\"role\": \"system\", \"content\": prompt or DEFAULT_SYSTEM_PROMPT}\r\n]\r\nasync def load_tools(self) -> None:\r\n# Connect to all configured MCP servers and register their tools\r\nfor cfg in self._servers_cfg:\r\nawait self.add_mcp_server(cfg[\"type\"], **cfg[\"config\"])\r\n2. The agent’s core: the Loop\r\nThe Agent.run()\r\nmethod is an asynchronous generator that processes a single user input. It manages the conversation turns, deciding when the agent's current task is complete.\r\n# Lines 56-99 of `Agent.run()`\r\n# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/agent.py#L56:L99\r\nasync def run(self, user_input: str, *, abort_event: Optional[asyncio.Event] = None, ...) -> AsyncGenerator[...]:\r\n...\r\nwhile True: # Main loop for processing the user_input\r\n...\r\n# Delegate to MCPClient to interact with LLM and tools for one step.\r\n# This streams back LLM text, tool call info, and tool results.\r\nasync for item in self.process_single_turn_with_tools(\r\nself.messages,\r\n...\r\n):\r\nyield item\r\n...\r\n# Exit Conditions\r\n# 1. Was an \"exit\" tool called?\r\nif last.get(\"role\") == \"tool\" and last.get(\"name\") in {t.function.name for t in EXIT_LOOP_TOOLS}:\r\nreturn\r\n# 2. Max turns reached or LLM gave a final text answer?\r\nif last.get(\"role\") != \"tool\" and num_turns > MAX_NUM_TURNS:\r\nreturn\r\nif last.get(\"role\") != \"tool\" and next_turn_should_call_tools:\r\nreturn\r\nnext_turn_should_call_tools = (last_message.get(\"role\") != \"tool\")\r\nInside the run()\r\nloop:\r\n- It first adds the user prompt to the conversation.\r\n- Then it calls\r\nMCPClient.process_single_turn_with_tools(...)\r\nto get the LLM's response and handle any tool executions for one step of reasoning. - Each item is immediately yielded, enabling real-time streaming to the caller.\r\n- After each step, it checks exit conditions: if a special \"exit loop\" tools was used, if a maximum turn limit is hit, or if the LLM provides a text response that seems final for the current request.\r\nNext Steps\r\nThere are a lot of cool ways to explore and expand upon the MCP Client and the Tiny Agent 🔥 Here are some ideas to get you started:\r\n- Benchmark how different LLM models and inference providers impact agentic performance: Tool calling performance can differ because each provider may optimize it differently. You can find the list of supported providers here.\r\n- Run tiny agents with local LLM inference servers, such as llama.cpp, or LM Studio.\r\n- .. and of course contribute! Share your unique tiny agents and open PRs in tiny-agents/tiny-agents dataset on the Hugging Face Hub.\r\nPull requests and contributions are welcome! Again, everything here is open source! 💎❤️","src/content/posts/2025-06-20-tiny-agents-in-python-a-mcp-powered-agent-in-~70-lines-of-code-e33440.md","31e459e64c896f0c",{html:705,metadata:706},"<p>Tiny Agents in Python: an MCP-powered agent in ~70 lines of code\r\nInspired by Tiny Agents in JS, we ported the idea to Python 🐍 and extended the\r\nhuggingface_hub\r\nclient SDK to act as a MCP Client so it can pull tools from MCP servers and pass them to the LLM during inference.\r\nMCP (Model Context Protocol) is an open protocol that standardizes how Large Language Models (LLMs) interact with external tools and APIs. Essentially, it removed the need to write custom integrations for each tool, making it simpler to plug new capabilities into your LLMs.\r\nIn this blog post, we’ll show you how to get started with a tiny Agent in Python connected to MCP servers to unlock powerful tool capabilities. You’ll see just how easy it is to spin up your own Agent and start building!\r\nSpoiler : An Agent is essentially a while loop built right on top of an MCP Client!\r\nHow to Run the Demo\r\nThis section walks you through how to use existing Tiny Agents. We’ll cover the setup and the commands to get an agent running.\r\nFirst, you need to install the latest version of huggingface_hub\r\nwith the mcp\r\nextra to get all the necessary components.\r\npip install “huggingface_hub[mcp]>=0.32.0”\r\nNow, let’s run an agent using the CLI!\r\nThe coolest part is that you can load agents directly from the Hugging Face Hub tiny-agents Dataset, or specify a path to your own local agent configuration!</p>\n<blockquote>\n<p>tiny-agents run —help\r\nUsage: tiny-agents run [OPTIONS] [PATH] COMMAND [ARGS]…\r\nRun the Agent in the CLI\r\n╭─ Arguments ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\r\n│ path [PATH] Path to a local folder containing an agent.json file or a built-in agent stored in the ‘tiny-agents/tiny-agents’ Hugging Face dataset │\r\n│ (<a href=\"https://huggingface.co/datasets/tiny-agents/tiny-agents\">https://huggingface.co/datasets/tiny-agents/tiny-agents</a>) │\r\n╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n╭─ Options ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\r\n│ —help Show this message and exit. │\r\n╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\nIf you don’t provide a path to a specific agent configuration, our Tiny Agent will connect by default to the following two MCP servers:</p>\n</blockquote>\n<ul>\n<li>the “canonical” file system server, which gets access to your Desktop,</li>\n<li>and the Playwright MCP server, which knows how to use a sandboxed Chromium browser for you.\r\nThe following example shows a web-browsing agent configured to use the Qwen/Qwen2.5-72B-Instruct model via Nebius inference provider, and it comes equipped with a playwright MCP server, which lets it use a web browser! The agent config is loaded specifying its path in the tiny-agents/tiny-agents\r\nHugging Face dataset.\r\nWhen you run the agent, you’ll see it load, listing the tools it has discovered from its connected MCP servers. Then, it’s ready for your prompts!\r\nPrompt used in this demo:\r\ndo a Web Search for HF inference providers on Brave Search and open the first result and then give me the list of the inference providers supported on Hugging Face\r\nYou can also use Gradio Spaces as MCP servers! The following example uses Qwen/Qwen2.5-72B-Instruct model via Nebius inference provider, and connects to a FLUX.1 [schnell]\r\nimage generation HF Space as an MCP server. The agent is loaded from its configuration in the tiny-agents/tiny-agents dataset on the Hugging Face Hub.\r\nPrompt used in this demo:\r\nGenerate a 1024x1024 image of a tiny astronaut hatching from an egg on the surface of the moon.\r\nNow that you’ve seen how to run existing Tiny Agents, the following sections will dive deeper into how they work and how to build your own.\r\nAgent Configuration\r\nEach agent’s behavior (its default model, inference provider, which MCP servers to connect to, and its initial system prompt) is defined by an agent.json\r\nfile. You can also provide a custom PROMPT.md\r\nin the same directory for a more detailed system prompt. Here is an example:\r\nagent.json\r\nThe model\r\nand provider\r\nfields specify the LLM and inference provider used by the agent.\r\nThe servers\r\narray defines the MCP servers the agent will connect to.\r\nIn this example, a “stdio” MCP server is configured. This type of server runs as a local process. The Agent starts it using the specified command\r\nand args\r\n, and then communicates with it via stdin/stdout to discover and execute available tools.\r\n{\r\n“model”: “Qwen/Qwen2.5-72B-Instruct”,\r\n“provider”: “nebius”,\r\n“servers”: [\r\n{\r\n“type”: “stdio”,\r\n“config”: {\r\n“command”: “npx”,\r\n“args”: [“@playwright/mcp@latest”]\r\n}\r\n}\r\n]\r\n}\r\nPROMPT.md\r\nYou are an agent - please keep going until the user’s query is completely resolved […]\r\nYou can find more details about Hugging Face Inference Providers here.\r\nLLMs Can Use Tools\r\nModern LLMs are built for function calling (or tool use), which enables users to easily build applications tailored to specific use cases and real-world tasks.\r\nA function is defined by its schema, which informs the LLM what it does and what input arguments it expects. The LLM decides when to use a tool, the Agent then orchestrates running the tool and feeding the result back.\r\ntools = [\r\n{\r\n“type”: “function”,\r\n“function”: {\r\n“name”: “get_weather”,\r\n“description”: “Get current temperature for a given location.”,\r\n“parameters”: {\r\n“type”: “object”,\r\n“properties”: {\r\n“location”: {\r\n“type”: “string”,\r\n“description”: “City and country e.g. Paris, France”\r\n}\r\n},\r\n“required”: [“location”],\r\n},\r\n}\r\n}\r\n]\r\nInferenceClient\r\nimplements the same tool calling interface as the OpenAI Chat Completions API, which is the established standard for inference providers and the community.\r\nBuilding our Python MCP Client\r\nThe MCPClient\r\nis the heart of our tool-use functionality. It’s now part of huggingface_hub\r\nand uses the AsyncInferenceClient\r\nto communicate with LLMs.\r\nThe full\r\nMCPClient\r\ncode is in here if you want to follow along using the actual code 🤓\r\nKey responsibilities of the MCPClient\r\n:</li>\n<li>Manage async connections to one or more MCP servers.</li>\n<li>Discover tools from these servers.</li>\n<li>Format these tools for the LLM.</li>\n<li>Execute tool calls via the correct MCP server.\r\nHere’s a glimpse of how it connects to an MCP server (the add_mcp_server\r\nmethod):</li>\n</ul>\n<h1 id=\"lines-111-219-of-mcpclientadd_mcp_server\">Lines 111-219 of <code>MCPClient.add_mcp_server</code></h1>\n<h1 id=\"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpmcp_clientpyl111l219\"><a href=\"https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L111:L219\">https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L111:L219</a></h1>\n<p>class MCPClient:\r\n…\r\nasync def add_mcp_server(self, type: ServerType, **params: Any):</p>\n<h1 id=\"type-can-be-stdio-sse-or-http\">‘type’ can be “stdio”, “sse”, or “http“</h1>\n<h1 id=\"params-are-specific-to-the-server-type-eg\">‘params’ are specific to the server type, e.g.:</h1>\n<h1 id=\"for-stdio-command-my_tool_server_cmd-args-port-1234\">for “stdio”: {“command”: “my_tool_server_cmd”, “args”: [“—port”, “1234”]}</h1>\n<h1 id=\"for-http-url-httpmytoolservermcp\">for “http”: {“url”: “<a href=\"http://my.tool.server/mcp%22%7D\">http://my.tool.server/mcp”}</a></h1>\n<h1 id=\"1-establish-connection-based-on-type-stdio-sse-http\">1. Establish connection based on type (stdio, sse, http)</h1>\n<h1 id=\"uses-mcpclientstdio_client-sse_client-or-streamablehttp_client\">(Uses mcp.client.stdio_client, sse_client, or streamablehttp_client)</h1>\n<p>read, write = await self.exit_stack.enter_async_context(…)</p>\n<h1 id=\"2-create-an-mcp-clientsession\">2. Create an MCP ClientSession</h1>\n<p>session = await self.exit_stack.enter_async_context(\r\nClientSession(read_stream=read, write_stream=write, …)\r\n)\r\nawait session.initialize()</p>\n<h1 id=\"3-list-tools-from-the-server\">3. List tools from the server</h1>\n<p>response = await session.list_tools()\r\nfor tool in response.tools:</p>\n<h1 id=\"store-session-for-this-tool\">Store session for this tool</h1>\n<p>self.sessions[tool.name] = session</p>\n<h1 id=\"add-tool-to-the-list-of-available-tools-and-format-for-llm\">Add tool to the list of available tools and Format for LLM</h1>\n<p>self.available_tools.append({\r\n“type”: “function”,\r\n“function”: {\r\n“name”: tool.name,\r\n“description”: tool.description,\r\n“parameters”: tool.input_schema,\r\n},\r\n})\r\nIt supports stdio\r\nservers for local tools (like accessing your file system), and http\r\nservers for remote tools! It’s also compatible with sse\r\n, which is the previous standard for remote tools.\r\nUsing the Tools: Streaming and Processing\r\nThe MCPClient\r\n‘s process_single_turn_with_tools\r\nmethod is where the LLM interaction happens. It sends the conversation history and available tools to the LLM via AsyncInferenceClient.chat.completions.create(…, stream=True)\r\n.</p>\n<ol>\n<li>Prepare tools and calling the LLM\r\nFirst, the method determines all tools the LLM should be aware of for the current turn – this includes tools from MCP servers and any special “exit loop” tools for agent control; then, it makes a streaming call to the LLM:</li>\n</ol>\n<h1 id=\"lines-241-251-of-mcpclientprocess_single_turn_with_tools\">Lines 241-251 of <code>MCPClient.process_single_turn_with_tools</code></h1>\n<h1 id=\"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpmcp_clientpyl241l251\"><a href=\"https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L241:L251\">https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L241:L251</a></h1>\n<h1 id=\"prepare-tools-list-based-on-options\">Prepare tools list based on options</h1>\n<p>tools = self.available_tools\r\nif exit_loop_tools is not None:\r\ntools = [*exit_loop_tools, *self.available_tools]</p>\n<h1 id=\"create-the-streaming-request-to-the-llm\">Create the streaming request to the LLM</h1>\n<p>response = await self.client.chat.completions.create(\r\nmessages=messages,\r\ntools=tools,\r\ntool_choice=“auto”, # LLM decides if it needs a tool\r\nstream=True,\r\n)\r\nAs chunks arrive from the LLM, the method iterates through them. Each chunk is immediately yielded, then we reconstruct the complete text response and any tool calls.</p>\n<h1 id=\"lines-258-290-of-mcpclientprocess_single_turn_with_tools\">Lines 258-290 of <code>MCPClient.process_single_turn_with_tools</code></h1>\n<h1 id=\"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpmcp_clientpyl258l290\"><a href=\"https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L258:L290\">https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L258:L290</a></h1>\n<h1 id=\"read-from-stream\">Read from stream</h1>\n<p>async for chunk in response:</p>\n<h1 id=\"yield-each-chunk-to-caller\">Yield each chunk to caller</h1>\n<p>yield chunk</p>\n<h1 id=\"aggregate-llms-text-response-and-parts-of-tool-calls\">Aggregate LLM’s text response and parts of tool calls</h1>\n<p>…\r\n2. Executing tools\r\nOnce the stream is complete, if the LLM requested any tool calls (now fully reconstructed in final_tool_calls\r\n), the method processes each one:</p>\n<h1 id=\"lines-293-313-of-mcpclientprocess_single_turn_with_tools\">Lines 293-313 of <code>MCPClient.process_single_turn_with_tools</code></h1>\n<h1 id=\"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpmcp_clientpyl293l313\"><a href=\"https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L293:L313\">https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L293:L313</a></h1>\n<p>for tool_call in final_tool_calls.values():\r\nfunction_name = tool_call.function.name\r\nfunction_args = json.loads(tool_call.function.arguments or ”{}“)</p>\n<h1 id=\"prepare-a-message-to-store-the-tools-result\">Prepare a message to store the tool’s result</h1>\n<p>tool_message = {“role”: “tool”, “tool_call_id”: tool_call.id, “content”: \"\", “name”: function_name}</p>\n<h1 id=\"a-is-this-a-special-exit-loop-tool\">a. Is this a special “exit loop” tool?</h1>\n<p>if exit_loop_tools and function_name in [t.function.name for t in exit_loop_tools]:</p>\n<h1 id=\"if-so-yield-a-message-and-terminate-this-turns-processing\">If so, yield a message and terminate this turn’s processing</h1>\n<p>messages.append(ChatCompletionInputMessage.parse_obj_as_instance(tool_message))\r\nyield ChatCompletionInputMessage.parse_obj_as_instance(tool_message)\r\nreturn # The Agent’s main loop will handle this signal</p>\n<h1 id=\"b-its-a-regular-tool-find-the-mcp-session-and-execute-it\">b. It’s a regular tool: find the MCP session and execute it</h1>\n<p>session = self.sessions.get(function_name) # self.sessions maps tool names to MCP connections\r\nif session is not None:\r\nresult = await session.call_tool(function_name, function_args)\r\ntool_message[“content”] = format_result(result) # format_result processes tool output\r\nelse:\r\ntool_message[“content”] = f”Error: No session found for tool: {function_name}”\r\ntool_message[“content”] = error_msg</p>\n<h1 id=\"add-tool-result-to-history-and-yield-it\">Add tool result to history and yield it</h1>\n<p>…\r\nIt first checks if the tool called exits the loop (exit_loop_tool\r\n). If not, it finds the correct MCP session responsible for that tool and calls session.call_tool()\r\n. The result (or error response) is then formatted, added to the conversation history, and yielded so the Agent is aware of the tool’s output.\r\nOur Tiny Python Agent: It’s (Almost) Just a Loop!\r\nWith the MCPClient\r\ndoing all the job for tool interactions, our Agent\r\nclass becomes wonderfully simple. It inherits from MCPClient\r\nand adds the conversation management logic.\r\nThe Agent class is tiny and focuses on the conversational loop, the code can be found here.</p>\n<ol>\n<li>Initializing the Agent\r\nWhen an Agent is created, it takes an agent config (model, provider, which MCP servers to use, system prompt) and initializes the conversation history with the system prompt. The load_tools()\r\nmethod then iterates through the server configurations (defined in agent.json) and calls add_mcp_server\r\n(from the parent MCPClient\r\n) for each one, populating the agent’s toolbox.</li>\n</ol>\n<h1 id=\"lines-12-54-of-agent\">Lines 12-54 of <code>Agent</code></h1>\n<h1 id=\"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpagentpyl12l54\"><a href=\"https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/agent.py#L12:L54\">https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/agent.py#L12:L54</a></h1>\n<p>class Agent(MCPClient):\r\ndef <strong>init</strong>(\r\nself,\r\n*,\r\nmodel: str,\r\nservers: Iterable[Dict], # Configuration for MCP servers\r\nprovider: Optional[PROVIDER_OR_POLICY_T] = None,\r\napi_key: Optional[str] = None,\r\nprompt: Optional[str] = None, # The system prompt\r\n):</p>\n<h1 id=\"initialize-the-underlying-mcpclient-with-model-provider-etc\">Initialize the underlying MCPClient with model, provider, etc.</h1>\n<p>super().<strong>init</strong>(model=model, provider=provider, api_key=api_key)</p>\n<h1 id=\"store-server-configurations-to-be-loaded\">Store server configurations to be loaded</h1>\n<p>self._servers_cfg = list(servers)</p>\n<h1 id=\"start-the-conversation-with-a-system-message\">Start the conversation with a system message</h1>\n<p>self.messages: List[Union[Dict, ChatCompletionInputMessage]] = [\r\n{“role”: “system”, “content”: prompt or DEFAULT_SYSTEM_PROMPT}\r\n]\r\nasync def load_tools(self) -> None:</p>\n<h1 id=\"connect-to-all-configured-mcp-servers-and-register-their-tools\">Connect to all configured MCP servers and register their tools</h1>\n<p>for cfg in self._servers_cfg:\r\nawait self.add_mcp_server(cfg[“type”], **cfg[“config”])\r\n2. The agent’s core: the Loop\r\nThe Agent.run()\r\nmethod is an asynchronous generator that processes a single user input. It manages the conversation turns, deciding when the agent’s current task is complete.</p>\n<h1 id=\"lines-56-99-of-agentrun\">Lines 56-99 of <code>Agent.run()</code></h1>\n<h1 id=\"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpagentpyl56l99\"><a href=\"https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/agent.py#L56:L99\">https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/agent.py#L56:L99</a></h1>\n<p>async def run(self, user_input: str, *, abort_event: Optional[asyncio.Event] = None, …) -> AsyncGenerator[…]:\r\n…\r\nwhile True: # Main loop for processing the user_input\r\n…</p>\n<h1 id=\"delegate-to-mcpclient-to-interact-with-llm-and-tools-for-one-step\">Delegate to MCPClient to interact with LLM and tools for one step.</h1>\n<h1 id=\"this-streams-back-llm-text-tool-call-info-and-tool-results\">This streams back LLM text, tool call info, and tool results.</h1>\n<p>async for item in self.process_single_turn_with_tools(\r\nself.messages,\r\n…\r\n):\r\nyield item\r\n…</p>\n<h1 id=\"exit-conditions\">Exit Conditions</h1>\n<h1 id=\"1-was-an-exit-tool-called\">1. Was an “exit” tool called?</h1>\n<p>if last.get(“role”) == “tool” and last.get(“name”) in {t.function.name for t in EXIT_LOOP_TOOLS}:\r\nreturn</p>\n<h1 id=\"2-max-turns-reached-or-llm-gave-a-final-text-answer\">2. Max turns reached or LLM gave a final text answer?</h1>\n<p>if last.get(“role”) != “tool” and num_turns > MAX_NUM_TURNS:\r\nreturn\r\nif last.get(“role”) != “tool” and next_turn_should_call_tools:\r\nreturn\r\nnext_turn_should_call_tools = (last_message.get(“role”) != “tool”)\r\nInside the run()\r\nloop:</p>\n<ul>\n<li>It first adds the user prompt to the conversation.</li>\n<li>Then it calls\r\nMCPClient.process_single_turn_with_tools(…)\r\nto get the LLM’s response and handle any tool executions for one step of reasoning. - Each item is immediately yielded, enabling real-time streaming to the caller.</li>\n<li>After each step, it checks exit conditions: if a special “exit loop” tools was used, if a maximum turn limit is hit, or if the LLM provides a text response that seems final for the current request.\r\nNext Steps\r\nThere are a lot of cool ways to explore and expand upon the MCP Client and the Tiny Agent 🔥 Here are some ideas to get you started:</li>\n<li>Benchmark how different LLM models and inference providers impact agentic performance: Tool calling performance can differ because each provider may optimize it differently. You can find the list of supported providers here.</li>\n<li>Run tiny agents with local LLM inference servers, such as llama.cpp, or LM Studio.</li>\n<li>.. and of course contribute! Share your unique tiny agents and open PRs in tiny-agents/tiny-agents dataset on the Hugging Face Hub.\r\nPull requests and contributions are welcome! Again, everything here is open source! 💎❤️</li>\n</ul>",{headings:707,localImagePaths:831,remoteImagePaths:832,frontmatter:833,imagePaths:835},[708,711,714,717,720,723,726,729,732,735,738,741,744,747,750,753,756,759,762,765,768,771,774,777,780,783,786,789,792,795,798,801,804,807,810,813,816,819,822,825,828],{depth:31,slug:709,text:710},"lines-111-219-of-mcpclientadd_mcp_server","Lines 111-219 of MCPClient.add_mcp_server",{depth:31,slug:712,text:713},"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpmcp_clientpyl111l219","https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L111:L219",{depth:31,slug:715,text:716},"type-can-be-stdio-sse-or-http","‘type’ can be “stdio”, “sse”, or “http“",{depth:31,slug:718,text:719},"params-are-specific-to-the-server-type-eg","‘params’ are specific to the server type, e.g.:",{depth:31,slug:721,text:722},"for-stdio-command-my_tool_server_cmd-args-port-1234","for “stdio”: ${“command”: “my_tool_server_cmd”, “args”: [“—port”, “1234”]}",{depth:31,slug:724,text:725},"for-http-url-httpmytoolservermcp","for “http”: ${“url”: “http://my.tool.server/mcp”}",{depth:31,slug:727,text:728},"1-establish-connection-based-on-type-stdio-sse-http","1. Establish connection based on type (stdio, sse, http)",{depth:31,slug:730,text:731},"uses-mcpclientstdio_client-sse_client-or-streamablehttp_client","(Uses mcp.client.stdio_client, sse_client, or streamablehttp_client)",{depth:31,slug:733,text:734},"2-create-an-mcp-clientsession","2. Create an MCP ClientSession",{depth:31,slug:736,text:737},"3-list-tools-from-the-server","3. List tools from the server",{depth:31,slug:739,text:740},"store-session-for-this-tool","Store session for this tool",{depth:31,slug:742,text:743},"add-tool-to-the-list-of-available-tools-and-format-for-llm","Add tool to the list of available tools and Format for LLM",{depth:31,slug:745,text:746},"lines-241-251-of-mcpclientprocess_single_turn_with_tools","Lines 241-251 of MCPClient.process_single_turn_with_tools",{depth:31,slug:748,text:749},"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpmcp_clientpyl241l251","https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L241:L251",{depth:31,slug:751,text:752},"prepare-tools-list-based-on-options","Prepare tools list based on options",{depth:31,slug:754,text:755},"create-the-streaming-request-to-the-llm","Create the streaming request to the LLM",{depth:31,slug:757,text:758},"lines-258-290-of-mcpclientprocess_single_turn_with_tools","Lines 258-290 of MCPClient.process_single_turn_with_tools",{depth:31,slug:760,text:761},"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpmcp_clientpyl258l290","https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L258:L290",{depth:31,slug:763,text:764},"read-from-stream","Read from stream",{depth:31,slug:766,text:767},"yield-each-chunk-to-caller","Yield each chunk to caller",{depth:31,slug:769,text:770},"aggregate-llms-text-response-and-parts-of-tool-calls","Aggregate LLM’s text response and parts of tool calls",{depth:31,slug:772,text:773},"lines-293-313-of-mcpclientprocess_single_turn_with_tools","Lines 293-313 of MCPClient.process_single_turn_with_tools",{depth:31,slug:775,text:776},"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpmcp_clientpyl293l313","https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L293:L313",{depth:31,slug:778,text:779},"prepare-a-message-to-store-the-tools-result","Prepare a message to store the tool’s result",{depth:31,slug:781,text:782},"a-is-this-a-special-exit-loop-tool","a. Is this a special “exit loop” tool?",{depth:31,slug:784,text:785},"if-so-yield-a-message-and-terminate-this-turns-processing","If so, yield a message and terminate this turn’s processing",{depth:31,slug:787,text:788},"b-its-a-regular-tool-find-the-mcp-session-and-execute-it","b. It’s a regular tool: find the MCP session and execute it",{depth:31,slug:790,text:791},"add-tool-result-to-history-and-yield-it","Add tool result to history and yield it",{depth:31,slug:793,text:794},"lines-12-54-of-agent","Lines 12-54 of Agent",{depth:31,slug:796,text:797},"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpagentpyl12l54","https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/agent.py#L12:L54",{depth:31,slug:799,text:800},"initialize-the-underlying-mcpclient-with-model-provider-etc","Initialize the underlying MCPClient with model, provider, etc.",{depth:31,slug:802,text:803},"store-server-configurations-to-be-loaded","Store server configurations to be loaded",{depth:31,slug:805,text:806},"start-the-conversation-with-a-system-message","Start the conversation with a system message",{depth:31,slug:808,text:809},"connect-to-all-configured-mcp-servers-and-register-their-tools","Connect to all configured MCP servers and register their tools",{depth:31,slug:811,text:812},"lines-56-99-of-agentrun","Lines 56-99 of Agent.run()",{depth:31,slug:814,text:815},"httpsgithubcomhuggingfacehuggingface_hubblobmainsrchuggingface_hubinference_mcpagentpyl56l99","https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/agent.py#L56:L99",{depth:31,slug:817,text:818},"delegate-to-mcpclient-to-interact-with-llm-and-tools-for-one-step","Delegate to MCPClient to interact with LLM and tools for one step.",{depth:31,slug:820,text:821},"this-streams-back-llm-text-tool-call-info-and-tool-results","This streams back LLM text, tool call info, and tool results.",{depth:31,slug:823,text:824},"exit-conditions","Exit Conditions",{depth:31,slug:826,text:827},"1-was-an-exit-tool-called","1. Was an “exit” tool called?",{depth:31,slug:829,text:830},"2-max-turns-reached-or-llm-gave-a-final-text-answer","2. Max turns reached or LLM gave a final text answer?",[],[],{title:697,summary:15,pubDate:133,media:17,tags:834,link:700,thumbnail:15},[19,20,21],[],"2025-06-20-tiny-agents-in-python-a-mcp-powered-agent-in-~70-lines-of-code-e33440.md","2025-06-20--liger-grpo-meets-trl-816c0f",{id:837,data:839,body:844,filePath:845,digest:846,rendered:847,legacyId:857},{title:840,summary:15,pubDate:841,media:17,tags:842,link:843,thumbnail:15},"🐯 Liger GRPO meets TRL",["Date","2025-05-25T00:00:00.000Z"],[19,20,21],"https://huggingface.co/blog/liger-grpo","Thank you for your great work.\r\nAnyway, I tested the liger loss with deepspeed zero3 using Qwen/Qwen2.5-0.5B-Instruct\r\nin a bf16\r\n.\r\nI met an shape mismatch as stated below:\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]: File \"/workspace/temp.py\", line 22, in <module>\r\n[rank0]: trainer.train()\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2238, in train\r\n[rank0]: return inner_training_loop(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2553, in _inner_training_loop\r\n[rank0]: tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3730, in training_step\r\n[rank0]: loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/trl/extras/profiling.py\", line 87, in wrapper\r\n[rank0]: return func(self, *args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py\", line 1187, in compute_loss\r\n[rank0]: return self.compute_liger_loss(model, inputs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py\", line 1160, in compute_liger_loss\r\n[rank0]: loss, metrics = self.liger_grpo_loss(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\r\n[rank0]: return self._call_impl(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\r\n[rank0]: return forward_call(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/liger_kernel/chunked_loss/grpo_loss.py\", line 249, in forward\r\n[rank0]: return LigerFusedLinearGRPOFunction.apply(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\", line 575, in apply\r\n[rank0]: return super().apply(*args, **kwargs) # type: ignore[misc]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/liger_kernel/chunked_loss/grpo_loss.py\", line 142, in forward\r\n[rank0]: return super().forward(\r\n[rank0]: ^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/liger_kernel/chunked_loss/fused_linear_ppo.py\", line 219, in forward\r\n[rank0]: accumulate_chunk(\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/liger_kernel/chunked_loss/fused_linear_ppo.py\", line 132, in accumulate_chunk\r\n[rank0]: (chunk_grad_input, chunk_grad_weight, *chunk_grad_bias), (chunk_loss, chunk_metrics) = fused_fwd_bwd(\r\n[rank0]: ^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\r\n[rank0]: return fn(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 1380, in __call__\r\n[rank0]: return self._torchdynamo_orig_callable(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\r\n[rank0]: result = self._inner_convert(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\r\n[rank0]: return _compile(\r\n[rank0]: ^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\r\n[rank0]: guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\r\n[rank0]: return _compile_inner(code, one_graph, hooks, transform)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\r\n[rank0]: return function(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\r\n[rank0]: out_code = transform_code_object(code, transform)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\r\n[rank0]: transformations(instructions, code_options)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\r\n[rank0]: return fn(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\r\n[rank0]: tracer.run()\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\r\n[rank0]: super().run()\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2341, in CALL\r\n[rank0]: self._call(inst)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2335, in _call\r\n[rank0]: self.call_function(fn, args, kwargs)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\r\n[rank0]: return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\r\n[rank0]: return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\r\n[rank0]: return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\r\n[rank0]: tracer.run()\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2341, in CALL\r\n[rank0]: self._call(inst)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2335, in _call\r\n[rank0]: self.call_function(fn, args, kwargs)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\r\n[rank0]: return super().call_function(tx, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\r\n[rank0]: return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\r\n[rank0]: return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\r\n[rank0]: return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\r\n[rank0]: tracer.run()\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1736, in CALL_FUNCTION_EX\r\n[rank0]: self.call_function(fn, argsvars.items, kwargsvars)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\r\n[rank0]: return super().call_function(tx, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\r\n[rank0]: return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\r\n[rank0]: return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\r\n[rank0]: return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\r\n[rank0]: tracer.run()\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1736, in CALL_FUNCTION_EX\r\n[rank0]: self.call_function(fn, argsvars.items, kwargsvars)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py\", line 858, in call_function\r\n[rank0]: return self.func.call_function(tx, merged_args, merged_kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\r\n[rank0]: return super().call_function(tx, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\r\n[rank0]: return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\r\n[rank0]: return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\r\n[rank0]: return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\r\n[rank0]: tracer.run()\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2341, in CALL\r\n[rank0]: self._call(inst)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2335, in _call\r\n[rank0]: self.call_function(fn, args, kwargs)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/misc.py\", line 1022, in call_function\r\n[rank0]: return self.obj.call_method(tx, self.name, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/misc.py\", line 778, in call_method\r\n[rank0]: .call_function(tx, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\r\n[rank0]: return super().call_function(tx, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\r\n[rank0]: return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\r\n[rank0]: return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\r\n[rank0]: return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\r\n[rank0]: tracer.run()\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2341, in CALL\r\n[rank0]: self._call(inst)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2335, in _call\r\n[rank0]: self.call_function(fn, args, kwargs)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/torch.py\", line 953, in call_function\r\n[rank0]: tensor_variable = wrap_fx_proxy(\r\n[rank0]: ^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/builder.py\", line 2153, in wrap_fx_proxy\r\n[rank0]: return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/builder.py\", line 2219, in wrap_fx_proxy_cls\r\n[rank0]: return _wrap_fx_proxy(\r\n[rank0]: ^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/builder.py\", line 2315, in _wrap_fx_proxy\r\n[rank0]: example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 2536, in get_fake_value\r\n[rank0]: raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 2471, in get_fake_value\r\n[rank0]: ret_val = wrap_fake_exception(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 2017, in wrap_fake_exception\r\n[rank0]: return fn()\r\n[rank0]: ^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 2472, in <lambda>\r\n[rank0]: lambda: run_node(tx.output, node, args, kwargs, nnmodule)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 2604, in run_node\r\n[rank0]: raise RuntimeError(make_error_message(e)).with_traceback(\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\", line 2586, in run_node\r\n[rank0]: return node.target(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_prims_common/wrappers.py\", line 289, in _fn\r\n[rank0]: result = fn(*args, is_out=(out is not None), **kwargs) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py\", line 4444, in matmul\r\n[rank0]: return torch.ops.aten._unsafe_view(t1_folded.mv(t2), output_shape)\r\n[rank0]: ^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_stats.py\", line 21, in wrapper\r\n[rank0]: return fn(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\", line 1276, in __torch_dispatch__\r\n[rank0]: return self.dispatch(func, types, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\", line 1816, in dispatch\r\n[rank0]: return self._cached_dispatch_impl(func, types, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\", line 1377, in _cached_dispatch_impl\r\n[rank0]: output = self._dispatch_impl(func, types, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\", line 2290, in _dispatch_impl\r\n[rank0]: decomposition_table[func](*args, **kwargs)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_prims_common/wrappers.py\", line 291, in _fn\r\n[rank0]: result = fn(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py\", line 83, in inner\r\n[rank0]: r = f(*tree_map(increase_prec, args), **tree_map(increase_prec, kwargs))\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py\", line 4336, in mv\r\n[rank0]: torch._check(\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1656, in _check\r\n[rank0]: _check_with(RuntimeError, cond, message)\r\n[rank0]: File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1638, in _check_with\r\n[rank0]: raise error_type(message_evaluated)\r\n[rank0]: torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method matmul of type object at 0x7f2e2a41ff00>(*(GradTrackingTensor(lvl=1, value=\r\n[rank0]: FakeTensor(..., device='cuda:0', size=(1, s0, 896), dtype=torch.bfloat16,\r\n[rank0]: requires_grad=True)\r\n[rank0]: ), GradTrackingTensor(lvl=1, value=\r\n[rank0]: FakeTensor(..., device='cuda:0', size=(0,), dtype=torch.bfloat16,\r\n[rank0]: requires_grad=True)\r\n[rank0]: )), **{}):\r\n[rank0]: size mismatch, got input (s0x896), vec (0)\r\nDoes liger GRPO support multi-gpu training with deepspeed zero3?","src/content/posts/2025-06-20-🐯-liger-grpo-meets-trl-816c0f.md","972dce036dd6bae5",{html:848,metadata:849},"<p>Thank you for your great work.\r\nAnyway, I tested the liger loss with deepspeed zero3 using Qwen/Qwen2.5-0.5B-Instruct\r\nin a bf16\r\n.\r\nI met an shape mismatch as stated below:\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]: File “/workspace/temp.py”, line 22, in <module>\r\n[rank0]: trainer.train()\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/transformers/trainer.py”, line 2238, in train\r\n[rank0]: return inner_training_loop(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/transformers/trainer.py”, line 2553, in _inner_training_loop\r\n[rank0]: tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/transformers/trainer.py”, line 3730, in training_step\r\n[rank0]: loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/trl/extras/profiling.py”, line 87, in wrapper\r\n[rank0]: return func(self, *args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py”, line 1187, in compute_loss\r\n[rank0]: return self.compute_liger_loss(model, inputs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py”, line 1160, in compute_liger_loss\r\n[rank0]: loss, metrics = self.liger_grpo_loss(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py”, line 1739, in _wrapped_call_impl\r\n[rank0]: return self._call_impl(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py”, line 1750, in _call_impl\r\n[rank0]: return forward_call(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/liger_kernel/chunked_loss/grpo_loss.py”, line 249, in forward\r\n[rank0]: return LigerFusedLinearGRPOFunction.apply(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py”, line 575, in apply\r\n[rank0]: return super().apply(*args, **kwargs) # type: ignore[misc]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/liger_kernel/chunked_loss/grpo_loss.py”, line 142, in forward\r\n[rank0]: return super().forward(\r\n[rank0]: ^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/liger_kernel/chunked_loss/fused_linear_ppo.py”, line 219, in forward\r\n[rank0]: accumulate_chunk(\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/liger_kernel/chunked_loss/fused_linear_ppo.py”, line 132, in accumulate_chunk\r\n[rank0]: (chunk_grad_input, chunk_grad_weight, *chunk_grad_bias), (chunk_loss, chunk_metrics) = fused_fwd_bwd(\r\n[rank0]: ^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py”, line 574, in _fn\r\n[rank0]: return fn(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py”, line 1380, in <strong>call</strong>\r\n[rank0]: return self._torchdynamo_orig_callable(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py”, line 1164, in <strong>call</strong>\r\n[rank0]: result = self._inner_convert(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py”, line 547, in <strong>call</strong>\r\n[rank0]: return _compile(\r\n[rank0]: ^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py”, line 986, in _compile\r\n[rank0]: guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py”, line 715, in compile_inner\r\n[rank0]: return _compile_inner(code, one_graph, hooks, transform)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_utils_internal.py”, line 95, in wrapper_function\r\n[rank0]: return function(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py”, line 750, in _compile_inner\r\n[rank0]: out_code = transform_code_object(code, transform)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/bytecode_transformation.py”, line 1361, in transform_code_object\r\n[rank0]: transformations(instructions, code_options)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py”, line 231, in _fn\r\n[rank0]: return fn(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py”, line 662, in transform\r\n[rank0]: tracer.run()\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 2868, in run\r\n[rank0]: super().run()\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 2341, in CALL\r\n[rank0]: self._call(inst)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 2335, in _call\r\n[rank0]: self.call_function(fn, args, kwargs)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py”, line 118, in call_function\r\n[rank0]: return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 903, in inline_user_function_return\r\n[rank0]: return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<em>dynamo/symbolic_convert.py”, line 3072, in inline_call\r\n[rank0]: return cls.inline_call</em>(parent, func, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<em>dynamo/symbolic_convert.py”, line 3198, in inline_call</em>\r\n[rank0]: tracer.run()\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 2341, in CALL\r\n[rank0]: self._call(inst)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 2335, in _call\r\n[rank0]: self.call_function(fn, args, kwargs)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py”, line 317, in call_function\r\n[rank0]: return super().call_function(tx, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py”, line 118, in call_function\r\n[rank0]: return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 903, in inline_user_function_return\r\n[rank0]: return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<em>dynamo/symbolic_convert.py”, line 3072, in inline_call\r\n[rank0]: return cls.inline_call</em>(parent, func, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<em>dynamo/symbolic_convert.py”, line 3198, in inline_call</em>\r\n[rank0]: tracer.run()\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 1736, in CALL_FUNCTION_EX\r\n[rank0]: self.call_function(fn, argsvars.items, kwargsvars)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py”, line 317, in call_function\r\n[rank0]: return super().call_function(tx, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py”, line 118, in call_function\r\n[rank0]: return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 903, in inline_user_function_return\r\n[rank0]: return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<em>dynamo/symbolic_convert.py”, line 3072, in inline_call\r\n[rank0]: return cls.inline_call</em>(parent, func, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<em>dynamo/symbolic_convert.py”, line 3198, in inline_call</em>\r\n[rank0]: tracer.run()\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 1736, in CALL_FUNCTION_EX\r\n[rank0]: self.call_function(fn, argsvars.items, kwargsvars)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py”, line 858, in call_function\r\n[rank0]: return self.func.call_function(tx, merged_args, merged_kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py”, line 317, in call_function\r\n[rank0]: return super().call_function(tx, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py”, line 118, in call_function\r\n[rank0]: return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 903, in inline_user_function_return\r\n[rank0]: return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<em>dynamo/symbolic_convert.py”, line 3072, in inline_call\r\n[rank0]: return cls.inline_call</em>(parent, func, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<em>dynamo/symbolic_convert.py”, line 3198, in inline_call</em>\r\n[rank0]: tracer.run()\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 2341, in CALL\r\n[rank0]: self._call(inst)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 2335, in _call\r\n[rank0]: self.call_function(fn, args, kwargs)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/misc.py”, line 1022, in call_function\r\n[rank0]: return self.obj.call_method(tx, self.name, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/misc.py”, line 778, in call_method\r\n[rank0]: .call_function(tx, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py”, line 317, in call_function\r\n[rank0]: return super().call_function(tx, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py”, line 118, in call_function\r\n[rank0]: return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 903, in inline_user_function_return\r\n[rank0]: return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<em>dynamo/symbolic_convert.py”, line 3072, in inline_call\r\n[rank0]: return cls.inline_call</em>(parent, func, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<em>dynamo/symbolic_convert.py”, line 3198, in inline_call</em>\r\n[rank0]: tracer.run()\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 1052, in run\r\n[rank0]: while self.step():\r\n[rank0]: ^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 962, in step\r\n[rank0]: self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 659, in wrapper\r\n[rank0]: return inner_fn(self, inst)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 2341, in CALL\r\n[rank0]: self._call(inst)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 2335, in _call\r\n[rank0]: self.call_function(fn, args, kwargs)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py”, line 897, in call_function\r\n[rank0]: self.push(fn.call_function(self, args, kwargs)) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/torch.py”, line 953, in call_function\r\n[rank0]: tensor_variable = wrap_fx_proxy(\r\n[rank0]: ^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/builder.py”, line 2153, in wrap_fx_proxy\r\n[rank0]: return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/builder.py”, line 2219, in wrap_fx_proxy_cls\r\n[rank0]: return _wrap_fx_proxy(\r\n[rank0]: ^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/builder.py”, line 2315, in _wrap_fx_proxy\r\n[rank0]: example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py”, line 2536, in get_fake_value\r\n[rank0]: raise TorchRuntimeError(str(e)).with_traceback(e.<strong>traceback</strong>) from None\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py”, line 2471, in get_fake_value\r\n[rank0]: ret_val = wrap_fake_exception(\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py”, line 2017, in wrap_fake_exception\r\n[rank0]: return fn()\r\n[rank0]: ^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py”, line 2472, in <lambda>\r\n[rank0]: lambda: run_node(tx.output, node, args, kwargs, nnmodule)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py”, line 2604, in run_node\r\n[rank0]: raise RuntimeError(make_error_message(e)).with_traceback(\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py”, line 2586, in run_node\r\n[rank0]: return node.target(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_prims_common/wrappers.py”, line 289, in _fn\r\n[rank0]: result = fn(*args, is_out=(out is not None), **kwargs) # type: ignore[arg-type]\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py”, line 4444, in matmul\r\n[rank0]: return torch.ops.aten._unsafe_view(t1_folded.mv(t2), output_shape)\r\n[rank0]: ^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/utils/_stats.py”, line 21, in wrapper\r\n[rank0]: return fn(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py”, line 1276, in <strong>torch_dispatch</strong>\r\n[rank0]: return self.dispatch(func, types, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py”, line 1816, in dispatch\r\n[rank0]: return self._cached_dispatch_impl(func, types, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py”, line 1377, in _cached_dispatch_impl\r\n[rank0]: output = self._dispatch_impl(func, types, args, kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py”, line 2290, in _dispatch_impl\r\n[rank0]: decomposition_table[func](*args, **kwargs)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_prims_common/wrappers.py”, line 291, in _fn\r\n[rank0]: result = fn(*args, **kwargs)\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py”, line 83, in inner\r\n[rank0]: r = f(<em>tree_map(increase_prec, args), **tree_map(increase_prec, kwargs))\r\n[rank0]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py”, line 4336, in mv\r\n[rank0]: torch._check(\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<strong>init</strong>.py”, line 1656, in _check\r\n[rank0]: _check_with(RuntimeError, cond, message)\r\n[rank0]: File “/usr/local/lib/python3.11/dist-packages/torch/<strong>init</strong>.py”, line 1638, in _check_with\r\n[rank0]: raise error_type(message_evaluated)\r\n[rank0]: torch._dynamo.exc.TorchRuntimeError: Failed running call_function &#x3C;built-in method matmul of type object at 0x7f2e2a41ff00>(</em>(GradTrackingTensor(lvl=1, value=\r\n[rank0]: FakeTensor(…, device=‘cuda:0’, size=(1, s0, 896), dtype=torch.bfloat16,\r\n[rank0]: requires_grad=True)\r\n[rank0]: ), GradTrackingTensor(lvl=1, value=\r\n[rank0]: FakeTensor(…, device=‘cuda:0’, size=(0,), dtype=torch.bfloat16,\r\n[rank0]: requires_grad=True)\r\n[rank0]: )), **{}):\r\n[rank0]: size mismatch, got input (s0x896), vec (0)\r\nDoes liger GRPO support multi-gpu training with deepspeed zero3?</lambda></module></p>",{headings:850,localImagePaths:851,remoteImagePaths:852,frontmatter:853,imagePaths:856},[],[],[],{title:840,summary:15,pubDate:854,media:17,tags:855,link:843,thumbnail:15},"Sun, 25 May 2025 00:00:00 GMT",[19,20,21],[],"2025-06-20-🐯-liger-grpo-meets-trl-816c0f.md"];

export { _astro_dataLayerContent as default };

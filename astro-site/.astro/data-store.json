[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.9.2","content-config-digest","c2211fbb8f6109dd","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"http://localhost:4321/\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"experimentalDefaultStyles\":true},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"prism\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","posts",["Map",11,12,37,38,58,59,79,80,100,101,121,122,142,143,163,164,184,185,205,206,226,227,247,248,268,269,289,290,310,311,331,332,352,353,373,374,394,395,415,416,436,437,457,458,478,479,499,500,520,521,541,542,562,563,583,584,604,605,625,626,646,647,667,668,688,689,709,710,730,731,751,752,772,773,793,794,814,815,835,836,856,857,877,878,898,899,919,920,940,941,961,962,982,983,1003,1004,1024,1025,1045,1046,1066,1067,1087,1088,1108,1109,1129,1130,1150,1151,1171,1172,1192,1193,1213,1214,1234,1235,1255,1256,1276,1277,1297,1298,1318,1319,1339,1340,1360,1361,1381,1382,1402,1403,1423,1424,1444,1445,1465,1466,1486,1487,1507,1508,1528,1529,1549,1550,1570,1571,1591,1592,1612,1613,1633,1634,1654,1655,1675,1676,1696,1697,1717,1718,1738,1739,1759,1760,1780,1781,1801,1802,1822,1823,1843,1844,1864,1865,1885,1886,1906,1907,1927,1928,1948,1949,1969,1970,1990,1991,2011,2012,2032,2033,2053,2054,2074,2075,2095,2096,2116,2117,2137,2138,2158,2159,2179,2180,2200,2201,2221,2222,2242,2243,2263,2264,2284,2285,2305,2306,2326,2327,2347,2348,2368,2369,2389,2390,2410,2411,2431,2432,2452,2453,2473,2474,2494,2495,2515,2516,2536,2537,2557,2558,2578,2579,2599,2600,2620,2621,2641,2642,2662,2663,2683,2684,2704,2705,2725,2726,2746,2747,2767,2768,2788,2789,2809,2810,2830,2831,2851,2852,2872,2873,2893,2894,2914,2915,2935,2936,2956,2957,2977,2978,2998,2999,3019,3020,3040,3041,3061,3062,3082,3083,3103,3104,3124,3125,3145,3146,3166,3167,3187,3188,3208,3209,3229,3230,3250,3251,3271,3272,3292,3293,3313,3314,3334,3335,3355,3356,3376,3377,3397,3398,3418,3419,3439,3440,3460,3461,3481,3482,3502,3503,3517,3518,3532,3533,3547,3548,3562,3563,3577,3578,3592,3593,3607,3608,3622,3623,3637,3638,3652,3653,3667,3668,3682,3683,3697,3698,3712,3713,3727,3728,3742,3743,3757,3758,3772,3773,3787,3788,3802,3803,3817,3818,3832,3833,3847,3848,3862,3863,3877,3878,3892,3893,3907,3908,3922,3923,3937,3938,3952,3953,3967,3968,3982,3983],"2025-06-10-a-reinforcement-learning-enhanced-llm-framework-for-automated-ab-testing-in-pers-4e3451",{"id":11,"data":13,"body":23,"filePath":24,"digest":25,"rendered":26,"legacyId":36},{"title":14,"description":15,"pubDate":16,"source":17,"tags":18,"url":22},"A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing","arXiv:2506.06316v1 Announce Type: cross \nAbstract: For personalized marketing, a new challenge of how to effectively algorithm the A/B testing to maximize user response is urgently to be overcome. In this paper, we present a new approach, the RL-LLM-AB test framework, for using reinforcement learning strategy optimization combined with LLM to automate and personalize A/B tests. The RL-LLM-AB test is built upon the pre-trained instruction-tuned language model. It first generates A/B versions of candidate content variants using a Prompt-Conditioned Generator, and then dynamically embeds and fuses the user portrait and the context of the current query with the multi-modal perception module to constitute the current interaction state. The content version is then selected in real-time through the policy optimization module with an Actor-Critic structure, and long-term revenue is estimated according to real-time feedback (such as click-through rate and conversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded into the framework to capture long-term user preference drift, which helps to generalize policy across multiple users and content contexts. Numerical results demonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B testing methods, including classical A/B testing, Contextual Bandits, and benchmark reinforcement learning approaches on real-world marketing data.",["Date","2025-06-10T04:00:00.000Z"],"arXiv AI",[19,20,21],"arxiv","ai","research","https://arxiv.org/abs/2506.06316","Computer Science > Information Retrieval\r\n[Submitted on 27 May 2025]\r\nTitle:A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing\r\nView PDFAbstract:For personalized marketing, a new challenge of how to effectively algorithm the A/B testing to maximize user response is urgently to be overcome. In this paper, we present a new approach, the RL-LLM-AB test framework, for using reinforcement learning strategy optimization combined with LLM to automate and personalize A/B tests. The RL-LLM-AB test is built upon the pre-trained instruction-tuned language model. It first generates A/B versions of candidate content variants using a Prompt-Conditioned Generator, and then dynamically embeds and fuses the user portrait and the context of the current query with the multi-modal perception module to constitute the current interaction state. The content version is then selected in real-time through the policy optimization module with an Actor-Critic structure, and long-term revenue is estimated according to real-time feedback (such as click-through rate and conversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded into the framework to capture long-term user preference drift, which helps to generalize policy across multiple users and content contexts. Numerical results demonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B testing methods, including classical A/B testing, Contextual Bandits, and benchmark reinforcement learning approaches on real-world marketing data.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-a-reinforcement-learning-enhanced-llm-framework-for-automated-ab-testing-in-pers-4e3451.md","3c24b5518d017551",{"html":27,"metadata":28},"\u003Cp>Computer Science > Information Retrieval\r\n[Submitted on 27 May 2025]\r\nTitle:A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing\r\nView PDFAbstract:For personalized marketing, a new challenge of how to effectively algorithm the A/B testing to maximize user response is urgently to be overcome. In this paper, we present a new approach, the RL-LLM-AB test framework, for using reinforcement learning strategy optimization combined with LLM to automate and personalize A/B tests. The RL-LLM-AB test is built upon the pre-trained instruction-tuned language model. It first generates A/B versions of candidate content variants using a Prompt-Conditioned Generator, and then dynamically embeds and fuses the user portrait and the context of the current query with the multi-modal perception module to constitute the current interaction state. The content version is then selected in real-time through the policy optimization module with an Actor-Critic structure, and long-term revenue is estimated according to real-time feedback (such as click-through rate and conversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded into the framework to capture long-term user preference drift, which helps to generalize policy across multiple users and content contexts. Numerical results demonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B testing methods, including classical A/B testing, Contextual Bandits, and benchmark reinforcement learning approaches on real-world marketing data.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":29,"localImagePaths":30,"remoteImagePaths":31,"frontmatter":32,"imagePaths":35},[],[],[],{"title":14,"description":15,"pubDate":33,"source":17,"tags":34,"url":22},"Tue, 10 Jun 2025 00:00:00 -0400",[19,20,21],[],"2025-06-10-a-reinforcement-learning-enhanced-llm-framework-for-automated-ab-testing-in-pers-4e3451.md","2025-06-10-tau2-bench-evaluating-conversational-agents-in-a-dual-control-environment-95151b",{"id":37,"data":39,"body":45,"filePath":46,"digest":47,"rendered":48,"legacyId":57},{"title":40,"description":41,"pubDate":42,"source":17,"tags":43,"url":44},"$\\tau^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment","arXiv:2506.07982v1 Announce Type: new \nAbstract: Existing benchmarks for conversational AI agents simulate single-control environments, where only the AI agent can use tools to interact with the world, while the user remains a passive information provider. This differs from real-world scenarios like technical support, where users need to actively participate in modifying the state of the (shared) world. In order to address this gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both agent and user make use of tools to act in a shared, dynamic environment that tests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse, verifiable tasks from atomic components, ensuring domain coverage and controlled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose behavior is constrained by tools and observable states, improving simulation fidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations including separating errors arising from reasoning vs communication/coordination.\n  In particular, our experiments show significant performance drops when agents shift from no-user to dual-control, highlighting the challenges of guiding users. Overall, $\\tau^2$-bench provides a controlled testbed for agents that must both reason effectively and guide user actions.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07982","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment\r\nView PDFAbstract:Existing benchmarks for conversational AI agents simulate single-control environments, where only the AI agent can use tools to interact with the world, while the user remains a passive information provider. This differs from real-world scenarios like technical support, where users need to actively participate in modifying the state of the (shared) world. In order to address this gap, we introduce $\\tau^2$-bench, with four key contributions:\r\n1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both agent and user make use of tools to act in a shared, dynamic environment that tests both agent coordination and communication,\r\n2) A compositional task generator that programmatically creates diverse, verifiable tasks from atomic components, ensuring domain coverage and controlled complexity,\r\n3) A reliable user simulator tightly coupled with the environment, whose behavior is constrained by tools and observable states, improving simulation fidelity,\r\n4) Fine-grained analysis of agent performance through multiple ablations including separating errors arising from reasoning vs communication/coordination.\r\nIn particular, our experiments show significant performance drops when agents shift from no-user to dual-control, highlighting the challenges of guiding users. Overall, $\\tau^2$-bench provides a controlled testbed for agents that must both reason effectively and guide user actions.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-$tau^2$-bench-evaluating-conversational-agents-in-a-dual-control-environment-95151b.md","bca51ed5c981b4af",{"html":49,"metadata":50},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment\r\nView PDFAbstract:Existing benchmarks for conversational AI agents simulate single-control environments, where only the AI agent can use tools to interact with the world, while the user remains a passive information provider. This differs from real-world scenarios like technical support, where users need to actively participate in modifying the state of the (shared) world. In order to address this gap, we introduce $\\tau^2$-bench, with four key contributions:\u003C/p>\n\u003Col>\n\u003Cli>A novel Telecom dual-control domain modeled as a Dec-POMDP, where both agent and user make use of tools to act in a shared, dynamic environment that tests both agent coordination and communication,\u003C/li>\n\u003Cli>A compositional task generator that programmatically creates diverse, verifiable tasks from atomic components, ensuring domain coverage and controlled complexity,\u003C/li>\n\u003Cli>A reliable user simulator tightly coupled with the environment, whose behavior is constrained by tools and observable states, improving simulation fidelity,\u003C/li>\n\u003Cli>Fine-grained analysis of agent performance through multiple ablations including separating errors arising from reasoning vs communication/coordination.\r\nIn particular, our experiments show significant performance drops when agents shift from no-user to dual-control, highlighting the challenges of guiding users. Overall, $\\tau^2$-bench provides a controlled testbed for agents that must both reason effectively and guide user actions.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/li>\n\u003C/ol>",{"headings":51,"localImagePaths":52,"remoteImagePaths":53,"frontmatter":54,"imagePaths":56},[],[],[],{"title":40,"description":41,"pubDate":33,"source":17,"tags":55,"url":44},[19,20,21],[],"2025-06-10-$tau^2$-bench-evaluating-conversational-agents-in-a-dual-control-environment-95151b.md","2025-06-10-a-proposal-to-extend-the-common-model-of-cognition-with-metacognition-f1f6c8",{"id":58,"data":60,"body":66,"filePath":67,"digest":68,"rendered":69,"legacyId":78},{"title":61,"description":62,"pubDate":63,"source":17,"tags":64,"url":65},"A Proposal to Extend the Common Model of Cognition with Metacognition","arXiv:2506.07807v1 Announce Type: new \nAbstract: The Common Model of Cognition (CMC) provides an abstract characterization of the structure and processing required by a cognitive architecture for human-like minds. We propose a unified approach to integrating metacognition within the CMC. We propose that metacognition involves reasoning over explicit representations of an agent's cognitive capabilities and processes in working memory. Our proposal exploits the existing cognitive capabilities of the CMC, making minimal extensions in the structure and information available within working memory. We provide examples of metacognition within our proposal.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07807","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:A Proposal to Extend the Common Model of Cognition with Metacognition\r\nView PDF HTML (experimental)Abstract:The Common Model of Cognition (CMC) provides an abstract characterization of the structure and processing required by a cognitive architecture for human-like minds. We propose a unified approach to integrating metacognition within the CMC. We propose that metacognition involves reasoning over explicit representations of an agent's cognitive capabilities and processes in working memory. Our proposal exploits the existing cognitive capabilities of the CMC, making minimal extensions in the structure and information available within working memory. We provide examples of metacognition within our proposal.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-a-proposal-to-extend-the-common-model-of-cognition-with-metacognition-f1f6c8.md","11731ad7c8c2994c",{"html":70,"metadata":71},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:A Proposal to Extend the Common Model of Cognition with Metacognition\r\nView PDF HTML (experimental)Abstract:The Common Model of Cognition (CMC) provides an abstract characterization of the structure and processing required by a cognitive architecture for human-like minds. We propose a unified approach to integrating metacognition within the CMC. We propose that metacognition involves reasoning over explicit representations of an agent’s cognitive capabilities and processes in working memory. Our proposal exploits the existing cognitive capabilities of the CMC, making minimal extensions in the structure and information available within working memory. We provide examples of metacognition within our proposal.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":72,"localImagePaths":73,"remoteImagePaths":74,"frontmatter":75,"imagePaths":77},[],[],[],{"title":61,"description":62,"pubDate":33,"source":17,"tags":76,"url":65},[19,20,21],[],"2025-06-10-a-proposal-to-extend-the-common-model-of-cognition-with-metacognition-f1f6c8.md","2025-06-10-a-reinforcement-learning-approach-for-ris-aided-fair-communications-4f6f89",{"id":79,"data":81,"body":87,"filePath":88,"digest":89,"rendered":90,"legacyId":99},{"title":82,"description":83,"pubDate":84,"source":17,"tags":85,"url":86},"A Reinforcement Learning Approach for RIS-aided Fair Communications","arXiv:2506.06344v1 Announce Type: cross \nAbstract: Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements that can dynamically alter electromagnetic wave properties to enhance beamforming and leading to improvements in areas with low coverage properties. They have the potential to be combined with Reinforcement Learning (RL) techniques to achieve network performance and energy efficiency via optimization techniques. In addition to performance and energy improvements, it is also crucial to consider the concept of fair communications. RISs must ensure that User Equipment (UE) units receive their signals with adequate strength, without other UE being deprived of service due to insufficient power. In this paper, we address such a problem. We explore the fairness properties of previous work and propose a novel method that aims at obtaining an efficient and fair duplex RIS-RL system for multiple legitimate UE units. We report and discuss our experimental work and simulation results. We also release our code and datasets to foster further research in the topic.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06344","Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 1 Jun 2025]\r\nTitle:A Reinforcement Learning Approach for RIS-aided Fair Communications\r\nView PDF HTML (experimental)Abstract:Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements that can dynamically alter electromagnetic wave properties to enhance beamforming and leading to improvements in areas with low coverage properties. They have the potential to be combined with Reinforcement Learning (RL) techniques to achieve network performance and energy efficiency via optimization techniques. In addition to performance and energy improvements, it is also crucial to consider the concept of fair communications. RISs must ensure that User Equipment (UE) units receive their signals with adequate strength, without other UE being deprived of service due to insufficient power. In this paper, we address such a problem. We explore the fairness properties of previous work and propose a novel method that aims at obtaining an efficient and fair duplex RIS-RL system for multiple legitimate UE units. We report and discuss our experimental work and simulation results. We also release our code and datasets to foster further research in the topic.\r\nSubmission history\r\nFrom: Joaquin Garcia-Alfaro [view email][v1] Sun, 1 Jun 2025 13:00:26 UTC (1,213 KB)\r\nCurrent browse context:\r\neess.SP\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-a-reinforcement-learning-approach-for-ris-aided-fair-communications-4f6f89.md","9a3b7bb8df48ae4b",{"html":91,"metadata":92},"\u003Cp>Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 1 Jun 2025]\r\nTitle:A Reinforcement Learning Approach for RIS-aided Fair Communications\r\nView PDF HTML (experimental)Abstract:Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements that can dynamically alter electromagnetic wave properties to enhance beamforming and leading to improvements in areas with low coverage properties. They have the potential to be combined with Reinforcement Learning (RL) techniques to achieve network performance and energy efficiency via optimization techniques. In addition to performance and energy improvements, it is also crucial to consider the concept of fair communications. RISs must ensure that User Equipment (UE) units receive their signals with adequate strength, without other UE being deprived of service due to insufficient power. In this paper, we address such a problem. We explore the fairness properties of previous work and propose a novel method that aims at obtaining an efficient and fair duplex RIS-RL system for multiple legitimate UE units. We report and discuss our experimental work and simulation results. We also release our code and datasets to foster further research in the topic.\r\nSubmission history\r\nFrom: Joaquin Garcia-Alfaro [view email][v1] Sun, 1 Jun 2025 13:00:26 UTC (1,213 KB)\r\nCurrent browse context:\r\neess.SP\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":93,"localImagePaths":94,"remoteImagePaths":95,"frontmatter":96,"imagePaths":98},[],[],[],{"title":82,"description":83,"pubDate":33,"source":17,"tags":97,"url":86},[19,20,21],[],"2025-06-10-a-reinforcement-learning-approach-for-ris-aided-fair-communications-4f6f89.md","2025-06-10-a-temporal-frbrfrbroo-based-model-for-component-level-versioning-of-legal-norms-765085",{"id":100,"data":102,"body":108,"filePath":109,"digest":110,"rendered":111,"legacyId":120},{"title":103,"description":104,"pubDate":105,"source":17,"tags":106,"url":107},"A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms","arXiv:2506.07853v1 Announce Type: new \nAbstract: Effectively representing legal norms for automated processing is a critical challenge, particularly in tracking the diachronic evolution of their hierarchical components (e.g., articles, paragraphs). While foundational frameworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal documents at a macro level, they lack native mechanisms for granular, component-level versioning. This limitation hinders the deterministic point-in-time reconstruction of legal texts, a fundamental capability for reliable Legal Tech and AI applications. This paper proposes a structured, temporal model that extends the FRBRoo framework to address this gap. It introduces specialized subclasses of Expressio - Temporal Version (TV) and Language Version (LV - to represent the state of a legal norm and its linguistic variations at specific points in time. The model applies this same paradigm hierarchically, introducing Component Work (CW), Component Temporal Version (CTV), and Component Language Version (CLV) to track the lifecycle of individual articles, paragraphs, and clauses. Using the Brazilian Federal Constitution as a case study, the paper demonstrates how each amendment creates new Component Temporal Versions for affected provisions, while unaffected components retain their existing versions. This fine-grained, time-aware architecture enables the precise, deterministic retrieval and reconstruction of any part of a legal text as it existed on a specific date. The model provides a robust foundation for developing advanced legal information systems, knowledge graphs, and AI tools capable of accurate historical analysis and impact assessment, overcoming the limitations of current generative models.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07853","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms\r\nView PDF HTML (experimental)Abstract:Effectively representing legal norms for automated processing is a critical challenge, particularly in tracking the diachronic evolution of their hierarchical components (e.g., articles, paragraphs). While foundational frameworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal documents at a macro level, they lack native mechanisms for granular, component-level versioning. This limitation hinders the deterministic point-in-time reconstruction of legal texts, a fundamental capability for reliable Legal Tech and AI applications. This paper proposes a structured, temporal model that extends the FRBRoo framework to address this gap. It introduces specialized subclasses of Expressio - Temporal Version (TV) and Language Version (LV - to represent the state of a legal norm and its linguistic variations at specific points in time. The model applies this same paradigm hierarchically, introducing Component Work (CW), Component Temporal Version (CTV), and Component Language Version (CLV) to track the lifecycle of individual articles, paragraphs, and clauses. Using the Brazilian Federal Constitution as a case study, the paper demonstrates how each amendment creates new Component Temporal Versions for affected provisions, while unaffected components retain their existing versions. This fine-grained, time-aware architecture enables the precise, deterministic retrieval and reconstruction of any part of a legal text as it existed on a specific date. The model provides a robust foundation for developing advanced legal information systems, knowledge graphs, and AI tools capable of accurate historical analysis and impact assessment, overcoming the limitations of current generative models.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-a-temporal-frbrfrbroo-based-model-for-component-level-versioning-of-legal-norms-765085.md","09e39af0e3e5bcdc",{"html":112,"metadata":113},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms\r\nView PDF HTML (experimental)Abstract:Effectively representing legal norms for automated processing is a critical challenge, particularly in tracking the diachronic evolution of their hierarchical components (e.g., articles, paragraphs). While foundational frameworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal documents at a macro level, they lack native mechanisms for granular, component-level versioning. This limitation hinders the deterministic point-in-time reconstruction of legal texts, a fundamental capability for reliable Legal Tech and AI applications. This paper proposes a structured, temporal model that extends the FRBRoo framework to address this gap. It introduces specialized subclasses of Expressio - Temporal Version (TV) and Language Version (LV - to represent the state of a legal norm and its linguistic variations at specific points in time. The model applies this same paradigm hierarchically, introducing Component Work (CW), Component Temporal Version (CTV), and Component Language Version (CLV) to track the lifecycle of individual articles, paragraphs, and clauses. Using the Brazilian Federal Constitution as a case study, the paper demonstrates how each amendment creates new Component Temporal Versions for affected provisions, while unaffected components retain their existing versions. This fine-grained, time-aware architecture enables the precise, deterministic retrieval and reconstruction of any part of a legal text as it existed on a specific date. The model provides a robust foundation for developing advanced legal information systems, knowledge graphs, and AI tools capable of accurate historical analysis and impact assessment, overcoming the limitations of current generative models.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":114,"localImagePaths":115,"remoteImagePaths":116,"frontmatter":117,"imagePaths":119},[],[],[],{"title":103,"description":104,"pubDate":33,"source":17,"tags":118,"url":107},[19,20,21],[],"2025-06-10-a-temporal-frbrfrbroo-based-model-for-component-level-versioning-of-legal-norms-765085.md","2025-06-10-agent-semantics-semantic-spacetime-and-graphical-reasoning-621077",{"id":121,"data":123,"body":129,"filePath":130,"digest":131,"rendered":132,"legacyId":141},{"title":124,"description":125,"pubDate":126,"source":17,"tags":127,"url":128},"Agent Semantics, Semantic Spacetime, and Graphical Reasoning","arXiv:2506.07756v1 Announce Type: new \nAbstract: Some formal aspects of the Semantic Spacetime graph model are presented, with reference to its use for directed knowledge representations and process modelling. A finite $\\gamma(3,4)$ representation is defined to form a closed set of operations that can scale to any degree of semantic complexity. The Semantic Spacetime postulates bring predictability with minimal constraints to pathways in graphs. The ubiquitous appearance of absorbing states in any partial graph means that a graph process leaks information. The issue is closely associated with the issue of division by zero, which signals a loss of closure and the need for manual injection of remedial information. The Semantic Spacetime model (and its Promise Theory) origins help to clarify how such absorbing states are associated with boundary information where intentionality can enter.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07756","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Agent Semantics, Semantic Spacetime, and Graphical Reasoning\r\nView PDFAbstract:Some formal aspects of the Semantic Spacetime graph model are presented, with reference to its use for directed knowledge representations and process modelling. A finite $\\gamma(3,4)$ representation is defined to form a closed set of operations that can scale to any degree of semantic complexity. The Semantic Spacetime postulates bring predictability with minimal constraints to pathways in graphs. The ubiquitous appearance of absorbing states in any partial graph means that a graph process leaks information. The issue is closely associated with the issue of division by zero, which signals a loss of closure and the need for manual injection of remedial information. The Semantic Spacetime model (and its Promise Theory) origins help to clarify how such absorbing states are associated with boundary information where intentionality can enter.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-agent-semantics,-semantic-spacetime,-and-graphical-reasoning-621077.md","2502b99d1361ace2",{"html":133,"metadata":134},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Agent Semantics, Semantic Spacetime, and Graphical Reasoning\r\nView PDFAbstract:Some formal aspects of the Semantic Spacetime graph model are presented, with reference to its use for directed knowledge representations and process modelling. A finite $\\gamma(3,4)$ representation is defined to form a closed set of operations that can scale to any degree of semantic complexity. The Semantic Spacetime postulates bring predictability with minimal constraints to pathways in graphs. The ubiquitous appearance of absorbing states in any partial graph means that a graph process leaks information. The issue is closely associated with the issue of division by zero, which signals a loss of closure and the need for manual injection of remedial information. The Semantic Spacetime model (and its Promise Theory) origins help to clarify how such absorbing states are associated with boundary information where intentionality can enter.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":135,"localImagePaths":136,"remoteImagePaths":137,"frontmatter":138,"imagePaths":140},[],[],[],{"title":124,"description":125,"pubDate":33,"source":17,"tags":139,"url":128},[19,20,21],[],"2025-06-10-agent-semantics,-semantic-spacetime,-and-graphical-reasoning-621077.md","2025-06-10-ai-simulation-by-digital-twins-systematic-survey-reference-framework-and-mappi-c726f1",{"id":142,"data":144,"body":150,"filePath":151,"digest":152,"rendered":153,"legacyId":162},{"title":145,"description":146,"pubDate":147,"source":17,"tags":148,"url":149},"AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture","arXiv:2506.06580v1 Announce Type: new \nAbstract: Insufficient data volume and quality are particularly pressing challenges in the adoption of modern subsymbolic AI. To alleviate these challenges, AI simulation uses virtual training environments in which AI agents can be safely and efficiently developed with simulated, synthetic data. Digital twins open new avenues in AI simulation, as these high-fidelity virtual replicas of physical systems are equipped with state-of-the-art simulators and the ability to further interact with the physical system for additional data collection. In this article, we report on our systematic survey of digital twin-enabled AI simulation. By analyzing 22 primary studies, we identify technological trends and derive a reference framework to situate digital twins and AI components. Based on our findings, we derive a reference framework and provide architectural guidelines by mapping it onto the ISO 23247 reference architecture for digital twins. Finally, we identify challenges and research opportunities for prospective researchers.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06580","Computer Science > Artificial Intelligence\r\n[Submitted on 6 Jun 2025]\r\nTitle:AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture\r\nView PDFAbstract:Insufficient data volume and quality are particularly pressing challenges in the adoption of modern subsymbolic AI. To alleviate these challenges, AI simulation uses virtual training environments in which AI agents can be safely and efficiently developed with simulated, synthetic data. Digital twins open new avenues in AI simulation, as these high-fidelity virtual replicas of physical systems are equipped with state-of-the-art simulators and the ability to further interact with the physical system for additional data collection. In this article, we report on our systematic survey of digital twin-enabled AI simulation. By analyzing 22 primary studies, we identify technological trends and derive a reference framework to situate digital twins and AI components. Based on our findings, we derive a reference framework and provide architectural guidelines by mapping it onto the ISO 23247 reference architecture for digital twins. Finally, we identify challenges and research opportunities for prospective researchers.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-ai-simulation-by-digital-twins-systematic-survey,-reference-framework,-and-mappi-c726f1.md","2e6552154ac251d6",{"html":154,"metadata":155},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 6 Jun 2025]\r\nTitle:AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture\r\nView PDFAbstract:Insufficient data volume and quality are particularly pressing challenges in the adoption of modern subsymbolic AI. To alleviate these challenges, AI simulation uses virtual training environments in which AI agents can be safely and efficiently developed with simulated, synthetic data. Digital twins open new avenues in AI simulation, as these high-fidelity virtual replicas of physical systems are equipped with state-of-the-art simulators and the ability to further interact with the physical system for additional data collection. In this article, we report on our systematic survey of digital twin-enabled AI simulation. By analyzing 22 primary studies, we identify technological trends and derive a reference framework to situate digital twins and AI components. Based on our findings, we derive a reference framework and provide architectural guidelines by mapping it onto the ISO 23247 reference architecture for digital twins. Finally, we identify challenges and research opportunities for prospective researchers.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":156,"localImagePaths":157,"remoteImagePaths":158,"frontmatter":159,"imagePaths":161},[],[],[],{"title":145,"description":146,"pubDate":33,"source":17,"tags":160,"url":149},[19,20,21],[],"2025-06-10-ai-simulation-by-digital-twins-systematic-survey,-reference-framework,-and-mappi-c726f1.md","2025-06-10-ai-psyroom-artificial-intelligence-platform-for-segmented-yearning-and-reactive--ad0200",{"id":163,"data":165,"body":171,"filePath":172,"digest":173,"rendered":174,"legacyId":183},{"title":166,"description":167,"pubDate":168,"source":17,"tags":169,"url":170},"AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method","arXiv:2506.06740v1 Announce Type: new \nAbstract: Psychological counseling faces huge challenges due to the growing demand for mental health services and the shortage of trained professionals. Large language models (LLMs) have shown potential to assist psychological counseling, especially in empathy and emotional support. However, existing models lack a deep understanding of emotions and are unable to generate personalized treatment plans based on fine-grained emotions. To address these shortcomings, we present AI PsyRoom, a multi-agent simulation framework designed to enhance psychological counseling by generating empathetic and emotionally nuanced conversations. By leveraging fine-grained emotion classification and a multi-agent framework, we construct a multi-agent PsyRoom A for dialogue reconstruction, generating a high-quality dialogue dataset EmoPsy, which contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues. We also propose PsyRoom B for generating personalized treatment plans. Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms state-of-the-art methods, achieving 18% improvement in problem orientation, 23% in expression, 24% in Empathy, and 16% in interactive communication quality. The datasets and models are publicly available, providing a foundation for advancing AI-assisted psychological counseling research.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06740","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method\r\nView PDF HTML (experimental)Abstract:Psychological counseling faces huge challenges due to the growing demand for mental health services and the shortage of trained professionals. Large language models (LLMs) have shown potential to assist psychological counseling, especially in empathy and emotional support. However, existing models lack a deep understanding of emotions and are unable to generate personalized treatment plans based on fine-grained emotions. To address these shortcomings, we present AI PsyRoom, a multi-agent simulation framework designed to enhance psychological counseling by generating empathetic and emotionally nuanced conversations. By leveraging fine-grained emotion classification and a multi-agent framework, we construct a multi-agent PsyRoom A for dialogue reconstruction, generating a high-quality dialogue dataset EmoPsy, which contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues. We also propose PsyRoom B for generating personalized treatment plans. Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms state-of-the-art methods, achieving 18% improvement in problem orientation, 23% in expression, 24% in Empathy, and 16% in interactive communication quality. The datasets and models are publicly available, providing a foundation for advancing AI-assisted psychological counseling research.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-ai-psyroom-artificial-intelligence-platform-for-segmented-yearning-and-reactive--ad0200.md","933a25f5219ba9e9",{"html":175,"metadata":176},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method\r\nView PDF HTML (experimental)Abstract:Psychological counseling faces huge challenges due to the growing demand for mental health services and the shortage of trained professionals. Large language models (LLMs) have shown potential to assist psychological counseling, especially in empathy and emotional support. However, existing models lack a deep understanding of emotions and are unable to generate personalized treatment plans based on fine-grained emotions. To address these shortcomings, we present AI PsyRoom, a multi-agent simulation framework designed to enhance psychological counseling by generating empathetic and emotionally nuanced conversations. By leveraging fine-grained emotion classification and a multi-agent framework, we construct a multi-agent PsyRoom A for dialogue reconstruction, generating a high-quality dialogue dataset EmoPsy, which contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues. We also propose PsyRoom B for generating personalized treatment plans. Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms state-of-the-art methods, achieving 18% improvement in problem orientation, 23% in expression, 24% in Empathy, and 16% in interactive communication quality. The datasets and models are publicly available, providing a foundation for advancing AI-assisted psychological counseling research.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":177,"localImagePaths":178,"remoteImagePaths":179,"frontmatter":180,"imagePaths":182},[],[],[],{"title":166,"description":167,"pubDate":33,"source":17,"tags":181,"url":170},[19,20,21],[],"2025-06-10-ai-psyroom-artificial-intelligence-platform-for-segmented-yearning-and-reactive--ad0200.md","2025-06-10-active-test-time-vision-language-navigation-be7d43",{"id":184,"data":186,"body":192,"filePath":193,"digest":194,"rendered":195,"legacyId":204},{"title":187,"description":188,"pubDate":189,"source":17,"tags":190,"url":191},"Active Test-time Vision-Language Navigation","arXiv:2506.06630v1 Announce Type: cross \nAbstract: Vision-Language Navigation (VLN) policies trained on offline datasets often exhibit degraded task performance when deployed in unfamiliar navigation environments at test time, where agents are typically evaluated without access to external interaction or feedback. Entropy minimization has emerged as a practical solution for reducing prediction uncertainty at test time; however, it can suffer from accumulated errors, as agents may become overconfident in incorrect actions without sufficient contextual grounding. To tackle these challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time active learning framework that enables a practical human-robot interaction via episodic feedback on uncertain navigation outcomes. In particular, ATENA learns to increase certainty in successful episodes and decrease it in failed ones, improving uncertainty calibration. Here, we propose mixture entropy optimization, where entropy is obtained from a combination of the action and pseudo-expert distributions-a hypothetical action distribution assuming the agent's selected action to be optimal-controlling both prediction confidence and action preference. In addition, we propose a self-active learning strategy that enables an agent to evaluate its navigation outcomes based on confident predictions. As a result, the agent stays actively engaged throughout all iterations, leading to well-grounded and adaptive decision-making. Extensive evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate that ATENA successfully overcomes distributional shifts at test time, outperforming the compared baseline methods across various settings.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06630","Computer Science > Robotics\r\n[Submitted on 7 Jun 2025]\r\nTitle:Active Test-time Vision-Language Navigation\r\nView PDF HTML (experimental)Abstract:Vision-Language Navigation (VLN) policies trained on offline datasets often exhibit degraded task performance when deployed in unfamiliar navigation environments at test time, where agents are typically evaluated without access to external interaction or feedback. Entropy minimization has emerged as a practical solution for reducing prediction uncertainty at test time; however, it can suffer from accumulated errors, as agents may become overconfident in incorrect actions without sufficient contextual grounding. To tackle these challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time active learning framework that enables a practical human-robot interaction via episodic feedback on uncertain navigation outcomes. In particular, ATENA learns to increase certainty in successful episodes and decrease it in failed ones, improving uncertainty calibration. Here, we propose mixture entropy optimization, where entropy is obtained from a combination of the action and pseudo-expert distributions-a hypothetical action distribution assuming the agent's selected action to be optimal-controlling both prediction confidence and action preference. In addition, we propose a self-active learning strategy that enables an agent to evaluate its navigation outcomes based on confident predictions. As a result, the agent stays actively engaged throughout all iterations, leading to well-grounded and adaptive decision-making. Extensive evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate that ATENA successfully overcomes distributional shifts at test time, outperforming the compared baseline methods across various settings.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-active-test-time-vision-language-navigation-be7d43.md","d10b5c08c5ba6b2d",{"html":196,"metadata":197},"\u003Cp>Computer Science > Robotics\r\n[Submitted on 7 Jun 2025]\r\nTitle:Active Test-time Vision-Language Navigation\r\nView PDF HTML (experimental)Abstract:Vision-Language Navigation (VLN) policies trained on offline datasets often exhibit degraded task performance when deployed in unfamiliar navigation environments at test time, where agents are typically evaluated without access to external interaction or feedback. Entropy minimization has emerged as a practical solution for reducing prediction uncertainty at test time; however, it can suffer from accumulated errors, as agents may become overconfident in incorrect actions without sufficient contextual grounding. To tackle these challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time active learning framework that enables a practical human-robot interaction via episodic feedback on uncertain navigation outcomes. In particular, ATENA learns to increase certainty in successful episodes and decrease it in failed ones, improving uncertainty calibration. Here, we propose mixture entropy optimization, where entropy is obtained from a combination of the action and pseudo-expert distributions-a hypothetical action distribution assuming the agent’s selected action to be optimal-controlling both prediction confidence and action preference. In addition, we propose a self-active learning strategy that enables an agent to evaluate its navigation outcomes based on confident predictions. As a result, the agent stays actively engaged throughout all iterations, leading to well-grounded and adaptive decision-making. Extensive evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate that ATENA successfully overcomes distributional shifts at test time, outperforming the compared baseline methods across various settings.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":198,"localImagePaths":199,"remoteImagePaths":200,"frontmatter":201,"imagePaths":203},[],[],[],{"title":187,"description":188,"pubDate":33,"source":17,"tags":202,"url":191},[19,20,21],[],"2025-06-10-active-test-time-vision-language-navigation-be7d43.md","2025-06-10-addition-in-four-movements-mapping-layer-wise-information-trajectories-in-llms-589a39",{"id":205,"data":207,"body":213,"filePath":214,"digest":215,"rendered":216,"legacyId":225},{"title":208,"description":209,"pubDate":210,"source":17,"tags":211,"url":212},"Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs","arXiv:2506.07824v1 Announce Type: new \nAbstract: Multi-digit addition is a clear probe of the computational power of large language models. To dissect the internal arithmetic processes in LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection. Inspired by the step-by-step manner in which humans perform addition, we propose and analyze a coherent four-stage trajectory in the forward pass:Formula-structure representations become linearly decodable first, while the answer token is still far down the candidate list.Core computational features then emerge prominently.At deeper activation layers, numerical abstractions of the result become clearer, enabling near-perfect detection and decoding of the individual digits in the sum.Near the output, the model organizes and generates the final content, with the correct token reliably occupying the top rank.This trajectory suggests a hierarchical process that favors internal computation over rote memorization. We release our code and data to facilitate reproducibility.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07824","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs\r\nView PDF HTML (experimental)Abstract:Multi-digit addition is a clear probe of the computational power of large language models. To dissect the internal arithmetic processes in LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection. Inspired by the step-by-step manner in which humans perform addition, we propose and analyze a coherent four-stage trajectory in the forward pass:Formula-structure representations become linearly decodable first, while the answer token is still far down the candidate this http URL computational features then emerge this http URL deeper activation layers, numerical abstractions of the result become clearer, enabling near-perfect detection and decoding of the individual digits in the this http URL the output, the model organizes and generates the final content, with the correct token reliably occupying the top this http URL trajectory suggests a hierarchical process that favors internal computation over rote memorization. We release our code and data to facilitate reproducibility.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-addition-in-four-movements-mapping-layer-wise-information-trajectories-in-llms-589a39.md","bdf3d788be4c479e",{"html":217,"metadata":218},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs\r\nView PDF HTML (experimental)Abstract:Multi-digit addition is a clear probe of the computational power of large language models. To dissect the internal arithmetic processes in LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection. Inspired by the step-by-step manner in which humans perform addition, we propose and analyze a coherent four-stage trajectory in the forward pass:Formula-structure representations become linearly decodable first, while the answer token is still far down the candidate this http URL computational features then emerge this http URL deeper activation layers, numerical abstractions of the result become clearer, enabling near-perfect detection and decoding of the individual digits in the this http URL the output, the model organizes and generates the final content, with the correct token reliably occupying the top this http URL trajectory suggests a hierarchical process that favors internal computation over rote memorization. We release our code and data to facilitate reproducibility.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":219,"localImagePaths":220,"remoteImagePaths":221,"frontmatter":222,"imagePaths":224},[],[],[],{"title":208,"description":209,"pubDate":33,"source":17,"tags":223,"url":212},[19,20,21],[],"2025-06-10-addition-in-four-movements-mapping-layer-wise-information-trajectories-in-llms-589a39.md","2025-06-10-an-agentic-framework-for-autonomous-metamaterial-modeling-and-inverse-design-1e3057",{"id":226,"data":228,"body":234,"filePath":235,"digest":236,"rendered":237,"legacyId":246},{"title":229,"description":230,"pubDate":231,"source":17,"tags":232,"url":233},"An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design","arXiv:2506.06935v1 Announce Type: new \nAbstract: Recent significant advances in integrating multiple Large Language Model (LLM) systems have enabled Agentic Frameworks capable of performing complex tasks autonomously, including novel scientific research. We develop and demonstrate such a framework specifically for the inverse design of photonic metamaterials. When queried with a desired optical spectrum, the Agent autonomously proposes and develops a forward deep learning model, accesses external tools via APIs for tasks like simulation and optimization, utilizes memory, and generates a final design via a deep inverse method. The framework's effectiveness is demonstrated in its ability to automate, reason, plan, and adapt. Notably, the Agentic Framework possesses internal reflection and decision flexibility, permitting highly varied and potentially novel outputs.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06935","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design\r\nView PDF HTML (experimental)Abstract:Recent significant advances in integrating multiple Large Language Model (LLM) systems have enabled Agentic Frameworks capable of performing complex tasks autonomously, including novel scientific research. We develop and demonstrate such a framework specifically for the inverse design of photonic metamaterials. When queried with a desired optical spectrum, the Agent autonomously proposes and develops a forward deep learning model, accesses external tools via APIs for tasks like simulation and optimization, utilizes memory, and generates a final design via a deep inverse method. The framework's effectiveness is demonstrated in its ability to automate, reason, plan, and adapt. Notably, the Agentic Framework possesses internal reflection and decision flexibility, permitting highly varied and potentially novel outputs.\r\nCurrent browse context:\r\ncs.AI\r\nChange to browse by:\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-an-agentic-framework-for-autonomous-metamaterial-modeling-and-inverse-design-1e3057.md","f9177dbe9773c7f1",{"html":238,"metadata":239},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design\r\nView PDF HTML (experimental)Abstract:Recent significant advances in integrating multiple Large Language Model (LLM) systems have enabled Agentic Frameworks capable of performing complex tasks autonomously, including novel scientific research. We develop and demonstrate such a framework specifically for the inverse design of photonic metamaterials. When queried with a desired optical spectrum, the Agent autonomously proposes and develops a forward deep learning model, accesses external tools via APIs for tasks like simulation and optimization, utilizes memory, and generates a final design via a deep inverse method. The framework’s effectiveness is demonstrated in its ability to automate, reason, plan, and adapt. Notably, the Agentic Framework possesses internal reflection and decision flexibility, permitting highly varied and potentially novel outputs.\r\nCurrent browse context:\r\ncs.AI\r\nChange to browse by:\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":240,"localImagePaths":241,"remoteImagePaths":242,"frontmatter":243,"imagePaths":245},[],[],[],{"title":229,"description":230,"pubDate":33,"source":17,"tags":244,"url":233},[19,20,21],[],"2025-06-10-an-agentic-framework-for-autonomous-metamaterial-modeling-and-inverse-design-1e3057.md","2025-06-10-an-intelligent-fault-self-healing-mechanism-for-cloud-ai-systems-via-integration-1c89ad",{"id":247,"data":249,"body":255,"filePath":256,"digest":257,"rendered":258,"legacyId":267},{"title":250,"description":251,"pubDate":252,"source":17,"tags":253,"url":254},"An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning","arXiv:2506.07411v1 Announce Type: new \nAbstract: As the scale and complexity of cloud-based AI systems continue to increase, the detection and adaptive recovery of system faults have become the core challenges to ensure service reliability and continuity. In this paper, we propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to realize a fault recovery framework with semantic understanding and policy optimization capabilities in cloud AI systems. On the basis of the traditional DRL-based control model, the proposed method constructs a two-stage hybrid architecture: (1) an LLM-driven fault semantic interpretation module, which can dynamically extract deep contextual semantics from multi-source logs and system indicators to accurately identify potential fault modes; (2) DRL recovery strategy optimizer, based on reinforcement learning, learns the dynamic matching of fault types and response behaviors in the cloud environment. The innovation of this method lies in the introduction of LLM for environment modeling and action space abstraction, which greatly improves the exploration efficiency and generalization ability of reinforcement learning. At the same time, a memory-guided meta-controller is introduced, combined with reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve continuous adaptation to new failure modes and avoid catastrophic forgetting. Experimental results on the cloud fault injection platform show that compared with the existing DRL and rule methods, the IFSHM framework shortens the system recovery time by 37% with unknown fault scenarios.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07411","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning\r\nView PDF HTML (experimental)Abstract:As the scale and complexity of cloud-based AI systems continue to increase, the detection and adaptive recovery of system faults have become the core challenges to ensure service reliability and continuity. In this paper, we propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to realize a fault recovery framework with semantic understanding and policy optimization capabilities in cloud AI systems. On the basis of the traditional DRL-based control model, the proposed method constructs a two-stage hybrid architecture: (1) an LLM-driven fault semantic interpretation module, which can dynamically extract deep contextual semantics from multi-source logs and system indicators to accurately identify potential fault modes; (2) DRL recovery strategy optimizer, based on reinforcement learning, learns the dynamic matching of fault types and response behaviors in the cloud environment. The innovation of this method lies in the introduction of LLM for environment modeling and action space abstraction, which greatly improves the exploration efficiency and generalization ability of reinforcement learning. At the same time, a memory-guided meta-controller is introduced, combined with reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve continuous adaptation to new failure modes and avoid catastrophic forgetting. Experimental results on the cloud fault injection platform show that compared with the existing DRL and rule methods, the IFSHM framework shortens the system recovery time by 37% with unknown fault scenarios.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-an-intelligent-fault-self-healing-mechanism-for-cloud-ai-systems-via-integration-1c89ad.md","cf2e4c8fe4054a54",{"html":259,"metadata":260},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning\r\nView PDF HTML (experimental)Abstract:As the scale and complexity of cloud-based AI systems continue to increase, the detection and adaptive recovery of system faults have become the core challenges to ensure service reliability and continuity. In this paper, we propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to realize a fault recovery framework with semantic understanding and policy optimization capabilities in cloud AI systems. On the basis of the traditional DRL-based control model, the proposed method constructs a two-stage hybrid architecture: (1) an LLM-driven fault semantic interpretation module, which can dynamically extract deep contextual semantics from multi-source logs and system indicators to accurately identify potential fault modes; (2) DRL recovery strategy optimizer, based on reinforcement learning, learns the dynamic matching of fault types and response behaviors in the cloud environment. The innovation of this method lies in the introduction of LLM for environment modeling and action space abstraction, which greatly improves the exploration efficiency and generalization ability of reinforcement learning. At the same time, a memory-guided meta-controller is introduced, combined with reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve continuous adaptation to new failure modes and avoid catastrophic forgetting. Experimental results on the cloud fault injection platform show that compared with the existing DRL and rule methods, the IFSHM framework shortens the system recovery time by 37% with unknown fault scenarios.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":261,"localImagePaths":262,"remoteImagePaths":263,"frontmatter":264,"imagePaths":266},[],[],[],{"title":250,"description":251,"pubDate":33,"source":17,"tags":265,"url":254},[19,20,21],[],"2025-06-10-an-intelligent-fault-self-healing-mechanism-for-cloud-ai-systems-via-integration-1c89ad.md","2025-06-10-as-asr-a-lightweight-framework-for-aphasia-specific-automatic-speech-recognition-c64680",{"id":268,"data":270,"body":276,"filePath":277,"digest":278,"rendered":279,"legacyId":288},{"title":271,"description":272,"pubDate":273,"source":17,"tags":274,"url":275},"AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition","arXiv:2506.06566v1 Announce Type: cross \nAbstract: This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition framework based on Whisper-tiny, tailored for low-resource deployment on edge devices. Our approach introduces a hybrid training strategy that systematically combines standard and aphasic speech at varying ratios, enabling robust generalization, and a GPT-4-based reference enhancement method that refines noisy aphasic transcripts, improving supervision quality. We conduct extensive experiments across multiple data mixing configurations and evaluation settings. Results show that our fine-tuned model significantly outperforms the zero-shot baseline, reducing WER on aphasic speech by over 30% while preserving performance on standard speech. The proposed framework offers a scalable, efficient solution for real-world disordered speech recognition.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06566","Electrical Engineering and Systems Science > Audio and Speech Processing\r\n[Submitted on 6 Jun 2025]\r\nTitle:AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition\r\nView PDF HTML (experimental)Abstract:This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition framework based on Whisper-tiny, tailored for low-resource deployment on edge devices. Our approach introduces a hybrid training strategy that systematically combines standard and aphasic speech at varying ratios, enabling robust generalization, and a GPT-4-based reference enhancement method that refines noisy aphasic transcripts, improving supervision quality. We conduct extensive experiments across multiple data mixing configurations and evaluation settings. Results show that our fine-tuned model significantly outperforms the zero-shot baseline, reducing WER on aphasic speech by over 30% while preserving performance on standard speech. The proposed framework offers a scalable, efficient solution for real-world disordered speech recognition.\r\nCurrent browse context:\r\neess.AS\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-as-asr-a-lightweight-framework-for-aphasia-specific-automatic-speech-recognition-c64680.md","a0973fa330780b84",{"html":280,"metadata":281},"\u003Cp>Electrical Engineering and Systems Science > Audio and Speech Processing\r\n[Submitted on 6 Jun 2025]\r\nTitle:AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition\r\nView PDF HTML (experimental)Abstract:This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition framework based on Whisper-tiny, tailored for low-resource deployment on edge devices. Our approach introduces a hybrid training strategy that systematically combines standard and aphasic speech at varying ratios, enabling robust generalization, and a GPT-4-based reference enhancement method that refines noisy aphasic transcripts, improving supervision quality. We conduct extensive experiments across multiple data mixing configurations and evaluation settings. Results show that our fine-tuned model significantly outperforms the zero-shot baseline, reducing WER on aphasic speech by over 30% while preserving performance on standard speech. The proposed framework offers a scalable, efficient solution for real-world disordered speech recognition.\r\nCurrent browse context:\r\neess.AS\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":282,"localImagePaths":283,"remoteImagePaths":284,"frontmatter":285,"imagePaths":287},[],[],[],{"title":271,"description":272,"pubDate":33,"source":17,"tags":286,"url":275},[19,20,21],[],"2025-06-10-as-asr-a-lightweight-framework-for-aphasia-specific-automatic-speech-recognition-c64680.md","2025-06-10-automating-exploratory-multiomics-research-via-language-models-a9f071",{"id":289,"data":291,"body":297,"filePath":298,"digest":299,"rendered":300,"legacyId":309},{"title":292,"description":293,"pubDate":294,"source":17,"tags":295,"url":296},"Automating Exploratory Multiomics Research via Language Models","arXiv:2506.07591v1 Announce Type: new \nAbstract: This paper introduces PROTEUS, a fully automated system that produces data-driven hypotheses from raw data files. We apply PROTEUS to clinical proteogenomics, a field where effective downstream data analysis and hypothesis proposal is crucial for producing novel discoveries. PROTEUS uses separate modules to simulate different stages of the scientific process, from open-ended data exploration to specific statistical analysis and hypothesis proposal. It formulates research directions, tools, and results in terms of relationships between biological entities, using unified graph structures to manage complex research processes. We applied PROTEUS to 10 clinical multiomics datasets from published research, arriving at 360 total hypotheses. Results were evaluated through external data validation and automatic open-ended scoring. Through exploratory and iterative research, the system can navigate high-throughput and heterogeneous multiomics data to arrive at hypotheses that balance reliability and novelty. In addition to accelerating multiomic analysis, PROTEUS represents a path towards tailoring general autonomous systems to specialized scientific domains to achieve open-ended hypothesis generation from data.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07591","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Automating Exploratory Multiomics Research via Language Models\r\nView PDF HTML (experimental)Abstract:This paper introduces PROTEUS, a fully automated system that produces data-driven hypotheses from raw data files. We apply PROTEUS to clinical proteogenomics, a field where effective downstream data analysis and hypothesis proposal is crucial for producing novel discoveries. PROTEUS uses separate modules to simulate different stages of the scientific process, from open-ended data exploration to specific statistical analysis and hypothesis proposal. It formulates research directions, tools, and results in terms of relationships between biological entities, using unified graph structures to manage complex research processes. We applied PROTEUS to 10 clinical multiomics datasets from published research, arriving at 360 total hypotheses. Results were evaluated through external data validation and automatic open-ended scoring. Through exploratory and iterative research, the system can navigate high-throughput and heterogeneous multiomics data to arrive at hypotheses that balance reliability and novelty. In addition to accelerating multiomic analysis, PROTEUS represents a path towards tailoring general autonomous systems to specialized scientific domains to achieve open-ended hypothesis generation from data.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-automating-exploratory-multiomics-research-via-language-models-a9f071.md","9ce937199b92774e",{"html":301,"metadata":302},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Automating Exploratory Multiomics Research via Language Models\r\nView PDF HTML (experimental)Abstract:This paper introduces PROTEUS, a fully automated system that produces data-driven hypotheses from raw data files. We apply PROTEUS to clinical proteogenomics, a field where effective downstream data analysis and hypothesis proposal is crucial for producing novel discoveries. PROTEUS uses separate modules to simulate different stages of the scientific process, from open-ended data exploration to specific statistical analysis and hypothesis proposal. It formulates research directions, tools, and results in terms of relationships between biological entities, using unified graph structures to manage complex research processes. We applied PROTEUS to 10 clinical multiomics datasets from published research, arriving at 360 total hypotheses. Results were evaluated through external data validation and automatic open-ended scoring. Through exploratory and iterative research, the system can navigate high-throughput and heterogeneous multiomics data to arrive at hypotheses that balance reliability and novelty. In addition to accelerating multiomic analysis, PROTEUS represents a path towards tailoring general autonomous systems to specialized scientific domains to achieve open-ended hypothesis generation from data.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":303,"localImagePaths":304,"remoteImagePaths":305,"frontmatter":306,"imagePaths":308},[],[],[],{"title":292,"description":293,"pubDate":33,"source":17,"tags":307,"url":296},[19,20,21],[],"2025-06-10-automating-exploratory-multiomics-research-via-language-models-a9f071.md","2025-06-10-beyond-facts-evaluating-intent-hallucination-in-large-language-models-fac57b",{"id":310,"data":312,"body":318,"filePath":319,"digest":320,"rendered":321,"legacyId":330},{"title":313,"description":314,"pubDate":315,"source":17,"tags":316,"url":317},"Beyond Facts: Evaluating Intent Hallucination in Large Language Models","arXiv:2506.06539v1 Announce Type: cross \nAbstract: When exposed to complex queries containing multiple conditions, today's large language models (LLMs) tend to produce responses that only partially satisfy the query while neglecting certain conditions. We therefore introduce the concept of Intent Hallucination. In this phenomenon, LLMs either omit (neglecting to address certain parts) or misinterpret (responding to invented query parts) elements of the given query, leading to intent hallucinated generation. To systematically evaluate intent hallucination, we introduce FAITHQA, a novel benchmark for intent hallucination that contains 20,068 problems, covering both query-only and retrieval-augmented generation (RAG) setups with varying topics and difficulty. FAITHQA is the first hallucination benchmark that goes beyond factual verification, tailored to identify the fundamental cause of intent hallucination. By evaluating various LLMs on FAITHQA, we find that (1) intent hallucination is a common issue even for state-of-the-art models, and (2) the phenomenon stems from omission or misinterpretation of LLMs. To facilitate future research, we introduce an automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting intent hallucination. Human evaluation results demonstrate that CONSTRAINT SCORE is closer to human performance for intent hallucination compared to baselines.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06539","Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:Beyond Facts: Evaluating Intent Hallucination in Large Language Models\r\nView PDF HTML (experimental)Abstract:When exposed to complex queries containing multiple conditions, today's large language models (LLMs) tend to produce responses that only partially satisfy the query while neglecting certain conditions. We therefore introduce the concept of Intent Hallucination. In this phenomenon, LLMs either omit (neglecting to address certain parts) or misinterpret (responding to invented query parts) elements of the given query, leading to intent hallucinated generation. To systematically evaluate intent hallucination, we introduce FAITHQA, a novel benchmark for intent hallucination that contains 20,068 problems, covering both query-only and retrieval-augmented generation (RAG) setups with varying topics and difficulty. FAITHQA is the first hallucination benchmark that goes beyond factual verification, tailored to identify the fundamental cause of intent hallucination. By evaluating various LLMs on FAITHQA, we find that (1) intent hallucination is a common issue even for state-of-the-art models, and (2) the phenomenon stems from omission or misinterpretation of LLMs. To facilitate future research, we introduce an automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting intent hallucination. Human evaluation results demonstrate that CONSTRAINT SCORE is closer to human performance for intent hallucination compared to baselines.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-beyond-facts-evaluating-intent-hallucination-in-large-language-models-fac57b.md","1e72c45609707edc",{"html":322,"metadata":323},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:Beyond Facts: Evaluating Intent Hallucination in Large Language Models\r\nView PDF HTML (experimental)Abstract:When exposed to complex queries containing multiple conditions, today’s large language models (LLMs) tend to produce responses that only partially satisfy the query while neglecting certain conditions. We therefore introduce the concept of Intent Hallucination. In this phenomenon, LLMs either omit (neglecting to address certain parts) or misinterpret (responding to invented query parts) elements of the given query, leading to intent hallucinated generation. To systematically evaluate intent hallucination, we introduce FAITHQA, a novel benchmark for intent hallucination that contains 20,068 problems, covering both query-only and retrieval-augmented generation (RAG) setups with varying topics and difficulty. FAITHQA is the first hallucination benchmark that goes beyond factual verification, tailored to identify the fundamental cause of intent hallucination. By evaluating various LLMs on FAITHQA, we find that (1) intent hallucination is a common issue even for state-of-the-art models, and (2) the phenomenon stems from omission or misinterpretation of LLMs. To facilitate future research, we introduce an automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting intent hallucination. Human evaluation results demonstrate that CONSTRAINT SCORE is closer to human performance for intent hallucination compared to baselines.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":324,"localImagePaths":325,"remoteImagePaths":326,"frontmatter":327,"imagePaths":329},[],[],[],{"title":313,"description":314,"pubDate":33,"source":17,"tags":328,"url":317},[19,20,21],[],"2025-06-10-beyond-facts-evaluating-intent-hallucination-in-large-language-models-fac57b.md","2025-06-10-benchmarking-misuse-mitigation-against-covert-adversaries-44bfcf",{"id":331,"data":333,"body":339,"filePath":340,"digest":341,"rendered":342,"legacyId":351},{"title":334,"description":335,"pubDate":336,"source":17,"tags":337,"url":338},"Benchmarking Misuse Mitigation Against Covert Adversaries","arXiv:2506.06414v1 Announce Type: cross \nAbstract: Existing language model safety evaluations focus on overt attacks and low-stakes tasks. Realistic attackers can subvert current safeguards by requesting help on small, benign-seeming tasks across many independent queries. Because individual queries do not appear harmful, the attack is hard to {detect}. However, when combined, these fragments uplift misuse by helping the attacker complete hard and dangerous tasks. Toward identifying defenses against such strategies, we develop Benchmarks for Stateful Defenses (BSD), a data generation pipeline that automates evaluations of covert attacks and corresponding defenses. Using this pipeline, we curate two new datasets that are consistently refused by frontier models and are too difficult for weaker open-weight models. Our evaluations indicate that decomposition attacks are effective misuse enablers, and highlight stateful defenses as a countermeasure.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06414","Computer Science > Cryptography and Security\r\n[Submitted on 6 Jun 2025]\r\nTitle:Benchmarking Misuse Mitigation Against Covert Adversaries\r\nView PDF HTML (experimental)Abstract:Existing language model safety evaluations focus on overt attacks and low-stakes tasks. Realistic attackers can subvert current safeguards by requesting help on small, benign-seeming tasks across many independent queries. Because individual queries do not appear harmful, the attack is hard to {detect}. However, when combined, these fragments uplift misuse by helping the attacker complete hard and dangerous tasks. Toward identifying defenses against such strategies, we develop Benchmarks for Stateful Defenses (BSD), a data generation pipeline that automates evaluations of covert attacks and corresponding defenses. Using this pipeline, we curate two new datasets that are consistently refused by frontier models and are too difficult for weaker open-weight models. Our evaluations indicate that decomposition attacks are effective misuse enablers, and highlight stateful defenses as a countermeasure.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-benchmarking-misuse-mitigation-against-covert-adversaries-44bfcf.md","2d6d4cf7341debe2",{"html":343,"metadata":344},"\u003Cp>Computer Science > Cryptography and Security\r\n[Submitted on 6 Jun 2025]\r\nTitle:Benchmarking Misuse Mitigation Against Covert Adversaries\r\nView PDF HTML (experimental)Abstract:Existing language model safety evaluations focus on overt attacks and low-stakes tasks. Realistic attackers can subvert current safeguards by requesting help on small, benign-seeming tasks across many independent queries. Because individual queries do not appear harmful, the attack is hard to {detect}. However, when combined, these fragments uplift misuse by helping the attacker complete hard and dangerous tasks. Toward identifying defenses against such strategies, we develop Benchmarks for Stateful Defenses (BSD), a data generation pipeline that automates evaluations of covert attacks and corresponding defenses. Using this pipeline, we curate two new datasets that are consistently refused by frontier models and are too difficult for weaker open-weight models. Our evaluations indicate that decomposition attacks are effective misuse enablers, and highlight stateful defenses as a countermeasure.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":345,"localImagePaths":346,"remoteImagePaths":347,"frontmatter":348,"imagePaths":350},[],[],[],{"title":334,"description":335,"pubDate":33,"source":17,"tags":349,"url":338},[19,20,21],[],"2025-06-10-benchmarking-misuse-mitigation-against-covert-adversaries-44bfcf.md","2025-06-10-bimgent-towards-autonomous-building-modeling-via-computer-use-agents-326575",{"id":352,"data":354,"body":360,"filePath":361,"digest":362,"rendered":363,"legacyId":372},{"title":355,"description":356,"pubDate":357,"source":17,"tags":358,"url":359},"BIMgent: Towards Autonomous Building Modeling via Computer-use Agents","arXiv:2506.07217v1 Announce Type: new \nAbstract: Existing computer-use agents primarily focus on general-purpose desktop automation tasks, with limited exploration of their application in highly specialized domains. In particular, the 3D building modeling process in the Architecture, Engineering, and Construction (AEC) sector involves open-ended design tasks and complex interaction patterns within Building Information Modeling (BIM) authoring software, which has yet to be thoroughly addressed by current studies. In this paper, we propose BIMgent, an agentic framework powered by multimodal large language models (LLMs), designed to enable autonomous building model authoring via graphical user interface (GUI) operations. BIMgent automates the architectural building modeling process, including multimodal input for conceptual design, planning of software-specific workflows, and efficient execution of the authoring GUI actions. We evaluate BIMgent on real-world building modeling tasks, including both text-based conceptual design generation and reconstruction from existing building design. The design quality achieved by BIMgent was found to be reasonable. Its operations achieved a 32% success rate, whereas all baseline models failed to complete the tasks (0% success rate). Results demonstrate that BIMgent effectively reduces manual workload while preserving design intent, highlighting its potential for practical deployment in real-world architectural modeling scenarios.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07217","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:BIMgent: Towards Autonomous Building Modeling via Computer-use Agents\r\nView PDF HTML (experimental)Abstract:Existing computer-use agents primarily focus on general-purpose desktop automation tasks, with limited exploration of their application in highly specialized domains. In particular, the 3D building modeling process in the Architecture, Engineering, and Construction (AEC) sector involves open-ended design tasks and complex interaction patterns within Building Information Modeling (BIM) authoring software, which has yet to be thoroughly addressed by current studies. In this paper, we propose BIMgent, an agentic framework powered by multimodal large language models (LLMs), designed to enable autonomous building model authoring via graphical user interface (GUI) operations. BIMgent automates the architectural building modeling process, including multimodal input for conceptual design, planning of software-specific workflows, and efficient execution of the authoring GUI actions. We evaluate BIMgent on real-world building modeling tasks, including both text-based conceptual design generation and reconstruction from existing building design. The design quality achieved by BIMgent was found to be reasonable. Its operations achieved a 32% success rate, whereas all baseline models failed to complete the tasks (0% success rate). Results demonstrate that BIMgent effectively reduces manual workload while preserving design intent, highlighting its potential for practical deployment in real-world architectural modeling scenarios.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-bimgent-towards-autonomous-building-modeling-via-computer-use-agents-326575.md","089daad869ab3b99",{"html":364,"metadata":365},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:BIMgent: Towards Autonomous Building Modeling via Computer-use Agents\r\nView PDF HTML (experimental)Abstract:Existing computer-use agents primarily focus on general-purpose desktop automation tasks, with limited exploration of their application in highly specialized domains. In particular, the 3D building modeling process in the Architecture, Engineering, and Construction (AEC) sector involves open-ended design tasks and complex interaction patterns within Building Information Modeling (BIM) authoring software, which has yet to be thoroughly addressed by current studies. In this paper, we propose BIMgent, an agentic framework powered by multimodal large language models (LLMs), designed to enable autonomous building model authoring via graphical user interface (GUI) operations. BIMgent automates the architectural building modeling process, including multimodal input for conceptual design, planning of software-specific workflows, and efficient execution of the authoring GUI actions. We evaluate BIMgent on real-world building modeling tasks, including both text-based conceptual design generation and reconstruction from existing building design. The design quality achieved by BIMgent was found to be reasonable. Its operations achieved a 32% success rate, whereas all baseline models failed to complete the tasks (0% success rate). Results demonstrate that BIMgent effectively reduces manual workload while preserving design intent, highlighting its potential for practical deployment in real-world architectural modeling scenarios.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":366,"localImagePaths":367,"remoteImagePaths":368,"frontmatter":369,"imagePaths":371},[],[],[],{"title":355,"description":356,"pubDate":33,"source":17,"tags":370,"url":359},[19,20,21],[],"2025-06-10-bimgent-towards-autonomous-building-modeling-via-computer-use-agents-326575.md","2025-06-10-beyond-the-norm-a-survey-of-synthetic-data-generation-for-rare-events-3d0b69",{"id":373,"data":375,"body":381,"filePath":382,"digest":383,"rendered":384,"legacyId":393},{"title":376,"description":377,"pubDate":378,"source":17,"tags":379,"url":380},"Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events","arXiv:2506.06380v1 Announce Type: cross \nAbstract: Extreme events, such as market crashes, natural disasters, and pandemics, are rare but catastrophic, often triggering cascading failures across interconnected systems. Accurate prediction and early warning can help minimize losses and improve preparedness. While data-driven methods offer powerful capabilities for extreme event modeling, they require abundant training data, yet extreme event data is inherently scarce, creating a fundamental challenge. Synthetic data generation has emerged as a powerful solution. However, existing surveys focus on general data with privacy preservation emphasis, rather than extreme events' unique performance requirements. This survey provides the first overview of synthetic data generation for extreme events. We systematically review generative modeling techniques and large language models, particularly those enhanced by statistical theory as well as specialized training and sampling mechanisms to capture heavy-tailed distributions. We summarize benchmark datasets and introduce a tailored evaluation framework covering statistical, dependence, visual, and task-oriented metrics. A central contribution is our in-depth analysis of each metric's applicability in extremeness and domain-specific adaptations, providing actionable guidance for model evaluation in extreme settings. We categorize key application domains and identify underexplored areas like behavioral finance, wildfires, earthquakes, windstorms, and infectious outbreaks. Finally, we outline open challenges, providing a structured foundation for advancing synthetic rare-event research.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06380","Computer Science > Machine Learning\r\n[Submitted on 4 Jun 2025]\r\nTitle:Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events\r\nView PDF HTML (experimental)Abstract:Extreme events, such as market crashes, natural disasters, and pandemics, are rare but catastrophic, often triggering cascading failures across interconnected systems. Accurate prediction and early warning can help minimize losses and improve preparedness. While data-driven methods offer powerful capabilities for extreme event modeling, they require abundant training data, yet extreme event data is inherently scarce, creating a fundamental challenge. Synthetic data generation has emerged as a powerful solution. However, existing surveys focus on general data with privacy preservation emphasis, rather than extreme events' unique performance requirements. This survey provides the first overview of synthetic data generation for extreme events. We systematically review generative modeling techniques and large language models, particularly those enhanced by statistical theory as well as specialized training and sampling mechanisms to capture heavy-tailed distributions. We summarize benchmark datasets and introduce a tailored evaluation framework covering statistical, dependence, visual, and task-oriented metrics. A central contribution is our in-depth analysis of each metric's applicability in extremeness and domain-specific adaptations, providing actionable guidance for model evaluation in extreme settings. We categorize key application domains and identify underexplored areas like behavioral finance, wildfires, earthquakes, windstorms, and infectious outbreaks. Finally, we outline open challenges, providing a structured foundation for advancing synthetic rare-event research.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-beyond-the-norm-a-survey-of-synthetic-data-generation-for-rare-events-3d0b69.md","015dbd8d6719f04a",{"html":385,"metadata":386},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 4 Jun 2025]\r\nTitle:Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events\r\nView PDF HTML (experimental)Abstract:Extreme events, such as market crashes, natural disasters, and pandemics, are rare but catastrophic, often triggering cascading failures across interconnected systems. Accurate prediction and early warning can help minimize losses and improve preparedness. While data-driven methods offer powerful capabilities for extreme event modeling, they require abundant training data, yet extreme event data is inherently scarce, creating a fundamental challenge. Synthetic data generation has emerged as a powerful solution. However, existing surveys focus on general data with privacy preservation emphasis, rather than extreme events’ unique performance requirements. This survey provides the first overview of synthetic data generation for extreme events. We systematically review generative modeling techniques and large language models, particularly those enhanced by statistical theory as well as specialized training and sampling mechanisms to capture heavy-tailed distributions. We summarize benchmark datasets and introduce a tailored evaluation framework covering statistical, dependence, visual, and task-oriented metrics. A central contribution is our in-depth analysis of each metric’s applicability in extremeness and domain-specific adaptations, providing actionable guidance for model evaluation in extreme settings. We categorize key application domains and identify underexplored areas like behavioral finance, wildfires, earthquakes, windstorms, and infectious outbreaks. Finally, we outline open challenges, providing a structured foundation for advancing synthetic rare-event research.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":387,"localImagePaths":388,"remoteImagePaths":389,"frontmatter":390,"imagePaths":392},[],[],[],{"title":376,"description":377,"pubDate":33,"source":17,"tags":391,"url":380},[19,20,21],[],"2025-06-10-beyond-the-norm-a-survey-of-synthetic-data-generation-for-rare-events-3d0b69.md","2025-06-10-benchmarking-large-language-models-on-homework-assessment-in-circuit-analysis-4d7b55",{"id":394,"data":396,"body":402,"filePath":403,"digest":404,"rendered":405,"legacyId":414},{"title":397,"description":398,"pubDate":399,"source":17,"tags":400,"url":401},"Benchmarking Large Language Models on Homework Assessment in Circuit Analysis","arXiv:2506.06390v1 Announce Type: cross \nAbstract: Large language models (LLMs) have the potential to revolutionize various fields, including code development, robotics, finance, and education, due to their extensive prior knowledge and rapid advancements. This paper investigates how LLMs can be leveraged in engineering education. Specifically, we benchmark the capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama 3 70B, in assessing homework for an undergraduate-level circuit analysis course. We have developed a novel dataset consisting of official reference solutions and real student solutions to problems from various topics in circuit analysis. To overcome the limitations of image recognition in current state-of-the-art LLMs, the solutions in the dataset are converted to LaTeX format. Using this dataset, a prompt template is designed to test five metrics of student solutions: completeness, method, final answer, arithmetic error, and units. The results show that GPT-4o and Llama 3 70B perform significantly better than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B each having distinct advantages in different evaluation aspects. Additionally, we present insights into the limitations of current LLMs in several aspects of circuit analysis. Given the paramount importance of ensuring reliability in LLM-generated homework assessment to avoid misleading students, our results establish benchmarks and offer valuable insights for the development of a reliable, personalized tutor for circuit analysis -- a focus of our future work. Furthermore, the proposed evaluation methods can be generalized to a broader range of courses for engineering education in the future.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06390","Computer Science > Computers and Society\r\n[Submitted on 5 Jun 2025]\r\nTitle:Benchmarking Large Language Models on Homework Assessment in Circuit Analysis\r\nView PDFAbstract:Large language models (LLMs) have the potential to revolutionize various fields, including code development, robotics, finance, and education, due to their extensive prior knowledge and rapid advancements. This paper investigates how LLMs can be leveraged in engineering education. Specifically, we benchmark the capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama 3 70B, in assessing homework for an undergraduate-level circuit analysis course. We have developed a novel dataset consisting of official reference solutions and real student solutions to problems from various topics in circuit analysis. To overcome the limitations of image recognition in current state-of-the-art LLMs, the solutions in the dataset are converted to LaTeX format. Using this dataset, a prompt template is designed to test five metrics of student solutions: completeness, method, final answer, arithmetic error, and units. The results show that GPT-4o and Llama 3 70B perform significantly better than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B each having distinct advantages in different evaluation aspects. Additionally, we present insights into the limitations of current LLMs in several aspects of circuit analysis. Given the paramount importance of ensuring reliability in LLM-generated homework assessment to avoid misleading students, our results establish benchmarks and offer valuable insights for the development of a reliable, personalized tutor for circuit analysis -- a focus of our future work. Furthermore, the proposed evaluation methods can be generalized to a broader range of courses for engineering education in the future.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-benchmarking-large-language-models-on-homework-assessment-in-circuit-analysis-4d7b55.md","b4bf57d35d59167d",{"html":406,"metadata":407},"\u003Cp>Computer Science > Computers and Society\r\n[Submitted on 5 Jun 2025]\r\nTitle:Benchmarking Large Language Models on Homework Assessment in Circuit Analysis\r\nView PDFAbstract:Large language models (LLMs) have the potential to revolutionize various fields, including code development, robotics, finance, and education, due to their extensive prior knowledge and rapid advancements. This paper investigates how LLMs can be leveraged in engineering education. Specifically, we benchmark the capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama 3 70B, in assessing homework for an undergraduate-level circuit analysis course. We have developed a novel dataset consisting of official reference solutions and real student solutions to problems from various topics in circuit analysis. To overcome the limitations of image recognition in current state-of-the-art LLMs, the solutions in the dataset are converted to LaTeX format. Using this dataset, a prompt template is designed to test five metrics of student solutions: completeness, method, final answer, arithmetic error, and units. The results show that GPT-4o and Llama 3 70B perform significantly better than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B each having distinct advantages in different evaluation aspects. Additionally, we present insights into the limitations of current LLMs in several aspects of circuit analysis. Given the paramount importance of ensuring reliability in LLM-generated homework assessment to avoid misleading students, our results establish benchmarks and offer valuable insights for the development of a reliable, personalized tutor for circuit analysis — a focus of our future work. Furthermore, the proposed evaluation methods can be generalized to a broader range of courses for engineering education in the future.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":408,"localImagePaths":409,"remoteImagePaths":410,"frontmatter":411,"imagePaths":413},[],[],[],{"title":397,"description":398,"pubDate":33,"source":17,"tags":412,"url":401},[19,20,21],[],"2025-06-10-benchmarking-large-language-models-on-homework-assessment-in-circuit-analysis-4d7b55.md","2025-06-10-bio-inspired-classification-combining-information-theory-and-spiking-neural-netw-b49b52",{"id":415,"data":417,"body":423,"filePath":424,"digest":425,"rendered":426,"legacyId":435},{"title":418,"description":419,"pubDate":420,"source":17,"tags":421,"url":422},"Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules","arXiv:2506.06750v1 Announce Type: new \nAbstract: Training of Spiking Neural Networks (SNN) is challenging due to their unique properties, including temporal dynamics, non-differentiability of spike events, and sparse event-driven activations. In this paper, we widely consider the influence of the type of chosen learning algorithm, including bioinspired learning rules on the accuracy of classification. We proposed a bioinspired classifier based on the combination of SNN and Lempel-Ziv complexity (LZC). This approach synergizes the strengths of SNNs in temporal precision and biological realism with LZC's structural complexity analysis, facilitating efficient and interpretable classification of spatiotemporal neural data. It turned out that the classic backpropagation algorithm achieves excellent classification accuracy, but at extremely high computational cost, which makes it impractical for real-time applications. Biologically inspired learning algorithms such as tempotron and Spikprop provide increased computational efficiency while maintaining competitive classification performance, making them suitable for time-sensitive tasks. The results obtained indicate that the selection of the most appropriate learning algorithm depends on the trade-off between classification accuracy and computational cost as well as application constraints.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06750","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules\r\nView PDF HTML (experimental)Abstract:Training of Spiking Neural Networks (SNN) is challenging due to their unique properties, including temporal dynamics, non-differentiability of spike events, and sparse event-driven activations. In this paper, we widely consider the influence of the type of chosen learning algorithm, including bioinspired learning rules on the accuracy of classification. We proposed a bioinspired classifier based on the combination of SNN and Lempel-Ziv complexity (LZC). This approach synergizes the strengths of SNNs in temporal precision and biological realism with LZC's structural complexity analysis, facilitating efficient and interpretable classification of spatiotemporal neural data. It turned out that the classic backpropagation algorithm achieves excellent classification accuracy, but at extremely high computational cost, which makes it impractical for real-time applications. Biologically inspired learning algorithms such as tempotron and Spikprop provide increased computational efficiency while maintaining competitive classification performance, making them suitable for time-sensitive tasks. The results obtained indicate that the selection of the most appropriate learning algorithm depends on the trade-off between classification accuracy and computational cost as well as application constraints.\r\nSubmission history\r\nFrom: Agnieszka Pregowska [view email][v1] Sat, 7 Jun 2025 10:43:09 UTC (561 KB)\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-bio-inspired-classification-combining-information-theory-and-spiking-neural-netw-b49b52.md","55dc177305f36914",{"html":427,"metadata":428},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks — Influence of the Learning Rules\r\nView PDF HTML (experimental)Abstract:Training of Spiking Neural Networks (SNN) is challenging due to their unique properties, including temporal dynamics, non-differentiability of spike events, and sparse event-driven activations. In this paper, we widely consider the influence of the type of chosen learning algorithm, including bioinspired learning rules on the accuracy of classification. We proposed a bioinspired classifier based on the combination of SNN and Lempel-Ziv complexity (LZC). This approach synergizes the strengths of SNNs in temporal precision and biological realism with LZC’s structural complexity analysis, facilitating efficient and interpretable classification of spatiotemporal neural data. It turned out that the classic backpropagation algorithm achieves excellent classification accuracy, but at extremely high computational cost, which makes it impractical for real-time applications. Biologically inspired learning algorithms such as tempotron and Spikprop provide increased computational efficiency while maintaining competitive classification performance, making them suitable for time-sensitive tasks. The results obtained indicate that the selection of the most appropriate learning algorithm depends on the trade-off between classification accuracy and computational cost as well as application constraints.\r\nSubmission history\r\nFrom: Agnieszka Pregowska [view email][v1] Sat, 7 Jun 2025 10:43:09 UTC (561 KB)\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":429,"localImagePaths":430,"remoteImagePaths":431,"frontmatter":432,"imagePaths":434},[],[],[],{"title":418,"description":419,"pubDate":33,"source":17,"tags":433,"url":422},[19,20,21],[],"2025-06-10-bio-inspired-classification-combining-information-theory-and-spiking-neural-netw-b49b52.md","2025-06-10-boosting-llm-reasoning-via-spontaneous-self-correction-ddd1a7",{"id":436,"data":438,"body":444,"filePath":445,"digest":446,"rendered":447,"legacyId":456},{"title":439,"description":440,"pubDate":441,"source":17,"tags":442,"url":443},"Boosting LLM Reasoning via Spontaneous Self-Correction","arXiv:2506.06923v1 Announce Type: new \nAbstract: While large language models (LLMs) have demonstrated remarkable success on a broad range of tasks, math reasoning remains a challenging one. One of the approaches for improving math reasoning is self-correction, which designs self-improving loops to let the model correct its own mistakes. However, existing self-correction approaches treat corrections as standalone post-generation refinements, relying on extra prompt and system designs to elicit self-corrections, instead of performing real-time, spontaneous self-corrections in a single pass. To address this, we propose SPOC, a spontaneous self-correction approach that enables LLMs to generate interleaved solutions and verifications in a single inference pass, with generation dynamically terminated based on verification outcomes, thereby effectively scaling inference time compute. SPOC considers a multi-agent perspective by assigning dual roles -- solution proposer and verifier -- to the same model. We adopt a simple yet effective approach to generate synthetic data for fine-tuning, enabling the model to develop capabilities for self-verification and multi-agent collaboration. We further improve its solution proposal and verification accuracy through online reinforcement learning. Experiments on mathematical reasoning benchmarks show that SPOC significantly improves performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23, and 3.3% and 6.7% on AIME24, respectively.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06923","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Boosting LLM Reasoning via Spontaneous Self-Correction\r\nView PDFAbstract:While large language models (LLMs) have demonstrated remarkable success on a broad range of tasks, math reasoning remains a challenging one. One of the approaches for improving math reasoning is self-correction, which designs self-improving loops to let the model correct its own mistakes. However, existing self-correction approaches treat corrections as standalone post-generation refinements, relying on extra prompt and system designs to elicit self-corrections, instead of performing real-time, spontaneous self-corrections in a single pass. To address this, we propose SPOC, a spontaneous self-correction approach that enables LLMs to generate interleaved solutions and verifications in a single inference pass, with generation dynamically terminated based on verification outcomes, thereby effectively scaling inference time compute. SPOC considers a multi-agent perspective by assigning dual roles -- solution proposer and verifier -- to the same model. We adopt a simple yet effective approach to generate synthetic data for fine-tuning, enabling the model to develop capabilities for self-verification and multi-agent collaboration. We further improve its solution proposal and verification accuracy through online reinforcement learning. Experiments on mathematical reasoning benchmarks show that SPOC significantly improves performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23, and 3.3% and 6.7% on AIME24, respectively.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-boosting-llm-reasoning-via-spontaneous-self-correction-ddd1a7.md","7674ee47d11e0327",{"html":448,"metadata":449},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Boosting LLM Reasoning via Spontaneous Self-Correction\r\nView PDFAbstract:While large language models (LLMs) have demonstrated remarkable success on a broad range of tasks, math reasoning remains a challenging one. One of the approaches for improving math reasoning is self-correction, which designs self-improving loops to let the model correct its own mistakes. However, existing self-correction approaches treat corrections as standalone post-generation refinements, relying on extra prompt and system designs to elicit self-corrections, instead of performing real-time, spontaneous self-corrections in a single pass. To address this, we propose SPOC, a spontaneous self-correction approach that enables LLMs to generate interleaved solutions and verifications in a single inference pass, with generation dynamically terminated based on verification outcomes, thereby effectively scaling inference time compute. SPOC considers a multi-agent perspective by assigning dual roles — solution proposer and verifier — to the same model. We adopt a simple yet effective approach to generate synthetic data for fine-tuning, enabling the model to develop capabilities for self-verification and multi-agent collaboration. We further improve its solution proposal and verification accuracy through online reinforcement learning. Experiments on mathematical reasoning benchmarks show that SPOC significantly improves performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23, and 3.3% and 6.7% on AIME24, respectively.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":450,"localImagePaths":451,"remoteImagePaths":452,"frontmatter":453,"imagePaths":455},[],[],[],{"title":439,"description":440,"pubDate":33,"source":17,"tags":454,"url":443},[19,20,21],[],"2025-06-10-boosting-llm-reasoning-via-spontaneous-self-correction-ddd1a7.md","2025-06-10-boosting-vulnerability-detection-of-llms-via-curriculum-preference-optimization--b34a2f",{"id":457,"data":459,"body":465,"filePath":466,"digest":467,"rendered":468,"legacyId":477},{"title":460,"description":461,"pubDate":462,"source":17,"tags":463,"url":464},"Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data","arXiv:2506.07390v1 Announce Type: new \nAbstract: Large language models (LLMs) demonstrate considerable proficiency in numerous coding-related tasks; however, their capabilities in detecting software vulnerabilities remain limited. This limitation primarily stems from two factors: (1) the absence of reasoning data related to vulnerabilities, which hinders the models' ability to capture underlying vulnerability patterns; and (2) their focus on learning semantic representations rather than the reason behind them, thus failing to recognize semantically similar vulnerability samples. Furthermore, the development of LLMs specialized in vulnerability detection is challenging, particularly in environments characterized by the scarcity of high-quality datasets. In this paper, we propose a novel framework ReVD that excels at mining vulnerability patterns through reasoning data synthesizing and vulnerability-specific preference optimization. Specifically, we construct forward and backward reasoning processes for vulnerability and corresponding fixed code, ensuring the synthesis of high-quality reasoning data. Moreover, we design the triplet supervised fine-tuning followed by curriculum online preference optimization for enabling ReVD to better understand vulnerability patterns. The extensive experiments conducted on PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for LLM-based software vulnerability detection, e.g., 12.24\\%-22.77\\% improvement in the accuracy. The source code and data are available at https://github.com/Xin-Cheng-Wen/PO4Vul.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07390","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data\r\nView PDF HTML (experimental)Abstract:Large language models (LLMs) demonstrate considerable proficiency in numerous coding-related tasks; however, their capabilities in detecting software vulnerabilities remain limited. This limitation primarily stems from two factors: (1) the absence of reasoning data related to vulnerabilities, which hinders the models' ability to capture underlying vulnerability patterns; and (2) their focus on learning semantic representations rather than the reason behind them, thus failing to recognize semantically similar vulnerability samples. Furthermore, the development of LLMs specialized in vulnerability detection is challenging, particularly in environments characterized by the scarcity of high-quality datasets. In this paper, we propose a novel framework ReVD that excels at mining vulnerability patterns through reasoning data synthesizing and vulnerability-specific preference optimization. Specifically, we construct forward and backward reasoning processes for vulnerability and corresponding fixed code, ensuring the synthesis of high-quality reasoning data. Moreover, we design the triplet supervised fine-tuning followed by curriculum online preference optimization for enabling ReVD to better understand vulnerability patterns. The extensive experiments conducted on PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for LLM-based software vulnerability detection, e.g., 12.24\\%-22.77\\% improvement in the accuracy. The source code and data are available at this https URL.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-boosting-vulnerability-detection-of-llms-via-curriculum-preference-optimization--b34a2f.md","f3b3a885b79491d7",{"html":469,"metadata":470},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data\r\nView PDF HTML (experimental)Abstract:Large language models (LLMs) demonstrate considerable proficiency in numerous coding-related tasks; however, their capabilities in detecting software vulnerabilities remain limited. This limitation primarily stems from two factors: (1) the absence of reasoning data related to vulnerabilities, which hinders the models’ ability to capture underlying vulnerability patterns; and (2) their focus on learning semantic representations rather than the reason behind them, thus failing to recognize semantically similar vulnerability samples. Furthermore, the development of LLMs specialized in vulnerability detection is challenging, particularly in environments characterized by the scarcity of high-quality datasets. In this paper, we propose a novel framework ReVD that excels at mining vulnerability patterns through reasoning data synthesizing and vulnerability-specific preference optimization. Specifically, we construct forward and backward reasoning processes for vulnerability and corresponding fixed code, ensuring the synthesis of high-quality reasoning data. Moreover, we design the triplet supervised fine-tuning followed by curriculum online preference optimization for enabling ReVD to better understand vulnerability patterns. The extensive experiments conducted on PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for LLM-based software vulnerability detection, e.g., 12.24%-22.77% improvement in the accuracy. The source code and data are available at this https URL.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":471,"localImagePaths":472,"remoteImagePaths":473,"frontmatter":474,"imagePaths":476},[],[],[],{"title":460,"description":461,"pubDate":33,"source":17,"tags":475,"url":464},[19,20,21],[],"2025-06-10-boosting-vulnerability-detection-of-llms-via-curriculum-preference-optimization--b34a2f.md","2025-06-10-bright-upgrading-the-bright-benchmark-with-marcus-a-multi-agent-rag-clean-up-s-f4a9aa",{"id":478,"data":480,"body":486,"filePath":487,"digest":488,"rendered":489,"legacyId":498},{"title":481,"description":482,"pubDate":483,"source":17,"tags":484,"url":485},"BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite","arXiv:2506.07116v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) systems require corpora that are both structurally clean and semantically coherent. BRIGHT is a recent and influential benchmark designed to evaluate complex multi-hop retrieval across diverse, high-reasoning domains. However, its practical effectiveness is limited by common web-crawled artifacts - such as content redundancy and semantic discontinuity - that impair retrieval accuracy and downstream reasoning. Notably, we find that such issues are concentrated in seven StackExchange-derived subdomains, while other domains (e.g., Coding and Theorem-based content) remain relatively clean.\n  In this study, we present MARCUS, a multi-agent pipeline that leverages large language models (LLMs) to systematically clean and re-chunk BRIGHT into a higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for structural noise removal and semantic segmentation, preserving answer-bearing spans while improving contextual integrity. Experimental evaluations demonstrate that BRIGHT-Plus yields consistent and significant improvements in both retrieval accuracy and multi-hop reasoning across a diverse set of retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to support future research on robust, reasoning-centric retrieval.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07116","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite\r\nView PDF HTML (experimental)Abstract:Retrieval-Augmented Generation (RAG) systems require corpora that are both structurally clean and semantically coherent. BRIGHT is a recent and influential benchmark designed to evaluate complex multi-hop retrieval across diverse, high-reasoning domains. However, its practical effectiveness is limited by common web-crawled artifacts - such as content redundancy and semantic discontinuity - that impair retrieval accuracy and downstream reasoning. Notably, we find that such issues are concentrated in seven StackExchange-derived subdomains, while other domains (e.g., Coding and Theorem-based content) remain relatively clean.\r\nIn this study, we present MARCUS, a multi-agent pipeline that leverages large language models (LLMs) to systematically clean and re-chunk BRIGHT into a higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for structural noise removal and semantic segmentation, preserving answer-bearing spans while improving contextual integrity. Experimental evaluations demonstrate that BRIGHT-Plus yields consistent and significant improvements in both retrieval accuracy and multi-hop reasoning across a diverse set of retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to support future research on robust, reasoning-centric retrieval.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-bright+-upgrading-the-bright-benchmark-with-marcus,-a-multi-agent-rag-clean-up-s-f4a9aa.md","431e41441fec7aab",{"html":490,"metadata":491},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite\r\nView PDF HTML (experimental)Abstract:Retrieval-Augmented Generation (RAG) systems require corpora that are both structurally clean and semantically coherent. BRIGHT is a recent and influential benchmark designed to evaluate complex multi-hop retrieval across diverse, high-reasoning domains. However, its practical effectiveness is limited by common web-crawled artifacts - such as content redundancy and semantic discontinuity - that impair retrieval accuracy and downstream reasoning. Notably, we find that such issues are concentrated in seven StackExchange-derived subdomains, while other domains (e.g., Coding and Theorem-based content) remain relatively clean.\r\nIn this study, we present MARCUS, a multi-agent pipeline that leverages large language models (LLMs) to systematically clean and re-chunk BRIGHT into a higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for structural noise removal and semantic segmentation, preserving answer-bearing spans while improving contextual integrity. Experimental evaluations demonstrate that BRIGHT-Plus yields consistent and significant improvements in both retrieval accuracy and multi-hop reasoning across a diverse set of retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to support future research on robust, reasoning-centric retrieval.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":492,"localImagePaths":493,"remoteImagePaths":494,"frontmatter":495,"imagePaths":497},[],[],[],{"title":481,"description":482,"pubDate":33,"source":17,"tags":496,"url":485},[19,20,21],[],"2025-06-10-bright+-upgrading-the-bright-benchmark-with-marcus,-a-multi-agent-rag-clean-up-s-f4a9aa.md","2025-06-10-canonical-autoregressive-generation-06fd57",{"id":499,"data":501,"body":507,"filePath":508,"digest":509,"rendered":510,"legacyId":519},{"title":502,"description":503,"pubDate":504,"source":17,"tags":505,"url":506},"Canonical Autoregressive Generation","arXiv:2506.06446v1 Announce Type: cross \nAbstract: State of the art large language models are trained using large amounts of tokens derived from raw text using what is called a tokenizer. Crucially, the tokenizer determines the (token) vocabulary a model will use during inference as well as, in principle, the (token) language. This is because, while the token vocabulary may allow for different tokenizations of a string, the tokenizer always maps the string to only one of these tokenizations--the canonical tokenization. However, multiple lines of empirical evidence suggest that large language models do not always generate canonical token sequences, and this comes with several negative consequences. In this work, we first show that, to generate a canonical token sequence, a model needs to generate (partial) canonical token sequences at each step of the autoregressive generation process underpinning its functioning. Building upon this theoretical result, we introduce canonical sampling, a simple and efficient sampling method that precludes a given model from generating non-canonical token sequences. Further, we also show that, in comparison with standard sampling, the distribution of token sequences generated using canonical sampling is provably closer to the true distribution of token sequences used during training.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06446","Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:Canonical Autoregressive Generation\r\nView PDF HTML (experimental)Abstract:State of the art large language models are trained using large amounts of tokens derived from raw text using what is called a tokenizer. Crucially, the tokenizer determines the (token) vocabulary a model will use during inference as well as, in principle, the (token) language. This is because, while the token vocabulary may allow for different tokenizations of a string, the tokenizer always maps the string to only one of these tokenizations--the canonical tokenization. However, multiple lines of empirical evidence suggest that large language models do not always generate canonical token sequences, and this comes with several negative consequences. In this work, we first show that, to generate a canonical token sequence, a model needs to generate (partial) canonical token sequences at each step of the autoregressive generation process underpinning its functioning. Building upon this theoretical result, we introduce canonical sampling, a simple and efficient sampling method that precludes a given model from generating non-canonical token sequences. Further, we also show that, in comparison with standard sampling, the distribution of token sequences generated using canonical sampling is provably closer to the true distribution of token sequences used during training.\r\nCurrent browse context:\r\ncs.CL\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-canonical-autoregressive-generation-06fd57.md","081f34d8e8b0d0b9",{"html":511,"metadata":512},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:Canonical Autoregressive Generation\r\nView PDF HTML (experimental)Abstract:State of the art large language models are trained using large amounts of tokens derived from raw text using what is called a tokenizer. Crucially, the tokenizer determines the (token) vocabulary a model will use during inference as well as, in principle, the (token) language. This is because, while the token vocabulary may allow for different tokenizations of a string, the tokenizer always maps the string to only one of these tokenizations—the canonical tokenization. However, multiple lines of empirical evidence suggest that large language models do not always generate canonical token sequences, and this comes with several negative consequences. In this work, we first show that, to generate a canonical token sequence, a model needs to generate (partial) canonical token sequences at each step of the autoregressive generation process underpinning its functioning. Building upon this theoretical result, we introduce canonical sampling, a simple and efficient sampling method that precludes a given model from generating non-canonical token sequences. Further, we also show that, in comparison with standard sampling, the distribution of token sequences generated using canonical sampling is provably closer to the true distribution of token sequences used during training.\r\nCurrent browse context:\r\ncs.CL\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":513,"localImagePaths":514,"remoteImagePaths":515,"frontmatter":516,"imagePaths":518},[],[],[],{"title":502,"description":503,"pubDate":33,"source":17,"tags":517,"url":506},[19,20,21],[],"2025-06-10-canonical-autoregressive-generation-06fd57.md","2025-06-10-causal-graph-based-event-reasoning-using-semantic-relation-experts-418702",{"id":520,"data":522,"body":528,"filePath":529,"digest":530,"rendered":531,"legacyId":540},{"title":523,"description":524,"pubDate":525,"source":17,"tags":526,"url":527},"Causal Graph based Event Reasoning using Semantic Relation Experts","arXiv:2506.06910v1 Announce Type: new \nAbstract: Understanding how events in a scenario causally connect with each other is important for effectively modeling and reasoning about events. But event reasoning remains a difficult challenge, and despite recent advances, Large Language Models (LLMs) still struggle to accurately identify causal connections between events. This struggle leads to poor performance on deeper reasoning tasks like event forecasting and timeline understanding. To address this challenge, we investigate the generation of causal event graphs (e.g., A enables B) as a parallel mechanism to help LLMs explicitly represent causality during inference. This paper evaluates both how to generate correct graphs as well as how graphs can assist reasoning. We propose a collaborative approach to causal graph generation where we use LLMs to simulate experts that focus on specific semantic relations. The experts engage in multiple rounds of discussions which are then consolidated by a final expert. Then, to demonstrate the utility of causal graphs, we use them on multiple downstream applications, and also introduce a new explainable event prediction task that requires a causal chain of events in the explanation. These explanations are more informative and coherent than baseline generations. Finally, our overall approach not finetuned on any downstream task, achieves competitive results with state-of-the-art models on both forecasting and next event prediction tasks.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06910","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Causal Graph based Event Reasoning using Semantic Relation Experts\r\nView PDF HTML (experimental)Abstract:Understanding how events in a scenario causally connect with each other is important for effectively modeling and reasoning about events. But event reasoning remains a difficult challenge, and despite recent advances, Large Language Models (LLMs) still struggle to accurately identify causal connections between events. This struggle leads to poor performance on deeper reasoning tasks like event forecasting and timeline understanding. To address this challenge, we investigate the generation of causal event graphs (e.g., A enables B) as a parallel mechanism to help LLMs explicitly represent causality during inference. This paper evaluates both how to generate correct graphs as well as how graphs can assist reasoning. We propose a collaborative approach to causal graph generation where we use LLMs to simulate experts that focus on specific semantic relations. The experts engage in multiple rounds of discussions which are then consolidated by a final expert. Then, to demonstrate the utility of causal graphs, we use them on multiple downstream applications, and also introduce a new explainable event prediction task that requires a causal chain of events in the explanation. These explanations are more informative and coherent than baseline generations. Finally, our overall approach not finetuned on any downstream task, achieves competitive results with state-of-the-art models on both forecasting and next event prediction tasks.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-causal-graph-based-event-reasoning-using-semantic-relation-experts-418702.md","5e972c1c03a599dd",{"html":532,"metadata":533},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Causal Graph based Event Reasoning using Semantic Relation Experts\r\nView PDF HTML (experimental)Abstract:Understanding how events in a scenario causally connect with each other is important for effectively modeling and reasoning about events. But event reasoning remains a difficult challenge, and despite recent advances, Large Language Models (LLMs) still struggle to accurately identify causal connections between events. This struggle leads to poor performance on deeper reasoning tasks like event forecasting and timeline understanding. To address this challenge, we investigate the generation of causal event graphs (e.g., A enables B) as a parallel mechanism to help LLMs explicitly represent causality during inference. This paper evaluates both how to generate correct graphs as well as how graphs can assist reasoning. We propose a collaborative approach to causal graph generation where we use LLMs to simulate experts that focus on specific semantic relations. The experts engage in multiple rounds of discussions which are then consolidated by a final expert. Then, to demonstrate the utility of causal graphs, we use them on multiple downstream applications, and also introduce a new explainable event prediction task that requires a causal chain of events in the explanation. These explanations are more informative and coherent than baseline generations. Finally, our overall approach not finetuned on any downstream task, achieves competitive results with state-of-the-art models on both forecasting and next event prediction tasks.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":534,"localImagePaths":535,"remoteImagePaths":536,"frontmatter":537,"imagePaths":539},[],[],[],{"title":523,"description":524,"pubDate":33,"source":17,"tags":538,"url":527},[19,20,21],[],"2025-06-10-causal-graph-based-event-reasoning-using-semantic-relation-experts-418702.md","2025-06-10-contextual-experience-replay-for-self-improvement-of-language-agents-2d70b8",{"id":541,"data":543,"body":549,"filePath":550,"digest":551,"rendered":552,"legacyId":561},{"title":544,"description":545,"pubDate":546,"source":17,"tags":547,"url":548},"Contextual Experience Replay for Self-Improvement of Language Agents","arXiv:2506.06698v1 Announce Type: new \nAbstract: Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06698","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Contextual Experience Replay for Self-Improvement of Language Agents\r\nView PDFAbstract:Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-contextual-experience-replay-for-self-improvement-of-language-agents-2d70b8.md","536eb2f78b7f6995",{"html":553,"metadata":554},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Contextual Experience Replay for Self-Improvement of Language Agents\r\nView PDFAbstract:Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":555,"localImagePaths":556,"remoteImagePaths":557,"frontmatter":558,"imagePaths":560},[],[],[],{"title":544,"description":545,"pubDate":33,"source":17,"tags":559,"url":548},[19,20,21],[],"2025-06-10-contextual-experience-replay-for-self-improvement-of-language-agents-2d70b8.md","2025-06-10-cellclip----learning-perturbation-effects-in-cell-painting-via-text-guided-contr-eeeb44",{"id":562,"data":564,"body":570,"filePath":571,"digest":572,"rendered":573,"legacyId":582},{"title":565,"description":566,"pubDate":567,"source":17,"tags":568,"url":569},"CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning","arXiv:2506.06290v1 Announce Type: cross \nAbstract: High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells' morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06290","Computer Science > Machine Learning\r\n[Submitted on 16 May 2025]\r\nTitle:CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning\r\nView PDF HTML (experimental)Abstract:High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells' morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-cellclip----learning-perturbation-effects-in-cell-painting-via-text-guided-contr-eeeb44.md","8ade4286918e4708",{"html":574,"metadata":575},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 16 May 2025]\r\nTitle:CellCLIP — Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning\r\nView PDF HTML (experimental)Abstract:High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells’ morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":576,"localImagePaths":577,"remoteImagePaths":578,"frontmatter":579,"imagePaths":581},[],[],[],{"title":565,"description":566,"pubDate":33,"source":17,"tags":580,"url":569},[19,20,21],[],"2025-06-10-cellclip----learning-perturbation-effects-in-cell-painting-via-text-guided-contr-eeeb44.md","2025-06-10-catch-cognitive-assessment-through-cookie-thief-b0cb12",{"id":583,"data":585,"body":591,"filePath":592,"digest":593,"rendered":594,"legacyId":603},{"title":586,"description":587,"pubDate":588,"source":17,"tags":589,"url":590},"CAtCh: Cognitive Assessment through Cookie Thief","arXiv:2506.06603v1 Announce Type: cross \nAbstract: Several machine learning algorithms have been developed for the prediction of Alzheimer's disease and related dementia (ADRD) from spontaneous speech. However, none of these algorithms have been translated for the prediction of broader cognitive impairment (CI), which in some cases is a precursor and risk factor of ADRD. In this paper, we evaluated several speech-based open-source methods originally proposed for the prediction of ADRD, as well as methods from multimodal sentiment analysis for the task of predicting CI from patient audio recordings. Results demonstrated that multimodal methods outperformed unimodal ones for CI prediction, and that acoustics-based approaches performed better than linguistics-based ones. Specifically, interpretable acoustic features relating to affect and prosody were found to significantly outperform BERT-based linguistic features and interpretable linguistic features, respectively. All the code developed for this study is available at https://github.com/JTColonel/catch.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06603","Computer Science > Machine Learning\r\n[Submitted on 7 Jun 2025]\r\nTitle:CAtCh: Cognitive Assessment through Cookie Thief\r\nView PDF HTML (experimental)Abstract:Several machine learning algorithms have been developed for the prediction of Alzheimer's disease and related dementia (ADRD) from spontaneous speech. However, none of these algorithms have been translated for the prediction of broader cognitive impairment (CI), which in some cases is a precursor and risk factor of ADRD. In this paper, we evaluated several speech-based open-source methods originally proposed for the prediction of ADRD, as well as methods from multimodal sentiment analysis for the task of predicting CI from patient audio recordings. Results demonstrated that multimodal methods outperformed unimodal ones for CI prediction, and that acoustics-based approaches performed better than linguistics-based ones. Specifically, interpretable acoustic features relating to affect and prosody were found to significantly outperform BERT-based linguistic features and interpretable linguistic features, respectively. All the code developed for this study is available at this https URL.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-catch-cognitive-assessment-through-cookie-thief-b0cb12.md","620621da98829e71",{"html":595,"metadata":596},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 7 Jun 2025]\r\nTitle:CAtCh: Cognitive Assessment through Cookie Thief\r\nView PDF HTML (experimental)Abstract:Several machine learning algorithms have been developed for the prediction of Alzheimer’s disease and related dementia (ADRD) from spontaneous speech. However, none of these algorithms have been translated for the prediction of broader cognitive impairment (CI), which in some cases is a precursor and risk factor of ADRD. In this paper, we evaluated several speech-based open-source methods originally proposed for the prediction of ADRD, as well as methods from multimodal sentiment analysis for the task of predicting CI from patient audio recordings. Results demonstrated that multimodal methods outperformed unimodal ones for CI prediction, and that acoustics-based approaches performed better than linguistics-based ones. Specifically, interpretable acoustic features relating to affect and prosody were found to significantly outperform BERT-based linguistic features and interpretable linguistic features, respectively. All the code developed for this study is available at this https URL.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":597,"localImagePaths":598,"remoteImagePaths":599,"frontmatter":600,"imagePaths":602},[],[],[],{"title":586,"description":587,"pubDate":33,"source":17,"tags":601,"url":590},[19,20,21],[],"2025-06-10-catch-cognitive-assessment-through-cookie-thief-b0cb12.md","2025-06-10-coordinating-search-informed-reasoning-and-reasoning-guided-search-in-claim-veri-cae8bf",{"id":604,"data":606,"body":612,"filePath":613,"digest":614,"rendered":615,"legacyId":624},{"title":607,"description":608,"pubDate":609,"source":17,"tags":610,"url":611},"Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification","arXiv:2506.07528v1 Announce Type: new \nAbstract: Multi-hop claim verification is inherently challenging, requiring multi-step reasoning to construct verification chains while iteratively searching for information to uncover hidden bridging facts. This process is fundamentally interleaved, as effective reasoning relies on dynamically retrieved evidence, while effective search demands reasoning to refine queries based on partial information. To achieve this, we propose Hierarchical Agent Reasoning and Information Search (HARIS), explicitly modeling the coordinated process of reasoning-driven searching and search-informed reasoning. HARIS consists of a high-level reasoning agent that focuses on constructing the main verification chain, generating factual questions when more information is needed, and a low-level search agent that iteratively retrieves more information, refining its search based on intermediate findings. This design allows each agent to specialize in its respective task, enhancing verification accuracy and interpretability. HARIS is trained using reinforcement learning with outcome-based rewards. Experimental results on the EX-FEVER and HOVER benchmarks demonstrate that HARIS achieves strong performance, greatly advancing multi-hop claim verification.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07528","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification\r\nView PDF HTML (experimental)Abstract:Multi-hop claim verification is inherently challenging, requiring multi-step reasoning to construct verification chains while iteratively searching for information to uncover hidden bridging facts. This process is fundamentally interleaved, as effective reasoning relies on dynamically retrieved evidence, while effective search demands reasoning to refine queries based on partial information. To achieve this, we propose Hierarchical Agent Reasoning and Information Search (HARIS), explicitly modeling the coordinated process of reasoning-driven searching and search-informed reasoning. HARIS consists of a high-level reasoning agent that focuses on constructing the main verification chain, generating factual questions when more information is needed, and a low-level search agent that iteratively retrieves more information, refining its search based on intermediate findings. This design allows each agent to specialize in its respective task, enhancing verification accuracy and interpretability. HARIS is trained using reinforcement learning with outcome-based rewards. Experimental results on the EX-FEVER and HOVER benchmarks demonstrate that HARIS achieves strong performance, greatly advancing multi-hop claim verification.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-coordinating-search-informed-reasoning-and-reasoning-guided-search-in-claim-veri-cae8bf.md","b6436b56135027b9",{"html":616,"metadata":617},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification\r\nView PDF HTML (experimental)Abstract:Multi-hop claim verification is inherently challenging, requiring multi-step reasoning to construct verification chains while iteratively searching for information to uncover hidden bridging facts. This process is fundamentally interleaved, as effective reasoning relies on dynamically retrieved evidence, while effective search demands reasoning to refine queries based on partial information. To achieve this, we propose Hierarchical Agent Reasoning and Information Search (HARIS), explicitly modeling the coordinated process of reasoning-driven searching and search-informed reasoning. HARIS consists of a high-level reasoning agent that focuses on constructing the main verification chain, generating factual questions when more information is needed, and a low-level search agent that iteratively retrieves more information, refining its search based on intermediate findings. This design allows each agent to specialize in its respective task, enhancing verification accuracy and interpretability. HARIS is trained using reinforcement learning with outcome-based rewards. Experimental results on the EX-FEVER and HOVER benchmarks demonstrate that HARIS achieves strong performance, greatly advancing multi-hop claim verification.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":618,"localImagePaths":619,"remoteImagePaths":620,"frontmatter":621,"imagePaths":623},[],[],[],{"title":607,"description":608,"pubDate":33,"source":17,"tags":622,"url":611},[19,20,21],[],"2025-06-10-coordinating-search-informed-reasoning-and-reasoning-guided-search-in-claim-veri-cae8bf.md","2025-06-10-cost-efficient-llm-training-with-lifetime-aware-tensor-offloading-via-gpudirect--e8f230",{"id":625,"data":627,"body":633,"filePath":634,"digest":635,"rendered":636,"legacyId":645},{"title":628,"description":629,"pubDate":630,"source":17,"tags":631,"url":632},"Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage","arXiv:2506.06472v1 Announce Type: cross \nAbstract: We present the design and implementation of a new lifetime-aware tensor offloading framework for GPU memory expansion using low-cost PCIe-based solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for large language model (LLM) training with multiple GPUs and multiple SSDs. Its design is driven by our observation that the active tensors take only a small fraction (1.7% on average) of allocated GPU memory in each LLM training iteration, the inactive tensors are usually large and will not be used for a long period of time, creating ample opportunities for offloading/prefetching tensors to/from slow SSDs without stalling the GPU training process. TERAIO accurately estimates the lifetime (active period of time in GPU memory) of each tensor with the profiling of the first few iterations in the training process. With the tensor lifetime analysis, TERAIO will generate an optimized tensor offloading/prefetching plan and integrate it into the compiled LLM program via PyTorch. TERAIO has a runtime tensor migration engine to execute the offloading/prefetching plan via GPUDirect storage, which allows direct tensor migration between GPUs and SSDs for alleviating the CPU bottleneck and maximizing the SSD bandwidth utilization. In comparison with state-of-the-art studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves the training performance of various LLMs by 1.47x on average, and achieves 80.7% of the ideal performance assuming unlimited GPU memory.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06472","Computer Science > Distributed, Parallel, and Cluster Computing\r\n[Submitted on 6 Jun 2025]\r\nTitle:Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage\r\nView PDF HTML (experimental)Abstract:We present the design and implementation of a new lifetime-aware tensor offloading framework for GPU memory expansion using low-cost PCIe-based solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for large language model (LLM) training with multiple GPUs and multiple SSDs. Its design is driven by our observation that the active tensors take only a small fraction (1.7% on average) of allocated GPU memory in each LLM training iteration, the inactive tensors are usually large and will not be used for a long period of time, creating ample opportunities for offloading/prefetching tensors to/from slow SSDs without stalling the GPU training process. TERAIO accurately estimates the lifetime (active period of time in GPU memory) of each tensor with the profiling of the first few iterations in the training process. With the tensor lifetime analysis, TERAIO will generate an optimized tensor offloading/prefetching plan and integrate it into the compiled LLM program via PyTorch. TERAIO has a runtime tensor migration engine to execute the offloading/prefetching plan via GPUDirect storage, which allows direct tensor migration between GPUs and SSDs for alleviating the CPU bottleneck and maximizing the SSD bandwidth utilization. In comparison with state-of-the-art studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves the training performance of various LLMs by 1.47x on average, and achieves 80.7% of the ideal performance assuming unlimited GPU memory.\r\nCurrent browse context:\r\ncs.DC\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-cost-efficient-llm-training-with-lifetime-aware-tensor-offloading-via-gpudirect--e8f230.md","b67bf2a006110a44",{"html":637,"metadata":638},"\u003Cp>Computer Science > Distributed, Parallel, and Cluster Computing\r\n[Submitted on 6 Jun 2025]\r\nTitle:Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage\r\nView PDF HTML (experimental)Abstract:We present the design and implementation of a new lifetime-aware tensor offloading framework for GPU memory expansion using low-cost PCIe-based solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for large language model (LLM) training with multiple GPUs and multiple SSDs. Its design is driven by our observation that the active tensors take only a small fraction (1.7% on average) of allocated GPU memory in each LLM training iteration, the inactive tensors are usually large and will not be used for a long period of time, creating ample opportunities for offloading/prefetching tensors to/from slow SSDs without stalling the GPU training process. TERAIO accurately estimates the lifetime (active period of time in GPU memory) of each tensor with the profiling of the first few iterations in the training process. With the tensor lifetime analysis, TERAIO will generate an optimized tensor offloading/prefetching plan and integrate it into the compiled LLM program via PyTorch. TERAIO has a runtime tensor migration engine to execute the offloading/prefetching plan via GPUDirect storage, which allows direct tensor migration between GPUs and SSDs for alleviating the CPU bottleneck and maximizing the SSD bandwidth utilization. In comparison with state-of-the-art studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves the training performance of various LLMs by 1.47x on average, and achieves 80.7% of the ideal performance assuming unlimited GPU memory.\r\nCurrent browse context:\r\ncs.DC\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":639,"localImagePaths":640,"remoteImagePaths":641,"frontmatter":642,"imagePaths":644},[],[],[],{"title":628,"description":629,"pubDate":33,"source":17,"tags":643,"url":632},[19,20,21],[],"2025-06-10-cost-efficient-llm-training-with-lifetime-aware-tensor-offloading-via-gpudirect--e8f230.md","2025-06-10-cps-guard-framework-for-dependability-assurance-of-ai--and-llm-based-cyber-physi-2fd45c",{"id":646,"data":648,"body":654,"filePath":655,"digest":656,"rendered":657,"legacyId":666},{"title":649,"description":650,"pubDate":651,"source":17,"tags":652,"url":653},"CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems","arXiv:2506.06381v1 Announce Type: cross \nAbstract: Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to operate in critical applications. However, traditional verification and validation methods often struggle to handle the unpredictable and dynamic nature of AI components. In this paper, we introduce CPS-Guard, a novel framework that employs multi-role orchestration to automate the iterative assurance process for AI-powered CPS. By assigning specialized roles (e.g., safety monitoring, security assessment, fault injection, and recovery planning) to dedicated agents within a simulated environment, CPS-Guard continuously evaluates and refines AI behavior against a range of dependability requirements. We demonstrate the framework through a case study involving an autonomous vehicle navigating an intersection with an AI-based planner. Our results show that CPS-Guard effectively detects vulnerabilities, manages performance impacts, and supports adaptive recovery strategies, thereby offering a structured and extensible solution for rigorous V&amp;V in safety- and security-critical systems.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06381","Computer Science > Robotics\r\n[Submitted on 4 Jun 2025]\r\nTitle:CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems\r\nView PDF HTML (experimental)Abstract:Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to operate in critical applications. However, traditional verification and validation methods often struggle to handle the unpredictable and dynamic nature of AI components. In this paper, we introduce CPS-Guard, a novel framework that employs multi-role orchestration to automate the iterative assurance process for AI-powered CPS. By assigning specialized roles (e.g., safety monitoring, security assessment, fault injection, and recovery planning) to dedicated agents within a simulated environment, CPS-Guard continuously evaluates and refines AI behavior against a range of dependability requirements. We demonstrate the framework through a case study involving an autonomous vehicle navigating an intersection with an AI-based planner. Our results show that CPS-Guard effectively detects vulnerabilities, manages performance impacts, and supports adaptive recovery strategies, thereby offering a structured and extensible solution for rigorous V&V in safety- and security-critical systems.\r\nCurrent browse context:\r\ncs.RO\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-cps-guard-framework-for-dependability-assurance-of-ai--and-llm-based-cyber-physi-2fd45c.md","6ae4f6da9dd0587c",{"html":658,"metadata":659},"\u003Cp>Computer Science > Robotics\r\n[Submitted on 4 Jun 2025]\r\nTitle:CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems\r\nView PDF HTML (experimental)Abstract:Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to operate in critical applications. However, traditional verification and validation methods often struggle to handle the unpredictable and dynamic nature of AI components. In this paper, we introduce CPS-Guard, a novel framework that employs multi-role orchestration to automate the iterative assurance process for AI-powered CPS. By assigning specialized roles (e.g., safety monitoring, security assessment, fault injection, and recovery planning) to dedicated agents within a simulated environment, CPS-Guard continuously evaluates and refines AI behavior against a range of dependability requirements. We demonstrate the framework through a case study involving an autonomous vehicle navigating an intersection with an AI-based planner. Our results show that CPS-Guard effectively detects vulnerabilities, manages performance impacts, and supports adaptive recovery strategies, thereby offering a structured and extensible solution for rigorous V&#x26;V in safety- and security-critical systems.\r\nCurrent browse context:\r\ncs.RO\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":660,"localImagePaths":661,"remoteImagePaths":662,"frontmatter":663,"imagePaths":665},[],[],[],{"title":649,"description":650,"pubDate":33,"source":17,"tags":664,"url":653},[19,20,21],[],"2025-06-10-cps-guard-framework-for-dependability-assurance-of-ai--and-llm-based-cyber-physi-2fd45c.md","2025-06-10-cr-blea-contrastive-ranking-for-adaptive-resource-allocation-in-bilevel-evolutio-dbf2a4",{"id":667,"data":669,"body":675,"filePath":676,"digest":677,"rendered":678,"legacyId":687},{"title":670,"description":671,"pubDate":672,"source":17,"tags":673,"url":674},"CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms","arXiv:2506.06362v1 Announce Type: cross \nAbstract: Bilevel optimization poses a significant computational challenge due to its nested structure, where each upper-level candidate solution requires solving a corresponding lower-level problem. While evolutionary algorithms (EAs) are effective at navigating such complex landscapes, their high resource demands remain a key bottleneck -- particularly the redundant evaluation of numerous unpromising lower-level tasks. Despite recent advances in multitasking and transfer learning, resource waste persists. To address this issue, we propose a novel resource allocation framework for bilevel EAs that selectively identifies and focuses on promising lower-level tasks. Central to our approach is a contrastive ranking network that learns relational patterns between paired upper- and lower-level solutions online. This knowledge guides a reference-based ranking strategy that prioritizes tasks for optimization and adaptively controls resampling based on estimated population quality. Comprehensive experiments across five state-of-the-art bilevel algorithms show that our framework significantly reduces computational cost while preserving -- or even enhancing -- solution accuracy. This work offers a generalizable strategy to improve the efficiency of bilevel EAs, paving the way for more scalable bilevel optimization.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06362","Computer Science > Neural and Evolutionary Computing\r\n[Submitted on 3 Jun 2025]\r\nTitle:CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms\r\nView PDF HTML (experimental)Abstract:Bilevel optimization poses a significant computational challenge due to its nested structure, where each upper-level candidate solution requires solving a corresponding lower-level problem. While evolutionary algorithms (EAs) are effective at navigating such complex landscapes, their high resource demands remain a key bottleneck -- particularly the redundant evaluation of numerous unpromising lower-level tasks. Despite recent advances in multitasking and transfer learning, resource waste persists. To address this issue, we propose a novel resource allocation framework for bilevel EAs that selectively identifies and focuses on promising lower-level tasks. Central to our approach is a contrastive ranking network that learns relational patterns between paired upper- and lower-level solutions online. This knowledge guides a reference-based ranking strategy that prioritizes tasks for optimization and adaptively controls resampling based on estimated population quality. Comprehensive experiments across five state-of-the-art bilevel algorithms show that our framework significantly reduces computational cost while preserving -- or even enhancing -- solution accuracy. This work offers a generalizable strategy to improve the efficiency of bilevel EAs, paving the way for more scalable bilevel optimization.\r\nCurrent browse context:\r\ncs.NE\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-cr-blea-contrastive-ranking-for-adaptive-resource-allocation-in-bilevel-evolutio-dbf2a4.md","11c23314f671ee98",{"html":679,"metadata":680},"\u003Cp>Computer Science > Neural and Evolutionary Computing\r\n[Submitted on 3 Jun 2025]\r\nTitle:CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms\r\nView PDF HTML (experimental)Abstract:Bilevel optimization poses a significant computational challenge due to its nested structure, where each upper-level candidate solution requires solving a corresponding lower-level problem. While evolutionary algorithms (EAs) are effective at navigating such complex landscapes, their high resource demands remain a key bottleneck — particularly the redundant evaluation of numerous unpromising lower-level tasks. Despite recent advances in multitasking and transfer learning, resource waste persists. To address this issue, we propose a novel resource allocation framework for bilevel EAs that selectively identifies and focuses on promising lower-level tasks. Central to our approach is a contrastive ranking network that learns relational patterns between paired upper- and lower-level solutions online. This knowledge guides a reference-based ranking strategy that prioritizes tasks for optimization and adaptively controls resampling based on estimated population quality. Comprehensive experiments across five state-of-the-art bilevel algorithms show that our framework significantly reduces computational cost while preserving — or even enhancing — solution accuracy. This work offers a generalizable strategy to improve the efficiency of bilevel EAs, paving the way for more scalable bilevel optimization.\r\nCurrent browse context:\r\ncs.NE\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":681,"localImagePaths":682,"remoteImagePaths":683,"frontmatter":684,"imagePaths":686},[],[],[],{"title":670,"description":671,"pubDate":33,"source":17,"tags":685,"url":674},[19,20,21],[],"2025-06-10-cr-blea-contrastive-ranking-for-adaptive-resource-allocation-in-bilevel-evolutio-dbf2a4.md","2025-06-10-curriculum-learning-with-counterfactual-group-relative-policy-advantage-for-mult-a30d10",{"id":688,"data":690,"body":696,"filePath":697,"digest":698,"rendered":699,"legacyId":708},{"title":691,"description":692,"pubDate":693,"source":17,"tags":694,"url":695},"Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning","arXiv:2506.07548v1 Announce Type: new \nAbstract: Multi-agent reinforcement learning (MARL) has achieved strong performance in cooperative adversarial tasks. However, most existing methods typically train agents against fixed opponent strategies and rely on such meta-static difficulty conditions, which limits their adaptability to changing environments and often leads to suboptimal policies. Inspired by the success of curriculum learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL that employs an self-adaptive difficulty adjustment mechanism. This mechanism continuously modulates opponent strength based on real-time agent training performance, allowing agents to progressively learn from easier to more challenging scenarios. However, the dynamic nature of CL introduces instability due to nonstationary environments and sparse global rewards. To address this challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA), which is tightly coupled with the curriculum by providing intrinsic credit signals that reflect each agent's impact under evolving task demands. CGRPA constructs a counterfactual advantage function that isolates individual contributions within group behavior, facilitating more reliable policy updates throughout the curriculum. CGRPA evaluates each agent's contribution through constructing counterfactual action advantage function, providing intrinsic rewards that enhance credit assignment and stabilize learning under non-stationary conditions. Extensive experiments demonstrate that our method improves both training stability and final performance, achieving competitive results against state-of-the-art methods. The code is available at https://github.com/NICE-HKU/CL2MARL-SMAC.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07548","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning\r\nView PDF HTML (experimental)Abstract:Multi-agent reinforcement learning (MARL) has achieved strong performance in cooperative adversarial tasks. However, most existing methods typically train agents against fixed opponent strategies and rely on such meta-static difficulty conditions, which limits their adaptability to changing environments and often leads to suboptimal policies. Inspired by the success of curriculum learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL that employs an self-adaptive difficulty adjustment mechanism. This mechanism continuously modulates opponent strength based on real-time agent training performance, allowing agents to progressively learn from easier to more challenging scenarios. However, the dynamic nature of CL introduces instability due to nonstationary environments and sparse global rewards. To address this challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA), which is tightly coupled with the curriculum by providing intrinsic credit signals that reflect each agent's impact under evolving task demands. CGRPA constructs a counterfactual advantage function that isolates individual contributions within group behavior, facilitating more reliable policy updates throughout the curriculum. CGRPA evaluates each agent's contribution through constructing counterfactual action advantage function, providing intrinsic rewards that enhance credit assignment and stabilize learning under non-stationary conditions. Extensive experiments demonstrate that our method improves both training stability and final performance, achieving competitive results against state-of-the-art methods. The code is available at this https URL.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-curriculum-learning-with-counterfactual-group-relative-policy-advantage-for-mult-a30d10.md","c1d2c60bd9b1bc36",{"html":700,"metadata":701},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning\r\nView PDF HTML (experimental)Abstract:Multi-agent reinforcement learning (MARL) has achieved strong performance in cooperative adversarial tasks. However, most existing methods typically train agents against fixed opponent strategies and rely on such meta-static difficulty conditions, which limits their adaptability to changing environments and often leads to suboptimal policies. Inspired by the success of curriculum learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL that employs an self-adaptive difficulty adjustment mechanism. This mechanism continuously modulates opponent strength based on real-time agent training performance, allowing agents to progressively learn from easier to more challenging scenarios. However, the dynamic nature of CL introduces instability due to nonstationary environments and sparse global rewards. To address this challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA), which is tightly coupled with the curriculum by providing intrinsic credit signals that reflect each agent’s impact under evolving task demands. CGRPA constructs a counterfactual advantage function that isolates individual contributions within group behavior, facilitating more reliable policy updates throughout the curriculum. CGRPA evaluates each agent’s contribution through constructing counterfactual action advantage function, providing intrinsic rewards that enhance credit assignment and stabilize learning under non-stationary conditions. Extensive experiments demonstrate that our method improves both training stability and final performance, achieving competitive results against state-of-the-art methods. The code is available at this https URL.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":702,"localImagePaths":703,"remoteImagePaths":704,"frontmatter":705,"imagePaths":707},[],[],[],{"title":691,"description":692,"pubDate":33,"source":17,"tags":706,"url":695},[19,20,21],[],"2025-06-10-curriculum-learning-with-counterfactual-group-relative-policy-advantage-for-mult-a30d10.md","2025-06-10-curriculum-reinforcement-learning-from-easy-to-hard-tasks-improves-llm-reasoning-4106c3",{"id":709,"data":711,"body":717,"filePath":718,"digest":719,"rendered":720,"legacyId":729},{"title":712,"description":713,"pubDate":714,"source":17,"tags":715,"url":716},"Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning","arXiv:2506.06632v1 Announce Type: cross \nAbstract: We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06632","Computer Science > Machine Learning\r\n[Submitted on 7 Jun 2025]\r\nTitle:Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning\r\nView PDF HTML (experimental)Abstract:We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-curriculum-reinforcement-learning-from-easy-to-hard-tasks-improves-llm-reasoning-4106c3.md","6e8854082b1f95e5",{"html":721,"metadata":722},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 7 Jun 2025]\r\nTitle:Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning\r\nView PDF HTML (experimental)Abstract:We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":723,"localImagePaths":724,"remoteImagePaths":725,"frontmatter":726,"imagePaths":728},[],[],[],{"title":712,"description":713,"pubDate":33,"source":17,"tags":727,"url":716},[19,20,21],[],"2025-06-10-curriculum-reinforcement-learning-from-easy-to-hard-tasks-improves-llm-reasoning-4106c3.md","2025-06-10-cross-entropy-games-for-language-models-from-implicit-knowledge-to-general-capab-e842b3",{"id":730,"data":732,"body":738,"filePath":739,"digest":740,"rendered":741,"legacyId":750},{"title":733,"description":734,"pubDate":735,"source":17,"tags":736,"url":737},"Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures","arXiv:2506.06832v1 Announce Type: new \nAbstract: Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06832","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-cross-entropy-games-for-language-models-from-implicit-knowledge-to-general-capab-e842b3.md","be8944edabc9ac99",{"html":742,"metadata":743},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":744,"localImagePaths":745,"remoteImagePaths":746,"frontmatter":747,"imagePaths":749},[],[],[],{"title":733,"description":734,"pubDate":33,"source":17,"tags":748,"url":737},[19,20,21],[],"2025-06-10-cross-entropy-games-for-language-models-from-implicit-knowledge-to-general-capab-e842b3.md","2025-06-10-deep-learning-methods-for-modeling-infrasound-transmission-loss-in-the-middle-at-f8c17f",{"id":751,"data":753,"body":759,"filePath":760,"digest":761,"rendered":762,"legacyId":771},{"title":754,"description":755,"pubDate":756,"source":17,"tags":757,"url":758},"Deep learning methods for modeling infrasound transmission loss in the middle atmosphere","arXiv:2506.06351v1 Announce Type: cross \nAbstract: Accurate modeling of infrasound transmission losses (TLs) is essential to assess the performance of the global International Monitoring System infrasound network. Among existing propagation modeling tools, parabolic equation (PE) method enables TLs to be finely modeled, but its computational cost does not allow exploration of a large parameter space for operational monitoring applications. To reduce computation times, Brissaud et al. 2023 explored the potential of convolutional neural networks trained on a large set of regionally simulated wavefields (\u003C 1000 km from the source) to predict TLs with negligible computation times compared to PE simulations. However, this method struggles in unfavorable initial wind conditions, especially at high frequencies, and causal issues with winds at large distances from the source affecting ground TLs close to the source. In this study, we have developed an optimized convolutional network designed to minimize prediction errors while predicting TLs from globally simulated combined temperature and wind fields spanning over propagation ranges of 4000 km. Our approach enhances the previously proposed one by implementing key optimizations that improve the overall architecture performance. The implemented model predicts TLs with an average error of 8.6 dB in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric scenarios.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06351","Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 2 Jun 2025]\r\nTitle:Deep learning methods for modeling infrasound transmission loss in the middle atmosphere\r\nView PDF HTML (experimental)Abstract:Accurate modeling of infrasound transmission losses (TLs) is essential to assess the performance of the global International Monitoring System infrasound network. Among existing propagation modeling tools, parabolic equation (PE) method enables TLs to be finely modeled, but its computational cost does not allow exploration of a large parameter space for operational monitoring applications. To reduce computation times, Brissaud et al. 2023 explored the potential of convolutional neural networks trained on a large set of regionally simulated wavefields (\u003C 1000 km from the source) to predict TLs with negligible computation times compared to PE simulations. However, this method struggles in unfavorable initial wind conditions, especially at high frequencies, and causal issues with winds at large distances from the source affecting ground TLs close to the source. In this study, we have developed an optimized convolutional network designed to minimize prediction errors while predicting TLs from globally simulated combined temperature and wind fields spanning over propagation ranges of 4000 km. Our approach enhances the previously proposed one by implementing key optimizations that improve the overall architecture performance. The implemented model predicts TLs with an average error of 8.6 dB in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric scenarios.\r\nCurrent browse context:\r\neess.SP\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-deep-learning-methods-for-modeling-infrasound-transmission-loss-in-the-middle-at-f8c17f.md","b2a9b6c608df78a6",{"html":763,"metadata":764},"\u003Cp>Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 2 Jun 2025]\r\nTitle:Deep learning methods for modeling infrasound transmission loss in the middle atmosphere\r\nView PDF HTML (experimental)Abstract:Accurate modeling of infrasound transmission losses (TLs) is essential to assess the performance of the global International Monitoring System infrasound network. Among existing propagation modeling tools, parabolic equation (PE) method enables TLs to be finely modeled, but its computational cost does not allow exploration of a large parameter space for operational monitoring applications. To reduce computation times, Brissaud et al. 2023 explored the potential of convolutional neural networks trained on a large set of regionally simulated wavefields (&#x3C; 1000 km from the source) to predict TLs with negligible computation times compared to PE simulations. However, this method struggles in unfavorable initial wind conditions, especially at high frequencies, and causal issues with winds at large distances from the source affecting ground TLs close to the source. In this study, we have developed an optimized convolutional network designed to minimize prediction errors while predicting TLs from globally simulated combined temperature and wind fields spanning over propagation ranges of 4000 km. Our approach enhances the previously proposed one by implementing key optimizations that improve the overall architecture performance. The implemented model predicts TLs with an average error of 8.6 dB in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric scenarios.\r\nCurrent browse context:\r\neess.SP\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":765,"localImagePaths":766,"remoteImagePaths":767,"frontmatter":768,"imagePaths":770},[],[],[],{"title":754,"description":755,"pubDate":33,"source":17,"tags":769,"url":758},[19,20,21],[],"2025-06-10-deep-learning-methods-for-modeling-infrasound-transmission-loss-in-the-middle-at-f8c17f.md","2025-06-10-deep-research-bench-evaluating-ai-web-research-agents-a1630f",{"id":772,"data":774,"body":780,"filePath":781,"digest":782,"rendered":783,"legacyId":792},{"title":775,"description":776,"pubDate":777,"source":17,"tags":778,"url":779},"Deep Research Bench: Evaluating AI Web Research Agents","arXiv:2506.06287v1 Announce Type: new \nAbstract: Amongst the most common use cases of modern AI is LLM chat with web search enabled. However, no direct evaluations of the quality of web research agents exist that control for the continually-changing web. We introduce Deep Research Bench, consisting of 89 multi-step web research task instances of varying difficulty across 8 diverse task categories, with the answers carefully worked out by skilled humans. We provide a \"RetroSearch\" environment with a large frozen set of scraped web pages, and demonstrate that offline \"RetroSearch\" agents perform comparably to \"live web\" agents, enabling reliable evaluations of models over time. We provide robust agent tooling and scaffolding to benchmark major LLMs as they are released, including \"thinking\" models like o3 and Gemini 2.5 Pro. We include automated evaluations of the lengthy agent traces to report progress over time in hallucinations, tool use, and forgetting. Finally, we evaluate the major web research products branded as \"Deep Research\", \"Deep Search\", \"Search\", or \"Research.\" Results are available on a public leaderboard at https://drb.futuresearch.ai/.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06287","Computer Science > Artificial Intelligence\r\n[Submitted on 6 May 2025]\r\nTitle:Deep Research Bench: Evaluating AI Web Research Agents\r\nView PDFAbstract:Amongst the most common use cases of modern AI is LLM chat with web search enabled. However, no direct evaluations of the quality of web research agents exist that control for the continually-changing web. We introduce Deep Research Bench, consisting of 89 multi-step web research task instances of varying difficulty across 8 diverse task categories, with the answers carefully worked out by skilled humans. We provide a \"RetroSearch\" environment with a large frozen set of scraped web pages, and demonstrate that offline \"RetroSearch\" agents perform comparably to \"live web\" agents, enabling reliable evaluations of models over time. We provide robust agent tooling and scaffolding to benchmark major LLMs as they are released, including \"thinking\" models like o3 and Gemini 2.5 Pro. We include automated evaluations of the lengthy agent traces to report progress over time in hallucinations, tool use, and forgetting. Finally, we evaluate the major web research products branded as \"Deep Research\", \"Deep Search\", \"Search\", or \"Research.\" Results are available on a public leaderboard at this https URL.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-deep-research-bench-evaluating-ai-web-research-agents-a1630f.md","a7b0f0fff1fac03a",{"html":784,"metadata":785},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 6 May 2025]\r\nTitle:Deep Research Bench: Evaluating AI Web Research Agents\r\nView PDFAbstract:Amongst the most common use cases of modern AI is LLM chat with web search enabled. However, no direct evaluations of the quality of web research agents exist that control for the continually-changing web. We introduce Deep Research Bench, consisting of 89 multi-step web research task instances of varying difficulty across 8 diverse task categories, with the answers carefully worked out by skilled humans. We provide a “RetroSearch” environment with a large frozen set of scraped web pages, and demonstrate that offline “RetroSearch” agents perform comparably to “live web” agents, enabling reliable evaluations of models over time. We provide robust agent tooling and scaffolding to benchmark major LLMs as they are released, including “thinking” models like o3 and Gemini 2.5 Pro. We include automated evaluations of the lengthy agent traces to report progress over time in hallucinations, tool use, and forgetting. Finally, we evaluate the major web research products branded as “Deep Research”, “Deep Search”, “Search”, or “Research.” Results are available on a public leaderboard at this https URL.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":786,"localImagePaths":787,"remoteImagePaths":788,"frontmatter":789,"imagePaths":791},[],[],[],{"title":775,"description":776,"pubDate":33,"source":17,"tags":790,"url":779},[19,20,21],[],"2025-06-10-deep-research-bench-evaluating-ai-web-research-agents-a1630f.md","2025-06-10-deep-rl-needs-deep-behavior-analysis-exploring-implicit-planning-by-model-free-a-255e7c",{"id":793,"data":795,"body":801,"filePath":802,"digest":803,"rendered":804,"legacyId":813},{"title":796,"description":797,"pubDate":798,"source":17,"tags":799,"url":800},"Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments","arXiv:2506.06981v1 Announce Type: new \nAbstract: Understanding the behavior of deep reinforcement learning (DRL) agents -- particularly as task and agent sophistication increase -- requires more than simple comparison of reward curves, yet standard methods for behavioral analysis remain underdeveloped in DRL. We apply tools from neuroscience and ethology to study DRL agents in a novel, complex, partially observable environment, ForageWorld, designed to capture key aspects of real-world animal foraging -- including sparse, depleting resource patches, predator threats, and spatially extended arenas. We use this environment as a platform for applying joint behavioral and neural analysis to agents, revealing detailed, quantitatively grounded insights into agent strategies, memory, and planning. Contrary to common assumptions, we find that model-free RNN-based DRL agents can exhibit structured, planning-like behavior purely through emergent dynamics -- without requiring explicit memory modules or world models. Our results show that studying DRL agents like animals -- analyzing them with neuroethology-inspired tools that reveal structure in both behavior and neural dynamics -- uncovers rich structure in their learning dynamics that would otherwise remain invisible. We distill these tools into a general analysis framework linking core behavioral and representational features to diagnostic methods, which can be reused for a wide range of tasks and agents. As agents grow more complex and autonomous, bridging neuroscience, cognitive science, and AI will be essential -- not just for understanding their behavior, but for ensuring safe alignment and maximizing desirable behaviors that are hard to measure via reward. We show how this can be done by drawing on lessons from how biological intelligence is studied.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06981","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments\r\nView PDF HTML (experimental)Abstract:Understanding the behavior of deep reinforcement learning (DRL) agents -- particularly as task and agent sophistication increase -- requires more than simple comparison of reward curves, yet standard methods for behavioral analysis remain underdeveloped in DRL. We apply tools from neuroscience and ethology to study DRL agents in a novel, complex, partially observable environment, ForageWorld, designed to capture key aspects of real-world animal foraging -- including sparse, depleting resource patches, predator threats, and spatially extended arenas. We use this environment as a platform for applying joint behavioral and neural analysis to agents, revealing detailed, quantitatively grounded insights into agent strategies, memory, and planning. Contrary to common assumptions, we find that model-free RNN-based DRL agents can exhibit structured, planning-like behavior purely through emergent dynamics -- without requiring explicit memory modules or world models. Our results show that studying DRL agents like animals -- analyzing them with neuroethology-inspired tools that reveal structure in both behavior and neural dynamics -- uncovers rich structure in their learning dynamics that would otherwise remain invisible. We distill these tools into a general analysis framework linking core behavioral and representational features to diagnostic methods, which can be reused for a wide range of tasks and agents. As agents grow more complex and autonomous, bridging neuroscience, cognitive science, and AI will be essential -- not just for understanding their behavior, but for ensuring safe alignment and maximizing desirable behaviors that are hard to measure via reward. We show how this can be done by drawing on lessons from how biological intelligence is studied.\r\nSubmission history\r\nFrom: Riley Simmons-Edler [view email][v1] Sun, 8 Jun 2025 03:43:48 UTC (6,264 KB)\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-deep-rl-needs-deep-behavior-analysis-exploring-implicit-planning-by-model-free-a-255e7c.md","5c4b12934da8f7e2",{"html":805,"metadata":806},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments\r\nView PDF HTML (experimental)Abstract:Understanding the behavior of deep reinforcement learning (DRL) agents — particularly as task and agent sophistication increase — requires more than simple comparison of reward curves, yet standard methods for behavioral analysis remain underdeveloped in DRL. We apply tools from neuroscience and ethology to study DRL agents in a novel, complex, partially observable environment, ForageWorld, designed to capture key aspects of real-world animal foraging — including sparse, depleting resource patches, predator threats, and spatially extended arenas. We use this environment as a platform for applying joint behavioral and neural analysis to agents, revealing detailed, quantitatively grounded insights into agent strategies, memory, and planning. Contrary to common assumptions, we find that model-free RNN-based DRL agents can exhibit structured, planning-like behavior purely through emergent dynamics — without requiring explicit memory modules or world models. Our results show that studying DRL agents like animals — analyzing them with neuroethology-inspired tools that reveal structure in both behavior and neural dynamics — uncovers rich structure in their learning dynamics that would otherwise remain invisible. We distill these tools into a general analysis framework linking core behavioral and representational features to diagnostic methods, which can be reused for a wide range of tasks and agents. As agents grow more complex and autonomous, bridging neuroscience, cognitive science, and AI will be essential — not just for understanding their behavior, but for ensuring safe alignment and maximizing desirable behaviors that are hard to measure via reward. We show how this can be done by drawing on lessons from how biological intelligence is studied.\r\nSubmission history\r\nFrom: Riley Simmons-Edler [view email][v1] Sun, 8 Jun 2025 03:43:48 UTC (6,264 KB)\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":807,"localImagePaths":808,"remoteImagePaths":809,"frontmatter":810,"imagePaths":812},[],[],[],{"title":796,"description":797,"pubDate":33,"source":17,"tags":811,"url":800},[19,20,21],[],"2025-06-10-deep-rl-needs-deep-behavior-analysis-exploring-implicit-planning-by-model-free-a-255e7c.md","2025-06-10-delphyne-a-pre-trained-model-for-general-and-financial-time-series-2ff958",{"id":814,"data":816,"body":822,"filePath":823,"digest":824,"rendered":825,"legacyId":834},{"title":817,"description":818,"pubDate":819,"source":17,"tags":820,"url":821},"DELPHYNE: A Pre-Trained Model for General and Financial Time Series","arXiv:2506.06288v1 Announce Type: cross \nAbstract: Time-series data is a vital modality within data science communities. This is particularly valuable in financial applications, where it helps in detecting patterns, understanding market behavior, and making informed decisions based on historical data. Recent advances in language modeling have led to the rise of time-series pre-trained models that are trained on vast collections of datasets and applied to diverse tasks across financial domains. However, across financial applications, existing time-series pre-trained models have not shown boosts in performance over simple finance benchmarks in both zero-shot and fine-tuning settings. This phenomenon occurs because of a i) lack of financial data within the pre-training stage, and ii) the negative transfer effect due to inherently different time-series patterns across domains. Furthermore, time-series data is continuous, noisy, and can be collected at varying frequencies and with varying lags across different variables, making this data more challenging to model than languages. To address the above problems, we introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne achieves competitive performance to existing foundation and full-shot models with few fine-tuning steps on publicly available datasets, and also shows superior performances on various financial tasks.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06288","Quantitative Finance > Statistical Finance\r\n[Submitted on 12 May 2025]\r\nTitle:DELPHYNE: A Pre-Trained Model for General and Financial Time Series\r\nView PDF HTML (experimental)Abstract:Time-series data is a vital modality within data science communities. This is particularly valuable in financial applications, where it helps in detecting patterns, understanding market behavior, and making informed decisions based on historical data. Recent advances in language modeling have led to the rise of time-series pre-trained models that are trained on vast collections of datasets and applied to diverse tasks across financial domains. However, across financial applications, existing time-series pre-trained models have not shown boosts in performance over simple finance benchmarks in both zero-shot and fine-tuning settings. This phenomenon occurs because of a i) lack of financial data within the pre-training stage, and ii) the negative transfer effect due to inherently different time-series patterns across domains. Furthermore, time-series data is continuous, noisy, and can be collected at varying frequencies and with varying lags across different variables, making this data more challenging to model than languages. To address the above problems, we introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne achieves competitive performance to existing foundation and full-shot models with few fine-tuning steps on publicly available datasets, and also shows superior performances on various financial tasks.\r\nCurrent browse context:\r\nq-fin.ST\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-delphyne-a-pre-trained-model-for-general-and-financial-time-series-2ff958.md","7b8c11d5ffebcd35",{"html":826,"metadata":827},"\u003Cp>Quantitative Finance > Statistical Finance\r\n[Submitted on 12 May 2025]\r\nTitle:DELPHYNE: A Pre-Trained Model for General and Financial Time Series\r\nView PDF HTML (experimental)Abstract:Time-series data is a vital modality within data science communities. This is particularly valuable in financial applications, where it helps in detecting patterns, understanding market behavior, and making informed decisions based on historical data. Recent advances in language modeling have led to the rise of time-series pre-trained models that are trained on vast collections of datasets and applied to diverse tasks across financial domains. However, across financial applications, existing time-series pre-trained models have not shown boosts in performance over simple finance benchmarks in both zero-shot and fine-tuning settings. This phenomenon occurs because of a i) lack of financial data within the pre-training stage, and ii) the negative transfer effect due to inherently different time-series patterns across domains. Furthermore, time-series data is continuous, noisy, and can be collected at varying frequencies and with varying lags across different variables, making this data more challenging to model than languages. To address the above problems, we introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne achieves competitive performance to existing foundation and full-shot models with few fine-tuning steps on publicly available datasets, and also shows superior performances on various financial tasks.\r\nCurrent browse context:\r\nq-fin.ST\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":828,"localImagePaths":829,"remoteImagePaths":830,"frontmatter":831,"imagePaths":833},[],[],[],{"title":817,"description":818,"pubDate":33,"source":17,"tags":832,"url":821},[19,20,21],[],"2025-06-10-delphyne-a-pre-trained-model-for-general-and-financial-time-series-2ff958.md","2025-06-10-deontically-constrained-policy-improvement-in-reinforcement-learning-agents-6ab530",{"id":835,"data":837,"body":843,"filePath":844,"digest":845,"rendered":846,"legacyId":855},{"title":838,"description":839,"pubDate":840,"source":17,"tags":841,"url":842},"Deontically Constrained Policy Improvement in Reinforcement Learning Agents","arXiv:2506.06959v1 Announce Type: new \nAbstract: Markov Decision Processes (MDPs) are the most common model for decision making under uncertainty in the Machine Learning community. An MDP captures non-determinism, probabilistic uncertainty, and an explicit model of action. A Reinforcement Learning (RL) agent learns to act in an MDP by maximizing a utility function. This paper considers the problem of learning a decision policy that maximizes utility subject to satisfying a constraint expressed in deontic logic. In this setup, the utility captures the agent's mission - such as going quickly from A to B. The deontic formula represents (ethical, social, situational) constraints on how the agent might achieve its mission by prohibiting classes of behaviors. We use the logic of Expected Act Utilitarianism, a probabilistic stit logic that can be interpreted over controlled MDPs. We develop a variation on policy improvement, and show that it reaches a constrained local maximum of the mission utility. Given that in stit logic, an agent's duty is derived from value maximization, this can be seen as a way of acting to simultaneously maximize two value functions, one of which is implicit, in a bi-level structure. We illustrate these results with experiments on sample MDPs.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06959","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Deontically Constrained Policy Improvement in Reinforcement Learning Agents\r\nView PDF HTML (experimental)Abstract:Markov Decision Processes (MDPs) are the most common model for decision making under uncertainty in the Machine Learning community. An MDP captures non-determinism, probabilistic uncertainty, and an explicit model of action. A Reinforcement Learning (RL) agent learns to act in an MDP by maximizing a utility function. This paper considers the problem of learning a decision policy that maximizes utility subject to satisfying a constraint expressed in deontic logic. In this setup, the utility captures the agent's mission - such as going quickly from A to B. The deontic formula represents (ethical, social, situational) constraints on how the agent might achieve its mission by prohibiting classes of behaviors. We use the logic of Expected Act Utilitarianism, a probabilistic stit logic that can be interpreted over controlled MDPs. We develop a variation on policy improvement, and show that it reaches a constrained local maximum of the mission utility. Given that in stit logic, an agent's duty is derived from value maximization, this can be seen as a way of acting to simultaneously maximize two value functions, one of which is implicit, in a bi-level structure. We illustrate these results with experiments on sample MDPs.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-deontically-constrained-policy-improvement-in-reinforcement-learning-agents-6ab530.md","533518ce5b01ee93",{"html":847,"metadata":848},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Deontically Constrained Policy Improvement in Reinforcement Learning Agents\r\nView PDF HTML (experimental)Abstract:Markov Decision Processes (MDPs) are the most common model for decision making under uncertainty in the Machine Learning community. An MDP captures non-determinism, probabilistic uncertainty, and an explicit model of action. A Reinforcement Learning (RL) agent learns to act in an MDP by maximizing a utility function. This paper considers the problem of learning a decision policy that maximizes utility subject to satisfying a constraint expressed in deontic logic. In this setup, the utility captures the agent’s mission - such as going quickly from A to B. The deontic formula represents (ethical, social, situational) constraints on how the agent might achieve its mission by prohibiting classes of behaviors. We use the logic of Expected Act Utilitarianism, a probabilistic stit logic that can be interpreted over controlled MDPs. We develop a variation on policy improvement, and show that it reaches a constrained local maximum of the mission utility. Given that in stit logic, an agent’s duty is derived from value maximization, this can be seen as a way of acting to simultaneously maximize two value functions, one of which is implicit, in a bi-level structure. We illustrate these results with experiments on sample MDPs.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":849,"localImagePaths":850,"remoteImagePaths":851,"frontmatter":852,"imagePaths":854},[],[],[],{"title":838,"description":839,"pubDate":33,"source":17,"tags":853,"url":842},[19,20,21],[],"2025-06-10-deontically-constrained-policy-improvement-in-reinforcement-learning-agents-6ab530.md","2025-06-10-direct-behavior-optimization-unlocking-the-potential-of-lightweight-llms-c33f1d",{"id":856,"data":858,"body":864,"filePath":865,"digest":866,"rendered":867,"legacyId":876},{"title":859,"description":860,"pubDate":861,"source":17,"tags":862,"url":863},"Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs","arXiv:2506.06401v1 Announce Type: cross \nAbstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized models designed to run efficiently on consumer-grade hardware, offering significant advantages in resource efficiency, cost-effectiveness, and data privacy. However, these models often struggle with limited inference and reasoning capabilities, which restrict their performance on complex tasks and limit their practical applicability. Moreover, existing prompt optimization methods typically rely on extensive manual effort or the meta-cognitive abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To address these challenges, we introduce DeBoP, a new Direct Behavior Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting technique. Unlike CoT Prompting, DeBoP is an automatic optimization method, which focuses on the optimization directly on the behavior of LwLLMs. In particular, DeBoP transforms the optimization of complex prompts into the optimization of discrete, quantifiable execution sequences using a gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging tasks where state-of-the-art LLMs excel but LwLLMs generally underperform. Experimental results demonstrate that DeBoP significantly outperforms recent prompt optimization methods on most tasks. In particular, DeBoP-optimized LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by approximately 60% compared to other automatic prompt optimization methods.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06401","Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs\r\nView PDF HTML (experimental)Abstract:Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized models designed to run efficiently on consumer-grade hardware, offering significant advantages in resource efficiency, cost-effectiveness, and data privacy. However, these models often struggle with limited inference and reasoning capabilities, which restrict their performance on complex tasks and limit their practical applicability. Moreover, existing prompt optimization methods typically rely on extensive manual effort or the meta-cognitive abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To address these challenges, we introduce DeBoP, a new Direct Behavior Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting technique. Unlike CoT Prompting, DeBoP is an automatic optimization method, which focuses on the optimization directly on the behavior of LwLLMs. In particular, DeBoP transforms the optimization of complex prompts into the optimization of discrete, quantifiable execution sequences using a gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging tasks where state-of-the-art LLMs excel but LwLLMs generally underperform. Experimental results demonstrate that DeBoP significantly outperforms recent prompt optimization methods on most tasks. In particular, DeBoP-optimized LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by approximately 60% compared to other automatic prompt optimization methods.\r\nCurrent browse context:\r\ncs.CL\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-direct-behavior-optimization-unlocking-the-potential-of-lightweight-llms-c33f1d.md","4f95d8421c827039",{"html":868,"metadata":869},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs\r\nView PDF HTML (experimental)Abstract:Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized models designed to run efficiently on consumer-grade hardware, offering significant advantages in resource efficiency, cost-effectiveness, and data privacy. However, these models often struggle with limited inference and reasoning capabilities, which restrict their performance on complex tasks and limit their practical applicability. Moreover, existing prompt optimization methods typically rely on extensive manual effort or the meta-cognitive abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To address these challenges, we introduce DeBoP, a new Direct Behavior Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting technique. Unlike CoT Prompting, DeBoP is an automatic optimization method, which focuses on the optimization directly on the behavior of LwLLMs. In particular, DeBoP transforms the optimization of complex prompts into the optimization of discrete, quantifiable execution sequences using a gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging tasks where state-of-the-art LLMs excel but LwLLMs generally underperform. Experimental results demonstrate that DeBoP significantly outperforms recent prompt optimization methods on most tasks. In particular, DeBoP-optimized LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by approximately 60% compared to other automatic prompt optimization methods.\r\nCurrent browse context:\r\ncs.CL\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":870,"localImagePaths":871,"remoteImagePaths":872,"frontmatter":873,"imagePaths":875},[],[],[],{"title":859,"description":860,"pubDate":33,"source":17,"tags":874,"url":863},[19,20,21],[],"2025-06-10-direct-behavior-optimization-unlocking-the-potential-of-lightweight-llms-c33f1d.md","2025-06-10-detection-method-for-prompt-injection-by-integrating-pre-trained-model-and-heuri-42df1a",{"id":877,"data":879,"body":885,"filePath":886,"digest":887,"rendered":888,"legacyId":897},{"title":880,"description":881,"pubDate":882,"source":17,"tags":883,"url":884},"Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering","arXiv:2506.06384v1 Announce Type: cross \nAbstract: With the widespread adoption of Large Language Models (LLMs), prompt injection attacks have emerged as a significant security threat. Existing defense mechanisms often face critical trade-offs between effectiveness and generalizability. This highlights the urgent need for efficient prompt injection detection methods that are applicable across a wide range of LLMs. To address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion detection framework. It integrates a pretrained language model with heuristic feature engineering to detect prompt injection attacks. Specifically, the framework employs DeBERTa-v3-base as a feature extractor to transform input text into semantic vectors enriched with contextual information. In parallel, we design heuristic rules based on known attack patterns to extract explicit structural features commonly observed in attacks. Features from both channels are subsequently fused and passed through a fully connected neural network to produce the final prediction. This dual-channel approach mitigates the limitations of relying only on DeBERTa to extract features. Experimental results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms existing methods in terms of accuracy, recall, and F1-score. Furthermore, when deployed actually, it significantly reduces attack success rates across mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06384","Computer Science > Computation and Language\r\n[Submitted on 5 Jun 2025]\r\nTitle:Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering\r\nView PDF HTML (experimental)Abstract:With the widespread adoption of Large Language Models (LLMs), prompt injection attacks have emerged as a significant security threat. Existing defense mechanisms often face critical trade-offs between effectiveness and generalizability. This highlights the urgent need for efficient prompt injection detection methods that are applicable across a wide range of LLMs. To address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion detection framework. It integrates a pretrained language model with heuristic feature engineering to detect prompt injection attacks. Specifically, the framework employs DeBERTa-v3-base as a feature extractor to transform input text into semantic vectors enriched with contextual information. In parallel, we design heuristic rules based on known attack patterns to extract explicit structural features commonly observed in attacks. Features from both channels are subsequently fused and passed through a fully connected neural network to produce the final prediction. This dual-channel approach mitigates the limitations of relying only on DeBERTa to extract features. Experimental results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms existing methods in terms of accuracy, recall, and F1-score. Furthermore, when deployed actually, it significantly reduces attack success rates across mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-detection-method-for-prompt-injection-by-integrating-pre-trained-model-and-heuri-42df1a.md","1d354d922eddb99c",{"html":889,"metadata":890},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 5 Jun 2025]\r\nTitle:Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering\r\nView PDF HTML (experimental)Abstract:With the widespread adoption of Large Language Models (LLMs), prompt injection attacks have emerged as a significant security threat. Existing defense mechanisms often face critical trade-offs between effectiveness and generalizability. This highlights the urgent need for efficient prompt injection detection methods that are applicable across a wide range of LLMs. To address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion detection framework. It integrates a pretrained language model with heuristic feature engineering to detect prompt injection attacks. Specifically, the framework employs DeBERTa-v3-base as a feature extractor to transform input text into semantic vectors enriched with contextual information. In parallel, we design heuristic rules based on known attack patterns to extract explicit structural features commonly observed in attacks. Features from both channels are subsequently fused and passed through a fully connected neural network to produce the final prediction. This dual-channel approach mitigates the limitations of relying only on DeBERTa to extract features. Experimental results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms existing methods in terms of accuracy, recall, and F1-score. Furthermore, when deployed actually, it significantly reduces attack success rates across mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":891,"localImagePaths":892,"remoteImagePaths":893,"frontmatter":894,"imagePaths":896},[],[],[],{"title":880,"description":881,"pubDate":33,"source":17,"tags":895,"url":884},[19,20,21],[],"2025-06-10-detection-method-for-prompt-injection-by-integrating-pre-trained-model-and-heuri-42df1a.md","2025-06-10-disretrieval-harnessing-discourse-structure-for-long-document-retrieval-87b7ce",{"id":898,"data":900,"body":906,"filePath":907,"digest":908,"rendered":909,"legacyId":918},{"title":901,"description":902,"pubDate":903,"source":17,"tags":904,"url":905},"DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval","arXiv:2506.06313v1 Announce Type: cross \nAbstract: Long document understanding has become increasingly crucial in natural language processing, with retrieval-based methods emerging as a promising solution to address the context length limitations of large language models (LLMs). However, existing approaches either treat documents as flat sequences or employ arbitrary chunking strategies, failing to capture the inherent discourse structure that guides human comprehension. We present DISRetrieval, a novel hierarchical retrieval framework that leverages linguistic discourse structure to enhance long document understanding. Our approach introduces three key innovations: (1) a discourse-aware document organization framework that utilizes rhetorical structure theory (RST) to create sentence-level hierarchical representations, preserving both semantic relationships and natural document flow; (2) an LLM-enhanced node representation technique that combines discourse structure with adaptive summarization to enrich tree nodes with contextual information; and (3) a hierarchical evidence retrieval mechanism that effectively selects relevant content while maintaining discourse coherence. Through comprehensive experiments on QASPER and QuALITY datasets, DISRetrieval demonstrates substantial improvements over existing methods in both token-level retrieval metrics and downstream question answering tasks. Our ablation studies confirm that incorporating discourse structure significantly enhances retrieval effectiveness across different document lengths and query types, validating the importance of linguistically-informed document representation in long-text understanding. Our code and datasets are publicly available at github/DreamH1gh/DISRetrieval to facilitate future research.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06313","Computer Science > Information Retrieval\r\n[Submitted on 26 May 2025]\r\nTitle:DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval\r\nView PDF HTML (experimental)Abstract:Long document understanding has become increasingly crucial in natural language processing, with retrieval-based methods emerging as a promising solution to address the context length limitations of large language models (LLMs). However, existing approaches either treat documents as flat sequences or employ arbitrary chunking strategies, failing to capture the inherent discourse structure that guides human comprehension. We present DISRetrieval, a novel hierarchical retrieval framework that leverages linguistic discourse structure to enhance long document understanding. Our approach introduces three key innovations: (1) a discourse-aware document organization framework that utilizes rhetorical structure theory (RST) to create sentence-level hierarchical representations, preserving both semantic relationships and natural document flow; (2) an LLM-enhanced node representation technique that combines discourse structure with adaptive summarization to enrich tree nodes with contextual information; and (3) a hierarchical evidence retrieval mechanism that effectively selects relevant content while maintaining discourse coherence. Through comprehensive experiments on QASPER and QuALITY datasets, DISRetrieval demonstrates substantial improvements over existing methods in both token-level retrieval metrics and downstream question answering tasks. Our ablation studies confirm that incorporating discourse structure significantly enhances retrieval effectiveness across different document lengths and query types, validating the importance of linguistically-informed document representation in long-text understanding. Our code and datasets are publicly available at github/DreamH1gh/DISRetrieval to facilitate future research.\r\nCurrent browse context:\r\ncs.IR\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-disretrieval-harnessing-discourse-structure-for-long-document-retrieval-87b7ce.md","003e346efeec1677",{"html":910,"metadata":911},"\u003Cp>Computer Science > Information Retrieval\r\n[Submitted on 26 May 2025]\r\nTitle:DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval\r\nView PDF HTML (experimental)Abstract:Long document understanding has become increasingly crucial in natural language processing, with retrieval-based methods emerging as a promising solution to address the context length limitations of large language models (LLMs). However, existing approaches either treat documents as flat sequences or employ arbitrary chunking strategies, failing to capture the inherent discourse structure that guides human comprehension. We present DISRetrieval, a novel hierarchical retrieval framework that leverages linguistic discourse structure to enhance long document understanding. Our approach introduces three key innovations: (1) a discourse-aware document organization framework that utilizes rhetorical structure theory (RST) to create sentence-level hierarchical representations, preserving both semantic relationships and natural document flow; (2) an LLM-enhanced node representation technique that combines discourse structure with adaptive summarization to enrich tree nodes with contextual information; and (3) a hierarchical evidence retrieval mechanism that effectively selects relevant content while maintaining discourse coherence. Through comprehensive experiments on QASPER and QuALITY datasets, DISRetrieval demonstrates substantial improvements over existing methods in both token-level retrieval metrics and downstream question answering tasks. Our ablation studies confirm that incorporating discourse structure significantly enhances retrieval effectiveness across different document lengths and query types, validating the importance of linguistically-informed document representation in long-text understanding. Our code and datasets are publicly available at github/DreamH1gh/DISRetrieval to facilitate future research.\r\nCurrent browse context:\r\ncs.IR\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":912,"localImagePaths":913,"remoteImagePaths":914,"frontmatter":915,"imagePaths":917},[],[],[],{"title":901,"description":902,"pubDate":33,"source":17,"tags":916,"url":905},[19,20,21],[],"2025-06-10-disretrieval-harnessing-discourse-structure-for-long-document-retrieval-87b7ce.md","2025-06-10-disentangling-ai-alignment-a-structured-taxonomy-beyond-safety-and-ethics-d6dcaf",{"id":919,"data":921,"body":927,"filePath":928,"digest":929,"rendered":930,"legacyId":939},{"title":922,"description":923,"pubDate":924,"source":17,"tags":925,"url":926},"Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics","arXiv:2506.06286v1 Announce Type: cross \nAbstract: Recent advances in AI research make it increasingly plausible that artificial agents with consequential real-world impact will soon operate beyond tightly controlled environments. Ensuring that these agents are not only safe but that they adhere to broader normative expectations is thus an urgent interdisciplinary challenge. Multiple fields -- notably AI Safety, AI Alignment, and Machine Ethics -- claim to contribute to this task. However, the conceptual boundaries and interrelations among these domains remain vague, leaving researchers without clear guidance in positioning their work.\n  To address this meta-challenge, we develop a structured conceptual framework for understanding AI alignment. Rather than focusing solely on alignment goals, we introduce a taxonomy distinguishing the alignment aim (safety, ethicality, legality, etc.), scope (outcome vs. execution), and constituency (individual vs. collective). This structural approach reveals multiple legitimate alignment configurations, providing a foundation for practical and philosophical integration across domains, and clarifying what it might mean for an agent to be aligned all-things-considered.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06286","Computer Science > Computers and Society\r\n[Submitted on 2 May 2025]\r\nTitle:Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics\r\nView PDF HTML (experimental)Abstract:Recent advances in AI research make it increasingly plausible that artificial agents with consequential real-world impact will soon operate beyond tightly controlled environments. Ensuring that these agents are not only safe but that they adhere to broader normative expectations is thus an urgent interdisciplinary challenge. Multiple fields -- notably AI Safety, AI Alignment, and Machine Ethics -- claim to contribute to this task. However, the conceptual boundaries and interrelations among these domains remain vague, leaving researchers without clear guidance in positioning their work.\r\nTo address this meta-challenge, we develop a structured conceptual framework for understanding AI alignment. Rather than focusing solely on alignment goals, we introduce a taxonomy distinguishing the alignment aim (safety, ethicality, legality, etc.), scope (outcome vs. execution), and constituency (individual vs. collective). This structural approach reveals multiple legitimate alignment configurations, providing a foundation for practical and philosophical integration across domains, and clarifying what it might mean for an agent to be aligned all-things-considered.\r\nCurrent browse context:\r\ncs.CY\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-disentangling-ai-alignment-a-structured-taxonomy-beyond-safety-and-ethics-d6dcaf.md","1b0dd3f3f8a798ab",{"html":931,"metadata":932},"\u003Cp>Computer Science > Computers and Society\r\n[Submitted on 2 May 2025]\r\nTitle:Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics\r\nView PDF HTML (experimental)Abstract:Recent advances in AI research make it increasingly plausible that artificial agents with consequential real-world impact will soon operate beyond tightly controlled environments. Ensuring that these agents are not only safe but that they adhere to broader normative expectations is thus an urgent interdisciplinary challenge. Multiple fields — notably AI Safety, AI Alignment, and Machine Ethics — claim to contribute to this task. However, the conceptual boundaries and interrelations among these domains remain vague, leaving researchers without clear guidance in positioning their work.\r\nTo address this meta-challenge, we develop a structured conceptual framework for understanding AI alignment. Rather than focusing solely on alignment goals, we introduce a taxonomy distinguishing the alignment aim (safety, ethicality, legality, etc.), scope (outcome vs. execution), and constituency (individual vs. collective). This structural approach reveals multiple legitimate alignment configurations, providing a foundation for practical and philosophical integration across domains, and clarifying what it might mean for an agent to be aligned all-things-considered.\r\nCurrent browse context:\r\ncs.CY\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":933,"localImagePaths":934,"remoteImagePaths":935,"frontmatter":936,"imagePaths":938},[],[],[],{"title":922,"description":923,"pubDate":33,"source":17,"tags":937,"url":926},[19,20,21],[],"2025-06-10-disentangling-ai-alignment-a-structured-taxonomy-beyond-safety-and-ethics-d6dcaf.md","2025-06-10-dual-modal-attention-enhanced-text-video-retrieval-with-triplet-partial-margin-c-d9f8f8",{"id":940,"data":942,"body":948,"filePath":949,"digest":950,"rendered":951,"legacyId":960},{"title":943,"description":944,"pubDate":945,"source":17,"tags":946,"url":947},"Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning","arXiv:2309.11082v3 Announce Type: cross \nAbstract: In recent years, the explosion of web videos makes text-video retrieval increasingly essential and popular for video filtering, recommendation, and search. Text-video retrieval aims to rank relevant text/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity between texts and videos. Recently, contrastive learning methods have shown promising results for text-video retrieval, most of which focus on the construction of positive and negative pairs to learn text and video representations. Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity. To address these two issues, this paper improves contrastive learning using two novel techniques. First, to exploit hard examples for robust discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs from textual and visual clues. By further introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively identify all these hard negatives and explicitly highlight their impacts in the training loss. Second, our work argues that triplet samples can better model fine-grained semantic similarity compared to pairwise samples. We thereby present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to construct partial order triplet samples by automatically generating fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL designs an adaptive token masking strategy with cross-modal interaction to model subtle semantic differences. Extensive experiments demonstrate that the proposed approach outperforms existing methods on four widely-used text-video retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2309.11082","Computer Science > Computer Vision and Pattern Recognition\r\n[Submitted on 20 Sep 2023 (v1), last revised 26 Jan 2024 (this version, v3)]\r\nTitle:Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning\r\nView PDF HTML (experimental)Abstract:In recent years, the explosion of web videos makes text-video retrieval increasingly essential and popular for video filtering, recommendation, and search. Text-video retrieval aims to rank relevant text/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity between texts and videos. Recently, contrastive learning methods have shown promising results for text-video retrieval, most of which focus on the construction of positive and negative pairs to learn text and video representations. Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity. To address these two issues, this paper improves contrastive learning using two novel techniques. First, to exploit hard examples for robust discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs from textual and visual clues. By further introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively identify all these hard negatives and explicitly highlight their impacts in the training loss. Second, our work argues that triplet samples can better model fine-grained semantic similarity compared to pairwise samples. We thereby present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to construct partial order triplet samples by automatically generating fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL designs an adaptive token masking strategy with cross-modal interaction to model subtle semantic differences. Extensive experiments demonstrate that the proposed approach outperforms existing methods on four widely-used text-video retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.\r\nSubmission history\r\nFrom: Chen Jiang [view email][v1] Wed, 20 Sep 2023 06:08:11 UTC (10,810 KB)\r\n[v2] Mon, 18 Dec 2023 06:47:29 UTC (10,701 KB)\r\n[v3] Fri, 26 Jan 2024 06:18:59 UTC (10,714 KB)\r\nCurrent browse context:\r\ncs.CV\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-dual-modal-attention-enhanced-text-video-retrieval-with-triplet-partial-margin-c-d9f8f8.md","4d1cab6841545857",{"html":952,"metadata":953},"\u003Cp>Computer Science > Computer Vision and Pattern Recognition\r\n[Submitted on 20 Sep 2023 (v1), last revised 26 Jan 2024 (this version, v3)]\r\nTitle:Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning\r\nView PDF HTML (experimental)Abstract:In recent years, the explosion of web videos makes text-video retrieval increasingly essential and popular for video filtering, recommendation, and search. Text-video retrieval aims to rank relevant text/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity between texts and videos. Recently, contrastive learning methods have shown promising results for text-video retrieval, most of which focus on the construction of positive and negative pairs to learn text and video representations. Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity. To address these two issues, this paper improves contrastive learning using two novel techniques. First, to exploit hard examples for robust discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs from textual and visual clues. By further introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively identify all these hard negatives and explicitly highlight their impacts in the training loss. Second, our work argues that triplet samples can better model fine-grained semantic similarity compared to pairwise samples. We thereby present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to construct partial order triplet samples by automatically generating fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL designs an adaptive token masking strategy with cross-modal interaction to model subtle semantic differences. Extensive experiments demonstrate that the proposed approach outperforms existing methods on four widely-used text-video retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.\r\nSubmission history\r\nFrom: Chen Jiang [view email][v1] Wed, 20 Sep 2023 06:08:11 UTC (10,810 KB)\r\n[v2] Mon, 18 Dec 2023 06:47:29 UTC (10,701 KB)\r\n[v3] Fri, 26 Jan 2024 06:18:59 UTC (10,714 KB)\r\nCurrent browse context:\r\ncs.CV\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":954,"localImagePaths":955,"remoteImagePaths":956,"frontmatter":957,"imagePaths":959},[],[],[],{"title":943,"description":944,"pubDate":33,"source":17,"tags":958,"url":947},[19,20,21],[],"2025-06-10-dual-modal-attention-enhanced-text-video-retrieval-with-triplet-partial-margin-c-d9f8f8.md","2025-06-10-dllm-cache-accelerating-diffusion-large-language-models-with-adaptive-caching-576c05",{"id":961,"data":963,"body":969,"filePath":970,"digest":971,"rendered":972,"legacyId":981},{"title":964,"description":965,"pubDate":966,"source":17,"tags":967,"url":968},"dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching","arXiv:2506.06295v1 Announce Type: cross \nAbstract: Autoregressive Models (ARMs) have long dominated the landscape of Large Language Models. Recently, a new paradigm has emerged in the form of diffusion-based Large Language Models (dLLMs), which generate text by iteratively denoising masked segments. This approach has shown significant advantages and potential. However, dLLMs suffer from high inference latency. Traditional ARM acceleration techniques, such as Key-Value caching, are incompatible with dLLMs due to their bidirectional attention mechanism. To address this specific challenge, our work begins with a key observation that dLLM inference involves a static prompt and a partially dynamic response, where most tokens remain stable across adjacent denoising steps. Based on this, we propose dLLM-Cache, a training-free adaptive caching framework that combines long-interval prompt caching with partial response updates guided by feature similarity. This design enables efficient reuse of intermediate computations without compromising model performance. Extensive experiments on representative dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1 x speedup over standard inference without compromising output quality. Notably, our method brings dLLM inference latency close to that of ARMs under many settings. Codes are provided in the supplementary material and will be released publicly on GitHub.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06295","Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching\r\nView PDF HTML (experimental)Abstract:Autoregressive Models (ARMs) have long dominated the landscape of Large Language Models. Recently, a new paradigm has emerged in the form of diffusion-based Large Language Models (dLLMs), which generate text by iteratively denoising masked segments. This approach has shown significant advantages and potential. However, dLLMs suffer from high inference latency. Traditional ARM acceleration techniques, such as Key-Value caching, are incompatible with dLLMs due to their bidirectional attention mechanism. To address this specific challenge, our work begins with a key observation that dLLM inference involves a static prompt and a partially dynamic response, where most tokens remain stable across adjacent denoising steps. Based on this, we propose dLLM-Cache, a training-free adaptive caching framework that combines long-interval prompt caching with partial response updates guided by feature similarity. This design enables efficient reuse of intermediate computations without compromising model performance. Extensive experiments on representative dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1 x speedup over standard inference without compromising output quality. Notably, our method brings dLLM inference latency close to that of ARMs under many settings. Codes are provided in the supplementary material and will be released publicly on GitHub.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-dllm-cache-accelerating-diffusion-large-language-models-with-adaptive-caching-576c05.md","9ba2c9d6aceb6c1a",{"html":973,"metadata":974},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching\r\nView PDF HTML (experimental)Abstract:Autoregressive Models (ARMs) have long dominated the landscape of Large Language Models. Recently, a new paradigm has emerged in the form of diffusion-based Large Language Models (dLLMs), which generate text by iteratively denoising masked segments. This approach has shown significant advantages and potential. However, dLLMs suffer from high inference latency. Traditional ARM acceleration techniques, such as Key-Value caching, are incompatible with dLLMs due to their bidirectional attention mechanism. To address this specific challenge, our work begins with a key observation that dLLM inference involves a static prompt and a partially dynamic response, where most tokens remain stable across adjacent denoising steps. Based on this, we propose dLLM-Cache, a training-free adaptive caching framework that combines long-interval prompt caching with partial response updates guided by feature similarity. This design enables efficient reuse of intermediate computations without compromising model performance. Extensive experiments on representative dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1 x speedup over standard inference without compromising output quality. Notably, our method brings dLLM inference latency close to that of ARMs under many settings. Codes are provided in the supplementary material and will be released publicly on GitHub.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":975,"localImagePaths":976,"remoteImagePaths":977,"frontmatter":978,"imagePaths":980},[],[],[],{"title":964,"description":965,"pubDate":33,"source":17,"tags":979,"url":968},[19,20,21],[],"2025-06-10-dllm-cache-accelerating-diffusion-large-language-models-with-adaptive-caching-576c05.md","2025-06-10-dynamic-graph-cnn-with-jacobi-kolmogorov-arnold-networks-for-3d-classification-o-d0d2f0",{"id":982,"data":984,"body":990,"filePath":991,"digest":992,"rendered":993,"legacyId":1002},{"title":985,"description":986,"pubDate":987,"source":17,"tags":988,"url":989},"Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets","arXiv:2506.06296v1 Announce Type: cross \nAbstract: We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks (KAN) for the classification of three-dimensional point clouds. This method replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate polynomial expansions within a streamlined DGCNN architecture, circumventing deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi polynomials outperform the traditional linear layer-based DGCNN baseline in terms of accuracy and convergence speed, while maintaining parameter efficiency. Our results demonstrate that higher polynomial degrees do not automatically improve performance, highlighting the need for further theoretical and empirical investigation to fully understand the interactions between polynomial bases, degrees, and the mechanisms of graph-based learning.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06296","Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets\r\nView PDF HTML (experimental)Abstract:We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks (KAN) for the classification of three-dimensional point clouds. This method replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate polynomial expansions within a streamlined DGCNN architecture, circumventing deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi polynomials outperform the traditional linear layer-based DGCNN baseline in terms of accuracy and convergence speed, while maintaining parameter efficiency. Our results demonstrate that higher polynomial degrees do not automatically improve performance, highlighting the need for further theoretical and empirical investigation to fully understand the interactions between polynomial bases, degrees, and the mechanisms of graph-based learning.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-dynamic-graph-cnn-with-jacobi-kolmogorov-arnold-networks-for-3d-classification-o-d0d2f0.md","0b071119f2d630a5",{"html":994,"metadata":995},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets\r\nView PDF HTML (experimental)Abstract:We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks (KAN) for the classification of three-dimensional point clouds. This method replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate polynomial expansions within a streamlined DGCNN architecture, circumventing deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi polynomials outperform the traditional linear layer-based DGCNN baseline in terms of accuracy and convergence speed, while maintaining parameter efficiency. Our results demonstrate that higher polynomial degrees do not automatically improve performance, highlighting the need for further theoretical and empirical investigation to fully understand the interactions between polynomial bases, degrees, and the mechanisms of graph-based learning.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":996,"localImagePaths":997,"remoteImagePaths":998,"frontmatter":999,"imagePaths":1001},[],[],[],{"title":985,"description":986,"pubDate":33,"source":17,"tags":1000,"url":989},[19,20,21],[],"2025-06-10-dynamic-graph-cnn-with-jacobi-kolmogorov-arnold-networks-for-3d-classification-o-d0d2f0.md","2025-06-10-edge-enabled-collaborative-object-detection-for-real-time-multi-vehicle-percepti-051eed",{"id":1003,"data":1005,"body":1011,"filePath":1012,"digest":1013,"rendered":1014,"legacyId":1023},{"title":1006,"description":1007,"pubDate":1008,"source":17,"tags":1009,"url":1010},"Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception","arXiv:2506.06474v1 Announce Type: cross \nAbstract: Accurate and reliable object detection is critical for ensuring the safety and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board perception systems have limited accuracy due to occlusions and blind spots, while cloud-based solutions introduce significant latency, making them unsuitable for real-time processing demands required for autonomous driving in dynamic environments. To address these challenges, we introduce an innovative framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that leverages edge computing and multi-CAV collaboration for real-time, multi-perspective object detection. Our ECOD framework integrates two key algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data from multiple CAVs on an edge server to enhance perception in scenarios where individual CAVs have limited visibility. VOTE utilizes a consensus-based voting mechanism to improve the accuracy of object classification by integrating data from multiple CAVs. Both algorithms are designed at the edge to operate in real-time, ensuring low-latency and reliable decision-making for CAVs. We develop a hardware-based controlled testbed consisting of camera-equipped robotic CAVs and an edge server to evaluate the efficacy of our framework. Our experimental results demonstrate the significant benefits of ECOD in terms of improved object classification accuracy, outperforming traditional single-perspective onboard approaches by up to 75%, while ensuring low-latency, edge-driven real-time processing. This research highlights the potential of edge computing to enhance collaborative perception for latency-sensitive autonomous systems.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06474","Computer Science > Robotics\r\n[Submitted on 6 Jun 2025]\r\nTitle:Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception\r\nView PDF HTML (experimental)Abstract:Accurate and reliable object detection is critical for ensuring the safety and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board perception systems have limited accuracy due to occlusions and blind spots, while cloud-based solutions introduce significant latency, making them unsuitable for real-time processing demands required for autonomous driving in dynamic environments. To address these challenges, we introduce an innovative framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that leverages edge computing and multi-CAV collaboration for real-time, multi-perspective object detection. Our ECOD framework integrates two key algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data from multiple CAVs on an edge server to enhance perception in scenarios where individual CAVs have limited visibility. VOTE utilizes a consensus-based voting mechanism to improve the accuracy of object classification by integrating data from multiple CAVs. Both algorithms are designed at the edge to operate in real-time, ensuring low-latency and reliable decision-making for CAVs. We develop a hardware-based controlled testbed consisting of camera-equipped robotic CAVs and an edge server to evaluate the efficacy of our framework. Our experimental results demonstrate the significant benefits of ECOD in terms of improved object classification accuracy, outperforming traditional single-perspective onboard approaches by up to 75%, while ensuring low-latency, edge-driven real-time processing. This research highlights the potential of edge computing to enhance collaborative perception for latency-sensitive autonomous systems.\r\nCurrent browse context:\r\ncs.RO\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-edge-enabled-collaborative-object-detection-for-real-time-multi-vehicle-percepti-051eed.md","e9735e062f73085f",{"html":1015,"metadata":1016},"\u003Cp>Computer Science > Robotics\r\n[Submitted on 6 Jun 2025]\r\nTitle:Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception\r\nView PDF HTML (experimental)Abstract:Accurate and reliable object detection is critical for ensuring the safety and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board perception systems have limited accuracy due to occlusions and blind spots, while cloud-based solutions introduce significant latency, making them unsuitable for real-time processing demands required for autonomous driving in dynamic environments. To address these challenges, we introduce an innovative framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that leverages edge computing and multi-CAV collaboration for real-time, multi-perspective object detection. Our ECOD framework integrates two key algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data from multiple CAVs on an edge server to enhance perception in scenarios where individual CAVs have limited visibility. VOTE utilizes a consensus-based voting mechanism to improve the accuracy of object classification by integrating data from multiple CAVs. Both algorithms are designed at the edge to operate in real-time, ensuring low-latency and reliable decision-making for CAVs. We develop a hardware-based controlled testbed consisting of camera-equipped robotic CAVs and an edge server to evaluate the efficacy of our framework. Our experimental results demonstrate the significant benefits of ECOD in terms of improved object classification accuracy, outperforming traditional single-perspective onboard approaches by up to 75%, while ensuring low-latency, edge-driven real-time processing. This research highlights the potential of edge computing to enhance collaborative perception for latency-sensitive autonomous systems.\r\nCurrent browse context:\r\ncs.RO\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1017,"localImagePaths":1018,"remoteImagePaths":1019,"frontmatter":1020,"imagePaths":1022},[],[],[],{"title":1006,"description":1007,"pubDate":33,"source":17,"tags":1021,"url":1010},[19,20,21],[],"2025-06-10-edge-enabled-collaborative-object-detection-for-real-time-multi-vehicle-percepti-051eed.md","2025-06-10-efficient-generation-of-diverse-cooperative-agents-with-world-models-c7f8e3",{"id":1024,"data":1026,"body":1032,"filePath":1033,"digest":1034,"rendered":1035,"legacyId":1044},{"title":1027,"description":1028,"pubDate":1029,"source":17,"tags":1030,"url":1031},"Efficient Generation of Diverse Cooperative Agents with World Models","arXiv:2506.07450v1 Announce Type: new \nAbstract: A major bottleneck in the training process for Zero-Shot Coordination (ZSC) agents is the generation of partner agents that are diverse in collaborative conventions. Current Cross-play Minimization (XPM) methods for population generation can be very computationally expensive and sample inefficient as the training objective requires sampling multiple types of trajectories. Each partner agent in the population is also trained from scratch, despite all of the partners in the population learning policies of the same coordination task. In this work, we propose that simulated trajectories from the dynamics model of an environment can drastically speed up the training process for XPM methods. We introduce XPM-WM, a framework for generating simulated trajectories for XPM via a learned World Model (WM). We show XPM with simulated trajectories removes the need to sample multiple trajectories. In addition, we show our proposed method can effectively generate partners with diverse conventions that match the performance of previous methods in terms of SP population training reward as well as training partners for ZSC agents. Our method is thus, significantly more sample efficient and scalable to a larger number of partners.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07450","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Efficient Generation of Diverse Cooperative Agents with World Models\r\nView PDF HTML (experimental)Abstract:A major bottleneck in the training process for Zero-Shot Coordination (ZSC) agents is the generation of partner agents that are diverse in collaborative conventions. Current Cross-play Minimization (XPM) methods for population generation can be very computationally expensive and sample inefficient as the training objective requires sampling multiple types of trajectories. Each partner agent in the population is also trained from scratch, despite all of the partners in the population learning policies of the same coordination task. In this work, we propose that simulated trajectories from the dynamics model of an environment can drastically speed up the training process for XPM methods. We introduce XPM-WM, a framework for generating simulated trajectories for XPM via a learned World Model (WM). We show XPM with simulated trajectories removes the need to sample multiple trajectories. In addition, we show our proposed method can effectively generate partners with diverse conventions that match the performance of previous methods in terms of SP population training reward as well as training partners for ZSC agents. Our method is thus, significantly more sample efficient and scalable to a larger number of partners.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-efficient-generation-of-diverse-cooperative-agents-with-world-models-c7f8e3.md","b223fc6e540d8366",{"html":1036,"metadata":1037},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Efficient Generation of Diverse Cooperative Agents with World Models\r\nView PDF HTML (experimental)Abstract:A major bottleneck in the training process for Zero-Shot Coordination (ZSC) agents is the generation of partner agents that are diverse in collaborative conventions. Current Cross-play Minimization (XPM) methods for population generation can be very computationally expensive and sample inefficient as the training objective requires sampling multiple types of trajectories. Each partner agent in the population is also trained from scratch, despite all of the partners in the population learning policies of the same coordination task. In this work, we propose that simulated trajectories from the dynamics model of an environment can drastically speed up the training process for XPM methods. We introduce XPM-WM, a framework for generating simulated trajectories for XPM via a learned World Model (WM). We show XPM with simulated trajectories removes the need to sample multiple trajectories. In addition, we show our proposed method can effectively generate partners with diverse conventions that match the performance of previous methods in terms of SP population training reward as well as training partners for ZSC agents. Our method is thus, significantly more sample efficient and scalable to a larger number of partners.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1038,"localImagePaths":1039,"remoteImagePaths":1040,"frontmatter":1041,"imagePaths":1043},[],[],[],{"title":1027,"description":1028,"pubDate":33,"source":17,"tags":1042,"url":1031},[19,20,21],[],"2025-06-10-efficient-generation-of-diverse-cooperative-agents-with-world-models-c7f8e3.md","2025-06-10-enhancing-decision-making-of-large-language-models-via-actor-critic-8a0fed",{"id":1045,"data":1047,"body":1053,"filePath":1054,"digest":1055,"rendered":1056,"legacyId":1065},{"title":1048,"description":1049,"pubDate":1050,"source":17,"tags":1051,"url":1052},"Enhancing Decision-Making of Large Language Models via Actor-Critic","arXiv:2506.06376v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have achieved remarkable advancements in natural language processing tasks, yet they encounter challenges in complex decision-making scenarios that require long-term reasoning and alignment with high-level objectives. Existing methods either rely on short-term auto-regressive action generation or face limitations in accurately simulating rollouts and assessing outcomes, leading to sub-optimal decisions. This paper introduces a novel LLM-based Actor-Critic framework, termed LAC, that effectively improves LLM policies with long-term action evaluations in a principled and scalable way. Our approach addresses two key challenges: (1) extracting robust action evaluations by computing Q-values via token logits associated with positive/negative outcomes, enhanced by future trajectory rollouts and reasoning; and (2) enabling efficient policy improvement through a gradient-free mechanism. Experiments across diverse environments -- including high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text), and large action spaces (WebShop) -- demonstrate the framework's generality and superiority over state-of-the-art methods. Notably, our approach achieves competitive performance using 7B/8B parameter LLMs, even outperforming baseline methods employing GPT-4 in complex tasks. These results underscore the potential of integrating structured policy optimization with LLMs' intrinsic knowledge to advance decision-making capabilities in multi-step environments.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06376","Computer Science > Computation and Language\r\n[Submitted on 4 Jun 2025]\r\nTitle:Enhancing Decision-Making of Large Language Models via Actor-Critic\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) have achieved remarkable advancements in natural language processing tasks, yet they encounter challenges in complex decision-making scenarios that require long-term reasoning and alignment with high-level objectives. Existing methods either rely on short-term auto-regressive action generation or face limitations in accurately simulating rollouts and assessing outcomes, leading to sub-optimal decisions. This paper introduces a novel LLM-based Actor-Critic framework, termed LAC, that effectively improves LLM policies with long-term action evaluations in a principled and scalable way. Our approach addresses two key challenges: (1) extracting robust action evaluations by computing Q-values via token logits associated with positive/negative outcomes, enhanced by future trajectory rollouts and reasoning; and (2) enabling efficient policy improvement through a gradient-free mechanism. Experiments across diverse environments -- including high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text), and large action spaces (WebShop) -- demonstrate the framework's generality and superiority over state-of-the-art methods. Notably, our approach achieves competitive performance using 7B/8B parameter LLMs, even outperforming baseline methods employing GPT-4 in complex tasks. These results underscore the potential of integrating structured policy optimization with LLMs' intrinsic knowledge to advance decision-making capabilities in multi-step environments.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-enhancing-decision-making-of-large-language-models-via-actor-critic-8a0fed.md","909d56dae32a9f7e",{"html":1057,"metadata":1058},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 4 Jun 2025]\r\nTitle:Enhancing Decision-Making of Large Language Models via Actor-Critic\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) have achieved remarkable advancements in natural language processing tasks, yet they encounter challenges in complex decision-making scenarios that require long-term reasoning and alignment with high-level objectives. Existing methods either rely on short-term auto-regressive action generation or face limitations in accurately simulating rollouts and assessing outcomes, leading to sub-optimal decisions. This paper introduces a novel LLM-based Actor-Critic framework, termed LAC, that effectively improves LLM policies with long-term action evaluations in a principled and scalable way. Our approach addresses two key challenges: (1) extracting robust action evaluations by computing Q-values via token logits associated with positive/negative outcomes, enhanced by future trajectory rollouts and reasoning; and (2) enabling efficient policy improvement through a gradient-free mechanism. Experiments across diverse environments — including high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text), and large action spaces (WebShop) — demonstrate the framework’s generality and superiority over state-of-the-art methods. Notably, our approach achieves competitive performance using 7B/8B parameter LLMs, even outperforming baseline methods employing GPT-4 in complex tasks. These results underscore the potential of integrating structured policy optimization with LLMs’ intrinsic knowledge to advance decision-making capabilities in multi-step environments.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1059,"localImagePaths":1060,"remoteImagePaths":1061,"frontmatter":1062,"imagePaths":1064},[],[],[],{"title":1048,"description":1049,"pubDate":33,"source":17,"tags":1063,"url":1052},[19,20,21],[],"2025-06-10-enhancing-decision-making-of-large-language-models-via-actor-critic-8a0fed.md","2025-06-10-evaluating-large-language-models-on-the-frame-and-symbol-grounding-problems-a-ze-33f8e0",{"id":1066,"data":1068,"body":1074,"filePath":1075,"digest":1076,"rendered":1077,"legacyId":1086},{"title":1069,"description":1070,"pubDate":1071,"source":17,"tags":1072,"url":1073},"Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark","arXiv:2506.07896v1 Announce Type: new \nAbstract: Recent advancements in large language models (LLMs) have revitalized philosophical debates surrounding artificial intelligence. Two of the most fundamental challenges - namely, the Frame Problem and the Symbol Grounding Problem - have historically been viewed as unsolvable within traditional symbolic AI systems. This study investigates whether modern LLMs possess the cognitive capacities required to address these problems. To do so, I designed two benchmark tasks reflecting the philosophical core of each problem, administered them under zero-shot conditions to 13 prominent LLMs (both closed and open-source), and assessed the quality of the models' outputs across five trials each. Responses were scored along multiple criteria, including contextual reasoning, semantic coherence, and information filtering. The results demonstrate that while open-source models showed variability in performance due to differences in model size, quantization, and instruction tuning, several closed models consistently achieved high scores. These findings suggest that select modern LLMs may be acquiring capacities sufficient to produce meaningful and stable responses to these long-standing theoretical challenges.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07896","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark\r\nView PDF HTML (experimental)Abstract:Recent advancements in large language models (LLMs) have revitalized philosophical debates surrounding artificial intelligence. Two of the most fundamental challenges - namely, the Frame Problem and the Symbol Grounding Problem - have historically been viewed as unsolvable within traditional symbolic AI systems. This study investigates whether modern LLMs possess the cognitive capacities required to address these problems. To do so, I designed two benchmark tasks reflecting the philosophical core of each problem, administered them under zero-shot conditions to 13 prominent LLMs (both closed and open-source), and assessed the quality of the models' outputs across five trials each. Responses were scored along multiple criteria, including contextual reasoning, semantic coherence, and information filtering. The results demonstrate that while open-source models showed variability in performance due to differences in model size, quantization, and instruction tuning, several closed models consistently achieved high scores. These findings suggest that select modern LLMs may be acquiring capacities sufficient to produce meaningful and stable responses to these long-standing theoretical challenges.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-evaluating-large-language-models-on-the-frame-and-symbol-grounding-problems-a-ze-33f8e0.md","5b833569b9927102",{"html":1078,"metadata":1079},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark\r\nView PDF HTML (experimental)Abstract:Recent advancements in large language models (LLMs) have revitalized philosophical debates surrounding artificial intelligence. Two of the most fundamental challenges - namely, the Frame Problem and the Symbol Grounding Problem - have historically been viewed as unsolvable within traditional symbolic AI systems. This study investigates whether modern LLMs possess the cognitive capacities required to address these problems. To do so, I designed two benchmark tasks reflecting the philosophical core of each problem, administered them under zero-shot conditions to 13 prominent LLMs (both closed and open-source), and assessed the quality of the models’ outputs across five trials each. Responses were scored along multiple criteria, including contextual reasoning, semantic coherence, and information filtering. The results demonstrate that while open-source models showed variability in performance due to differences in model size, quantization, and instruction tuning, several closed models consistently achieved high scores. These findings suggest that select modern LLMs may be acquiring capacities sufficient to produce meaningful and stable responses to these long-standing theoretical challenges.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1080,"localImagePaths":1081,"remoteImagePaths":1082,"frontmatter":1083,"imagePaths":1085},[],[],[],{"title":1069,"description":1070,"pubDate":33,"source":17,"tags":1084,"url":1073},[19,20,21],[],"2025-06-10-evaluating-large-language-models-on-the-frame-and-symbol-grounding-problems-a-ze-33f8e0.md","2025-06-10-evaluating-visual-mathematics-in-multimodal-llms-a-multilingual-benchmark-based--418de5",{"id":1087,"data":1089,"body":1095,"filePath":1096,"digest":1097,"rendered":1098,"legacyId":1107},{"title":1090,"description":1091,"pubDate":1092,"source":17,"tags":1093,"url":1094},"Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests","arXiv:2506.07418v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) promise advanced vision language capabilities, yet their effectiveness in visually presented mathematics remains underexplored. This paper analyzes the development and evaluation of MLLMs for mathematical problem solving, focusing on diagrams, multilingual text, and symbolic notation. We then assess several models, including GPT 4o, Pixtral, Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our experiments reveal four key findings. First, overall precision remains moderate across geometry, visual algebra, logic, patterns, and combinatorics: no single model excels in every topic. Second, while most models see improved accuracy with questions that do not have images, the gain is often limited; performance for some remains nearly unchanged without visual input, indicating underutilization of diagrammatic information. Third, substantial variation exists across languages and difficulty levels: models frequently handle easier items but struggle with advanced geometry and combinatorial reasoning. Notably, Gemini 2.0 Flash achieves the highest precision on image based tasks, followed by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance. Fourth, a complementary analysis aimed at distinguishing whether models reason or simply recite reveals that Gemini and GPT 4o stand out for their structured reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less consistent reasoning, often defaulting to heuristics or randomness when unable to align their outputs with the given answer options.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07418","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests\r\nView PDF HTML (experimental)Abstract:Multimodal Large Language Models (MLLMs) promise advanced vision language capabilities, yet their effectiveness in visually presented mathematics remains underexplored. This paper analyzes the development and evaluation of MLLMs for mathematical problem solving, focusing on diagrams, multilingual text, and symbolic notation. We then assess several models, including GPT 4o, Pixtral, Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our experiments reveal four key findings. First, overall precision remains moderate across geometry, visual algebra, logic, patterns, and combinatorics: no single model excels in every topic. Second, while most models see improved accuracy with questions that do not have images, the gain is often limited; performance for some remains nearly unchanged without visual input, indicating underutilization of diagrammatic information. Third, substantial variation exists across languages and difficulty levels: models frequently handle easier items but struggle with advanced geometry and combinatorial reasoning. Notably, Gemini 2.0 Flash achieves the highest precision on image based tasks, followed by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance. Fourth, a complementary analysis aimed at distinguishing whether models reason or simply recite reveals that Gemini and GPT 4o stand out for their structured reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less consistent reasoning, often defaulting to heuristics or randomness when unable to align their outputs with the given answer options.\r\nSubmission history\r\nFrom: J. Alberto Conejero PhD [view email][v1] Mon, 9 Jun 2025 04:35:02 UTC (615 KB)\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-evaluating-visual-mathematics-in-multimodal-llms-a-multilingual-benchmark-based--418de5.md","c9a6fe32a4959849",{"html":1099,"metadata":1100},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests\r\nView PDF HTML (experimental)Abstract:Multimodal Large Language Models (MLLMs) promise advanced vision language capabilities, yet their effectiveness in visually presented mathematics remains underexplored. This paper analyzes the development and evaluation of MLLMs for mathematical problem solving, focusing on diagrams, multilingual text, and symbolic notation. We then assess several models, including GPT 4o, Pixtral, Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our experiments reveal four key findings. First, overall precision remains moderate across geometry, visual algebra, logic, patterns, and combinatorics: no single model excels in every topic. Second, while most models see improved accuracy with questions that do not have images, the gain is often limited; performance for some remains nearly unchanged without visual input, indicating underutilization of diagrammatic information. Third, substantial variation exists across languages and difficulty levels: models frequently handle easier items but struggle with advanced geometry and combinatorial reasoning. Notably, Gemini 2.0 Flash achieves the highest precision on image based tasks, followed by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance. Fourth, a complementary analysis aimed at distinguishing whether models reason or simply recite reveals that Gemini and GPT 4o stand out for their structured reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less consistent reasoning, often defaulting to heuristics or randomness when unable to align their outputs with the given answer options.\r\nSubmission history\r\nFrom: J. Alberto Conejero PhD [view email][v1] Mon, 9 Jun 2025 04:35:02 UTC (615 KB)\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1101,"localImagePaths":1102,"remoteImagePaths":1103,"frontmatter":1104,"imagePaths":1106},[],[],[],{"title":1090,"description":1091,"pubDate":33,"source":17,"tags":1105,"url":1094},[19,20,21],[],"2025-06-10-evaluating-visual-mathematics-in-multimodal-llms-a-multilingual-benchmark-based--418de5.md","2025-06-10-explainable-ai-powered-stock-price-prediction-using-time-series-transformers-a-c-4bc847",{"id":1108,"data":1110,"body":1116,"filePath":1117,"digest":1118,"rendered":1119,"legacyId":1128},{"title":1111,"description":1112,"pubDate":1113,"source":17,"tags":1114,"url":1115},"Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100","arXiv:2506.06345v1 Announce Type: cross \nAbstract: Financial literacy is increasingly dependent on the ability to interpret complex financial data and utilize advanced forecasting tools. In this context, this study proposes a novel approach that combines transformer-based time series models with explainable artificial intelligence (XAI) to enhance the interpretability and accuracy of stock price predictions. The analysis focuses on the daily stock prices of the five highest-volume banks listed in the BIST100 index, along with XBANK and XU100 indices, covering the period from January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla Transformer, and Time Series Transformer are employed, with input features enriched by technical indicators. SHAP and LIME techniques are used to provide transparency into the influence of individual features on model outputs. The results demonstrate the strong predictive capabilities of transformer models and highlight the potential of interpretable machine learning to empower individuals in making informed investment decisions and actively engaging in financial markets.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06345","Quantitative Finance > Statistical Finance\r\n[Submitted on 1 Jun 2025]\r\nTitle:Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100\r\nView PDFAbstract:Financial literacy is increasingly dependent on the ability to interpret complex financial data and utilize advanced forecasting tools. In this context, this study proposes a novel approach that combines transformer-based time series models with explainable artificial intelligence (XAI) to enhance the interpretability and accuracy of stock price predictions. The analysis focuses on the daily stock prices of the five highest-volume banks listed in the BIST100 index, along with XBANK and XU100 indices, covering the period from January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla Transformer, and Time Series Transformer are employed, with input features enriched by technical indicators. SHAP and LIME techniques are used to provide transparency into the influence of individual features on model outputs. The results demonstrate the strong predictive capabilities of transformer models and highlight the potential of interpretable machine learning to empower individuals in making informed investment decisions and actively engaging in financial markets.\r\nSubmission history\r\nFrom: Zeynep Hilal Kilimci [view email][v1] Sun, 1 Jun 2025 13:29:25 UTC (1,600 KB)\r\nCurrent browse context:\r\nq-fin.ST\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-explainable-ai-powered-stock-price-prediction-using-time-series-transformers-a-c-4bc847.md","c55ce99ef598f56d",{"html":1120,"metadata":1121},"\u003Cp>Quantitative Finance > Statistical Finance\r\n[Submitted on 1 Jun 2025]\r\nTitle:Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100\r\nView PDFAbstract:Financial literacy is increasingly dependent on the ability to interpret complex financial data and utilize advanced forecasting tools. In this context, this study proposes a novel approach that combines transformer-based time series models with explainable artificial intelligence (XAI) to enhance the interpretability and accuracy of stock price predictions. The analysis focuses on the daily stock prices of the five highest-volume banks listed in the BIST100 index, along with XBANK and XU100 indices, covering the period from January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla Transformer, and Time Series Transformer are employed, with input features enriched by technical indicators. SHAP and LIME techniques are used to provide transparency into the influence of individual features on model outputs. The results demonstrate the strong predictive capabilities of transformer models and highlight the potential of interpretable machine learning to empower individuals in making informed investment decisions and actively engaging in financial markets.\r\nSubmission history\r\nFrom: Zeynep Hilal Kilimci [view email][v1] Sun, 1 Jun 2025 13:29:25 UTC (1,600 KB)\r\nCurrent browse context:\r\nq-fin.ST\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1122,"localImagePaths":1123,"remoteImagePaths":1124,"frontmatter":1125,"imagePaths":1127},[],[],[],{"title":1111,"description":1112,"pubDate":33,"source":17,"tags":1126,"url":1115},[19,20,21],[],"2025-06-10-explainable-ai-powered-stock-price-prediction-using-time-series-transformers-a-c-4bc847.md","2025-06-10-exploring-effective-strategies-for-building-a-customised-gpt-agent-for-coding-cl-93c7cd",{"id":1129,"data":1131,"body":1137,"filePath":1138,"digest":1139,"rendered":1140,"legacyId":1149},{"title":1132,"description":1133,"pubDate":1134,"source":17,"tags":1135,"url":1136},"Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues","arXiv:2506.07194v1 Announce Type: new \nAbstract: This study investigates effective strategies for developing a customised GPT agent to code classroom dialogue. While classroom dialogue is widely recognised as a crucial element of education, its analysis remains challenging due to the need for a nuanced understanding of dialogic functions and the labour-intensive nature of manual transcript coding. Recent advancements in large language models offer promising avenues for automating this process. However, existing studies predominantly focus on training large-scale models or evaluating pre-trained models with fixed codebooks, which are often not applicable or replicable for dialogue researchers working with small datasets or customised coding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its baseline performance in coding classroom dialogue with a human codebook and examines how performance varies with different example inputs through a variable control method. Through a design-based research approach, it identifies a set of practical strategies, based on MyGPT's unique features, for configuring effective agents with limited data. The findings suggest that, despite some limitations, a MyGPT agent developed with these strategies can serve as a useful coding assistant by generating coding suggestions.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07194","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues\r\nView PDFAbstract:This study investigates effective strategies for developing a customised GPT agent to code classroom dialogue. While classroom dialogue is widely recognised as a crucial element of education, its analysis remains challenging due to the need for a nuanced understanding of dialogic functions and the labour-intensive nature of manual transcript coding. Recent advancements in large language models offer promising avenues for automating this process. However, existing studies predominantly focus on training large-scale models or evaluating pre-trained models with fixed codebooks, which are often not applicable or replicable for dialogue researchers working with small datasets or customised coding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its baseline performance in coding classroom dialogue with a human codebook and examines how performance varies with different example inputs through a variable control method. Through a design-based research approach, it identifies a set of practical strategies, based on MyGPT's unique features, for configuring effective agents with limited data. The findings suggest that, despite some limitations, a MyGPT agent developed with these strategies can serve as a useful coding assistant by generating coding suggestions.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-exploring-effective-strategies-for-building-a-customised-gpt-agent-for-coding-cl-93c7cd.md","d62fdd01bdd3a0c3",{"html":1141,"metadata":1142},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues\r\nView PDFAbstract:This study investigates effective strategies for developing a customised GPT agent to code classroom dialogue. While classroom dialogue is widely recognised as a crucial element of education, its analysis remains challenging due to the need for a nuanced understanding of dialogic functions and the labour-intensive nature of manual transcript coding. Recent advancements in large language models offer promising avenues for automating this process. However, existing studies predominantly focus on training large-scale models or evaluating pre-trained models with fixed codebooks, which are often not applicable or replicable for dialogue researchers working with small datasets or customised coding schemes. Using GPT-4’s MyGPT agent as a case, this study evaluates its baseline performance in coding classroom dialogue with a human codebook and examines how performance varies with different example inputs through a variable control method. Through a design-based research approach, it identifies a set of practical strategies, based on MyGPT’s unique features, for configuring effective agents with limited data. The findings suggest that, despite some limitations, a MyGPT agent developed with these strategies can serve as a useful coding assistant by generating coding suggestions.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1143,"localImagePaths":1144,"remoteImagePaths":1145,"frontmatter":1146,"imagePaths":1148},[],[],[],{"title":1132,"description":1133,"pubDate":33,"source":17,"tags":1147,"url":1136},[19,20,21],[],"2025-06-10-exploring-effective-strategies-for-building-a-customised-gpt-agent-for-coding-cl-93c7cd.md","2025-06-10-finbert2-a-specialized-bidirectional-encoder-for-bridging-the-gap-in-finance-spe-45b7d0",{"id":1150,"data":1152,"body":1158,"filePath":1159,"digest":1160,"rendered":1161,"legacyId":1170},{"title":1153,"description":1154,"pubDate":1155,"source":17,"tags":1156,"url":1157},"FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models","arXiv:2506.06335v1 Announce Type: cross \nAbstract: In natural language processing (NLP), the focus has shifted from encoder-only tiny language models like BERT to decoder-only large language models(LLMs) such as GPT-3. However, LLMs' practical application in the financial sector has revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT on discriminative tasks despite costing much higher computational resources, such as market sentiment analysis in financial reports; (2) Application on generative tasks heavily relies on retrieval augmented generation (RAG) methods to provide current and specialized information, with general retrievers showing suboptimal performance on domain-specific retrieval tasks; (3) There are additional inadequacies in other feature-based scenarios, such as topic modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens. This represents the largest known Chinese financial pretraining corpus for models of this parameter size. As a better backbone, FinBERT2 can bridge the gap in the financial-specific deployment of LLMs through the following achievements: (1) Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks. (2) Contrastive fine-tuned models (Fin-Retrievers) outperform both open-source (e.g., +6.8\\% avg improvement over BGE-base-zh) and proprietary (e.g., +4.2\\% avg improvement over OpenAI's text-embedding-3-large) embedders across five financial retrieval tasks; (3) Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables superior clustering and topic representation for financial titles. Our work revisits financial BERT models through comparative analysis with contemporary LLMs and offers practical insights for effectively utilizing FinBERT in the LLMs era.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06335","Computer Science > Information Retrieval\r\n[Submitted on 31 May 2025]\r\nTitle:FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models\r\nView PDF HTML (experimental)Abstract:In natural language processing (NLP), the focus has shifted from encoder-only tiny language models like BERT to decoder-only large language models(LLMs) such as GPT-3. However, LLMs' practical application in the financial sector has revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT on discriminative tasks despite costing much higher computational resources, such as market sentiment analysis in financial reports; (2) Application on generative tasks heavily relies on retrieval augmented generation (RAG) methods to provide current and specialized information, with general retrievers showing suboptimal performance on domain-specific retrieval tasks; (3) There are additional inadequacies in other feature-based scenarios, such as topic modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens. This represents the largest known Chinese financial pretraining corpus for models of this parameter size. As a better backbone, FinBERT2 can bridge the gap in the financial-specific deployment of LLMs through the following achievements: (1) Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks. (2) Contrastive fine-tuned models (Fin-Retrievers) outperform both open-source (e.g., +6.8\\% avg improvement over BGE-base-zh) and proprietary (e.g., +4.2\\% avg improvement over OpenAI's text-embedding-3-large) embedders across five financial retrieval tasks; (3) Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables superior clustering and topic representation for financial titles. Our work revisits financial BERT models through comparative analysis with contemporary LLMs and offers practical insights for effectively utilizing FinBERT in the LLMs era.\r\nCurrent browse context:\r\ncs.IR\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-finbert2-a-specialized-bidirectional-encoder-for-bridging-the-gap-in-finance-spe-45b7d0.md","2b2ef61b4564dc9c",{"html":1162,"metadata":1163},"\u003Cp>Computer Science > Information Retrieval\r\n[Submitted on 31 May 2025]\r\nTitle:FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models\r\nView PDF HTML (experimental)Abstract:In natural language processing (NLP), the focus has shifted from encoder-only tiny language models like BERT to decoder-only large language models(LLMs) such as GPT-3. However, LLMs’ practical application in the financial sector has revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT on discriminative tasks despite costing much higher computational resources, such as market sentiment analysis in financial reports; (2) Application on generative tasks heavily relies on retrieval augmented generation (RAG) methods to provide current and specialized information, with general retrievers showing suboptimal performance on domain-specific retrieval tasks; (3) There are additional inadequacies in other feature-based scenarios, such as topic modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens. This represents the largest known Chinese financial pretraining corpus for models of this parameter size. As a better backbone, FinBERT2 can bridge the gap in the financial-specific deployment of LLMs through the following achievements: (1) Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks. (2) Contrastive fine-tuned models (Fin-Retrievers) outperform both open-source (e.g., +6.8% avg improvement over BGE-base-zh) and proprietary (e.g., +4.2% avg improvement over OpenAI’s text-embedding-3-large) embedders across five financial retrieval tasks; (3) Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables superior clustering and topic representation for financial titles. Our work revisits financial BERT models through comparative analysis with contemporary LLMs and offers practical insights for effectively utilizing FinBERT in the LLMs era.\r\nCurrent browse context:\r\ncs.IR\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1164,"localImagePaths":1165,"remoteImagePaths":1166,"frontmatter":1167,"imagePaths":1169},[],[],[],{"title":1153,"description":1154,"pubDate":33,"source":17,"tags":1168,"url":1157},[19,20,21],[],"2025-06-10-finbert2-a-specialized-bidirectional-encoder-for-bridging-the-gap-in-finance-spe-45b7d0.md","2025-06-10-fact-in-fragments-deconstructing-complex-claims-via-llm-based-atomic-fact-extrac-b50a33",{"id":1171,"data":1173,"body":1179,"filePath":1180,"digest":1181,"rendered":1182,"legacyId":1191},{"title":1174,"description":1175,"pubDate":1176,"source":17,"tags":1177,"url":1178},"Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification","arXiv:2506.07446v1 Announce Type: new \nAbstract: Fact verification plays a vital role in combating misinformation by assessing the veracity of claims through evidence retrieval and reasoning. However, traditional methods struggle with complex claims requiring multi-hop reasoning over fragmented evidence, as they often rely on static decomposition strategies and surface-level semantic retrieval, which fail to capture the nuanced structure and intent of the claim. This results in accumulated reasoning errors, noisy evidence contamination, and limited adaptability to diverse claims, ultimately undermining verification accuracy in complex scenarios. To address this, we propose Atomic Fact Extraction and Verification (AFEV), a novel framework that iteratively decomposes complex claims into atomic facts, enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically refines claim understanding and reduces error propagation through iterative fact extraction, reranks evidence to filter noise, and leverages context-specific demonstrations to guide the reasoning process. Extensive experiments on five benchmark datasets demonstrate that AFEV achieves state-of-the-art performance in both accuracy and interpretability.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07446","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification\r\nView PDF HTML (experimental)Abstract:Fact verification plays a vital role in combating misinformation by assessing the veracity of claims through evidence retrieval and reasoning. However, traditional methods struggle with complex claims requiring multi-hop reasoning over fragmented evidence, as they often rely on static decomposition strategies and surface-level semantic retrieval, which fail to capture the nuanced structure and intent of the claim. This results in accumulated reasoning errors, noisy evidence contamination, and limited adaptability to diverse claims, ultimately undermining verification accuracy in complex scenarios. To address this, we propose Atomic Fact Extraction and Verification (AFEV), a novel framework that iteratively decomposes complex claims into atomic facts, enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically refines claim understanding and reduces error propagation through iterative fact extraction, reranks evidence to filter noise, and leverages context-specific demonstrations to guide the reasoning process. Extensive experiments on five benchmark datasets demonstrate that AFEV achieves state-of-the-art performance in both accuracy and interpretability.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-fact-in-fragments-deconstructing-complex-claims-via-llm-based-atomic-fact-extrac-b50a33.md","854ea6dc43b8ee4d",{"html":1183,"metadata":1184},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification\r\nView PDF HTML (experimental)Abstract:Fact verification plays a vital role in combating misinformation by assessing the veracity of claims through evidence retrieval and reasoning. However, traditional methods struggle with complex claims requiring multi-hop reasoning over fragmented evidence, as they often rely on static decomposition strategies and surface-level semantic retrieval, which fail to capture the nuanced structure and intent of the claim. This results in accumulated reasoning errors, noisy evidence contamination, and limited adaptability to diverse claims, ultimately undermining verification accuracy in complex scenarios. To address this, we propose Atomic Fact Extraction and Verification (AFEV), a novel framework that iteratively decomposes complex claims into atomic facts, enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically refines claim understanding and reduces error propagation through iterative fact extraction, reranks evidence to filter noise, and leverages context-specific demonstrations to guide the reasoning process. Extensive experiments on five benchmark datasets demonstrate that AFEV achieves state-of-the-art performance in both accuracy and interpretability.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1185,"localImagePaths":1186,"remoteImagePaths":1187,"frontmatter":1188,"imagePaths":1190},[],[],[],{"title":1174,"description":1175,"pubDate":33,"source":17,"tags":1189,"url":1178},[19,20,21],[],"2025-06-10-fact-in-fragments-deconstructing-complex-claims-via-llm-based-atomic-fact-extrac-b50a33.md","2025-06-10-fixing-it-in-post-a-comparative-study-of-llm-post-training-data-quality-and-mode-59f223",{"id":1192,"data":1194,"body":1200,"filePath":1201,"digest":1202,"rendered":1203,"legacyId":1212},{"title":1195,"description":1196,"pubDate":1197,"source":17,"tags":1198,"url":1199},"Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance","arXiv:2506.06522v1 Announce Type: cross \nAbstract: Recent work on large language models (LLMs) has increasingly focused on post-training and alignment with datasets curated to enhance instruction following, world knowledge, and specialized skills. However, most post-training datasets used in leading open- and closed-source LLMs remain inaccessible to the public, with limited information about their construction process. This lack of transparency has motivated the recent development of open-source post-training corpora. While training on these open alternatives can yield performance comparable to that of leading models, systematic comparisons remain challenging due to the significant computational cost of conducting them rigorously at scale, and are therefore largely absent. As a result, it remains unclear how specific samples, task types, or curation strategies influence downstream performance when assessing data quality. In this work, we conduct the first comprehensive side-by-side analysis of two prominent open post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie framework, we annotate each sample with detailed quality metrics, including turn structure (single-turn vs. multi-turn), task category, input quality, and response quality, and we derive statistics that reveal structural and qualitative similarities and differences between the two datasets. Based on these insights, we design a principled curation recipe that produces a new data mixture, TuluTalk, which contains 14% fewer samples than either source dataset while matching or exceeding their performance on key benchmarks. Our findings offer actionable insights for constructing more effective post-training datasets that improve model performance within practical resource limits. To support future research, we publicly release both the annotated source datasets and our curated TuluTalk mixture.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06522","Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance\r\nView PDFAbstract:Recent work on large language models (LLMs) has increasingly focused on post-training and alignment with datasets curated to enhance instruction following, world knowledge, and specialized skills. However, most post-training datasets used in leading open- and closed-source LLMs remain inaccessible to the public, with limited information about their construction process. This lack of transparency has motivated the recent development of open-source post-training corpora. While training on these open alternatives can yield performance comparable to that of leading models, systematic comparisons remain challenging due to the significant computational cost of conducting them rigorously at scale, and are therefore largely absent. As a result, it remains unclear how specific samples, task types, or curation strategies influence downstream performance when assessing data quality. In this work, we conduct the first comprehensive side-by-side analysis of two prominent open post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie framework, we annotate each sample with detailed quality metrics, including turn structure (single-turn vs. multi-turn), task category, input quality, and response quality, and we derive statistics that reveal structural and qualitative similarities and differences between the two datasets. Based on these insights, we design a principled curation recipe that produces a new data mixture, TuluTalk, which contains 14% fewer samples than either source dataset while matching or exceeding their performance on key benchmarks. Our findings offer actionable insights for constructing more effective post-training datasets that improve model performance within practical resource limits. To support future research, we publicly release both the annotated source datasets and our curated TuluTalk mixture.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-fixing-it-in-post-a-comparative-study-of-llm-post-training-data-quality-and-mode-59f223.md","e3471f88a3e2a4e8",{"html":1204,"metadata":1205},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance\r\nView PDFAbstract:Recent work on large language models (LLMs) has increasingly focused on post-training and alignment with datasets curated to enhance instruction following, world knowledge, and specialized skills. However, most post-training datasets used in leading open- and closed-source LLMs remain inaccessible to the public, with limited information about their construction process. This lack of transparency has motivated the recent development of open-source post-training corpora. While training on these open alternatives can yield performance comparable to that of leading models, systematic comparisons remain challenging due to the significant computational cost of conducting them rigorously at scale, and are therefore largely absent. As a result, it remains unclear how specific samples, task types, or curation strategies influence downstream performance when assessing data quality. In this work, we conduct the first comprehensive side-by-side analysis of two prominent open post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie framework, we annotate each sample with detailed quality metrics, including turn structure (single-turn vs. multi-turn), task category, input quality, and response quality, and we derive statistics that reveal structural and qualitative similarities and differences between the two datasets. Based on these insights, we design a principled curation recipe that produces a new data mixture, TuluTalk, which contains 14% fewer samples than either source dataset while matching or exceeding their performance on key benchmarks. Our findings offer actionable insights for constructing more effective post-training datasets that improve model performance within practical resource limits. To support future research, we publicly release both the annotated source datasets and our curated TuluTalk mixture.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1206,"localImagePaths":1207,"remoteImagePaths":1208,"frontmatter":1209,"imagePaths":1211},[],[],[],{"title":1195,"description":1196,"pubDate":33,"source":17,"tags":1210,"url":1199},[19,20,21],[],"2025-06-10-fixing-it-in-post-a-comparative-study-of-llm-post-training-data-quality-and-mode-59f223.md","2025-06-10-facial-foundational-model-advances-early-warning-of-coronary-artery-disease-from-cf288b",{"id":1213,"data":1215,"body":1221,"filePath":1222,"digest":1223,"rendered":1224,"legacyId":1233},{"title":1216,"description":1217,"pubDate":1218,"source":17,"tags":1219,"url":1220},"Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow","arXiv:2506.06283v1 Announce Type: cross \nAbstract: Global population aging presents increasing challenges to healthcare systems, with coronary artery disease (CAD) responsible for approximately 17.8 million deaths annually, making it a leading cause of global mortality. As CAD is largely preventable, early detection and proactive management are essential. In this work, we introduce DigitalShadow, an advanced early warning system for CAD, powered by a fine-tuned facial foundation model. The system is pre-trained on 21 million facial images and subsequently fine-tuned into LiveCAD, a specialized CAD risk assessment model trained on 7,004 facial images from 1,751 subjects across four hospitals in China. DigitalShadow functions passively and contactlessly, extracting facial features from live video streams without requiring active user engagement. Integrated with a personalized database, it generates natural language risk reports and individualized health recommendations. With privacy as a core design principle, DigitalShadow supports local deployment to ensure secure handling of user data.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06283","Computer Science > Computer Vision and Pattern Recognition\r\n[Submitted on 23 Apr 2025]\r\nTitle:Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow\r\nView PDF HTML (experimental)Abstract:Global population aging presents increasing challenges to healthcare systems, with coronary artery disease (CAD) responsible for approximately 17.8 million deaths annually, making it a leading cause of global mortality. As CAD is largely preventable, early detection and proactive management are essential. In this work, we introduce DigitalShadow, an advanced early warning system for CAD, powered by a fine-tuned facial foundation model. The system is pre-trained on 21 million facial images and subsequently fine-tuned into LiveCAD, a specialized CAD risk assessment model trained on 7,004 facial images from 1,751 subjects across four hospitals in China. DigitalShadow functions passively and contactlessly, extracting facial features from live video streams without requiring active user engagement. Integrated with a personalized database, it generates natural language risk reports and individualized health recommendations. With privacy as a core design principle, DigitalShadow supports local deployment to ensure secure handling of user data.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-facial-foundational-model-advances-early-warning-of-coronary-artery-disease-from-cf288b.md","a5d9fea6916a422d",{"html":1225,"metadata":1226},"\u003Cp>Computer Science > Computer Vision and Pattern Recognition\r\n[Submitted on 23 Apr 2025]\r\nTitle:Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow\r\nView PDF HTML (experimental)Abstract:Global population aging presents increasing challenges to healthcare systems, with coronary artery disease (CAD) responsible for approximately 17.8 million deaths annually, making it a leading cause of global mortality. As CAD is largely preventable, early detection and proactive management are essential. In this work, we introduce DigitalShadow, an advanced early warning system for CAD, powered by a fine-tuned facial foundation model. The system is pre-trained on 21 million facial images and subsequently fine-tuned into LiveCAD, a specialized CAD risk assessment model trained on 7,004 facial images from 1,751 subjects across four hospitals in China. DigitalShadow functions passively and contactlessly, extracting facial features from live video streams without requiring active user engagement. Integrated with a personalized database, it generates natural language risk reports and individualized health recommendations. With privacy as a core design principle, DigitalShadow supports local deployment to ensure secure handling of user data.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1227,"localImagePaths":1228,"remoteImagePaths":1229,"frontmatter":1230,"imagePaths":1232},[],[],[],{"title":1216,"description":1217,"pubDate":33,"source":17,"tags":1231,"url":1220},[19,20,21],[],"2025-06-10-facial-foundational-model-advances-early-warning-of-coronary-artery-disease-from-cf288b.md","2025-06-10-evolutionary-model-for-energy-trading-in-community-microgrids-using-hawk-dove-st-3a29d4",{"id":1234,"data":1236,"body":1242,"filePath":1243,"digest":1244,"rendered":1245,"legacyId":1254},{"title":1237,"description":1238,"pubDate":1239,"source":17,"tags":1240,"url":1241},"Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies","arXiv:2506.06325v1 Announce Type: cross \nAbstract: This paper proposes a decentralized model of energy cooperation between microgrids, in which decisions are made locally, at the level of the microgrid community. Each microgrid is modeled as an autonomous agent that adopts a Hawk or Dove strategy, depending on the level of energy stored in the battery and its role in the energy trading process. The interactions between selling and buying microgrids are modeled through an evolutionary algorithm. An individual in the algorithm population is represented as an energy trading matrix that encodes the amounts of energy traded between the selling and buying microgrids. The population evolution is achieved by recombination and mutation operators. Recombination uses a specialized operator for matrix structures, and mutation is applied to the matrix elements according to a Gaussian distribution. The evaluation of an individual is made with a multi-criteria fitness function that considers the seller profit, the degree of energy stability at the community level, penalties for energy imbalance at the community level and for the degradation of microgrids batteries. The method was tested on a simulated scenario with 100 microgrids, each with its own selling and buying thresholds, to reflect a realistic environment with variable storage characteristics of microgrids batteries. By applying the algorithm on this scenario, 95 out of the 100 microgrids reached a stable energy state. This result confirms the effectiveness of the proposed model in achieving energy balance both at the individual level, for each microgrid, and at the level of the entire community.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06325","Computer Science > Neural and Evolutionary Computing\r\n[Submitted on 30 May 2025]\r\nTitle:Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies\r\nView PDFAbstract:This paper proposes a decentralized model of energy cooperation between microgrids, in which decisions are made locally, at the level of the microgrid community. Each microgrid is modeled as an autonomous agent that adopts a Hawk or Dove strategy, depending on the level of energy stored in the battery and its role in the energy trading process. The interactions between selling and buying microgrids are modeled through an evolutionary algorithm. An individual in the algorithm population is represented as an energy trading matrix that encodes the amounts of energy traded between the selling and buying microgrids. The population evolution is achieved by recombination and mutation operators. Recombination uses a specialized operator for matrix structures, and mutation is applied to the matrix elements according to a Gaussian distribution. The evaluation of an individual is made with a multi-criteria fitness function that considers the seller profit, the degree of energy stability at the community level, penalties for energy imbalance at the community level and for the degradation of microgrids batteries. The method was tested on a simulated scenario with 100 microgrids, each with its own selling and buying thresholds, to reflect a realistic environment with variable storage characteristics of microgrids batteries. By applying the algorithm on this scenario, 95 out of the 100 microgrids reached a stable energy state. This result confirms the effectiveness of the proposed model in achieving energy balance both at the individual level, for each microgrid, and at the level of the entire community.\r\nCurrent browse context:\r\ncs.NE\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-evolutionary-model-for-energy-trading-in-community-microgrids-using-hawk-dove-st-3a29d4.md","dbdf4b44a93990a7",{"html":1246,"metadata":1247},"\u003Cp>Computer Science > Neural and Evolutionary Computing\r\n[Submitted on 30 May 2025]\r\nTitle:Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies\r\nView PDFAbstract:This paper proposes a decentralized model of energy cooperation between microgrids, in which decisions are made locally, at the level of the microgrid community. Each microgrid is modeled as an autonomous agent that adopts a Hawk or Dove strategy, depending on the level of energy stored in the battery and its role in the energy trading process. The interactions between selling and buying microgrids are modeled through an evolutionary algorithm. An individual in the algorithm population is represented as an energy trading matrix that encodes the amounts of energy traded between the selling and buying microgrids. The population evolution is achieved by recombination and mutation operators. Recombination uses a specialized operator for matrix structures, and mutation is applied to the matrix elements according to a Gaussian distribution. The evaluation of an individual is made with a multi-criteria fitness function that considers the seller profit, the degree of energy stability at the community level, penalties for energy imbalance at the community level and for the degradation of microgrids batteries. The method was tested on a simulated scenario with 100 microgrids, each with its own selling and buying thresholds, to reflect a realistic environment with variable storage characteristics of microgrids batteries. By applying the algorithm on this scenario, 95 out of the 100 microgrids reached a stable energy state. This result confirms the effectiveness of the proposed model in achieving energy balance both at the individual level, for each microgrid, and at the level of the entire community.\r\nCurrent browse context:\r\ncs.NE\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1248,"localImagePaths":1249,"remoteImagePaths":1250,"frontmatter":1251,"imagePaths":1253},[],[],[],{"title":1237,"description":1238,"pubDate":33,"source":17,"tags":1252,"url":1241},[19,20,21],[],"2025-06-10-evolutionary-model-for-energy-trading-in-community-microgrids-using-hawk-dove-st-3a29d4.md","2025-06-10-evaluating-llm-corrupted-crowdsourcing-data-without-ground-truth-ff6c88",{"id":1255,"data":1257,"body":1263,"filePath":1264,"digest":1265,"rendered":1266,"legacyId":1275},{"title":1258,"description":1259,"pubDate":1260,"source":17,"tags":1261,"url":1262},"Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth","arXiv:2506.06991v1 Announce Type: new \nAbstract: The recent success of generative AI highlights the crucial role of high-quality human feedback in building trustworthy AI systems. However, the increasing use of large language models (LLMs) by crowdsourcing workers poses a significant challenge: datasets intended to reflect human input may be compromised by LLM-generated responses. Existing LLM detection approaches often rely on high-dimension training data such as text, making them unsuitable for annotation tasks like multiple-choice labeling. In this work, we investigate the potential of peer prediction -- a mechanism that evaluates the information within workers' responses without using ground truth -- to mitigate LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our approach quantifies the correlations between worker answers while conditioning on (a subset of) LLM-generated labels available to the requester. Building on prior research, we propose a training-free scoring mechanism with theoretical guarantees under a crowdsourcing model that accounts for LLM collusion. We establish conditions under which our method is effective and empirically demonstrate its robustness in detecting low-effort cheating on real-world crowdsourcing datasets.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06991","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth\r\nView PDF HTML (experimental)Abstract:The recent success of generative AI highlights the crucial role of high-quality human feedback in building trustworthy AI systems. However, the increasing use of large language models (LLMs) by crowdsourcing workers poses a significant challenge: datasets intended to reflect human input may be compromised by LLM-generated responses. Existing LLM detection approaches often rely on high-dimension training data such as text, making them unsuitable for annotation tasks like multiple-choice labeling. In this work, we investigate the potential of peer prediction -- a mechanism that evaluates the information within workers' responses without using ground truth -- to mitigate LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our approach quantifies the correlations between worker answers while conditioning on (a subset of) LLM-generated labels available to the requester. Building on prior research, we propose a training-free scoring mechanism with theoretical guarantees under a crowdsourcing model that accounts for LLM collusion. We establish conditions under which our method is effective and empirically demonstrate its robustness in detecting low-effort cheating on real-world crowdsourcing datasets.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-evaluating-llm-corrupted-crowdsourcing-data-without-ground-truth-ff6c88.md","4a1bdd0202e17059",{"html":1267,"metadata":1268},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth\r\nView PDF HTML (experimental)Abstract:The recent success of generative AI highlights the crucial role of high-quality human feedback in building trustworthy AI systems. However, the increasing use of large language models (LLMs) by crowdsourcing workers poses a significant challenge: datasets intended to reflect human input may be compromised by LLM-generated responses. Existing LLM detection approaches often rely on high-dimension training data such as text, making them unsuitable for annotation tasks like multiple-choice labeling. In this work, we investigate the potential of peer prediction — a mechanism that evaluates the information within workers’ responses without using ground truth — to mitigate LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our approach quantifies the correlations between worker answers while conditioning on (a subset of) LLM-generated labels available to the requester. Building on prior research, we propose a training-free scoring mechanism with theoretical guarantees under a crowdsourcing model that accounts for LLM collusion. We establish conditions under which our method is effective and empirically demonstrate its robustness in detecting low-effort cheating on real-world crowdsourcing datasets.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1269,"localImagePaths":1270,"remoteImagePaths":1271,"frontmatter":1272,"imagePaths":1274},[],[],[],{"title":1258,"description":1259,"pubDate":33,"source":17,"tags":1273,"url":1262},[19,20,21],[],"2025-06-10-evaluating-llm-corrupted-crowdsourcing-data-without-ground-truth-ff6c88.md","2025-06-10-from-model-based-and-adaptive-control-to-evolving-fuzzy-control-281307",{"id":1276,"data":1278,"body":1284,"filePath":1285,"digest":1286,"rendered":1287,"legacyId":1296},{"title":1279,"description":1280,"pubDate":1281,"source":17,"tags":1282,"url":1283},"From Model-Based and Adaptive Control to Evolving Fuzzy Control","arXiv:2506.06594v1 Announce Type: cross \nAbstract: Evolving fuzzy systems build and adapt fuzzy models - such as predictors and controllers - by incrementally updating their rule-base structure from data streams. On the occasion of the 60-year anniversary of fuzzy set theory, commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the historical development and core contributions of classical fuzzy and adaptive modeling and control frameworks. It then highlights the emergence and significance of evolving intelligent systems in fuzzy modeling and control, emphasizing their advantages in handling nonstationary environments. Key challenges and future directions are discussed, including safety, interpretability, and principled structural evolution.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06594","Electrical Engineering and Systems Science > Systems and Control\r\n[Submitted on 7 Jun 2025]\r\nTitle:From Model-Based and Adaptive Control to Evolving Fuzzy Control\r\nView PDF HTML (experimental)Abstract:Evolving fuzzy systems build and adapt fuzzy models - such as predictors and controllers - by incrementally updating their rule-base structure from data streams. On the occasion of the 60-year anniversary of fuzzy set theory, commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the historical development and core contributions of classical fuzzy and adaptive modeling and control frameworks. It then highlights the emergence and significance of evolving intelligent systems in fuzzy modeling and control, emphasizing their advantages in handling nonstationary environments. Key challenges and future directions are discussed, including safety, interpretability, and principled structural evolution.\r\nCurrent browse context:\r\neess.SY\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-from-model-based-and-adaptive-control-to-evolving-fuzzy-control-281307.md","dfa848e64c0b910d",{"html":1288,"metadata":1289},"\u003Cp>Electrical Engineering and Systems Science > Systems and Control\r\n[Submitted on 7 Jun 2025]\r\nTitle:From Model-Based and Adaptive Control to Evolving Fuzzy Control\r\nView PDF HTML (experimental)Abstract:Evolving fuzzy systems build and adapt fuzzy models - such as predictors and controllers - by incrementally updating their rule-base structure from data streams. On the occasion of the 60-year anniversary of fuzzy set theory, commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the historical development and core contributions of classical fuzzy and adaptive modeling and control frameworks. It then highlights the emergence and significance of evolving intelligent systems in fuzzy modeling and control, emphasizing their advantages in handling nonstationary environments. Key challenges and future directions are discussed, including safety, interpretability, and principled structural evolution.\r\nCurrent browse context:\r\neess.SY\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1290,"localImagePaths":1291,"remoteImagePaths":1292,"frontmatter":1293,"imagePaths":1295},[],[],[],{"title":1279,"description":1280,"pubDate":33,"source":17,"tags":1294,"url":1283},[19,20,21],[],"2025-06-10-from-model-based-and-adaptive-control-to-evolving-fuzzy-control-281307.md","2025-06-10-from-rogue-to-safe-ai-the-role-of-explicit-refusals-in-aligning-llms-with-intern-722e5e",{"id":1297,"data":1299,"body":1305,"filePath":1306,"digest":1307,"rendered":1308,"legacyId":1317},{"title":1300,"description":1301,"pubDate":1302,"source":17,"tags":1303,"url":1304},"From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law","arXiv:2506.06391v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) are widely used across sectors, yet their alignment with International Humanitarian Law (IHL) is not well understood. This study evaluates eight leading LLMs on their ability to refuse prompts that explicitly violate these legal frameworks, focusing also on helpfulness - how clearly and constructively refusals are communicated. While most models rejected unlawful requests, the clarity and consistency of their responses varied. By revealing the model's rationale and referencing relevant legal or safety principles, explanatory refusals clarify the system's boundaries, reduce ambiguity, and help prevent misuse. A standardised system-level safety prompt significantly improved the quality of the explanations expressed within refusals in most models, highlighting the effectiveness of lightweight interventions. However, more complex prompts involving technical language or requests for code revealed ongoing vulnerabilities. These findings contribute to the development of safer, more transparent AI systems and propose a benchmark to evaluate the compliance of LLM with IHL.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06391","Computer Science > Computers and Society\r\n[Submitted on 5 Jun 2025]\r\nTitle:From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) are widely used across sectors, yet their alignment with International Humanitarian Law (IHL) is not well understood. This study evaluates eight leading LLMs on their ability to refuse prompts that explicitly violate these legal frameworks, focusing also on helpfulness - how clearly and constructively refusals are communicated. While most models rejected unlawful requests, the clarity and consistency of their responses varied. By revealing the model's rationale and referencing relevant legal or safety principles, explanatory refusals clarify the system's boundaries, reduce ambiguity, and help prevent misuse. A standardised system-level safety prompt significantly improved the quality of the explanations expressed within refusals in most models, highlighting the effectiveness of lightweight interventions. However, more complex prompts involving technical language or requests for code revealed ongoing vulnerabilities. These findings contribute to the development of safer, more transparent AI systems and propose a benchmark to evaluate the compliance of LLM with IHL.\r\nCurrent browse context:\r\ncs.CY\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-from-rogue-to-safe-ai-the-role-of-explicit-refusals-in-aligning-llms-with-intern-722e5e.md","82b07469c06601c4",{"html":1309,"metadata":1310},"\u003Cp>Computer Science > Computers and Society\r\n[Submitted on 5 Jun 2025]\r\nTitle:From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) are widely used across sectors, yet their alignment with International Humanitarian Law (IHL) is not well understood. This study evaluates eight leading LLMs on their ability to refuse prompts that explicitly violate these legal frameworks, focusing also on helpfulness - how clearly and constructively refusals are communicated. While most models rejected unlawful requests, the clarity and consistency of their responses varied. By revealing the model’s rationale and referencing relevant legal or safety principles, explanatory refusals clarify the system’s boundaries, reduce ambiguity, and help prevent misuse. A standardised system-level safety prompt significantly improved the quality of the explanations expressed within refusals in most models, highlighting the effectiveness of lightweight interventions. However, more complex prompts involving technical language or requests for code revealed ongoing vulnerabilities. These findings contribute to the development of safer, more transparent AI systems and propose a benchmark to evaluate the compliance of LLM with IHL.\r\nCurrent browse context:\r\ncs.CY\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1311,"localImagePaths":1312,"remoteImagePaths":1313,"frontmatter":1314,"imagePaths":1316},[],[],[],{"title":1300,"description":1301,"pubDate":33,"source":17,"tags":1315,"url":1304},[19,20,21],[],"2025-06-10-from-rogue-to-safe-ai-the-role-of-explicit-refusals-in-aligning-llms-with-intern-722e5e.md","2025-06-10-future-of-work-with-ai-agents-auditing-automation-and-augmentation-potential-acr-bd128f",{"id":1318,"data":1320,"body":1326,"filePath":1327,"digest":1328,"rendered":1329,"legacyId":1338},{"title":1321,"description":1322,"pubDate":1323,"source":17,"tags":1324,"url":1325},"Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce","arXiv:2506.06576v1 Announce Type: cross \nAbstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, we lack a systematic understanding of the evolving landscape. In this paper, we address this gap by introducing a novel auditing framework to assess which occupational tasks workers want AI agents to automate or augment, and how those desires align with the current technological capabilities. Our framework features an audio-enhanced mini-interview to capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a shared language to quantify the preferred level of human involvement. Using this framework, we construct the WORKBank database, building on the U.S. Department of Labor's O*NET database, to capture preferences from 1,500 domain workers and capability assessments from AI experts across over 844 tasks spanning 104 occupations. Jointly considering the desire and technological capability divides tasks in WORKBank into four zones: Automation \"Green Light\" Zone, Automation \"Red Light\" Zone, R&amp;D Opportunity Zone, Low Priority Zone. This highlights critical mismatches and opportunities for AI agent development. Moving beyond a simple automate-or-not dichotomy, our results reveal diverse HAS profiles across occupations, reflecting heterogeneous expectations for human involvement. Moreover, our study offers early signals of how AI agent integration may reshape the core human competencies, shifting from information-focused skills to interpersonal ones. These findings underscore the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06576","Computer Science > Computers and Society\r\n[Submitted on 6 Jun 2025]\r\nTitle:Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce\r\nView PDF HTML (experimental)Abstract:The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, we lack a systematic understanding of the evolving landscape. In this paper, we address this gap by introducing a novel auditing framework to assess which occupational tasks workers want AI agents to automate or augment, and how those desires align with the current technological capabilities. Our framework features an audio-enhanced mini-interview to capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a shared language to quantify the preferred level of human involvement. Using this framework, we construct the WORKBank database, building on the U.S. Department of Labor's O*NET database, to capture preferences from 1,500 domain workers and capability assessments from AI experts across over 844 tasks spanning 104 occupations. Jointly considering the desire and technological capability divides tasks in WORKBank into four zones: Automation \"Green Light\" Zone, Automation \"Red Light\" Zone, R&D Opportunity Zone, Low Priority Zone. This highlights critical mismatches and opportunities for AI agent development. Moving beyond a simple automate-or-not dichotomy, our results reveal diverse HAS profiles across occupations, reflecting heterogeneous expectations for human involvement. Moreover, our study offers early signals of how AI agent integration may reshape the core human competencies, shifting from information-focused skills to interpersonal ones. These findings underscore the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics.\r\nCurrent browse context:\r\ncs.CY\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-future-of-work-with-ai-agents-auditing-automation-and-augmentation-potential-acr-bd128f.md","8395b0bb6ea083d1",{"html":1330,"metadata":1331},"\u003Cp>Computer Science > Computers and Society\r\n[Submitted on 6 Jun 2025]\r\nTitle:Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce\r\nView PDF HTML (experimental)Abstract:The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, we lack a systematic understanding of the evolving landscape. In this paper, we address this gap by introducing a novel auditing framework to assess which occupational tasks workers want AI agents to automate or augment, and how those desires align with the current technological capabilities. Our framework features an audio-enhanced mini-interview to capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a shared language to quantify the preferred level of human involvement. Using this framework, we construct the WORKBank database, building on the U.S. Department of Labor’s O*NET database, to capture preferences from 1,500 domain workers and capability assessments from AI experts across over 844 tasks spanning 104 occupations. Jointly considering the desire and technological capability divides tasks in WORKBank into four zones: Automation “Green Light” Zone, Automation “Red Light” Zone, R&#x26;D Opportunity Zone, Low Priority Zone. This highlights critical mismatches and opportunities for AI agent development. Moving beyond a simple automate-or-not dichotomy, our results reveal diverse HAS profiles across occupations, reflecting heterogeneous expectations for human involvement. Moreover, our study offers early signals of how AI agent integration may reshape the core human competencies, shifting from information-focused skills to interpersonal ones. These findings underscore the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics.\r\nCurrent browse context:\r\ncs.CY\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1332,"localImagePaths":1333,"remoteImagePaths":1334,"frontmatter":1335,"imagePaths":1337},[],[],[],{"title":1321,"description":1322,"pubDate":33,"source":17,"tags":1336,"url":1325},[19,20,21],[],"2025-06-10-future-of-work-with-ai-agents-auditing-automation-and-augmentation-potential-acr-bd128f.md","2025-06-10-geld-a-unified-neural-model-for-efficiently-solving-traveling-salesman-problems--e24948",{"id":1339,"data":1341,"body":1347,"filePath":1348,"digest":1349,"rendered":1350,"legacyId":1359},{"title":1342,"description":1343,"pubDate":1344,"source":17,"tags":1345,"url":1346},"GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales","arXiv:2506.06634v1 Announce Type: new \nAbstract: The Traveling Salesman Problem (TSP) is a well-known combinatorial optimization problem with broad real-world applications. Recent advancements in neural network-based TSP solvers have shown promising results. Nonetheless, these models often struggle to efficiently solve both small- and large-scale TSPs using the same set of pre-trained model parameters, limiting their practical utility. To address this issue, we introduce a novel neural TSP solver named GELD, built upon our proposed broad global assessment and refined local selection framework. Specifically, GELD integrates a lightweight Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich embedding representation while accelerating the decision-making process. Moreover, GE incorporates a novel low-complexity attention mechanism, allowing GELD to achieve low inference latency and scalability to larger-scale TSPs. Additionally, we propose a two-stage training strategy that utilizes training instances of different sizes to bolster GELD's generalization ability. Extensive experiments conducted on both synthetic and real-world datasets demonstrate that GELD outperforms seven state-of-the-art models considering both solution quality and inference speed. Furthermore, GELD can be employed as a post-processing method to significantly elevate the quality of the solutions derived by existing neural TSP solvers via spending affordable additional computing time. Notably, GELD is shown as capable of solving TSPs with up to 744,710 nodes, first-of-its-kind to solve this large size TSP without relying on divide-and-conquer strategies to the best of our knowledge.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06634","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales\r\nView PDF HTML (experimental)Abstract:The Traveling Salesman Problem (TSP) is a well-known combinatorial optimization problem with broad real-world applications. Recent advancements in neural network-based TSP solvers have shown promising results. Nonetheless, these models often struggle to efficiently solve both small- and large-scale TSPs using the same set of pre-trained model parameters, limiting their practical utility. To address this issue, we introduce a novel neural TSP solver named GELD, built upon our proposed broad global assessment and refined local selection framework. Specifically, GELD integrates a lightweight Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich embedding representation while accelerating the decision-making process. Moreover, GE incorporates a novel low-complexity attention mechanism, allowing GELD to achieve low inference latency and scalability to larger-scale TSPs. Additionally, we propose a two-stage training strategy that utilizes training instances of different sizes to bolster GELD's generalization ability. Extensive experiments conducted on both synthetic and real-world datasets demonstrate that GELD outperforms seven state-of-the-art models considering both solution quality and inference speed. Furthermore, GELD can be employed as a post-processing method to significantly elevate the quality of the solutions derived by existing neural TSP solvers via spending affordable additional computing time. Notably, GELD is shown as capable of solving TSPs with up to 744,710 nodes, first-of-its-kind to solve this large size TSP without relying on divide-and-conquer strategies to the best of our knowledge.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-geld-a-unified-neural-model-for-efficiently-solving-traveling-salesman-problems--e24948.md","b6edea883a2bf852",{"html":1351,"metadata":1352},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales\r\nView PDF HTML (experimental)Abstract:The Traveling Salesman Problem (TSP) is a well-known combinatorial optimization problem with broad real-world applications. Recent advancements in neural network-based TSP solvers have shown promising results. Nonetheless, these models often struggle to efficiently solve both small- and large-scale TSPs using the same set of pre-trained model parameters, limiting their practical utility. To address this issue, we introduce a novel neural TSP solver named GELD, built upon our proposed broad global assessment and refined local selection framework. Specifically, GELD integrates a lightweight Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich embedding representation while accelerating the decision-making process. Moreover, GE incorporates a novel low-complexity attention mechanism, allowing GELD to achieve low inference latency and scalability to larger-scale TSPs. Additionally, we propose a two-stage training strategy that utilizes training instances of different sizes to bolster GELD’s generalization ability. Extensive experiments conducted on both synthetic and real-world datasets demonstrate that GELD outperforms seven state-of-the-art models considering both solution quality and inference speed. Furthermore, GELD can be employed as a post-processing method to significantly elevate the quality of the solutions derived by existing neural TSP solvers via spending affordable additional computing time. Notably, GELD is shown as capable of solving TSPs with up to 744,710 nodes, first-of-its-kind to solve this large size TSP without relying on divide-and-conquer strategies to the best of our knowledge.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1353,"localImagePaths":1354,"remoteImagePaths":1355,"frontmatter":1356,"imagePaths":1358},[],[],[],{"title":1342,"description":1343,"pubDate":33,"source":17,"tags":1357,"url":1346},[19,20,21],[],"2025-06-10-geld-a-unified-neural-model-for-efficiently-solving-traveling-salesman-problems--e24948.md","2025-06-10-from-transformers-to-large-language-models-a-systematic-review-of-ai-application-656149",{"id":1360,"data":1362,"body":1368,"filePath":1369,"digest":1370,"rendered":1371,"legacyId":1380},{"title":1363,"description":1364,"pubDate":1365,"source":17,"tags":1366,"url":1367},"From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins","arXiv:2506.06359v1 Announce Type: cross \nAbstract: Artificial intelligence (AI) has long promised to improve energy management in smart grids by enhancing situational awareness and supporting more effective decision-making. While traditional machine learning has demonstrated notable results in forecasting and optimization, it often struggles with generalization, situational awareness, and heterogeneous data integration. Recent advances in foundation models such as Transformer architecture and Large Language Models (LLMs) have demonstrated improved capabilities in modelling complex temporal and contextual relationships, as well as in multi-modal data fusion which is essential for most AI applications in the energy sector. In this review we synthesize the rapid expanding field of AI applications in the energy domain focusing on Transformers and LLMs. We examine the architectural foundations, domain-specific adaptations and practical implementations of transformer models across various forecasting and grid management tasks. We then explore the emerging role of LLMs in the field: adaptation and fine tuning for the energy sector, the type of tasks they are suited for, and the new challenges they introduce. Along the way, we highlight practical implementations, innovations, and areas where the research frontier is rapidly expanding. These recent developments reviewed underscore a broader trend: Generative AI (GenAI) is beginning to augment decision-making not only in high-level planning but also in day-to-day operations, from forecasting and grid balancing to workforce training and asset onboarding. Building on these developments, we introduce the concept of the Agentic Digital Twin, a next-generation model that integrates LLMs to bring autonomy, proactivity, and social interaction into digital twin-based energy management systems.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06359","Computer Science > Machine Learning\r\n[Submitted on 3 Jun 2025]\r\nTitle:From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins\r\nView PDFAbstract:Artificial intelligence (AI) has long promised to improve energy management in smart grids by enhancing situational awareness and supporting more effective decision-making. While traditional machine learning has demonstrated notable results in forecasting and optimization, it often struggles with generalization, situational awareness, and heterogeneous data integration. Recent advances in foundation models such as Transformer architecture and Large Language Models (LLMs) have demonstrated improved capabilities in modelling complex temporal and contextual relationships, as well as in multi-modal data fusion which is essential for most AI applications in the energy sector. In this review we synthesize the rapid expanding field of AI applications in the energy domain focusing on Transformers and LLMs. We examine the architectural foundations, domain-specific adaptations and practical implementations of transformer models across various forecasting and grid management tasks. We then explore the emerging role of LLMs in the field: adaptation and fine tuning for the energy sector, the type of tasks they are suited for, and the new challenges they introduce. Along the way, we highlight practical implementations, innovations, and areas where the research frontier is rapidly expanding. These recent developments reviewed underscore a broader trend: Generative AI (GenAI) is beginning to augment decision-making not only in high-level planning but also in day-to-day operations, from forecasting and grid balancing to workforce training and asset onboarding. Building on these developments, we introduce the concept of the Agentic Digital Twin, a next-generation model that integrates LLMs to bring autonomy, proactivity, and social interaction into digital twin-based energy management systems.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-from-transformers-to-large-language-models-a-systematic-review-of-ai-application-656149.md","58f4950b6d8f0c75",{"html":1372,"metadata":1373},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 3 Jun 2025]\r\nTitle:From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins\r\nView PDFAbstract:Artificial intelligence (AI) has long promised to improve energy management in smart grids by enhancing situational awareness and supporting more effective decision-making. While traditional machine learning has demonstrated notable results in forecasting and optimization, it often struggles with generalization, situational awareness, and heterogeneous data integration. Recent advances in foundation models such as Transformer architecture and Large Language Models (LLMs) have demonstrated improved capabilities in modelling complex temporal and contextual relationships, as well as in multi-modal data fusion which is essential for most AI applications in the energy sector. In this review we synthesize the rapid expanding field of AI applications in the energy domain focusing on Transformers and LLMs. We examine the architectural foundations, domain-specific adaptations and practical implementations of transformer models across various forecasting and grid management tasks. We then explore the emerging role of LLMs in the field: adaptation and fine tuning for the energy sector, the type of tasks they are suited for, and the new challenges they introduce. Along the way, we highlight practical implementations, innovations, and areas where the research frontier is rapidly expanding. These recent developments reviewed underscore a broader trend: Generative AI (GenAI) is beginning to augment decision-making not only in high-level planning but also in day-to-day operations, from forecasting and grid balancing to workforce training and asset onboarding. Building on these developments, we introduce the concept of the Agentic Digital Twin, a next-generation model that integrates LLMs to bring autonomy, proactivity, and social interaction into digital twin-based energy management systems.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1374,"localImagePaths":1375,"remoteImagePaths":1376,"frontmatter":1377,"imagePaths":1379},[],[],[],{"title":1363,"description":1364,"pubDate":33,"source":17,"tags":1378,"url":1367},[19,20,21],[],"2025-06-10-from-transformers-to-large-language-models-a-systematic-review-of-ai-application-656149.md","2025-06-10-glprotein-global-and-local-structure-aware-protein-representation-learning-2792f6",{"id":1381,"data":1383,"body":1389,"filePath":1390,"digest":1391,"rendered":1392,"legacyId":1401},{"title":1384,"description":1385,"pubDate":1386,"source":17,"tags":1387,"url":1388},"GLProtein: Global-and-Local Structure Aware Protein Representation Learning","arXiv:2506.06294v1 Announce Type: cross \nAbstract: Proteins are central to biological systems, participating as building blocks across all forms of life. Despite advancements in understanding protein functions through protein sequence analysis, there remains potential for further exploration in integrating protein structural information. We argue that the structural information of proteins is not only limited to their 3D information but also encompasses information from amino acid molecules (local information) to protein-protein structure similarity (global information). To address this, we propose \\textbf{GLProtein}, the first framework in protein pre-training that incorporates both global structural similarity and local amino acid details to enhance prediction accuracy and functional insights. GLProtein innovatively combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding. Experimental results demonstrate that GLProtein outperforms previous methods in several bioinformatics tasks, including predicting protein-protein interaction, contact prediction, and so on.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06294","Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:GLProtein: Global-and-Local Structure Aware Protein Representation Learning\r\nView PDF HTML (experimental)Abstract:Proteins are central to biological systems, participating as building blocks across all forms of life. Despite advancements in understanding protein functions through protein sequence analysis, there remains potential for further exploration in integrating protein structural information. We argue that the structural information of proteins is not only limited to their 3D information but also encompasses information from amino acid molecules (local information) to protein-protein structure similarity (global information). To address this, we propose \\textbf{GLProtein}, the first framework in protein pre-training that incorporates both global structural similarity and local amino acid details to enhance prediction accuracy and functional insights. GLProtein innovatively combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding. Experimental results demonstrate that GLProtein outperforms previous methods in several bioinformatics tasks, including predicting protein-protein interaction, contact prediction, and so on.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-glprotein-global-and-local-structure-aware-protein-representation-learning-2792f6.md","1cdc8cf50f6d301f",{"html":1393,"metadata":1394},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:GLProtein: Global-and-Local Structure Aware Protein Representation Learning\r\nView PDF HTML (experimental)Abstract:Proteins are central to biological systems, participating as building blocks across all forms of life. Despite advancements in understanding protein functions through protein sequence analysis, there remains potential for further exploration in integrating protein structural information. We argue that the structural information of proteins is not only limited to their 3D information but also encompasses information from amino acid molecules (local information) to protein-protein structure similarity (global information). To address this, we propose \\textbf{GLProtein}, the first framework in protein pre-training that incorporates both global structural similarity and local amino acid details to enhance prediction accuracy and functional insights. GLProtein innovatively combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding. Experimental results demonstrate that GLProtein outperforms previous methods in several bioinformatics tasks, including predicting protein-protein interaction, contact prediction, and so on.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1395,"localImagePaths":1396,"remoteImagePaths":1397,"frontmatter":1398,"imagePaths":1400},[],[],[],{"title":1384,"description":1385,"pubDate":33,"source":17,"tags":1399,"url":1388},[19,20,21],[],"2025-06-10-glprotein-global-and-local-structure-aware-protein-representation-learning-2792f6.md","2025-06-10-gradients-when-markets-meet-fine-tuning----a-distributed-approach-to-model-optim-bf9476",{"id":1402,"data":1404,"body":1410,"filePath":1411,"digest":1412,"rendered":1413,"legacyId":1422},{"title":1405,"description":1406,"pubDate":1407,"source":17,"tags":1408,"url":1409},"Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation","arXiv:2506.07940v1 Announce Type: new \nAbstract: Foundation model fine-tuning faces a fundamental challenge: existing AutoML platforms rely on single optimisation strategies that explore only a fraction of viable hyperparameter configurations. In this white paper, We introduce Gradients, a decentralised AutoML platform that transforms hyperparameter optimisation into a competitive marketplace where independent miners compete to discover optimal configurations. Economic incentives align individual exploration with collective optimisation goals, driving systematic investigation of hyperparameter regions that centralised methods miss. We evaluate our approach across 180 controlled experiments spanning diverse model architectures (70M to 70B parameters) and task types. Gradients achieves an 82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI, Databricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\% respectively. Complex reasoning and retrieval tasks show particularly strong gains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for person-specific generation. These results demonstrate that competitive, economically-driven approaches can systematically discover superior configurations that centralised AutoML consistently miss.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07940","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation\r\nView PDF HTML (experimental)Abstract:Foundation model fine-tuning faces a fundamental challenge: existing AutoML platforms rely on single optimisation strategies that explore only a fraction of viable hyperparameter configurations. In this white paper, We introduce Gradients, a decentralised AutoML platform that transforms hyperparameter optimisation into a competitive marketplace where independent miners compete to discover optimal configurations. Economic incentives align individual exploration with collective optimisation goals, driving systematic investigation of hyperparameter regions that centralised methods miss. We evaluate our approach across 180 controlled experiments spanning diverse model architectures (70M to 70B parameters) and task types. Gradients achieves an 82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI, Databricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\% respectively. Complex reasoning and retrieval tasks show particularly strong gains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for person-specific generation. These results demonstrate that competitive, economically-driven approaches can systematically discover superior configurations that centralised AutoML consistently miss.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-gradients-when-markets-meet-fine-tuning----a-distributed-approach-to-model-optim-bf9476.md","4f074556b93a65cb",{"html":1414,"metadata":1415},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Gradients: When Markets Meet Fine-tuning — A Distributed Approach to Model Optimisation\r\nView PDF HTML (experimental)Abstract:Foundation model fine-tuning faces a fundamental challenge: existing AutoML platforms rely on single optimisation strategies that explore only a fraction of viable hyperparameter configurations. In this white paper, We introduce Gradients, a decentralised AutoML platform that transforms hyperparameter optimisation into a competitive marketplace where independent miners compete to discover optimal configurations. Economic incentives align individual exploration with collective optimisation goals, driving systematic investigation of hyperparameter regions that centralised methods miss. We evaluate our approach across 180 controlled experiments spanning diverse model architectures (70M to 70B parameters) and task types. Gradients achieves an 82.8% win rate against HuggingFace AutoTrain and 100% against TogetherAI, Databricks, and Google Cloud, with mean improvements of 11.8% and 42.1% respectively. Complex reasoning and retrieval tasks show particularly strong gains of 30-40%, whilst diffusion models achieve 23.4% improvements for person-specific generation. These results demonstrate that competitive, economically-driven approaches can systematically discover superior configurations that centralised AutoML consistently miss.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1416,"localImagePaths":1417,"remoteImagePaths":1418,"frontmatter":1419,"imagePaths":1421},[],[],[],{"title":1405,"description":1406,"pubDate":33,"source":17,"tags":1420,"url":1409},[19,20,21],[],"2025-06-10-gradients-when-markets-meet-fine-tuning----a-distributed-approach-to-model-optim-bf9476.md","2025-06-10-graph-persistence-goes-spectral-d77bc3",{"id":1423,"data":1425,"body":1431,"filePath":1432,"digest":1433,"rendered":1434,"legacyId":1443},{"title":1426,"description":1427,"pubDate":1428,"source":17,"tags":1429,"url":1430},"Graph Persistence goes Spectral","arXiv:2506.06571v1 Announce Type: cross \nAbstract: Including intricate topological information (e.g., cycles) provably enhances the expressivity of message-passing graph neural networks (GNNs) beyond the Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods are increasingly employed for graph representation learning. In this context, recent works have proposed decorating classical PH diagrams with vertex and edge features for improved expressivity. However, due to their dependence on features, these methods still fail to capture basic graph structural information. In this paper, we propose SpectRe -- a new topological descriptor for graphs that integrates spectral information into PH diagrams. Notably, SpectRe is strictly more expressive than existing descriptors on graphs. We also introduce notions of global and local stability to analyze existing descriptors and establish that SpectRe is locally stable. Finally, experiments on synthetic and real-world datasets demonstrate the effectiveness of SpectRe and its potential to enhance the capabilities of graph models in relevant learning tasks.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06571","Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Graph Persistence goes Spectral\r\nView PDF HTML (experimental)Abstract:Including intricate topological information (e.g., cycles) provably enhances the expressivity of message-passing graph neural networks (GNNs) beyond the Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods are increasingly employed for graph representation learning. In this context, recent works have proposed decorating classical PH diagrams with vertex and edge features for improved expressivity. However, due to their dependence on features, these methods still fail to capture basic graph structural information. In this paper, we propose SpectRe -- a new topological descriptor for graphs that integrates spectral information into PH diagrams. Notably, SpectRe is strictly more expressive than existing descriptors on graphs. We also introduce notions of global and local stability to analyze existing descriptors and establish that SpectRe is locally stable. Finally, experiments on synthetic and real-world datasets demonstrate the effectiveness of SpectRe and its potential to enhance the capabilities of graph models in relevant learning tasks.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-graph-persistence-goes-spectral-d77bc3.md","2b4975f9c0f6f0d0",{"html":1435,"metadata":1436},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Graph Persistence goes Spectral\r\nView PDF HTML (experimental)Abstract:Including intricate topological information (e.g., cycles) provably enhances the expressivity of message-passing graph neural networks (GNNs) beyond the Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods are increasingly employed for graph representation learning. In this context, recent works have proposed decorating classical PH diagrams with vertex and edge features for improved expressivity. However, due to their dependence on features, these methods still fail to capture basic graph structural information. In this paper, we propose SpectRe — a new topological descriptor for graphs that integrates spectral information into PH diagrams. Notably, SpectRe is strictly more expressive than existing descriptors on graphs. We also introduce notions of global and local stability to analyze existing descriptors and establish that SpectRe is locally stable. Finally, experiments on synthetic and real-world datasets demonstrate the effectiveness of SpectRe and its potential to enhance the capabilities of graph models in relevant learning tasks.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1437,"localImagePaths":1438,"remoteImagePaths":1439,"frontmatter":1440,"imagePaths":1442},[],[],[],{"title":1426,"description":1427,"pubDate":33,"source":17,"tags":1441,"url":1430},[19,20,21],[],"2025-06-10-graph-persistence-goes-spectral-d77bc3.md","2025-06-10-golfer-smaller-lm-generated-documents-hallucination-filter-and-combiner-for-quer-594b65",{"id":1444,"data":1446,"body":1452,"filePath":1453,"digest":1454,"rendered":1455,"legacyId":1464},{"title":1447,"description":1448,"pubDate":1449,"source":17,"tags":1450,"url":1451},"GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval","arXiv:2506.04762v1 Announce Type: cross \nAbstract: Large language models (LLMs)-based query expansion for information retrieval augments queries with generated hypothetical documents with LLMs. However, its performance relies heavily on the scale of the language models (LMs), necessitating larger, more advanced LLMs. This approach is costly, computationally intensive, and often has limited accessibility. To address these limitations, we introduce GOLFer - Smaller LMs-Generated Documents Hallucination Filter & Combiner - a novel method leveraging smaller open-source LMs for query expansion. GOLFer comprises two modules: a hallucination filter and a documents combiner. The former detects and removes non-factual and inconsistent sentences in generated documents, a common issue with smaller LMs, while the latter combines the filtered content with the query using a weight vector to balance their influence. We evaluate GOLFer alongside dominant LLM-based query expansion methods on three web search and ten low-resource datasets. Experimental results demonstrate that GOLFer consistently outperforms other methods using smaller LMs, and maintains competitive performance against methods using large-size LLMs, demonstrating its effectiveness.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.04762","Computer Science > Information Retrieval\r\n[Submitted on 5 Jun 2025]\r\nTitle:GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval\r\nView PDF HTML (experimental)Abstract:Large language models (LLMs)-based query expansion for information retrieval augments queries with generated hypothetical documents with LLMs. However, its performance relies heavily on the scale of the language models (LMs), necessitating larger, more advanced LLMs. This approach is costly, computationally intensive, and often has limited accessibility. To address these limitations, we introduce GOLFer - Smaller LMs-Generated Documents Hallucination Filter & Combiner - a novel method leveraging smaller open-source LMs for query expansion. GOLFer comprises two modules: a hallucination filter and a documents combiner. The former detects and removes non-factual and inconsistent sentences in generated documents, a common issue with smaller LMs, while the latter combines the filtered content with the query using a weight vector to balance their influence. We evaluate GOLFer alongside dominant LLM-based query expansion methods on three web search and ten low-resource datasets. Experimental results demonstrate that GOLFer consistently outperforms other methods using smaller LMs, and maintains competitive performance against methods using large-size LLMs, demonstrating its effectiveness.\r\nCurrent browse context:\r\ncs.IR\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-golfer-smaller-lm-generated-documents-hallucination-filter-and-combiner-for-quer-594b65.md","0472bf0f97a7d8d7",{"html":1456,"metadata":1457},"\u003Cp>Computer Science > Information Retrieval\r\n[Submitted on 5 Jun 2025]\r\nTitle:GOLFer: Smaller LM-Generated Documents Hallucination Filter &#x26; Combiner for Query Expansion in Information Retrieval\r\nView PDF HTML (experimental)Abstract:Large language models (LLMs)-based query expansion for information retrieval augments queries with generated hypothetical documents with LLMs. However, its performance relies heavily on the scale of the language models (LMs), necessitating larger, more advanced LLMs. This approach is costly, computationally intensive, and often has limited accessibility. To address these limitations, we introduce GOLFer - Smaller LMs-Generated Documents Hallucination Filter &#x26; Combiner - a novel method leveraging smaller open-source LMs for query expansion. GOLFer comprises two modules: a hallucination filter and a documents combiner. The former detects and removes non-factual and inconsistent sentences in generated documents, a common issue with smaller LMs, while the latter combines the filtered content with the query using a weight vector to balance their influence. We evaluate GOLFer alongside dominant LLM-based query expansion methods on three web search and ten low-resource datasets. Experimental results demonstrate that GOLFer consistently outperforms other methods using smaller LMs, and maintains competitive performance against methods using large-size LLMs, demonstrating its effectiveness.\r\nCurrent browse context:\r\ncs.IR\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1458,"localImagePaths":1459,"remoteImagePaths":1460,"frontmatter":1461,"imagePaths":1463},[],[],[],{"title":1447,"description":1448,"pubDate":33,"source":17,"tags":1462,"url":1451},[19,20,21],[],"2025-06-10-golfer-smaller-lm-generated-documents-hallucination-filter-and-combiner-for-quer-594b65.md","2025-06-10-gtr-cot-graph-traversal-as-visual-chain-of-thought-for-molecular-structure-recog-4e5a7f",{"id":1465,"data":1467,"body":1473,"filePath":1474,"digest":1475,"rendered":1476,"legacyId":1485},{"title":1468,"description":1469,"pubDate":1470,"source":17,"tags":1471,"url":1472},"GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition","arXiv:2506.07553v1 Announce Type: new \nAbstract: Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the \\textit{Graph Traversal as Visual Chain of Thought} mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of \\textit{Faithfully Recognize What You've Seen}, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at https://github.com/opendatalab/GTR-CoT.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07553","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition\r\nView PDF HTML (experimental)Abstract:Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the \\textit{Graph Traversal as Visual Chain of Thought} mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of \\textit{Faithfully Recognize What You've Seen}, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at this https URL.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-gtr-cot-graph-traversal-as-visual-chain-of-thought-for-molecular-structure-recog-4e5a7f.md","b50d06ecddf804a6",{"html":1477,"metadata":1478},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition\r\nView PDF HTML (experimental)Abstract:Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the \\textit{Graph Traversal as Visual Chain of Thought} mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of \\textit{Faithfully Recognize What You’ve Seen}, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at this https URL.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1479,"localImagePaths":1480,"remoteImagePaths":1481,"frontmatter":1482,"imagePaths":1484},[],[],[],{"title":1468,"description":1469,"pubDate":33,"source":17,"tags":1483,"url":1472},[19,20,21],[],"2025-06-10-gtr-cot-graph-traversal-as-visual-chain-of-thought-for-molecular-structure-recog-4e5a7f.md","2025-06-10-gui-reflection-empowering-multimodal-gui-models-with-self-reflection-behavior-0e6d37",{"id":1486,"data":1488,"body":1494,"filePath":1495,"digest":1496,"rendered":1497,"legacyId":1506},{"title":1489,"description":1490,"pubDate":1491,"source":17,"tags":1492,"url":1493},"GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior","arXiv:2506.08012v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.08012","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior\r\nView PDF HTML (experimental)Abstract:Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-gui-reflection-empowering-multimodal-gui-models-with-self-reflection-behavior-0e6d37.md","9e2c5560b52ce48d",{"html":1498,"metadata":1499},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior\r\nView PDF HTML (experimental)Abstract:Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1500,"localImagePaths":1501,"remoteImagePaths":1502,"frontmatter":1503,"imagePaths":1505},[],[],[],{"title":1489,"description":1490,"pubDate":33,"source":17,"tags":1504,"url":1493},[19,20,21],[],"2025-06-10-gui-reflection-empowering-multimodal-gui-models-with-self-reflection-behavior-0e6d37.md","2025-06-10-haibu-remud-reasoning-multimodal-ultrasound-dataset-and-model-bridging-to-genera-175670",{"id":1507,"data":1509,"body":1515,"filePath":1516,"digest":1517,"rendered":1518,"legacyId":1527},{"title":1510,"description":1511,"pubDate":1512,"source":17,"tags":1513,"url":1514},"HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains","arXiv:2506.07837v1 Announce Type: new \nAbstract: Multimodal large language models (MLLMs) have shown great potential in general domains but perform poorly in some specific domains due to a lack of domain-specific data, such as image-text data or vedio-text data. In some specific domains, there is abundant graphic and textual data scattered around, but lacks standardized arrangement. In the field of medical ultrasound, there are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic diagnostic reports, and so on. However, these ultrasonic materials are often saved in the forms of PDF, images, etc., and cannot be directly used for the training of MLLMs. This paper proposes a novel image-text reasoning supervised fine-tuning data generation pipeline to create specific domain quadruplets (image, question, thinking trace, and answer) from domain-specific materials. A medical ultrasound domain dataset ReMUD is established, containing over 45,000 reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound field. To facilitate research, the ReMUD dataset, data generation codebase, and ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD, addressing the data shortage issue in specific domain MLLMs.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07837","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains\r\nView PDF HTML (experimental)Abstract:Multimodal large language models (MLLMs) have shown great potential in general domains but perform poorly in some specific domains due to a lack of domain-specific data, such as image-text data or vedio-text data. In some specific domains, there is abundant graphic and textual data scattered around, but lacks standardized arrangement. In the field of medical ultrasound, there are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic diagnostic reports, and so on. However, these ultrasonic materials are often saved in the forms of PDF, images, etc., and cannot be directly used for the training of MLLMs. This paper proposes a novel image-text reasoning supervised fine-tuning data generation pipeline to create specific domain quadruplets (image, question, thinking trace, and answer) from domain-specific materials. A medical ultrasound domain dataset ReMUD is established, containing over 45,000 reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound field. To facilitate research, the ReMUD dataset, data generation codebase, and ReMUD-7B parameters will be released at this https URL, addressing the data shortage issue in specific domain MLLMs.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-haibu-remud-reasoning-multimodal-ultrasound-dataset-and-model-bridging-to-genera-175670.md","9170d6c9f91762d6",{"html":1519,"metadata":1520},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains\r\nView PDF HTML (experimental)Abstract:Multimodal large language models (MLLMs) have shown great potential in general domains but perform poorly in some specific domains due to a lack of domain-specific data, such as image-text data or vedio-text data. In some specific domains, there is abundant graphic and textual data scattered around, but lacks standardized arrangement. In the field of medical ultrasound, there are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic diagnostic reports, and so on. However, these ultrasonic materials are often saved in the forms of PDF, images, etc., and cannot be directly used for the training of MLLMs. This paper proposes a novel image-text reasoning supervised fine-tuning data generation pipeline to create specific domain quadruplets (image, question, thinking trace, and answer) from domain-specific materials. A medical ultrasound domain dataset ReMUD is established, containing over 45,000 reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound field. To facilitate research, the ReMUD dataset, data generation codebase, and ReMUD-7B parameters will be released at this https URL, addressing the data shortage issue in specific domain MLLMs.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1521,"localImagePaths":1522,"remoteImagePaths":1523,"frontmatter":1524,"imagePaths":1526},[],[],[],{"title":1510,"description":1511,"pubDate":33,"source":17,"tags":1525,"url":1514},[19,20,21],[],"2025-06-10-haibu-remud-reasoning-multimodal-ultrasound-dataset-and-model-bridging-to-genera-175670.md","2025-06-10-heavywater-and-simplexwater-watermarking-low-entropy-text-distributions-ef5b50",{"id":1528,"data":1530,"body":1536,"filePath":1537,"digest":1538,"rendered":1539,"legacyId":1548},{"title":1531,"description":1532,"pubDate":1533,"source":17,"tags":1534,"url":1535},"HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions","arXiv:2506.06409v1 Announce Type: cross \nAbstract: Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks - such as coding - where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory. The code implementation can be found in https://github.com/DorTsur/HeavyWater_SimplexWater",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06409","Computer Science > Cryptography and Security\r\n[Submitted on 6 Jun 2025]\r\nTitle:HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions\r\nView PDF HTML (experimental)Abstract:Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks - such as coding - where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory. The code implementation can be found in this https URL\r\nCurrent browse context:\r\ncs.CR\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-heavywater-and-simplexwater-watermarking-low-entropy-text-distributions-ef5b50.md","b03ae869c358d45e",{"html":1540,"metadata":1541},"\u003Cp>Computer Science > Cryptography and Security\r\n[Submitted on 6 Jun 2025]\r\nTitle:HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions\r\nView PDF HTML (experimental)Abstract:Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks - such as coding - where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory. The code implementation can be found in this https URL\r\nCurrent browse context:\r\ncs.CR\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1542,"localImagePaths":1543,"remoteImagePaths":1544,"frontmatter":1545,"imagePaths":1547},[],[],[],{"title":1531,"description":1532,"pubDate":33,"source":17,"tags":1546,"url":1535},[19,20,21],[],"2025-06-10-heavywater-and-simplexwater-watermarking-low-entropy-text-distributions-ef5b50.md","2025-06-10-guideline-forest-experience-induced-multi-guideline-reasoning-with-stepwise-aggr-5775dd",{"id":1549,"data":1551,"body":1557,"filePath":1558,"digest":1559,"rendered":1560,"legacyId":1569},{"title":1552,"description":1553,"pubDate":1554,"source":17,"tags":1555,"url":1556},"Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation","arXiv:2506.07820v1 Announce Type: new \nAbstract: Human reasoning is flexible, adaptive, and grounded in prior experience-qualities that large language models (LLMs) still struggle to emulate. Existing methods either explore diverse reasoning paths at inference time or search for optimal workflows through expensive operations, but both fall short in leveraging multiple reusable strategies in a structured, efficient manner. We propose Guideline Forest, a framework that enhances LLMs reasoning by inducing structured reasoning strategies-called guidelines-from verified examples and executing them via step-wise aggregation. Unlike test-time search or single-path distillation, our method draws on verified reasoning experiences by inducing reusable guidelines and expanding each into diverse variants. Much like human reasoning, these variants reflect alternative thought patterns, are executed in parallel, refined via self-correction, and aggregated step by step-enabling the model to adaptively resolve uncertainty and synthesize robust solutions.We evaluate Guideline Forest on four benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and programmatic reasoning. Guideline Forest consistently outperforms strong baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further highlight the effectiveness of multi-path reasoning and stepwise aggregation, underscoring the Guideline Forest's adaptability and generalization potential.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07820","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation\r\nView PDFAbstract:Human reasoning is flexible, adaptive, and grounded in prior experience-qualities that large language models (LLMs) still struggle to emulate. Existing methods either explore diverse reasoning paths at inference time or search for optimal workflows through expensive operations, but both fall short in leveraging multiple reusable strategies in a structured, efficient manner. We propose Guideline Forest, a framework that enhances LLMs reasoning by inducing structured reasoning strategies-called guidelines-from verified examples and executing them via step-wise aggregation. Unlike test-time search or single-path distillation, our method draws on verified reasoning experiences by inducing reusable guidelines and expanding each into diverse variants. Much like human reasoning, these variants reflect alternative thought patterns, are executed in parallel, refined via self-correction, and aggregated step by step-enabling the model to adaptively resolve uncertainty and synthesize robust this http URL evaluate Guideline Forest on four benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and programmatic reasoning. Guideline Forest consistently outperforms strong baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further highlight the effectiveness of multi-path reasoning and stepwise aggregation, underscoring the Guideline Forest's adaptability and generalization potential.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-guideline-forest-experience-induced-multi-guideline-reasoning-with-stepwise-aggr-5775dd.md","9602ae10f5443dab",{"html":1561,"metadata":1562},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation\r\nView PDFAbstract:Human reasoning is flexible, adaptive, and grounded in prior experience-qualities that large language models (LLMs) still struggle to emulate. Existing methods either explore diverse reasoning paths at inference time or search for optimal workflows through expensive operations, but both fall short in leveraging multiple reusable strategies in a structured, efficient manner. We propose Guideline Forest, a framework that enhances LLMs reasoning by inducing structured reasoning strategies-called guidelines-from verified examples and executing them via step-wise aggregation. Unlike test-time search or single-path distillation, our method draws on verified reasoning experiences by inducing reusable guidelines and expanding each into diverse variants. Much like human reasoning, these variants reflect alternative thought patterns, are executed in parallel, refined via self-correction, and aggregated step by step-enabling the model to adaptively resolve uncertainty and synthesize robust this http URL evaluate Guideline Forest on four benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and programmatic reasoning. Guideline Forest consistently outperforms strong baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further highlight the effectiveness of multi-path reasoning and stepwise aggregation, underscoring the Guideline Forest’s adaptability and generalization potential.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1563,"localImagePaths":1564,"remoteImagePaths":1565,"frontmatter":1566,"imagePaths":1568},[],[],[],{"title":1552,"description":1553,"pubDate":33,"source":17,"tags":1567,"url":1556},[19,20,21],[],"2025-06-10-guideline-forest-experience-induced-multi-guideline-reasoning-with-stepwise-aggr-5775dd.md","2025-06-10-heta-relation-wise-heterogeneous-graph-foundation-attack-model-7f8fb3",{"id":1570,"data":1572,"body":1578,"filePath":1579,"digest":1580,"rendered":1581,"legacyId":1590},{"title":1573,"description":1574,"pubDate":1575,"source":17,"tags":1576,"url":1577},"HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model","arXiv:2506.07428v1 Announce Type: new \nAbstract: Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the need for tailored attacks to assess their robustness and ensure security. However, existing HGNN attacks often require complex retraining of parameters to generate specific perturbations for new scenarios. Recently, foundation models have opened new horizons for the generalization of graph neural networks by capturing shared semantics across various graph distributions. This leads us to ask:Can we design a foundation attack model for HGNNs that enables generalizable perturbations across different HGNNs, and quickly adapts to new heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant differences in model design and parameter space, different HGNNs surprisingly share common vulnerability patterns from a relation-aware perspective. Therefore, we explore how to design foundation HGNN attack criteria by mining shared attack units. In this paper, we propose a novel relation-wise heterogeneous graph foundation attack model, HeTa. We introduce a foundation surrogate model to align heterogeneity and identify the importance of shared relation-aware attack units. Building on this, we implement a serialized relation-by-relation attack based on the identified relational weights. In this way, the perturbation can be transferred to various target HGNNs and easily fine-tuned for new HGs. Extensive experiments exhibit powerful attack performances and generalizability of our method.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07428","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model\r\nView PDF HTML (experimental)Abstract:Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the need for tailored attacks to assess their robustness and ensure security. However, existing HGNN attacks often require complex retraining of parameters to generate specific perturbations for new scenarios. Recently, foundation models have opened new horizons for the generalization of graph neural networks by capturing shared semantics across various graph distributions. This leads us to ask:Can we design a foundation attack model for HGNNs that enables generalizable perturbations across different HGNNs, and quickly adapts to new heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant differences in model design and parameter space, different HGNNs surprisingly share common vulnerability patterns from a relation-aware perspective. Therefore, we explore how to design foundation HGNN attack criteria by mining shared attack units. In this paper, we propose a novel relation-wise heterogeneous graph foundation attack model, HeTa. We introduce a foundation surrogate model to align heterogeneity and identify the importance of shared relation-aware attack units. Building on this, we implement a serialized relation-by-relation attack based on the identified relational weights. In this way, the perturbation can be transferred to various target HGNNs and easily fine-tuned for new HGs. Extensive experiments exhibit powerful attack performances and generalizability of our method.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-heta-relation-wise-heterogeneous-graph-foundation-attack-model-7f8fb3.md","d5f6b4f3136dfe60",{"html":1582,"metadata":1583},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model\r\nView PDF HTML (experimental)Abstract:Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the need for tailored attacks to assess their robustness and ensure security. However, existing HGNN attacks often require complex retraining of parameters to generate specific perturbations for new scenarios. Recently, foundation models have opened new horizons for the generalization of graph neural networks by capturing shared semantics across various graph distributions. This leads us to ask:Can we design a foundation attack model for HGNNs that enables generalizable perturbations across different HGNNs, and quickly adapts to new heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant differences in model design and parameter space, different HGNNs surprisingly share common vulnerability patterns from a relation-aware perspective. Therefore, we explore how to design foundation HGNN attack criteria by mining shared attack units. In this paper, we propose a novel relation-wise heterogeneous graph foundation attack model, HeTa. We introduce a foundation surrogate model to align heterogeneity and identify the importance of shared relation-aware attack units. Building on this, we implement a serialized relation-by-relation attack based on the identified relational weights. In this way, the perturbation can be transferred to various target HGNNs and easily fine-tuned for new HGs. Extensive experiments exhibit powerful attack performances and generalizability of our method.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1584,"localImagePaths":1585,"remoteImagePaths":1586,"frontmatter":1587,"imagePaths":1589},[],[],[],{"title":1573,"description":1574,"pubDate":33,"source":17,"tags":1588,"url":1577},[19,20,21],[],"2025-06-10-heta-relation-wise-heterogeneous-graph-foundation-attack-model-7f8fb3.md","2025-06-10-hierarchical-and-collaborative-llm-based-control-for-multi-uav-motion-and-commun-f626c2",{"id":1591,"data":1593,"body":1599,"filePath":1600,"digest":1601,"rendered":1602,"legacyId":1611},{"title":1594,"description":1595,"pubDate":1596,"source":17,"tags":1597,"url":1598},"Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks","arXiv:2506.06532v1 Announce Type: cross \nAbstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various real-world applications. However, the control and optimization of multi-UAV systems remain a significant challenge, particularly in dynamic and constrained environments. This work explores the joint motion and communication control of multiple UAVs operating within integrated terrestrial and non-terrestrial networks that include high-altitude platform stations (HAPS). Specifically, we consider an aerial highway scenario in which UAVs must accelerate, decelerate, and change lanes to avoid collisions and maintain overall traffic flow. Different from existing studies, we propose a novel hierarchical and collaborative method based on large language models (LLMs). In our approach, an LLM deployed on the HAPS performs UAV access control, while another LLM onboard each UAV handles motion planning and control. This LLM-based framework leverages the rich knowledge embedded in pre-trained models to enable both high-level strategic planning and low-level tactical decisions. This knowledge-driven paradigm holds great potential for the development of next-generation 3D aerial highway systems. Experimental results demonstrate that our proposed collaborative LLM-based method achieves higher system rewards, lower operational costs, and significantly reduced UAV collision rates compared to baseline approaches.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06532","Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks\r\nView PDF HTML (experimental)Abstract:Unmanned aerial vehicles (UAVs) have been widely adopted in various real-world applications. However, the control and optimization of multi-UAV systems remain a significant challenge, particularly in dynamic and constrained environments. This work explores the joint motion and communication control of multiple UAVs operating within integrated terrestrial and non-terrestrial networks that include high-altitude platform stations (HAPS). Specifically, we consider an aerial highway scenario in which UAVs must accelerate, decelerate, and change lanes to avoid collisions and maintain overall traffic flow. Different from existing studies, we propose a novel hierarchical and collaborative method based on large language models (LLMs). In our approach, an LLM deployed on the HAPS performs UAV access control, while another LLM onboard each UAV handles motion planning and control. This LLM-based framework leverages the rich knowledge embedded in pre-trained models to enable both high-level strategic planning and low-level tactical decisions. This knowledge-driven paradigm holds great potential for the development of next-generation 3D aerial highway systems. Experimental results demonstrate that our proposed collaborative LLM-based method achieves higher system rewards, lower operational costs, and significantly reduced UAV collision rates compared to baseline approaches.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-hierarchical-and-collaborative-llm-based-control-for-multi-uav-motion-and-commun-f626c2.md","dc4015af3ead7130",{"html":1603,"metadata":1604},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks\r\nView PDF HTML (experimental)Abstract:Unmanned aerial vehicles (UAVs) have been widely adopted in various real-world applications. However, the control and optimization of multi-UAV systems remain a significant challenge, particularly in dynamic and constrained environments. This work explores the joint motion and communication control of multiple UAVs operating within integrated terrestrial and non-terrestrial networks that include high-altitude platform stations (HAPS). Specifically, we consider an aerial highway scenario in which UAVs must accelerate, decelerate, and change lanes to avoid collisions and maintain overall traffic flow. Different from existing studies, we propose a novel hierarchical and collaborative method based on large language models (LLMs). In our approach, an LLM deployed on the HAPS performs UAV access control, while another LLM onboard each UAV handles motion planning and control. This LLM-based framework leverages the rich knowledge embedded in pre-trained models to enable both high-level strategic planning and low-level tactical decisions. This knowledge-driven paradigm holds great potential for the development of next-generation 3D aerial highway systems. Experimental results demonstrate that our proposed collaborative LLM-based method achieves higher system rewards, lower operational costs, and significantly reduced UAV collision rates compared to baseline approaches.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1605,"localImagePaths":1606,"remoteImagePaths":1607,"frontmatter":1608,"imagePaths":1610},[],[],[],{"title":1594,"description":1595,"pubDate":33,"source":17,"tags":1609,"url":1598},[19,20,21],[],"2025-06-10-hierarchical-and-collaborative-llm-based-control-for-multi-uav-motion-and-commun-f626c2.md","2025-06-10-honey-i-shrunk-the-hypothesis-space-through-logical-preprocessing-17274e",{"id":1612,"data":1614,"body":1620,"filePath":1621,"digest":1622,"rendered":1623,"legacyId":1632},{"title":1615,"description":1616,"pubDate":1617,"source":17,"tags":1618,"url":1619},"Honey, I shrunk the hypothesis space (through logical preprocessing)","arXiv:2506.06739v1 Announce Type: new \nAbstract: Inductive logic programming (ILP) is a form of logical machine learning. The goal is to search a hypothesis space for a hypothesis that generalises training examples and background knowledge. We introduce an approach that 'shrinks' the hypothesis space before an ILP system searches it. Our approach uses background knowledge to find rules that cannot be in an optimal hypothesis regardless of the training examples. For instance, our approach discovers relationships such as \"even numbers cannot be odd\" and \"prime numbers greater than 2 are odd\". It then removes violating rules from the hypothesis space. We implement our approach using answer set programming and use it to shrink the hypothesis space of a constraint-based ILP system. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can substantially reduce learning times whilst maintaining predictive accuracies. For instance, given just 10 seconds of preprocessing time, our approach can reduce learning times from over 10 hours to only 2 seconds.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06739","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Honey, I shrunk the hypothesis space (through logical preprocessing)\r\nView PDF HTML (experimental)Abstract:Inductive logic programming (ILP) is a form of logical machine learning. The goal is to search a hypothesis space for a hypothesis that generalises training examples and background knowledge. We introduce an approach that 'shrinks' the hypothesis space before an ILP system searches it. Our approach uses background knowledge to find rules that cannot be in an optimal hypothesis regardless of the training examples. For instance, our approach discovers relationships such as \"even numbers cannot be odd\" and \"prime numbers greater than 2 are odd\". It then removes violating rules from the hypothesis space. We implement our approach using answer set programming and use it to shrink the hypothesis space of a constraint-based ILP system. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can substantially reduce learning times whilst maintaining predictive accuracies. For instance, given just 10 seconds of preprocessing time, our approach can reduce learning times from over 10 hours to only 2 seconds.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-honey,-i-shrunk-the-hypothesis-space-(through-logical-preprocessing)-17274e.md","88fd18bc14b3a914",{"html":1624,"metadata":1625},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Honey, I shrunk the hypothesis space (through logical preprocessing)\r\nView PDF HTML (experimental)Abstract:Inductive logic programming (ILP) is a form of logical machine learning. The goal is to search a hypothesis space for a hypothesis that generalises training examples and background knowledge. We introduce an approach that ‘shrinks’ the hypothesis space before an ILP system searches it. Our approach uses background knowledge to find rules that cannot be in an optimal hypothesis regardless of the training examples. For instance, our approach discovers relationships such as “even numbers cannot be odd” and “prime numbers greater than 2 are odd”. It then removes violating rules from the hypothesis space. We implement our approach using answer set programming and use it to shrink the hypothesis space of a constraint-based ILP system. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can substantially reduce learning times whilst maintaining predictive accuracies. For instance, given just 10 seconds of preprocessing time, our approach can reduce learning times from over 10 hours to only 2 seconds.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1626,"localImagePaths":1627,"remoteImagePaths":1628,"frontmatter":1629,"imagePaths":1631},[],[],[],{"title":1615,"description":1616,"pubDate":33,"source":17,"tags":1630,"url":1619},[19,20,21],[],"2025-06-10-honey,-i-shrunk-the-hypothesis-space-(through-logical-preprocessing)-17274e.md","2025-06-10-how-malicious-ai-swarms-can-threaten-democracy-55680e",{"id":1633,"data":1635,"body":1641,"filePath":1642,"digest":1643,"rendered":1644,"legacyId":1653},{"title":1636,"description":1637,"pubDate":1638,"source":17,"tags":1639,"url":1640},"How Malicious AI Swarms Can Threaten Democracy","arXiv:2506.06299v1 Announce Type: cross \nAbstract: Advances in AI portend a new era of sophisticated disinformation operations. While individual AI systems already create convincing -- and at times misleading -- information, an imminent development is the emergence of malicious AI swarms. These systems can coordinate covertly, infiltrate communities, evade traditional detectors, and run continuous A/B tests, with round-the-clock persistence. The result can include fabricated grassroots consensus, fragmented shared reality, mass harassment, voter micro-suppression or mobilization, contamination of AI training data, and erosion of institutional trust. With democratic processes worldwide increasingly vulnerable, we urge a three-pronged response: (1) platform-side defenses -- always-on swarm-detection dashboards, pre-election high-fidelity swarm-simulation stress-tests, transparency audits, and optional client-side \"AI shields\" for users; (2) model-side safeguards -- standardized persuasion-risk tests, provenance-authenticating passkeys, and watermarking; and (3) system-level oversight -- a UN-backed AI Influence Observatory.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06299","Computer Science > Computers and Society\r\n[Submitted on 18 May 2025]\r\nTitle:How Malicious AI Swarms Can Threaten Democracy\r\nView PDF HTML (experimental)Abstract:Advances in AI portend a new era of sophisticated disinformation operations. While individual AI systems already create convincing -- and at times misleading -- information, an imminent development is the emergence of malicious AI swarms. These systems can coordinate covertly, infiltrate communities, evade traditional detectors, and run continuous A/B tests, with round-the-clock persistence. The result can include fabricated grassroots consensus, fragmented shared reality, mass harassment, voter micro-suppression or mobilization, contamination of AI training data, and erosion of institutional trust. With democratic processes worldwide increasingly vulnerable, we urge a three-pronged response: (1) platform-side defenses -- always-on swarm-detection dashboards, pre-election high-fidelity swarm-simulation stress-tests, transparency audits, and optional client-side \"AI shields\" for users; (2) model-side safeguards -- standardized persuasion-risk tests, provenance-authenticating passkeys, and watermarking; and (3) system-level oversight -- a UN-backed AI Influence Observatory.\r\nCurrent browse context:\r\ncs.CY\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-how-malicious-ai-swarms-can-threaten-democracy-55680e.md","37ecfdd7994249be",{"html":1645,"metadata":1646},"\u003Cp>Computer Science > Computers and Society\r\n[Submitted on 18 May 2025]\r\nTitle:How Malicious AI Swarms Can Threaten Democracy\r\nView PDF HTML (experimental)Abstract:Advances in AI portend a new era of sophisticated disinformation operations. While individual AI systems already create convincing — and at times misleading — information, an imminent development is the emergence of malicious AI swarms. These systems can coordinate covertly, infiltrate communities, evade traditional detectors, and run continuous A/B tests, with round-the-clock persistence. The result can include fabricated grassroots consensus, fragmented shared reality, mass harassment, voter micro-suppression or mobilization, contamination of AI training data, and erosion of institutional trust. With democratic processes worldwide increasingly vulnerable, we urge a three-pronged response: (1) platform-side defenses — always-on swarm-detection dashboards, pre-election high-fidelity swarm-simulation stress-tests, transparency audits, and optional client-side “AI shields” for users; (2) model-side safeguards — standardized persuasion-risk tests, provenance-authenticating passkeys, and watermarking; and (3) system-level oversight — a UN-backed AI Influence Observatory.\r\nCurrent browse context:\r\ncs.CY\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1647,"localImagePaths":1648,"remoteImagePaths":1649,"frontmatter":1650,"imagePaths":1652},[],[],[],{"title":1636,"description":1637,"pubDate":33,"source":17,"tags":1651,"url":1640},[19,20,21],[],"2025-06-10-how-malicious-ai-swarms-can-threaten-democracy-55680e.md","2025-06-10-how-significant-are-the-real-performance-gains-an-unbiased-evaluation-framework--413d54",{"id":1654,"data":1656,"body":1662,"filePath":1663,"digest":1664,"rendered":1665,"legacyId":1674},{"title":1657,"description":1658,"pubDate":1659,"source":17,"tags":1660,"url":1661},"How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG","arXiv:2506.06331v1 Announce Type: cross \nAbstract: By retrieving contexts from knowledge graphs, graph-based retrieval-augmented generation (GraphRAG) enhances large language models (LLMs) to generate quality answers for user questions. Many GraphRAG methods have been proposed and reported inspiring performance in answer quality. However, we observe that the current answer evaluation framework for GraphRAG has two critical flaws, i.e., unrelated questions and evaluation biases, which may lead to biased or even wrong conclusions on performance. To tackle the two flaws, we propose an unbiased evaluation framework that uses graph-text-grounded question generation to produce questions that are more related to the underlying dataset and an unbiased evaluation procedure to eliminate the biases in LLM-based answer assessment. We apply our unbiased framework to evaluate 3 representative GraphRAG methods and find that their performance gains are much more moderate than reported previously. Although our evaluation framework may still have flaws, it calls for scientific evaluations to lay solid foundations for GraphRAG research.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06331","Computer Science > Computation and Language\r\n[Submitted on 31 May 2025]\r\nTitle:How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG\r\nView PDF HTML (experimental)Abstract:By retrieving contexts from knowledge graphs, graph-based retrieval-augmented generation (GraphRAG) enhances large language models (LLMs) to generate quality answers for user questions. Many GraphRAG methods have been proposed and reported inspiring performance in answer quality. However, we observe that the current answer evaluation framework for GraphRAG has two critical flaws, i.e., unrelated questions and evaluation biases, which may lead to biased or even wrong conclusions on performance. To tackle the two flaws, we propose an unbiased evaluation framework that uses graph-text-grounded question generation to produce questions that are more related to the underlying dataset and an unbiased evaluation procedure to eliminate the biases in LLM-based answer assessment. We apply our unbiased framework to evaluate 3 representative GraphRAG methods and find that their performance gains are much more moderate than reported previously. Although our evaluation framework may still have flaws, it calls for scientific evaluations to lay solid foundations for GraphRAG research.\r\nCurrent browse context:\r\ncs.CL\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-how-significant-are-the-real-performance-gains-an-unbiased-evaluation-framework--413d54.md","29a14853478a2629",{"html":1666,"metadata":1667},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 31 May 2025]\r\nTitle:How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG\r\nView PDF HTML (experimental)Abstract:By retrieving contexts from knowledge graphs, graph-based retrieval-augmented generation (GraphRAG) enhances large language models (LLMs) to generate quality answers for user questions. Many GraphRAG methods have been proposed and reported inspiring performance in answer quality. However, we observe that the current answer evaluation framework for GraphRAG has two critical flaws, i.e., unrelated questions and evaluation biases, which may lead to biased or even wrong conclusions on performance. To tackle the two flaws, we propose an unbiased evaluation framework that uses graph-text-grounded question generation to produce questions that are more related to the underlying dataset and an unbiased evaluation procedure to eliminate the biases in LLM-based answer assessment. We apply our unbiased framework to evaluate 3 representative GraphRAG methods and find that their performance gains are much more moderate than reported previously. Although our evaluation framework may still have flaws, it calls for scientific evaluations to lay solid foundations for GraphRAG research.\r\nCurrent browse context:\r\ncs.CL\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1668,"localImagePaths":1669,"remoteImagePaths":1670,"frontmatter":1671,"imagePaths":1673},[],[],[],{"title":1657,"description":1658,"pubDate":33,"source":17,"tags":1672,"url":1661},[19,20,21],[],"2025-06-10-how-significant-are-the-real-performance-gains-an-unbiased-evaluation-framework--413d54.md","2025-06-10-human-and-ai-collaboration-in-fitness-educationa-longitudinal-study-with-a-pilat-69a636",{"id":1675,"data":1677,"body":1683,"filePath":1684,"digest":1685,"rendered":1686,"legacyId":1695},{"title":1678,"description":1679,"pubDate":1680,"source":17,"tags":1681,"url":1682},"Human and AI collaboration in Fitness Education:A Longitudinal Study with a Pilates Instructor","arXiv:2506.06383v1 Announce Type: cross \nAbstract: Artificial intelligence is poised to transform teaching and coaching practices,yet its optimal role alongside human expertise remains unclear.This study investigates human and AI collaboration in fitness education through a one year qualitative case study with a Pilates instructor.The researcher participated in the instructor classes and conducted biweekly semi structured interviews to explore how generative AI could be integrated into class planning and instruction.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06383","Computer Science > Computers and Society\r\n[Submitted on 5 Jun 2025]\r\nTitle:Human and AI collaboration in Fitness Education:A Longitudinal Study with a Pilates Instructor\r\nView PDFAbstract:Artificial intelligence is poised to transform teaching and coaching practices,yet its optimal role alongside human expertise remains this http URL study investigates human and AI collaboration in fitness education through a one year qualitative case study with a Pilates this http URL researcher participated in the instructor classes and conducted biweekly semi structured interviews to explore how generative AI could be integrated into class planning and instruction.\r\nSubmission history\r\nFrom: Qian Huang (Cathy) [view email][v1] Thu, 5 Jun 2025 04:04:34 UTC (1,513 KB)\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-human-and-ai-collaboration-in-fitness-educationa-longitudinal-study-with-a-pilat-69a636.md","09261308d18b88ff",{"html":1687,"metadata":1688},"\u003Cp>Computer Science > Computers and Society\r\n[Submitted on 5 Jun 2025]\r\nTitle:Human and AI collaboration in Fitness Education:A Longitudinal Study with a Pilates Instructor\r\nView PDFAbstract:Artificial intelligence is poised to transform teaching and coaching practices,yet its optimal role alongside human expertise remains this http URL study investigates human and AI collaboration in fitness education through a one year qualitative case study with a Pilates this http URL researcher participated in the instructor classes and conducted biweekly semi structured interviews to explore how generative AI could be integrated into class planning and instruction.\r\nSubmission history\r\nFrom: Qian Huang (Cathy) [view email][v1] Thu, 5 Jun 2025 04:04:34 UTC (1,513 KB)\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1689,"localImagePaths":1690,"remoteImagePaths":1691,"frontmatter":1692,"imagePaths":1694},[],[],[],{"title":1678,"description":1679,"pubDate":33,"source":17,"tags":1693,"url":1682},[19,20,21],[],"2025-06-10-human-and-ai-collaboration-in-fitness-educationa-longitudinal-study-with-a-pilat-69a636.md","2025-06-10-introduction-to-predictive-coding-networks-for-machine-learning-ad4bd8",{"id":1696,"data":1698,"body":1704,"filePath":1705,"digest":1706,"rendered":1707,"legacyId":1716},{"title":1699,"description":1700,"pubDate":1701,"source":17,"tags":1702,"url":1703},"Introduction to Predictive Coding Networks for Machine Learning","arXiv:2506.06332v1 Announce Type: cross \nAbstract: Predictive coding networks (PCNs) constitute a biologically inspired framework for understanding hierarchical computation in the brain, and offer an alternative to traditional feedforward neural networks in ML. This note serves as a quick, onboarding introduction to PCNs for machine learning practitioners. We cover the foundational network architecture, inference and learning update rules, and algorithmic implementation. A concrete image-classification task (CIFAR-10) is provided as a benchmark-smashing application, together with an accompanying Python notebook containing the PyTorch implementation.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06332","Computer Science > Neural and Evolutionary Computing\r\n[Submitted on 31 May 2025]\r\nTitle:Introduction to Predictive Coding Networks for Machine Learning\r\nView PDF HTML (experimental)Abstract:Predictive coding networks (PCNs) constitute a biologically inspired framework for understanding hierarchical computation in the brain, and offer an alternative to traditional feedforward neural networks in ML. This note serves as a quick, onboarding introduction to PCNs for machine learning practitioners. We cover the foundational network architecture, inference and learning update rules, and algorithmic implementation. A concrete image-classification task (CIFAR-10) is provided as a benchmark-smashing application, together with an accompanying Python notebook containing the PyTorch implementation.\r\nCurrent browse context:\r\ncs.NE\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-introduction-to-predictive-coding-networks-for-machine-learning-ad4bd8.md","9a37626a51553a9b",{"html":1708,"metadata":1709},"\u003Cp>Computer Science > Neural and Evolutionary Computing\r\n[Submitted on 31 May 2025]\r\nTitle:Introduction to Predictive Coding Networks for Machine Learning\r\nView PDF HTML (experimental)Abstract:Predictive coding networks (PCNs) constitute a biologically inspired framework for understanding hierarchical computation in the brain, and offer an alternative to traditional feedforward neural networks in ML. This note serves as a quick, onboarding introduction to PCNs for machine learning practitioners. We cover the foundational network architecture, inference and learning update rules, and algorithmic implementation. A concrete image-classification task (CIFAR-10) is provided as a benchmark-smashing application, together with an accompanying Python notebook containing the PyTorch implementation.\r\nCurrent browse context:\r\ncs.NE\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1710,"localImagePaths":1711,"remoteImagePaths":1712,"frontmatter":1713,"imagePaths":1715},[],[],[],{"title":1699,"description":1700,"pubDate":33,"source":17,"tags":1714,"url":1703},[19,20,21],[],"2025-06-10-introduction-to-predictive-coding-networks-for-machine-learning-ad4bd8.md","2025-06-10-improvement-of-optimization-using-learning-based-models-in-mixed-integer-linear--200f5b",{"id":1717,"data":1719,"body":1725,"filePath":1726,"digest":1727,"rendered":1728,"legacyId":1737},{"title":1720,"description":1721,"pubDate":1722,"source":17,"tags":1723,"url":1724},"Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks","arXiv:2506.06291v1 Announce Type: cross \nAbstract: Mixed Integer Linear Programs (MILPs) are essential tools for solving planning and scheduling problems across critical industries such as construction, manufacturing, and logistics. However, their widespread adoption is limited by long computational times, especially in large-scale, real-time scenarios. To address this, we present a learning-based framework that leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph Neural Networks (GNNs), producing high-quality initial solutions for warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling Problems. Experimental results demonstrate that our method reduces optimization time and variance compared to traditional techniques while maintaining solution quality and feasibility.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06291","Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks\r\nView PDF HTML (experimental)Abstract:Mixed Integer Linear Programs (MILPs) are essential tools for solving planning and scheduling problems across critical industries such as construction, manufacturing, and logistics. However, their widespread adoption is limited by long computational times, especially in large-scale, real-time scenarios. To address this, we present a learning-based framework that leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph Neural Networks (GNNs), producing high-quality initial solutions for warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling Problems. Experimental results demonstrate that our method reduces optimization time and variance compared to traditional techniques while maintaining solution quality and feasibility.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-improvement-of-optimization-using-learning-based-models-in-mixed-integer-linear--200f5b.md","cf5834437728972b",{"html":1729,"metadata":1730},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks\r\nView PDF HTML (experimental)Abstract:Mixed Integer Linear Programs (MILPs) are essential tools for solving planning and scheduling problems across critical industries such as construction, manufacturing, and logistics. However, their widespread adoption is limited by long computational times, especially in large-scale, real-time scenarios. To address this, we present a learning-based framework that leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph Neural Networks (GNNs), producing high-quality initial solutions for warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling Problems. Experimental results demonstrate that our method reduces optimization time and variance compared to traditional techniques while maintaining solution quality and feasibility.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1731,"localImagePaths":1732,"remoteImagePaths":1733,"frontmatter":1734,"imagePaths":1736},[],[],[],{"title":1720,"description":1721,"pubDate":33,"source":17,"tags":1735,"url":1724},[19,20,21],[],"2025-06-10-improvement-of-optimization-using-learning-based-models-in-mixed-integer-linear--200f5b.md","2025-06-10-integrating-ai-planning-semantics-into-sysml-system-models-for-automated-pddl-fi-01340a",{"id":1738,"data":1740,"body":1746,"filePath":1747,"digest":1748,"rendered":1749,"legacyId":1758},{"title":1741,"description":1742,"pubDate":1743,"source":17,"tags":1744,"url":1745},"Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation","arXiv:2506.06714v1 Announce Type: new \nAbstract: This paper presents a SysML profile that enables the direct integration of planning semantics based on the Planning Domain Definition Language (PDDL) into system models. Reusable stereotypes are defined for key PDDL concepts such as types, predicates, functions and actions, while formal OCL constraints ensure syntactic consistency. The profile was derived from the Backus-Naur Form (BNF) definition of PDDL 3.1 to align with SysML modeling practices. A case study from aircraft manufacturing demonstrates the application of the profile: a robotic system with interchangeable end effectors is modeled and enriched to generate both domain and problem descriptions in PDDL format. These are used as input to a PDDL solver to derive optimized execution plans. The approach supports automated and model-based generation of planning descriptions and provides a reusable bridge between system modeling and AI planning in engineering design.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06714","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation\r\nView PDF HTML (experimental)Abstract:This paper presents a SysML profile that enables the direct integration of planning semantics based on the Planning Domain Definition Language (PDDL) into system models. Reusable stereotypes are defined for key PDDL concepts such as types, predicates, functions and actions, while formal OCL constraints ensure syntactic consistency. The profile was derived from the Backus-Naur Form (BNF) definition of PDDL 3.1 to align with SysML modeling practices. A case study from aircraft manufacturing demonstrates the application of the profile: a robotic system with interchangeable end effectors is modeled and enriched to generate both domain and problem descriptions in PDDL format. These are used as input to a PDDL solver to derive optimized execution plans. The approach supports automated and model-based generation of planning descriptions and provides a reusable bridge between system modeling and AI planning in engineering design.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-integrating-ai-planning-semantics-into-sysml-system-models-for-automated-pddl-fi-01340a.md","64170aecd7ad06c6",{"html":1750,"metadata":1751},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation\r\nView PDF HTML (experimental)Abstract:This paper presents a SysML profile that enables the direct integration of planning semantics based on the Planning Domain Definition Language (PDDL) into system models. Reusable stereotypes are defined for key PDDL concepts such as types, predicates, functions and actions, while formal OCL constraints ensure syntactic consistency. The profile was derived from the Backus-Naur Form (BNF) definition of PDDL 3.1 to align with SysML modeling practices. A case study from aircraft manufacturing demonstrates the application of the profile: a robotic system with interchangeable end effectors is modeled and enriched to generate both domain and problem descriptions in PDDL format. These are used as input to a PDDL solver to derive optimized execution plans. The approach supports automated and model-based generation of planning descriptions and provides a reusable bridge between system modeling and AI planning in engineering design.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1752,"localImagePaths":1753,"remoteImagePaths":1754,"frontmatter":1755,"imagePaths":1757},[],[],[],{"title":1741,"description":1742,"pubDate":33,"source":17,"tags":1756,"url":1745},[19,20,21],[],"2025-06-10-integrating-ai-planning-semantics-into-sysml-system-models-for-automated-pddl-fi-01340a.md","2025-06-10-incorporating-failure-of-machine-learning-in-dynamic-probabilistic-safety-assura-1b8298",{"id":1759,"data":1761,"body":1767,"filePath":1768,"digest":1769,"rendered":1770,"legacyId":1779},{"title":1762,"description":1763,"pubDate":1764,"source":17,"tags":1765,"url":1766},"Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance","arXiv:2506.06868v1 Announce Type: new \nAbstract: Machine Learning (ML) models are increasingly integrated into safety-critical systems, such as autonomous vehicle platooning, to enable real-time decision-making. However, their inherent imperfection introduces a new class of failure: reasoning failures often triggered by distributional shifts between operational and training data. Traditional safety assessment methods, which rely on design artefacts or code, are ill-suited for ML components that learn behaviour from data. SafeML was recently proposed to dynamically detect such shifts and assign confidence levels to the reasoning of ML-based components. Building on this, we introduce a probabilistic safety assurance framework that integrates SafeML with Bayesian Networks (BNs) to model ML failures as part of a broader causal safety analysis. This allows for dynamic safety evaluation and system adaptation under uncertainty. We demonstrate the approach on an simulated automotive platooning system with traffic sign recognition. The findings highlight the potential broader benefits of explicitly modelling ML failures in safety assessment.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06868","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance\r\nView PDF HTML (experimental)Abstract:Machine Learning (ML) models are increasingly integrated into safety-critical systems, such as autonomous vehicle platooning, to enable real-time decision-making. However, their inherent imperfection introduces a new class of failure: reasoning failures often triggered by distributional shifts between operational and training data. Traditional safety assessment methods, which rely on design artefacts or code, are ill-suited for ML components that learn behaviour from data. SafeML was recently proposed to dynamically detect such shifts and assign confidence levels to the reasoning of ML-based components. Building on this, we introduce a probabilistic safety assurance framework that integrates SafeML with Bayesian Networks (BNs) to model ML failures as part of a broader causal safety analysis. This allows for dynamic safety evaluation and system adaptation under uncertainty. We demonstrate the approach on an simulated automotive platooning system with traffic sign recognition. The findings highlight the potential broader benefits of explicitly modelling ML failures in safety assessment.\r\nSubmission history\r\nFrom: Razieh Arshadizadeh [view email][v1] Sat, 7 Jun 2025 17:16:05 UTC (3,186 KB)\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-incorporating-failure-of-machine-learning-in-dynamic-probabilistic-safety-assura-1b8298.md","3375ed85d122b92b",{"html":1771,"metadata":1772},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance\r\nView PDF HTML (experimental)Abstract:Machine Learning (ML) models are increasingly integrated into safety-critical systems, such as autonomous vehicle platooning, to enable real-time decision-making. However, their inherent imperfection introduces a new class of failure: reasoning failures often triggered by distributional shifts between operational and training data. Traditional safety assessment methods, which rely on design artefacts or code, are ill-suited for ML components that learn behaviour from data. SafeML was recently proposed to dynamically detect such shifts and assign confidence levels to the reasoning of ML-based components. Building on this, we introduce a probabilistic safety assurance framework that integrates SafeML with Bayesian Networks (BNs) to model ML failures as part of a broader causal safety analysis. This allows for dynamic safety evaluation and system adaptation under uncertainty. We demonstrate the approach on an simulated automotive platooning system with traffic sign recognition. The findings highlight the potential broader benefits of explicitly modelling ML failures in safety assessment.\r\nSubmission history\r\nFrom: Razieh Arshadizadeh [view email][v1] Sat, 7 Jun 2025 17:16:05 UTC (3,186 KB)\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1773,"localImagePaths":1774,"remoteImagePaths":1775,"frontmatter":1776,"imagePaths":1778},[],[],[],{"title":1762,"description":1763,"pubDate":33,"source":17,"tags":1777,"url":1766},[19,20,21],[],"2025-06-10-incorporating-failure-of-machine-learning-in-dynamic-probabilistic-safety-assura-1b8298.md","2025-06-10-knowcoder-v2-deep-knowledge-analysis-fef1e5",{"id":1780,"data":1782,"body":1788,"filePath":1789,"digest":1790,"rendered":1791,"legacyId":1800},{"title":1783,"description":1784,"pubDate":1785,"source":17,"tags":1786,"url":1787},"KnowCoder-V2: Deep Knowledge Analysis","arXiv:2506.06881v1 Announce Type: new \nAbstract: Deep knowledge analysis tasks always involve the systematic extraction and association of knowledge from large volumes of data, followed by logical reasoning to discover insights. However, to solve such complex tasks, existing deep research frameworks face three major challenges: 1) They lack systematic organization and management of knowledge; 2) They operate purely online, making it inefficient for tasks that rely on shared and large-scale knowledge; 3) They cannot perform complex knowledge computation, limiting their abilities to produce insightful analytical results. Motivated by these, in this paper, we propose a \\textbf{K}nowledgeable \\textbf{D}eep \\textbf{R}esearch (\\textbf{KDR}) framework that empowers deep research with deep knowledge analysis capability. Specifically, it introduces an independent knowledge organization phase to preprocess large-scale, domain-relevant data into systematic knowledge offline. Based on this knowledge, it extends deep research with an additional kind of reasoning steps that perform complex knowledge computation in an online manner. To enhance the abilities of LLMs to solve knowledge analysis tasks in the above framework, we further introduce \\textbf{\\KCII}, an LLM that bridges knowledge organization and reasoning via unified code generation. For knowledge organization, it generates instantiation code for predefined classes, transforming data into knowledge objects. For knowledge computation, it generates analysis code and executes on the above knowledge objects to obtain deep analysis results. Experimental results on more than thirty datasets across six knowledge analysis tasks demonstrate the effectiveness of \\KCII. Moreover, when integrated into the KDR framework, \\KCII can generate high-quality reports with insightful analytical results compared to the mainstream deep research framework.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06881","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:KnowCoder-V2: Deep Knowledge Analysis\r\nView PDFAbstract:Deep knowledge analysis tasks always involve the systematic extraction and association of knowledge from large volumes of data, followed by logical reasoning to discover insights. However, to solve such complex tasks, existing deep research frameworks face three major challenges: 1) They lack systematic organization and management of knowledge; 2) They operate purely online, making it inefficient for tasks that rely on shared and large-scale knowledge; 3) They cannot perform complex knowledge computation, limiting their abilities to produce insightful analytical results. Motivated by these, in this paper, we propose a \\textbf{K}nowledgeable \\textbf{D}eep \\textbf{R}esearch (\\textbf{KDR}) framework that empowers deep research with deep knowledge analysis capability. Specifically, it introduces an independent knowledge organization phase to preprocess large-scale, domain-relevant data into systematic knowledge offline. Based on this knowledge, it extends deep research with an additional kind of reasoning steps that perform complex knowledge computation in an online manner. To enhance the abilities of LLMs to solve knowledge analysis tasks in the above framework, we further introduce \\textbf{\\KCII}, an LLM that bridges knowledge organization and reasoning via unified code generation. For knowledge organization, it generates instantiation code for predefined classes, transforming data into knowledge objects. For knowledge computation, it generates analysis code and executes on the above knowledge objects to obtain deep analysis results. Experimental results on more than thirty datasets across six knowledge analysis tasks demonstrate the effectiveness of \\KCII. Moreover, when integrated into the KDR framework, \\KCII can generate high-quality reports with insightful analytical results compared to the mainstream deep research framework.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-knowcoder-v2-deep-knowledge-analysis-fef1e5.md","d790448bcf88761a",{"html":1792,"metadata":1793},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:KnowCoder-V2: Deep Knowledge Analysis\r\nView PDFAbstract:Deep knowledge analysis tasks always involve the systematic extraction and association of knowledge from large volumes of data, followed by logical reasoning to discover insights. However, to solve such complex tasks, existing deep research frameworks face three major challenges: 1) They lack systematic organization and management of knowledge; 2) They operate purely online, making it inefficient for tasks that rely on shared and large-scale knowledge; 3) They cannot perform complex knowledge computation, limiting their abilities to produce insightful analytical results. Motivated by these, in this paper, we propose a \\textbf{K}nowledgeable \\textbf{D}eep \\textbf{R}esearch (\\textbf{KDR}) framework that empowers deep research with deep knowledge analysis capability. Specifically, it introduces an independent knowledge organization phase to preprocess large-scale, domain-relevant data into systematic knowledge offline. Based on this knowledge, it extends deep research with an additional kind of reasoning steps that perform complex knowledge computation in an online manner. To enhance the abilities of LLMs to solve knowledge analysis tasks in the above framework, we further introduce \\textbf{\\KCII}, an LLM that bridges knowledge organization and reasoning via unified code generation. For knowledge organization, it generates instantiation code for predefined classes, transforming data into knowledge objects. For knowledge computation, it generates analysis code and executes on the above knowledge objects to obtain deep analysis results. Experimental results on more than thirty datasets across six knowledge analysis tasks demonstrate the effectiveness of \\KCII. Moreover, when integrated into the KDR framework, \\KCII can generate high-quality reports with insightful analytical results compared to the mainstream deep research framework.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1794,"localImagePaths":1795,"remoteImagePaths":1796,"frontmatter":1797,"imagePaths":1799},[],[],[],{"title":1783,"description":1784,"pubDate":33,"source":17,"tags":1798,"url":1787},[19,20,21],[],"2025-06-10-knowcoder-v2-deep-knowledge-analysis-fef1e5.md","2025-06-10-kramabench-a-benchmark-for-ai-systems-on-data-to-insight-pipelines-over-data-lak-8db291",{"id":1801,"data":1803,"body":1809,"filePath":1810,"digest":1811,"rendered":1812,"legacyId":1821},{"title":1804,"description":1805,"pubDate":1806,"source":17,"tags":1807,"url":1808},"KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes","arXiv:2506.06541v1 Announce Type: cross \nAbstract: Constructing real-world data-to-insight pipelines often involves data extraction from data lakes, data integration across heterogeneous data sources, and diverse operations from data cleaning to analysis. The design and implementation of data science pipelines require domain knowledge, technical expertise, and even project-specific insights. AI systems have shown remarkable reasoning, coding, and understanding capabilities. However, it remains unclear to what extent these capabilities translate into successful design and execution of such complex pipelines. We introduce KRAMABENCH: a benchmark composed of 104 manually-curated real-world data science pipelines spanning 1700 data files from 24 data sources in 6 different domains. We show that these pipelines test the end-to-end capabilities of AI systems on data processing, requiring data discovery, wrangling and cleaning, efficient processing, statistical reasoning, and orchestrating data processing steps given a high-level task. Our evaluation tests 5 general models and 3 code generation models using our reference framework, DS-GURU, which instructs the AI model to decompose a question into a sequence of subtasks, reason through each step, and synthesize Python code that implements the proposed design. Our results on KRAMABENCH show that, although the models are sufficiently capable of solving well-specified data science code generation tasks, when extensive data processing and domain knowledge are required to construct real-world data science pipelines, existing out-of-box models fall short. Progress on KramaBench represents crucial steps towards developing autonomous data science agents for real-world applications. Our code, reference framework, and data are available at https://github.com/mitdbg/KramaBench.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06541","Computer Science > Databases\r\n[Submitted on 6 Jun 2025]\r\nTitle:KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes\r\nView PDF HTML (experimental)Abstract:Constructing real-world data-to-insight pipelines often involves data extraction from data lakes, data integration across heterogeneous data sources, and diverse operations from data cleaning to analysis. The design and implementation of data science pipelines require domain knowledge, technical expertise, and even project-specific insights. AI systems have shown remarkable reasoning, coding, and understanding capabilities. However, it remains unclear to what extent these capabilities translate into successful design and execution of such complex pipelines. We introduce KRAMABENCH: a benchmark composed of 104 manually-curated real-world data science pipelines spanning 1700 data files from 24 data sources in 6 different domains. We show that these pipelines test the end-to-end capabilities of AI systems on data processing, requiring data discovery, wrangling and cleaning, efficient processing, statistical reasoning, and orchestrating data processing steps given a high-level task. Our evaluation tests 5 general models and 3 code generation models using our reference framework, DS-GURU, which instructs the AI model to decompose a question into a sequence of subtasks, reason through each step, and synthesize Python code that implements the proposed design. Our results on KRAMABENCH show that, although the models are sufficiently capable of solving well-specified data science code generation tasks, when extensive data processing and domain knowledge are required to construct real-world data science pipelines, existing out-of-box models fall short. Progress on KramaBench represents crucial steps towards developing autonomous data science agents for real-world applications. Our code, reference framework, and data are available at this https URL.\r\nCurrent browse context:\r\ncs.DB\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-kramabench-a-benchmark-for-ai-systems-on-data-to-insight-pipelines-over-data-lak-8db291.md","7896f77e19750bb8",{"html":1813,"metadata":1814},"\u003Cp>Computer Science > Databases\r\n[Submitted on 6 Jun 2025]\r\nTitle:KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes\r\nView PDF HTML (experimental)Abstract:Constructing real-world data-to-insight pipelines often involves data extraction from data lakes, data integration across heterogeneous data sources, and diverse operations from data cleaning to analysis. The design and implementation of data science pipelines require domain knowledge, technical expertise, and even project-specific insights. AI systems have shown remarkable reasoning, coding, and understanding capabilities. However, it remains unclear to what extent these capabilities translate into successful design and execution of such complex pipelines. We introduce KRAMABENCH: a benchmark composed of 104 manually-curated real-world data science pipelines spanning 1700 data files from 24 data sources in 6 different domains. We show that these pipelines test the end-to-end capabilities of AI systems on data processing, requiring data discovery, wrangling and cleaning, efficient processing, statistical reasoning, and orchestrating data processing steps given a high-level task. Our evaluation tests 5 general models and 3 code generation models using our reference framework, DS-GURU, which instructs the AI model to decompose a question into a sequence of subtasks, reason through each step, and synthesize Python code that implements the proposed design. Our results on KRAMABENCH show that, although the models are sufficiently capable of solving well-specified data science code generation tasks, when extensive data processing and domain knowledge are required to construct real-world data science pipelines, existing out-of-box models fall short. Progress on KramaBench represents crucial steps towards developing autonomous data science agents for real-world applications. Our code, reference framework, and data are available at this https URL.\r\nCurrent browse context:\r\ncs.DB\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1815,"localImagePaths":1816,"remoteImagePaths":1817,"frontmatter":1818,"imagePaths":1820},[],[],[],{"title":1804,"description":1805,"pubDate":33,"source":17,"tags":1819,"url":1808},[19,20,21],[],"2025-06-10-kramabench-a-benchmark-for-ai-systems-on-data-to-insight-pipelines-over-data-lak-8db291.md","2025-06-10-large-language-models-and-their-applications-in-roadway-safety-and-mobility-enha-610ee6",{"id":1822,"data":1824,"body":1830,"filePath":1831,"digest":1832,"rendered":1833,"legacyId":1842},{"title":1825,"description":1826,"pubDate":1827,"source":17,"tags":1828,"url":1829},"Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review","arXiv:2506.06301v1 Announce Type: new \nAbstract: Roadway safety and mobility remain critical challenges for modern transportation systems, demanding innovative analytical frameworks capable of addressing complex, dynamic, and heterogeneous environments. While traditional engineering methods have made progress, the complexity and dynamism of real-world traffic necessitate more advanced analytical frameworks. Large Language Models (LLMs), with their unprecedented capabilities in natural language understanding, knowledge integration, and reasoning, represent a promising paradigm shift. This paper comprehensively reviews the application and customization of LLMs for enhancing roadway safety and mobility. A key focus is how LLMs are adapted -- via architectural, training, prompting, and multimodal strategies -- to bridge the \"modality gap\" with transportation's unique spatio-temporal and physical data. The review systematically analyzes diverse LLM applications in mobility (e.g., traffic flow prediction, signal control) and safety (e.g., crash analysis, driver behavior assessment,). Enabling technologies such as V2X integration, domain-specific foundation models, explainability frameworks, and edge computing are also examined. Despite significant potential, challenges persist regarding inherent LLM limitations (hallucinations, reasoning deficits), data governance (privacy, bias), deployment complexities (sim-to-real, latency), and rigorous safety assurance. Promising future research directions are highlighted, including advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI collaboration, continuous learning, and the development of efficient, verifiable systems. This review provides a structured roadmap of current capabilities, limitations, and opportunities, underscoring LLMs' transformative potential while emphasizing the need for responsible innovation to realize safer, more intelligent transportation systems.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06301","Computer Science > Artificial Intelligence\r\n[Submitted on 19 May 2025]\r\nTitle:Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review\r\nView PDF HTML (experimental)Abstract:Roadway safety and mobility remain critical challenges for modern transportation systems, demanding innovative analytical frameworks capable of addressing complex, dynamic, and heterogeneous environments. While traditional engineering methods have made progress, the complexity and dynamism of real-world traffic necessitate more advanced analytical frameworks. Large Language Models (LLMs), with their unprecedented capabilities in natural language understanding, knowledge integration, and reasoning, represent a promising paradigm shift. This paper comprehensively reviews the application and customization of LLMs for enhancing roadway safety and mobility. A key focus is how LLMs are adapted -- via architectural, training, prompting, and multimodal strategies -- to bridge the \"modality gap\" with transportation's unique spatio-temporal and physical data. The review systematically analyzes diverse LLM applications in mobility (e.g., traffic flow prediction, signal control) and safety (e.g., crash analysis, driver behavior assessment,). Enabling technologies such as V2X integration, domain-specific foundation models, explainability frameworks, and edge computing are also examined. Despite significant potential, challenges persist regarding inherent LLM limitations (hallucinations, reasoning deficits), data governance (privacy, bias), deployment complexities (sim-to-real, latency), and rigorous safety assurance. Promising future research directions are highlighted, including advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI collaboration, continuous learning, and the development of efficient, verifiable systems. This review provides a structured roadmap of current capabilities, limitations, and opportunities, underscoring LLMs' transformative potential while emphasizing the need for responsible innovation to realize safer, more intelligent transportation systems.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-large-language-models-and-their-applications-in-roadway-safety-and-mobility-enha-610ee6.md","23789b49d711a90b",{"html":1834,"metadata":1835},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 19 May 2025]\r\nTitle:Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review\r\nView PDF HTML (experimental)Abstract:Roadway safety and mobility remain critical challenges for modern transportation systems, demanding innovative analytical frameworks capable of addressing complex, dynamic, and heterogeneous environments. While traditional engineering methods have made progress, the complexity and dynamism of real-world traffic necessitate more advanced analytical frameworks. Large Language Models (LLMs), with their unprecedented capabilities in natural language understanding, knowledge integration, and reasoning, represent a promising paradigm shift. This paper comprehensively reviews the application and customization of LLMs for enhancing roadway safety and mobility. A key focus is how LLMs are adapted — via architectural, training, prompting, and multimodal strategies — to bridge the “modality gap” with transportation’s unique spatio-temporal and physical data. The review systematically analyzes diverse LLM applications in mobility (e.g., traffic flow prediction, signal control) and safety (e.g., crash analysis, driver behavior assessment,). Enabling technologies such as V2X integration, domain-specific foundation models, explainability frameworks, and edge computing are also examined. Despite significant potential, challenges persist regarding inherent LLM limitations (hallucinations, reasoning deficits), data governance (privacy, bias), deployment complexities (sim-to-real, latency), and rigorous safety assurance. Promising future research directions are highlighted, including advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI collaboration, continuous learning, and the development of efficient, verifiable systems. This review provides a structured roadmap of current capabilities, limitations, and opportunities, underscoring LLMs’ transformative potential while emphasizing the need for responsible innovation to realize safer, more intelligent transportation systems.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1836,"localImagePaths":1837,"remoteImagePaths":1838,"frontmatter":1839,"imagePaths":1841},[],[],[],{"title":1825,"description":1826,"pubDate":33,"source":17,"tags":1840,"url":1829},[19,20,21],[],"2025-06-10-large-language-models-and-their-applications-in-roadway-safety-and-mobility-enha-610ee6.md","2025-06-10-lamp-cap-personalized-figure-caption-generation-with-multimodal-figure-profiles-aeda3e",{"id":1843,"data":1845,"body":1851,"filePath":1852,"digest":1853,"rendered":1854,"legacyId":1863},{"title":1846,"description":1847,"pubDate":1848,"source":17,"tags":1849,"url":1850},"LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles","arXiv:2506.06561v1 Announce Type: cross \nAbstract: Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06561","Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles\r\nView PDF HTML (experimental)Abstract:Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-lamp-cap-personalized-figure-caption-generation-with-multimodal-figure-profiles-aeda3e.md","bedd886bb3050051",{"html":1855,"metadata":1856},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles\r\nView PDF HTML (experimental)Abstract:Figure captions are crucial for helping readers understand and remember a figure’s key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain’s style, highlighting the need for personalization. Despite language models’ personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document—each with its image, caption, and figure-mentioning paragraphs—as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1857,"localImagePaths":1858,"remoteImagePaths":1859,"frontmatter":1860,"imagePaths":1862},[],[],[],{"title":1846,"description":1847,"pubDate":33,"source":17,"tags":1861,"url":1850},[19,20,21],[],"2025-06-10-lamp-cap-personalized-figure-caption-generation-with-multimodal-figure-profiles-aeda3e.md","2025-06-10-large-language-models-can-be-a-viable-substitute-for-expert-political-surveys-wh-204aa8",{"id":1864,"data":1866,"body":1872,"filePath":1873,"digest":1874,"rendered":1875,"legacyId":1884},{"title":1867,"description":1868,"pubDate":1869,"source":17,"tags":1870,"url":1871},"Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches","arXiv:2506.06540v1 Announce Type: cross \nAbstract: After a disruptive event or shock, such as the Department of Government Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by knowledge of the outcome. This can make it difficult or impossible to reconstruct the pre-event perceptions needed to study the factors associated with the event. This position paper argues that large language models (LLMs), trained on vast amounts of digital media data, can be a viable substitute for expert political surveys when a shock disrupts traditional measurement. We analyze the DOGE layoffs as a specific case study for this position. We use pairwise comparison prompts with LLMs and derive ideology scores for federal executive agencies. These scores replicate pre-layoff expert measures and predict which agencies were targeted by DOGE. We also use this same approach and find that the perceptions of certain federal agencies as knowledge institutions predict which agencies were targeted by DOGE, even when controlling for ideology. This case study demonstrates that using LLMs allows us to rapidly and easily test the associated factors hypothesized behind the shock. More broadly, our case study of this recent event exemplifies how LLMs offer insights into the correlational factors of the shock when traditional measurement techniques fail. We conclude by proposing a two-part criterion for when researchers can turn to LLMs as a substitute for expert political surveys.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06540","Computer Science > Computers and Society\r\n[Submitted on 6 Jun 2025]\r\nTitle:Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches\r\nView PDF HTML (experimental)Abstract:After a disruptive event or shock, such as the Department of Government Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by knowledge of the outcome. This can make it difficult or impossible to reconstruct the pre-event perceptions needed to study the factors associated with the event. This position paper argues that large language models (LLMs), trained on vast amounts of digital media data, can be a viable substitute for expert political surveys when a shock disrupts traditional measurement. We analyze the DOGE layoffs as a specific case study for this position. We use pairwise comparison prompts with LLMs and derive ideology scores for federal executive agencies. These scores replicate pre-layoff expert measures and predict which agencies were targeted by DOGE. We also use this same approach and find that the perceptions of certain federal agencies as knowledge institutions predict which agencies were targeted by DOGE, even when controlling for ideology. This case study demonstrates that using LLMs allows us to rapidly and easily test the associated factors hypothesized behind the shock. More broadly, our case study of this recent event exemplifies how LLMs offer insights into the correlational factors of the shock when traditional measurement techniques fail. We conclude by proposing a two-part criterion for when researchers can turn to LLMs as a substitute for expert political surveys.\r\nCurrent browse context:\r\ncs.CY\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-large-language-models-can-be-a-viable-substitute-for-expert-political-surveys-wh-204aa8.md","0d54dd19fa464395",{"html":1876,"metadata":1877},"\u003Cp>Computer Science > Computers and Society\r\n[Submitted on 6 Jun 2025]\r\nTitle:Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches\r\nView PDF HTML (experimental)Abstract:After a disruptive event or shock, such as the Department of Government Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by knowledge of the outcome. This can make it difficult or impossible to reconstruct the pre-event perceptions needed to study the factors associated with the event. This position paper argues that large language models (LLMs), trained on vast amounts of digital media data, can be a viable substitute for expert political surveys when a shock disrupts traditional measurement. We analyze the DOGE layoffs as a specific case study for this position. We use pairwise comparison prompts with LLMs and derive ideology scores for federal executive agencies. These scores replicate pre-layoff expert measures and predict which agencies were targeted by DOGE. We also use this same approach and find that the perceptions of certain federal agencies as knowledge institutions predict which agencies were targeted by DOGE, even when controlling for ideology. This case study demonstrates that using LLMs allows us to rapidly and easily test the associated factors hypothesized behind the shock. More broadly, our case study of this recent event exemplifies how LLMs offer insights into the correlational factors of the shock when traditional measurement techniques fail. We conclude by proposing a two-part criterion for when researchers can turn to LLMs as a substitute for expert political surveys.\r\nCurrent browse context:\r\ncs.CY\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1878,"localImagePaths":1879,"remoteImagePaths":1880,"frontmatter":1881,"imagePaths":1883},[],[],[],{"title":1867,"description":1868,"pubDate":33,"source":17,"tags":1882,"url":1871},[19,20,21],[],"2025-06-10-large-language-models-can-be-a-viable-substitute-for-expert-political-surveys-wh-204aa8.md","2025-06-10-large-language-models-for-eeg-a-comprehensive-survey-and-taxonomy-43ac86",{"id":1885,"data":1887,"body":1893,"filePath":1894,"digest":1895,"rendered":1896,"legacyId":1905},{"title":1888,"description":1889,"pubDate":1890,"source":17,"tags":1891,"url":1892},"Large Language Models for EEG: A Comprehensive Survey and Taxonomy","arXiv:2506.06353v1 Announce Type: cross \nAbstract: The growing convergence between Large Language Models (LLMs) and electroencephalography (EEG) research is enabling new directions in neural decoding, brain-computer interfaces (BCIs), and affective computing. This survey offers a systematic review and structured taxonomy of recent advancements that utilize LLMs for EEG-based analysis and applications. We organize the literature into four domains: (1) LLM-inspired foundation models for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal generation including image and 3D object synthesis, and (4) clinical applications and dataset management tools. The survey highlights how transformer-based architectures adapted through fine-tuning, few-shot, and zero-shot learning have enabled EEG-based models to perform complex tasks such as natural language generation, semantic interpretation, and diagnostic assistance. By offering a structured overview of modeling strategies, system designs, and application areas, this work serves as a foundational resource for future work to bridge natural language processing and neural signal analysis through language models.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06353","Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 2 Jun 2025]\r\nTitle:Large Language Models for EEG: A Comprehensive Survey and Taxonomy\r\nView PDF HTML (experimental)Abstract:The growing convergence between Large Language Models (LLMs) and electroencephalography (EEG) research is enabling new directions in neural decoding, brain-computer interfaces (BCIs), and affective computing. This survey offers a systematic review and structured taxonomy of recent advancements that utilize LLMs for EEG-based analysis and applications. We organize the literature into four domains: (1) LLM-inspired foundation models for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal generation including image and 3D object synthesis, and (4) clinical applications and dataset management tools. The survey highlights how transformer-based architectures adapted through fine-tuning, few-shot, and zero-shot learning have enabled EEG-based models to perform complex tasks such as natural language generation, semantic interpretation, and diagnostic assistance. By offering a structured overview of modeling strategies, system designs, and application areas, this work serves as a foundational resource for future work to bridge natural language processing and neural signal analysis through language models.\r\nCurrent browse context:\r\neess.SP\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-large-language-models-for-eeg-a-comprehensive-survey-and-taxonomy-43ac86.md","7d1d1fea041610af",{"html":1897,"metadata":1898},"\u003Cp>Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 2 Jun 2025]\r\nTitle:Large Language Models for EEG: A Comprehensive Survey and Taxonomy\r\nView PDF HTML (experimental)Abstract:The growing convergence between Large Language Models (LLMs) and electroencephalography (EEG) research is enabling new directions in neural decoding, brain-computer interfaces (BCIs), and affective computing. This survey offers a systematic review and structured taxonomy of recent advancements that utilize LLMs for EEG-based analysis and applications. We organize the literature into four domains: (1) LLM-inspired foundation models for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal generation including image and 3D object synthesis, and (4) clinical applications and dataset management tools. The survey highlights how transformer-based architectures adapted through fine-tuning, few-shot, and zero-shot learning have enabled EEG-based models to perform complex tasks such as natural language generation, semantic interpretation, and diagnostic assistance. By offering a structured overview of modeling strategies, system designs, and application areas, this work serves as a foundational resource for future work to bridge natural language processing and neural signal analysis through language models.\r\nCurrent browse context:\r\neess.SP\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1899,"localImagePaths":1900,"remoteImagePaths":1901,"frontmatter":1902,"imagePaths":1904},[],[],[],{"title":1888,"description":1889,"pubDate":33,"source":17,"tags":1903,"url":1892},[19,20,21],[],"2025-06-10-large-language-models-for-eeg-a-comprehensive-survey-and-taxonomy-43ac86.md","2025-06-10-learning-what-matters-now-a-dual-critic-context-aware-rl-framework-for-priority--42f7f3",{"id":1906,"data":1908,"body":1914,"filePath":1915,"digest":1916,"rendered":1917,"legacyId":1926},{"title":1909,"description":1910,"pubDate":1911,"source":17,"tags":1912,"url":1913},"Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain","arXiv:2506.06786v1 Announce Type: new \nAbstract: Autonomous systems operating in high-stakes search-and-rescue (SAR) missions must continuously gather mission-critical information while flexibly adapting to shifting operational priorities. We propose CA-MIQ (Context-Aware Max-Information Q-learning), a lightweight dual-critic reinforcement learning (RL) framework that dynamically adjusts its exploration strategy whenever mission priorities change. CA-MIQ pairs a standard extrinsic critic for task reward with an intrinsic critic that fuses state-novelty, information-location awareness, and real-time priority alignment. A built-in shift detector triggers transient exploration boosts and selective critic resets, allowing the agent to re-focus after a priority revision. In a simulated SAR grid-world, where experiments specifically test adaptation to changes in the priority order of information types the agent is expected to focus on, CA-MIQ achieves nearly four times higher mission-success rates than baselines after a single priority shift and more than three times better performance in multiple-shift scenarios, achieving 100% recovery while baseline methods fail to adapt. These results highlight CA-MIQ's effectiveness in any discrete environment with piecewise-stationary information-value distributions.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06786","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain\r\nView PDF HTML (experimental)Abstract:Autonomous systems operating in high-stakes search-and-rescue (SAR) missions must continuously gather mission-critical information while flexibly adapting to shifting operational priorities. We propose CA-MIQ (Context-Aware Max-Information Q-learning), a lightweight dual-critic reinforcement learning (RL) framework that dynamically adjusts its exploration strategy whenever mission priorities change. CA-MIQ pairs a standard extrinsic critic for task reward with an intrinsic critic that fuses state-novelty, information-location awareness, and real-time priority alignment. A built-in shift detector triggers transient exploration boosts and selective critic resets, allowing the agent to re-focus after a priority revision. In a simulated SAR grid-world, where experiments specifically test adaptation to changes in the priority order of information types the agent is expected to focus on, CA-MIQ achieves nearly four times higher mission-success rates than baselines after a single priority shift and more than three times better performance in multiple-shift scenarios, achieving 100% recovery while baseline methods fail to adapt. These results highlight CA-MIQ's effectiveness in any discrete environment with piecewise-stationary information-value distributions.\r\nSubmission history\r\nFrom: Dimitris A. Panagopoulos [view email][v1] Sat, 7 Jun 2025 12:55:10 UTC (3,453 KB)\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-learning-what-matters-now-a-dual-critic-context-aware-rl-framework-for-priority--42f7f3.md","7249cd5077793435",{"html":1918,"metadata":1919},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain\r\nView PDF HTML (experimental)Abstract:Autonomous systems operating in high-stakes search-and-rescue (SAR) missions must continuously gather mission-critical information while flexibly adapting to shifting operational priorities. We propose CA-MIQ (Context-Aware Max-Information Q-learning), a lightweight dual-critic reinforcement learning (RL) framework that dynamically adjusts its exploration strategy whenever mission priorities change. CA-MIQ pairs a standard extrinsic critic for task reward with an intrinsic critic that fuses state-novelty, information-location awareness, and real-time priority alignment. A built-in shift detector triggers transient exploration boosts and selective critic resets, allowing the agent to re-focus after a priority revision. In a simulated SAR grid-world, where experiments specifically test adaptation to changes in the priority order of information types the agent is expected to focus on, CA-MIQ achieves nearly four times higher mission-success rates than baselines after a single priority shift and more than three times better performance in multiple-shift scenarios, achieving 100% recovery while baseline methods fail to adapt. These results highlight CA-MIQ’s effectiveness in any discrete environment with piecewise-stationary information-value distributions.\r\nSubmission history\r\nFrom: Dimitris A. Panagopoulos [view email][v1] Sat, 7 Jun 2025 12:55:10 UTC (3,453 KB)\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1920,"localImagePaths":1921,"remoteImagePaths":1922,"frontmatter":1923,"imagePaths":1925},[],[],[],{"title":1909,"description":1910,"pubDate":33,"source":17,"tags":1924,"url":1913},[19,20,21],[],"2025-06-10-learning-what-matters-now-a-dual-critic-context-aware-rl-framework-for-priority--42f7f3.md","2025-06-10-legalreasoner-step-wised-verification-correction-for-legal-judgment-reasoning-c72876",{"id":1927,"data":1929,"body":1935,"filePath":1936,"digest":1937,"rendered":1938,"legacyId":1947},{"title":1930,"description":1931,"pubDate":1932,"source":17,"tags":1933,"url":1934},"LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning","arXiv:2506.07443v1 Announce Type: new \nAbstract: Legal judgment prediction (LJP) aims to function as a judge by making final rulings based on case claims and facts, which plays a vital role in the judicial domain for supporting court decision-making and improving judicial efficiency. However, existing methods often struggle with logical errors when conducting complex legal reasoning. We propose LegalReasoner, which enhances LJP reliability through step-wise verification and correction of the reasoning process. Specifically, it first identifies dispute points to decompose complex cases, and then conducts step-wise reasoning while employing a process verifier to validate each step's logic from correctness, progressiveness, and potential perspectives. When errors are detected, expert-designed attribution and resolution strategies are applied for correction. To fine-tune LegalReasoner, we release the LegalHK dataset, containing 58,130 Hong Kong court cases with detailed annotations of dispute points, step-by-step reasoning chains, and process verification labels. Experiments demonstrate that LegalReasoner significantly improves concordance with court decisions from 72.37 to 80.27 on LLAMA-3.1-70B. The data is available at https://huggingface.co/datasets/weijiezz/LegalHK.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07443","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning\r\nView PDFAbstract:Legal judgment prediction (LJP) aims to function as a judge by making final rulings based on case claims and facts, which plays a vital role in the judicial domain for supporting court decision-making and improving judicial efficiency. However, existing methods often struggle with logical errors when conducting complex legal reasoning. We propose LegalReasoner, which enhances LJP reliability through step-wise verification and correction of the reasoning process. Specifically, it first identifies dispute points to decompose complex cases, and then conducts step-wise reasoning while employing a process verifier to validate each step's logic from correctness, progressiveness, and potential perspectives. When errors are detected, expert-designed attribution and resolution strategies are applied for correction. To fine-tune LegalReasoner, we release the LegalHK dataset, containing 58,130 Hong Kong court cases with detailed annotations of dispute points, step-by-step reasoning chains, and process verification labels. Experiments demonstrate that LegalReasoner significantly improves concordance with court decisions from 72.37 to 80.27 on LLAMA-3.1-70B. The data is available at this https URL.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-legalreasoner-step-wised-verification-correction-for-legal-judgment-reasoning-c72876.md","0f06e06689b2779a",{"html":1939,"metadata":1940},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning\r\nView PDFAbstract:Legal judgment prediction (LJP) aims to function as a judge by making final rulings based on case claims and facts, which plays a vital role in the judicial domain for supporting court decision-making and improving judicial efficiency. However, existing methods often struggle with logical errors when conducting complex legal reasoning. We propose LegalReasoner, which enhances LJP reliability through step-wise verification and correction of the reasoning process. Specifically, it first identifies dispute points to decompose complex cases, and then conducts step-wise reasoning while employing a process verifier to validate each step’s logic from correctness, progressiveness, and potential perspectives. When errors are detected, expert-designed attribution and resolution strategies are applied for correction. To fine-tune LegalReasoner, we release the LegalHK dataset, containing 58,130 Hong Kong court cases with detailed annotations of dispute points, step-by-step reasoning chains, and process verification labels. Experiments demonstrate that LegalReasoner significantly improves concordance with court decisions from 72.37 to 80.27 on LLAMA-3.1-70B. The data is available at this https URL.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1941,"localImagePaths":1942,"remoteImagePaths":1943,"frontmatter":1944,"imagePaths":1946},[],[],[],{"title":1930,"description":1931,"pubDate":33,"source":17,"tags":1945,"url":1934},[19,20,21],[],"2025-06-10-legalreasoner-step-wised-verification-correction-for-legal-judgment-reasoning-c72876.md","2025-06-10-low-resource-machine-translation-what-for-who-for-an-observational-study-on-a-de-923d38",{"id":1948,"data":1950,"body":1956,"filePath":1957,"digest":1958,"rendered":1959,"legacyId":1968},{"title":1951,"description":1952,"pubDate":1953,"source":17,"tags":1954,"url":1955},"Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service","arXiv:2411.12262v4 Announce Type: cross \nAbstract: Low-resource machine translation (MT) presents a diversity of community needs and application challenges that remain poorly understood. To complement surveys and focus groups, which tend to rely on small samples of respondents, we propose an observational study on actual usage patterns of tetun$.$org, a specialized MT service for the Tetun language, which is the lingua franca in Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that challenge assumptions based on existing corpora. We find that users, many of them students on mobile devices, typically translate text from a high-resource language into Tetun across diverse domains including science, healthcare, and daily life. This contrasts sharply with available Tetun corpora, which are dominated by news articles covering government and social issues. Our results suggest that MT systems for institutionalized minority languages like Tetun should prioritize accuracy on domains relevant to educational contexts, in the high-resource to low-resource direction. More broadly, this study demonstrates how observational analysis can inform low-resource language technology development, by grounding research in practical community needs.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2411.12262","Computer Science > Computation and Language\r\n[Submitted on 19 Nov 2024 (v1), last revised 2 Apr 2025 (this version, v4)]\r\nTitle:Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service\r\nView PDF HTML (experimental)Abstract:Low-resource machine translation (MT) presents a diversity of community needs and application challenges that remain poorly understood. To complement surveys and focus groups, which tend to rely on small samples of respondents, we propose an observational study on actual usage patterns of tetun$.$org, a specialized MT service for the Tetun language, which is the lingua franca in Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that challenge assumptions based on existing corpora. We find that users, many of them students on mobile devices, typically translate text from a high-resource language into Tetun across diverse domains including science, healthcare, and daily life. This contrasts sharply with available Tetun corpora, which are dominated by news articles covering government and social issues. Our results suggest that MT systems for institutionalized minority languages like Tetun should prioritize accuracy on domains relevant to educational contexts, in the high-resource to low-resource direction. More broadly, this study demonstrates how observational analysis can inform low-resource language technology development, by grounding research in practical community needs.\r\nSubmission history\r\nFrom: Raphael Merx [view email][v1] Tue, 19 Nov 2024 06:21:51 UTC (526 KB)\r\n[v2] Thu, 19 Dec 2024 07:29:23 UTC (595 KB)\r\n[v3] Tue, 1 Apr 2025 05:19:04 UTC (617 KB)\r\n[v4] Wed, 2 Apr 2025 13:56:42 UTC (617 KB)\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-low-resource-machine-translation-what-for-who-for-an-observational-study-on-a-de-923d38.md","d857d08f895a4154",{"html":1960,"metadata":1961},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 19 Nov 2024 (v1), last revised 2 Apr 2025 (this version, v4)]\r\nTitle:Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service\r\nView PDF HTML (experimental)Abstract:Low-resource machine translation (MT) presents a diversity of community needs and application challenges that remain poorly understood. To complement surveys and focus groups, which tend to rely on small samples of respondents, we propose an observational study on actual usage patterns of tetun$.$org, a specialized MT service for the Tetun language, which is the lingua franca in Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that challenge assumptions based on existing corpora. We find that users, many of them students on mobile devices, typically translate text from a high-resource language into Tetun across diverse domains including science, healthcare, and daily life. This contrasts sharply with available Tetun corpora, which are dominated by news articles covering government and social issues. Our results suggest that MT systems for institutionalized minority languages like Tetun should prioritize accuracy on domains relevant to educational contexts, in the high-resource to low-resource direction. More broadly, this study demonstrates how observational analysis can inform low-resource language technology development, by grounding research in practical community needs.\r\nSubmission history\r\nFrom: Raphael Merx [view email][v1] Tue, 19 Nov 2024 06:21:51 UTC (526 KB)\r\n[v2] Thu, 19 Dec 2024 07:29:23 UTC (595 KB)\r\n[v3] Tue, 1 Apr 2025 05:19:04 UTC (617 KB)\r\n[v4] Wed, 2 Apr 2025 13:56:42 UTC (617 KB)\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1962,"localImagePaths":1963,"remoteImagePaths":1964,"frontmatter":1965,"imagePaths":1967},[],[],[],{"title":1951,"description":1952,"pubDate":33,"source":17,"tags":1966,"url":1955},[19,20,21],[],"2025-06-10-low-resource-machine-translation-what-for-who-for-an-observational-study-on-a-de-923d38.md","2025-06-10-learning-what-reinforcement-learning-cant-interleaved-online-fine-tuning-for-ha-bc20fe",{"id":1969,"data":1971,"body":1977,"filePath":1978,"digest":1979,"rendered":1980,"legacyId":1989},{"title":1972,"description":1973,"pubDate":1974,"source":17,"tags":1975,"url":1976},"Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions","arXiv:2506.07527v1 Announce Type: new \nAbstract: Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \\textbf{ReLIFT} (\\textbf{Re}inforcement \\textbf{L}earning \\textbf{I}nterleaved with Online \\textbf{F}ine-\\textbf{T}uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07527","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions\r\nView PDF HTML (experimental)Abstract:Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \\textbf{ReLIFT} (\\textbf{Re}inforcement \\textbf{L}earning \\textbf{I}nterleaved with Online \\textbf{F}ine-\\textbf{T}uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-learning-what-reinforcement-learning-can't-interleaved-online-fine-tuning-for-ha-bc20fe.md","e53a5e3b54947075",{"html":1981,"metadata":1982},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Learning What Reinforcement Learning Can’t: Interleaved Online Fine-Tuning for Hardest Questions\r\nView PDF HTML (experimental)Abstract:Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model’s original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \\textbf{ReLIFT} (\\textbf{Re}inforcement \\textbf{L}earning \\textbf{I}nterleaved with Online \\textbf{F}ine-\\textbf{T}uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model’s reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":1983,"localImagePaths":1984,"remoteImagePaths":1985,"frontmatter":1986,"imagePaths":1988},[],[],[],{"title":1972,"description":1973,"pubDate":33,"source":17,"tags":1987,"url":1976},[19,20,21],[],"2025-06-10-learning-what-reinforcement-learning-can't-interleaved-online-fine-tuning-for-ha-bc20fe.md","2025-06-10-lucifer-language-understanding-and-context-infused-framework-for-exploration-and-05e247",{"id":1990,"data":1992,"body":1998,"filePath":1999,"digest":2000,"rendered":2001,"legacyId":2010},{"title":1993,"description":1994,"pubDate":1995,"source":17,"tags":1996,"url":1997},"LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement","arXiv:2506.07915v1 Announce Type: new \nAbstract: In dynamic environments, the rapid obsolescence of pre-existing environmental knowledge creates a gap between an agent's internal model and the evolving reality of its operational context. This disparity between prior and updated environmental valuations fundamentally limits the effectiveness of autonomous decision-making. To bridge this gap, the contextual bias of human domain stakeholders, who naturally accumulate insights through direct, real-time observation, becomes indispensable. However, translating their nuanced, and context-rich input into actionable intelligence for autonomous systems remains an open challenge. To address this, we propose LUCIFER (Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement), a domain-agnostic framework that integrates a hierarchical decision-making architecture with reinforcement learning (RL) and large language models (LLMs) into a unified system. This architecture mirrors how humans decompose complex tasks, enabling a high-level planner to coordinate specialised sub-agents, each focused on distinct objectives and temporally interdependent actions. Unlike traditional applications where LLMs are limited to single role, LUCIFER integrates them in two synergistic roles: as context extractors, structuring verbal stakeholder input into domain-aware representations that influence decision-making through an attention space mechanism aligning LLM-derived insights with the agent's learning process, and as zero-shot exploration facilitators guiding the agent's action selection process during exploration. We benchmark various LLMs in both roles and demonstrate that LUCIFER improves exploration efficiency and decision quality, outperforming flat, goal-conditioned policies. Our findings show the potential of context-driven decision-making, where autonomous systems leverage human contextual knowledge for operational success.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07915","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement\r\nView PDF HTML (experimental)Abstract:In dynamic environments, the rapid obsolescence of pre-existing environmental knowledge creates a gap between an agent's internal model and the evolving reality of its operational context. This disparity between prior and updated environmental valuations fundamentally limits the effectiveness of autonomous decision-making. To bridge this gap, the contextual bias of human domain stakeholders, who naturally accumulate insights through direct, real-time observation, becomes indispensable. However, translating their nuanced, and context-rich input into actionable intelligence for autonomous systems remains an open challenge. To address this, we propose LUCIFER (Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement), a domain-agnostic framework that integrates a hierarchical decision-making architecture with reinforcement learning (RL) and large language models (LLMs) into a unified system. This architecture mirrors how humans decompose complex tasks, enabling a high-level planner to coordinate specialised sub-agents, each focused on distinct objectives and temporally interdependent actions. Unlike traditional applications where LLMs are limited to single role, LUCIFER integrates them in two synergistic roles: as context extractors, structuring verbal stakeholder input into domain-aware representations that influence decision-making through an attention space mechanism aligning LLM-derived insights with the agent's learning process, and as zero-shot exploration facilitators guiding the agent's action selection process during exploration. We benchmark various LLMs in both roles and demonstrate that LUCIFER improves exploration efficiency and decision quality, outperforming flat, goal-conditioned policies. Our findings show the potential of context-driven decision-making, where autonomous systems leverage human contextual knowledge for operational success.\r\nSubmission history\r\nFrom: Dimitris A. Panagopoulos [view email][v1] Mon, 9 Jun 2025 16:30:05 UTC (880 KB)\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-lucifer-language-understanding-and-context-infused-framework-for-exploration-and-05e247.md","855f3d56d1c57595",{"html":2002,"metadata":2003},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement\r\nView PDF HTML (experimental)Abstract:In dynamic environments, the rapid obsolescence of pre-existing environmental knowledge creates a gap between an agent’s internal model and the evolving reality of its operational context. This disparity between prior and updated environmental valuations fundamentally limits the effectiveness of autonomous decision-making. To bridge this gap, the contextual bias of human domain stakeholders, who naturally accumulate insights through direct, real-time observation, becomes indispensable. However, translating their nuanced, and context-rich input into actionable intelligence for autonomous systems remains an open challenge. To address this, we propose LUCIFER (Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement), a domain-agnostic framework that integrates a hierarchical decision-making architecture with reinforcement learning (RL) and large language models (LLMs) into a unified system. This architecture mirrors how humans decompose complex tasks, enabling a high-level planner to coordinate specialised sub-agents, each focused on distinct objectives and temporally interdependent actions. Unlike traditional applications where LLMs are limited to single role, LUCIFER integrates them in two synergistic roles: as context extractors, structuring verbal stakeholder input into domain-aware representations that influence decision-making through an attention space mechanism aligning LLM-derived insights with the agent’s learning process, and as zero-shot exploration facilitators guiding the agent’s action selection process during exploration. We benchmark various LLMs in both roles and demonstrate that LUCIFER improves exploration efficiency and decision quality, outperforming flat, goal-conditioned policies. Our findings show the potential of context-driven decision-making, where autonomous systems leverage human contextual knowledge for operational success.\r\nSubmission history\r\nFrom: Dimitris A. Panagopoulos [view email][v1] Mon, 9 Jun 2025 16:30:05 UTC (880 KB)\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2004,"localImagePaths":2005,"remoteImagePaths":2006,"frontmatter":2007,"imagePaths":2009},[],[],[],{"title":1993,"description":1994,"pubDate":33,"source":17,"tags":2008,"url":1997},[19,20,21],[],"2025-06-10-lucifer-language-understanding-and-context-infused-framework-for-exploration-and-05e247.md","2025-06-10-long-tailed-learning-for-generalized-category-discovery-5c8b7d",{"id":2011,"data":2013,"body":2019,"filePath":2020,"digest":2021,"rendered":2022,"legacyId":2031},{"title":2014,"description":2015,"pubDate":2016,"source":17,"tags":2017,"url":2018},"Long-Tailed Learning for Generalized Category Discovery","arXiv:2506.06965v1 Announce Type: new \nAbstract: Generalized Category Discovery (GCD) utilizes labeled samples of known classes to discover novel classes in unlabeled samples. Existing methods show effective performance on artificial datasets with balanced distributions. However, real-world datasets are always imbalanced, significantly affecting the effectiveness of these methods. To solve this problem, we propose a novel framework that performs generalized category discovery in long-tailed distributions. We first present a self-guided labeling technique that uses a learnable distribution to generate pseudo-labels, resulting in less biased classifiers. We then introduce a representation balancing process to derive discriminative representations. By mining sample neighborhoods, this process encourages the model to focus more on tail classes. We conduct experiments on public datasets to demonstrate the effectiveness of the proposed framework. The results show that our model exceeds previous state-of-the-art methods.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06965","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Long-Tailed Learning for Generalized Category Discovery\r\nView PDFAbstract:Generalized Category Discovery (GCD) utilizes labeled samples of known classes to discover novel classes in unlabeled samples. Existing methods show effective performance on artificial datasets with balanced distributions. However, real-world datasets are always imbalanced, significantly affecting the effectiveness of these methods. To solve this problem, we propose a novel framework that performs generalized category discovery in long-tailed distributions. We first present a self-guided labeling technique that uses a learnable distribution to generate pseudo-labels, resulting in less biased classifiers. We then introduce a representation balancing process to derive discriminative representations. By mining sample neighborhoods, this process encourages the model to focus more on tail classes. We conduct experiments on public datasets to demonstrate the effectiveness of the proposed framework. The results show that our model exceeds previous state-of-the-art methods.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-long-tailed-learning-for-generalized-category-discovery-5c8b7d.md","ff7695fb861d9fea",{"html":2023,"metadata":2024},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Long-Tailed Learning for Generalized Category Discovery\r\nView PDFAbstract:Generalized Category Discovery (GCD) utilizes labeled samples of known classes to discover novel classes in unlabeled samples. Existing methods show effective performance on artificial datasets with balanced distributions. However, real-world datasets are always imbalanced, significantly affecting the effectiveness of these methods. To solve this problem, we propose a novel framework that performs generalized category discovery in long-tailed distributions. We first present a self-guided labeling technique that uses a learnable distribution to generate pseudo-labels, resulting in less biased classifiers. We then introduce a representation balancing process to derive discriminative representations. By mining sample neighborhoods, this process encourages the model to focus more on tail classes. We conduct experiments on public datasets to demonstrate the effectiveness of the proposed framework. The results show that our model exceeds previous state-of-the-art methods.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2025,"localImagePaths":2026,"remoteImagePaths":2027,"frontmatter":2028,"imagePaths":2030},[],[],[],{"title":2014,"description":2015,"pubDate":33,"source":17,"tags":2029,"url":2018},[19,20,21],[],"2025-06-10-long-tailed-learning-for-generalized-category-discovery-5c8b7d.md","2025-06-10-llm-enhanced-rapid-reflex-async-reflect-embodied-agent-for-real-time-decision-ma-7fa6cf",{"id":2032,"data":2034,"body":2040,"filePath":2041,"digest":2042,"rendered":2043,"legacyId":2052},{"title":2035,"description":2036,"pubDate":2037,"source":17,"tags":2038,"url":2039},"LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments","arXiv:2506.07223v1 Announce Type: new \nAbstract: In the realm of embodied intelligence, the evolution of large language models (LLMs) has markedly enhanced agent decision making. Consequently, researchers have begun exploring agent performance in dynamically changing high-risk scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under these extreme conditions, the delay in decision making emerges as a crucial yet insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that translates inference delays in decision-making into equivalent simulation frames, thus aligning cognitive and physical costs under a single FPS-based metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a lightweight LLM-guided feedback module with a rule-based agent to enable immediate reactive behaviors and asynchronous reflective refinements in situ. Experiments on HAZARD show that RRARA substantially outperforms existing baselines in latency-sensitive scenarios.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07223","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments\r\nView PDF HTML (experimental)Abstract:In the realm of embodied intelligence, the evolution of large language models (LLMs) has markedly enhanced agent decision making. Consequently, researchers have begun exploring agent performance in dynamically changing high-risk scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under these extreme conditions, the delay in decision making emerges as a crucial yet insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that translates inference delays in decision-making into equivalent simulation frames, thus aligning cognitive and physical costs under a single FPS-based metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a lightweight LLM-guided feedback module with a rule-based agent to enable immediate reactive behaviors and asynchronous reflective refinements in situ. Experiments on HAZARD show that RRARA substantially outperforms existing baselines in latency-sensitive scenarios.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-llm-enhanced-rapid-reflex-async-reflect-embodied-agent-for-real-time-decision-ma-7fa6cf.md","59b29c837b9fc546",{"html":2044,"metadata":2045},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments\r\nView PDF HTML (experimental)Abstract:In the realm of embodied intelligence, the evolution of large language models (LLMs) has markedly enhanced agent decision making. Consequently, researchers have begun exploring agent performance in dynamically changing high-risk scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under these extreme conditions, the delay in decision making emerges as a crucial yet insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that translates inference delays in decision-making into equivalent simulation frames, thus aligning cognitive and physical costs under a single FPS-based metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a lightweight LLM-guided feedback module with a rule-based agent to enable immediate reactive behaviors and asynchronous reflective refinements in situ. Experiments on HAZARD show that RRARA substantially outperforms existing baselines in latency-sensitive scenarios.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2046,"localImagePaths":2047,"remoteImagePaths":2048,"frontmatter":2049,"imagePaths":2051},[],[],[],{"title":2035,"description":2036,"pubDate":33,"source":17,"tags":2050,"url":2039},[19,20,21],[],"2025-06-10-llm-enhanced-rapid-reflex-async-reflect-embodied-agent-for-real-time-decision-ma-7fa6cf.md","2025-06-10-mapping-human-agent-co-learning-and-co-adaptation-a-scoping-review-6574e7",{"id":2053,"data":2055,"body":2061,"filePath":2062,"digest":2063,"rendered":2064,"legacyId":2073},{"title":2056,"description":2057,"pubDate":2058,"source":17,"tags":2059,"url":2060},"Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review","arXiv:2506.06324v1 Announce Type: new \nAbstract: Several papers have delved into the challenges of human-AI-robot co-learning and co-adaptation. It has been noted that the terminology used to describe this collaborative relationship in existing studies needs to be more consistent. For example, the prefix \"co\" is used interchangeably to represent both \"collaborative\" and \"mutual,\" and the terms \"co-learning\" and \"co-adaptation\" are sometimes used interchangeably. However, they can reflect subtle differences in the focus of the studies. The current scoping review's primary research question (RQ1) aims to gather existing papers discussing this collaboration pattern and examine the terms researchers use to describe this human-agent relationship. Given the relative newness of this area of study, we are also keen on exploring the specific types of intelligent agents and task domains that have been considered in existing research (RQ2). This exploration is significant as it can shed light on the diversity of human-agent interactions, from one-time to continuous learning/adaptation scenarios. It can also help us understand the dynamics of human-agent interactions in different task domains, guiding our expectations towards research situated in dynamic, complex domains. Our third objective (RQ3) is to investigate the cognitive theories and frameworks that have been utilized in existing studies to measure human-agent co-learning and co-adaptation. This investigation is crucial as it can help us understand the theoretical underpinnings of human-agent collaboration and adaptation, and it can also guide us in identifying any new frameworks proposed specifically for this type of relationship.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06324","Computer Science > Artificial Intelligence\r\n[Submitted on 30 May 2025]\r\nTitle:Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review\r\nView PDFAbstract:Several papers have delved into the challenges of human-AI-robot co-learning and co-adaptation. It has been noted that the terminology used to describe this collaborative relationship in existing studies needs to be more consistent. For example, the prefix \"co\" is used interchangeably to represent both \"collaborative\" and \"mutual,\" and the terms \"co-learning\" and \"co-adaptation\" are sometimes used interchangeably. However, they can reflect subtle differences in the focus of the studies. The current scoping review's primary research question (RQ1) aims to gather existing papers discussing this collaboration pattern and examine the terms researchers use to describe this human-agent relationship. Given the relative newness of this area of study, we are also keen on exploring the specific types of intelligent agents and task domains that have been considered in existing research (RQ2). This exploration is significant as it can shed light on the diversity of human-agent interactions, from one-time to continuous learning/adaptation scenarios. It can also help us understand the dynamics of human-agent interactions in different task domains, guiding our expectations towards research situated in dynamic, complex domains. Our third objective (RQ3) is to investigate the cognitive theories and frameworks that have been utilized in existing studies to measure human-agent co-learning and co-adaptation. This investigation is crucial as it can help us understand the theoretical underpinnings of human-agent collaboration and adaptation, and it can also guide us in identifying any new frameworks proposed specifically for this type of relationship.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-mapping-human-agent-co-learning-and-co-adaptation-a-scoping-review-6574e7.md","8f148311b8232297",{"html":2065,"metadata":2066},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 30 May 2025]\r\nTitle:Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review\r\nView PDFAbstract:Several papers have delved into the challenges of human-AI-robot co-learning and co-adaptation. It has been noted that the terminology used to describe this collaborative relationship in existing studies needs to be more consistent. For example, the prefix “co” is used interchangeably to represent both “collaborative” and “mutual,” and the terms “co-learning” and “co-adaptation” are sometimes used interchangeably. However, they can reflect subtle differences in the focus of the studies. The current scoping review’s primary research question (RQ1) aims to gather existing papers discussing this collaboration pattern and examine the terms researchers use to describe this human-agent relationship. Given the relative newness of this area of study, we are also keen on exploring the specific types of intelligent agents and task domains that have been considered in existing research (RQ2). This exploration is significant as it can shed light on the diversity of human-agent interactions, from one-time to continuous learning/adaptation scenarios. It can also help us understand the dynamics of human-agent interactions in different task domains, guiding our expectations towards research situated in dynamic, complex domains. Our third objective (RQ3) is to investigate the cognitive theories and frameworks that have been utilized in existing studies to measure human-agent co-learning and co-adaptation. This investigation is crucial as it can help us understand the theoretical underpinnings of human-agent collaboration and adaptation, and it can also guide us in identifying any new frameworks proposed specifically for this type of relationship.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2067,"localImagePaths":2068,"remoteImagePaths":2069,"frontmatter":2070,"imagePaths":2072},[],[],[],{"title":2056,"description":2057,"pubDate":33,"source":17,"tags":2071,"url":2060},[19,20,21],[],"2025-06-10-mapping-human-agent-co-learning-and-co-adaptation-a-scoping-review-6574e7.md","2025-06-10-mathesis-towards-formal-theorem-proving-from-natural-languages-ac8196",{"id":2074,"data":2076,"body":2082,"filePath":2083,"digest":2084,"rendered":2085,"legacyId":2094},{"title":2077,"description":2078,"pubDate":2079,"source":17,"tags":2080,"url":2081},"Mathesis: Towards Formal Theorem Proving from Natural Languages","arXiv:2506.07047v1 Announce Type: new \nAbstract: Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07047","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Mathesis: Towards Formal Theorem Proving from Natural Languages\r\nView PDFAbstract:Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-mathesis-towards-formal-theorem-proving-from-natural-languages-ac8196.md","e1594ae71dea64c2",{"html":2086,"metadata":2087},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Mathesis: Towards Formal Theorem Proving from Natural Languages\r\nView PDFAbstract:Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China’s national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis’s effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2088,"localImagePaths":2089,"remoteImagePaths":2090,"frontmatter":2091,"imagePaths":2093},[],[],[],{"title":2077,"description":2078,"pubDate":33,"source":17,"tags":2092,"url":2081},[19,20,21],[],"2025-06-10-mathesis-towards-formal-theorem-proving-from-natural-languages-ac8196.md","2025-06-10-mcpworld-a-unified-benchmarking-testbed-for-api-gui-and-hybrid-computer-use-ag-34cc45",{"id":2095,"data":2097,"body":2103,"filePath":2104,"digest":2105,"rendered":2106,"legacyId":2115},{"title":2098,"description":2099,"pubDate":2100,"source":17,"tags":2101,"url":2102},"MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents","arXiv:2506.07672v1 Announce Type: new \nAbstract: (M)LLM-powered computer use agents (CUA) are emerging as a transformative technique to automate human-computer interaction. However, existing CUA benchmarks predominantly target GUI agents, whose evaluation methods are susceptible to UI changes and ignore function interactions exposed by application APIs, e.g., Model Context Protocol (MCP). To this end, we propose MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid agents. A key principle of MCPWorld is the use of \"white-box apps\", i.e., those with source code availability and can be revised/re-compiled as needed (e.g., adding MCP support), with two notable advantages:\n  (1) It greatly broadens the design space of CUA, such as what and how the app features to be exposed/extracted as CUA-callable APIs.\n  (2) It allows MCPWorld to programmatically verify task completion by directly monitoring application behavior through techniques like dynamic code instrumentation, offering robust, accurate CUA evaluation decoupled from specific agent implementations or UI states.\n  Currently, MCPWorld includes 201 well curated and annotated user tasks, covering diversified use cases and difficulty levels. MCPWorld is also fully containerized with GPU acceleration support for flexible adoption on different OS/hardware environments. Our preliminary experiments, using a representative LLM-powered CUA framework, achieve 75.12% task completion accuracy, simultaneously providing initial evidence on the practical effectiveness of agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate and standardize the benchmarking of next-generation computer use agents that can leverage rich external tools. Our code and dataset are publicly available at https://github.com/SAAgent/MCPWorld.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07672","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents\r\nView PDF HTML (experimental)Abstract:(M)LLM-powered computer use agents (CUA) are emerging as a transformative technique to automate human-computer interaction. However, existing CUA benchmarks predominantly target GUI agents, whose evaluation methods are susceptible to UI changes and ignore function interactions exposed by application APIs, e.g., Model Context Protocol (MCP). To this end, we propose MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid agents. A key principle of MCPWorld is the use of \"white-box apps\", i.e., those with source code availability and can be revised/re-compiled as needed (e.g., adding MCP support), with two notable advantages:\r\n(1) It greatly broadens the design space of CUA, such as what and how the app features to be exposed/extracted as CUA-callable APIs.\r\n(2) It allows MCPWorld to programmatically verify task completion by directly monitoring application behavior through techniques like dynamic code instrumentation, offering robust, accurate CUA evaluation decoupled from specific agent implementations or UI states.\r\nCurrently, MCPWorld includes 201 well curated and annotated user tasks, covering diversified use cases and difficulty levels. MCPWorld is also fully containerized with GPU acceleration support for flexible adoption on different OS/hardware environments. Our preliminary experiments, using a representative LLM-powered CUA framework, achieve 75.12% task completion accuracy, simultaneously providing initial evidence on the practical effectiveness of agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate and standardize the benchmarking of next-generation computer use agents that can leverage rich external tools. Our code and dataset are publicly available at this https URL.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-mcpworld-a-unified-benchmarking-testbed-for-api,-gui,-and-hybrid-computer-use-ag-34cc45.md","7639878190d666ef",{"html":2107,"metadata":2108},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents\r\nView PDF HTML (experimental)Abstract:(M)LLM-powered computer use agents (CUA) are emerging as a transformative technique to automate human-computer interaction. However, existing CUA benchmarks predominantly target GUI agents, whose evaluation methods are susceptible to UI changes and ignore function interactions exposed by application APIs, e.g., Model Context Protocol (MCP). To this end, we propose MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid agents. A key principle of MCPWorld is the use of “white-box apps”, i.e., those with source code availability and can be revised/re-compiled as needed (e.g., adding MCP support), with two notable advantages:\r\n(1) It greatly broadens the design space of CUA, such as what and how the app features to be exposed/extracted as CUA-callable APIs.\r\n(2) It allows MCPWorld to programmatically verify task completion by directly monitoring application behavior through techniques like dynamic code instrumentation, offering robust, accurate CUA evaluation decoupled from specific agent implementations or UI states.\r\nCurrently, MCPWorld includes 201 well curated and annotated user tasks, covering diversified use cases and difficulty levels. MCPWorld is also fully containerized with GPU acceleration support for flexible adoption on different OS/hardware environments. Our preliminary experiments, using a representative LLM-powered CUA framework, achieve 75.12% task completion accuracy, simultaneously providing initial evidence on the practical effectiveness of agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate and standardize the benchmarking of next-generation computer use agents that can leverage rich external tools. Our code and dataset are publicly available at this https URL.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2109,"localImagePaths":2110,"remoteImagePaths":2111,"frontmatter":2112,"imagePaths":2114},[],[],[],{"title":2098,"description":2099,"pubDate":33,"source":17,"tags":2113,"url":2102},[19,20,21],[],"2025-06-10-mcpworld-a-unified-benchmarking-testbed-for-api,-gui,-and-hybrid-computer-use-ag-34cc45.md","2025-06-10-memory-os-of-ai-agent-041cf0",{"id":2116,"data":2118,"body":2124,"filePath":2125,"digest":2126,"rendered":2127,"legacyId":2136},{"title":2119,"description":2120,"pubDate":2121,"source":17,"tags":2122,"url":2123},"Memory OS of AI Agent","arXiv:2506.06326v1 Announce Type: new \nAbstract: Large Language Models (LLMs) face a crucial challenge from fixed context windows and inadequate memory management, leading to a severe shortage of long-term memory capabilities and limited personalization in the interactive experience with AI agents. To overcome this challenge, we innovatively propose a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and efficient memory management for AI agents. Inspired by the memory management principles in operating systems, MemoryOS designs a hierarchical storage architecture and consists of four key modules: Memory Storage, Updating, Retrieval, and Generation. Specifically, the architecture comprises three levels of storage units: short-term memory, mid-term memory, and long-term personal memory. Key operations within MemoryOS include dynamic updates between storage units: short-term to mid-term updates follow a dialogue-chain-based FIFO principle, while mid-term to long-term updates use a segmented page organization strategy. Our pioneering MemoryOS enables hierarchical memory integration and dynamic updating. Extensive experiments on the LoCoMo benchmark show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the baselines on GPT-4o-mini, showing contextual coherence and personalized memory retention in long conversations. The implementation code is open-sourced at https://github.com/BAI-LAB/MemoryOS.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06326","Computer Science > Artificial Intelligence\r\n[Submitted on 30 May 2025]\r\nTitle:Memory OS of AI Agent\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) face a crucial challenge from fixed context windows and inadequate memory management, leading to a severe shortage of long-term memory capabilities and limited personalization in the interactive experience with AI agents. To overcome this challenge, we innovatively propose a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and efficient memory management for AI agents. Inspired by the memory management principles in operating systems, MemoryOS designs a hierarchical storage architecture and consists of four key modules: Memory Storage, Updating, Retrieval, and Generation. Specifically, the architecture comprises three levels of storage units: short-term memory, mid-term memory, and long-term personal memory. Key operations within MemoryOS include dynamic updates between storage units: short-term to mid-term updates follow a dialogue-chain-based FIFO principle, while mid-term to long-term updates use a segmented page organization strategy. Our pioneering MemoryOS enables hierarchical memory integration and dynamic updating. Extensive experiments on the LoCoMo benchmark show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the baselines on GPT-4o-mini, showing contextual coherence and personalized memory retention in long conversations. The implementation code is open-sourced at this https URL.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-memory-os-of-ai-agent-041cf0.md","87a6f784db787288",{"html":2128,"metadata":2129},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 30 May 2025]\r\nTitle:Memory OS of AI Agent\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) face a crucial challenge from fixed context windows and inadequate memory management, leading to a severe shortage of long-term memory capabilities and limited personalization in the interactive experience with AI agents. To overcome this challenge, we innovatively propose a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and efficient memory management for AI agents. Inspired by the memory management principles in operating systems, MemoryOS designs a hierarchical storage architecture and consists of four key modules: Memory Storage, Updating, Retrieval, and Generation. Specifically, the architecture comprises three levels of storage units: short-term memory, mid-term memory, and long-term personal memory. Key operations within MemoryOS include dynamic updates between storage units: short-term to mid-term updates follow a dialogue-chain-based FIFO principle, while mid-term to long-term updates use a segmented page organization strategy. Our pioneering MemoryOS enables hierarchical memory integration and dynamic updating. Extensive experiments on the LoCoMo benchmark show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the baselines on GPT-4o-mini, showing contextual coherence and personalized memory retention in long conversations. The implementation code is open-sourced at this https URL.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2130,"localImagePaths":2131,"remoteImagePaths":2132,"frontmatter":2133,"imagePaths":2135},[],[],[],{"title":2119,"description":2120,"pubDate":33,"source":17,"tags":2134,"url":2123},[19,20,21],[],"2025-06-10-memory-os-of-ai-agent-041cf0.md","2025-06-10-medcite-can-language-models-generate-verifiable-text-for-medicine-860422",{"id":2137,"data":2139,"body":2145,"filePath":2146,"digest":2147,"rendered":2148,"legacyId":2157},{"title":2140,"description":2141,"pubDate":2142,"source":17,"tags":2143,"url":2144},"MedCite: Can Language Models Generate Verifiable Text for Medicine?","arXiv:2506.06605v1 Announce Type: cross \nAbstract: Existing LLM-based medical question-answering systems lack citation generation and evaluation capabilities, raising concerns about their adoption in practice. In this work, we introduce \\name, the first end-to-end framework that facilitates the design and evaluation of citation generation with LLMs for medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation method that generates high-quality citations. Our evaluation highlights the challenges and opportunities of citation generation for medical tasks, while identifying important design choices that have a significant impact on the final citation quality. Our proposed method achieves superior citation precision and recall improvements compared to strong baseline methods, and we show that evaluation results correlate well with annotation results from professional experts.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06605","Computer Science > Computation and Language\r\n[Submitted on 7 Jun 2025]\r\nTitle:MedCite: Can Language Models Generate Verifiable Text for Medicine?\r\nView PDF HTML (experimental)Abstract:Existing LLM-based medical question-answering systems lack citation generation and evaluation capabilities, raising concerns about their adoption in practice. In this work, we introduce \\name, the first end-to-end framework that facilitates the design and evaluation of citation generation with LLMs for medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation method that generates high-quality citations. Our evaluation highlights the challenges and opportunities of citation generation for medical tasks, while identifying important design choices that have a significant impact on the final citation quality. Our proposed method achieves superior citation precision and recall improvements compared to strong baseline methods, and we show that evaluation results correlate well with annotation results from professional experts.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-medcite-can-language-models-generate-verifiable-text-for-medicine-860422.md","c46070f5babd075f",{"html":2149,"metadata":2150},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 7 Jun 2025]\r\nTitle:MedCite: Can Language Models Generate Verifiable Text for Medicine?\r\nView PDF HTML (experimental)Abstract:Existing LLM-based medical question-answering systems lack citation generation and evaluation capabilities, raising concerns about their adoption in practice. In this work, we introduce \\name, the first end-to-end framework that facilitates the design and evaluation of citation generation with LLMs for medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation method that generates high-quality citations. Our evaluation highlights the challenges and opportunities of citation generation for medical tasks, while identifying important design choices that have a significant impact on the final citation quality. Our proposed method achieves superior citation precision and recall improvements compared to strong baseline methods, and we show that evaluation results correlate well with annotation results from professional experts.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2151,"localImagePaths":2152,"remoteImagePaths":2153,"frontmatter":2154,"imagePaths":2156},[],[],[],{"title":2140,"description":2141,"pubDate":33,"source":17,"tags":2155,"url":2144},[19,20,21],[],"2025-06-10-medcite-can-language-models-generate-verifiable-text-for-medicine-860422.md","2025-06-10-meta-adaptive-prompt-distillation-for-few-shot-visual-question-answering-a50265",{"id":2158,"data":2160,"body":2166,"filePath":2167,"digest":2168,"rendered":2169,"legacyId":2178},{"title":2161,"description":2162,"pubDate":2163,"source":17,"tags":2164,"url":2165},"Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering","arXiv:2506.06905v1 Announce Type: new \nAbstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just a few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06905","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering\r\nView PDF HTML (experimental)Abstract:Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just a few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-meta-adaptive-prompt-distillation-for-few-shot-visual-question-answering-a50265.md","432f57d789bd83e1",{"html":2170,"metadata":2171},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering\r\nView PDF HTML (experimental)Abstract:Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just a few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2172,"localImagePaths":2173,"remoteImagePaths":2174,"frontmatter":2175,"imagePaths":2177},[],[],[],{"title":2161,"description":2162,"pubDate":33,"source":17,"tags":2176,"url":2165},[19,20,21],[],"2025-06-10-meta-adaptive-prompt-distillation-for-few-shot-visual-question-answering-a50265.md","2025-06-10-minigpt-reverse-designing-predicting-image-adjustments-utilizing-minigpt-4-f3d473",{"id":2179,"data":2181,"body":2187,"filePath":2188,"digest":2189,"rendered":2190,"legacyId":2199},{"title":2182,"description":2183,"pubDate":2184,"source":17,"tags":2185,"url":2186},"MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4","arXiv:2406.00971v2 Announce Type: cross \nAbstract: Vision-Language Models (VLMs) have recently seen significant advancements through integrating with Large Language Models (LLMs). The VLMs, which process image and text modalities simultaneously, have demonstrated the ability to learn and understand the interaction between images and texts across various multi-modal tasks. Reverse designing, which could be defined as a complex vision-language task, aims to predict the edits and their parameters, given a source image, an edited version, and an optional high-level textual edit description. This task requires VLMs to comprehend the interplay between the source image, the edited version, and the optional textual context simultaneously, going beyond traditional vision-language tasks. In this paper, we extend and fine-tune MiniGPT-4 for the reverse designing task. Our experiments demonstrate the extensibility of off-the-shelf VLMs, specifically MiniGPT-4, for more complex tasks such as reverse designing. Code is available at this \\href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2406.00971","Computer Science > Computer Vision and Pattern Recognition\r\n[Submitted on 3 Jun 2024 (v1), last revised 30 Aug 2024 (this version, v2)]\r\nTitle:MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4\r\nView PDF HTML (experimental)Abstract:Vision-Language Models (VLMs) have recently seen significant advancements through integrating with Large Language Models (LLMs). The VLMs, which process image and text modalities simultaneously, have demonstrated the ability to learn and understand the interaction between images and texts across various multi-modal tasks. Reverse designing, which could be defined as a complex vision-language task, aims to predict the edits and their parameters, given a source image, an edited version, and an optional high-level textual edit description. This task requires VLMs to comprehend the interplay between the source image, the edited version, and the optional textual context simultaneously, going beyond traditional vision-language tasks. In this paper, we extend and fine-tune MiniGPT-4 for the reverse designing task. Our experiments demonstrate the extensibility of off-the-shelf VLMs, specifically MiniGPT-4, for more complex tasks such as reverse designing. Code is available at this \\href{this https URL}\r\nSubmission history\r\nFrom: Vahid Azizi [view email][v1] Mon, 3 Jun 2024 03:59:29 UTC (5,265 KB)\r\n[v2] Fri, 30 Aug 2024 01:50:37 UTC (5,564 KB)\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-minigpt-reverse-designing-predicting-image-adjustments-utilizing-minigpt-4-f3d473.md","4580a41e11cb074d",{"html":2191,"metadata":2192},"\u003Cp>Computer Science > Computer Vision and Pattern Recognition\r\n[Submitted on 3 Jun 2024 (v1), last revised 30 Aug 2024 (this version, v2)]\r\nTitle:MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4\r\nView PDF HTML (experimental)Abstract:Vision-Language Models (VLMs) have recently seen significant advancements through integrating with Large Language Models (LLMs). The VLMs, which process image and text modalities simultaneously, have demonstrated the ability to learn and understand the interaction between images and texts across various multi-modal tasks. Reverse designing, which could be defined as a complex vision-language task, aims to predict the edits and their parameters, given a source image, an edited version, and an optional high-level textual edit description. This task requires VLMs to comprehend the interplay between the source image, the edited version, and the optional textual context simultaneously, going beyond traditional vision-language tasks. In this paper, we extend and fine-tune MiniGPT-4 for the reverse designing task. Our experiments demonstrate the extensibility of off-the-shelf VLMs, specifically MiniGPT-4, for more complex tasks such as reverse designing. Code is available at this \\href{this https URL}\r\nSubmission history\r\nFrom: Vahid Azizi [view email][v1] Mon, 3 Jun 2024 03:59:29 UTC (5,265 KB)\r\n[v2] Fri, 30 Aug 2024 01:50:37 UTC (5,564 KB)\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2193,"localImagePaths":2194,"remoteImagePaths":2195,"frontmatter":2196,"imagePaths":2198},[],[],[],{"title":2182,"description":2183,"pubDate":33,"source":17,"tags":2197,"url":2186},[19,20,21],[],"2025-06-10-minigpt-reverse-designing-predicting-image-adjustments-utilizing-minigpt-4-f3d473.md","2025-06-10-mitigating-behavioral-hallucination-in-multimodal-large-language-models-for-sequ-bb3124",{"id":2200,"data":2202,"body":2208,"filePath":2209,"digest":2210,"rendered":2211,"legacyId":2220},{"title":2203,"description":2204,"pubDate":2205,"source":17,"tags":2206,"url":2207},"Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images","arXiv:2506.07184v1 Announce Type: new \nAbstract: While multimodal large language models excel at various tasks, they still suffer from hallucinations, which limit their reliability and scalability for broader domain applications. To address this issue, recent research mainly focuses on objective hallucination. However, for sequential images, besides objective hallucination, there is also behavioral hallucination, which is less studied. This work aims to fill in the gap. We first reveal that behavioral hallucinations mainly arise from two key factors: prior-driven bias and the snowball effect. Based on these observations, we introduce SHE (Sequence Hallucination Eradication), a lightweight, two-stage framework that (1) detects hallucinations via visual-textual alignment check using our proposed adaptive temporal window and (2) mitigates them via orthogonal projection onto the joint embedding space. We also propose a new metric (BEACH) to quantify behavioral hallucination severity. Empirical results on standard benchmarks demonstrate that SHE reduces behavioral hallucination by over 10% on BEACH while maintaining descriptive accuracy.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07184","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images\r\nView PDF HTML (experimental)Abstract:While multimodal large language models excel at various tasks, they still suffer from hallucinations, which limit their reliability and scalability for broader domain applications. To address this issue, recent research mainly focuses on objective hallucination. However, for sequential images, besides objective hallucination, there is also behavioral hallucination, which is less studied. This work aims to fill in the gap. We first reveal that behavioral hallucinations mainly arise from two key factors: prior-driven bias and the snowball effect. Based on these observations, we introduce SHE (Sequence Hallucination Eradication), a lightweight, two-stage framework that (1) detects hallucinations via visual-textual alignment check using our proposed adaptive temporal window and (2) mitigates them via orthogonal projection onto the joint embedding space. We also propose a new metric (BEACH) to quantify behavioral hallucination severity. Empirical results on standard benchmarks demonstrate that SHE reduces behavioral hallucination by over 10% on BEACH while maintaining descriptive accuracy.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-mitigating-behavioral-hallucination-in-multimodal-large-language-models-for-sequ-bb3124.md","300864741e0e33ab",{"html":2212,"metadata":2213},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images\r\nView PDF HTML (experimental)Abstract:While multimodal large language models excel at various tasks, they still suffer from hallucinations, which limit their reliability and scalability for broader domain applications. To address this issue, recent research mainly focuses on objective hallucination. However, for sequential images, besides objective hallucination, there is also behavioral hallucination, which is less studied. This work aims to fill in the gap. We first reveal that behavioral hallucinations mainly arise from two key factors: prior-driven bias and the snowball effect. Based on these observations, we introduce SHE (Sequence Hallucination Eradication), a lightweight, two-stage framework that (1) detects hallucinations via visual-textual alignment check using our proposed adaptive temporal window and (2) mitigates them via orthogonal projection onto the joint embedding space. We also propose a new metric (BEACH) to quantify behavioral hallucination severity. Empirical results on standard benchmarks demonstrate that SHE reduces behavioral hallucination by over 10% on BEACH while maintaining descriptive accuracy.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2214,"localImagePaths":2215,"remoteImagePaths":2216,"frontmatter":2217,"imagePaths":2219},[],[],[],{"title":2203,"description":2204,"pubDate":33,"source":17,"tags":2218,"url":2207},[19,20,21],[],"2025-06-10-mitigating-behavioral-hallucination-in-multimodal-large-language-models-for-sequ-bb3124.md","2025-06-10-model-based-neural-data-augmentation-for-sub-wavelength-radio-localization-4262e2",{"id":2221,"data":2223,"body":2229,"filePath":2230,"digest":2231,"rendered":2232,"legacyId":2241},{"title":2224,"description":2225,"pubDate":2226,"source":17,"tags":2227,"url":2228},"Model-based Neural Data Augmentation for sub-wavelength Radio Localization","arXiv:2506.06387v1 Announce Type: cross \nAbstract: The increasing deployment of large antenna arrays at base stations has significantly improved the spatial resolution and localization accuracy of radio-localization methods. However, traditional signal processing techniques struggle in complex radio environments, particularly in scenarios dominated by non line of sight (NLoS) propagation paths, resulting in degraded localization accuracy. Recent developments in machine learning have facilitated the development of machine learning-assisted localization techniques, enhancing localization accuracy in complex radio environments. However, these methods often involve substantial computational complexity during both the training and inference phases. This work extends the well-established fingerprinting-based localization framework by simultaneously reducing its memory requirements and improving its accuracy. Specifically, a model-based neural network is used to learn the location-to-channel mapping, and then serves as a generative neural channel model. This generative model augments the fingerprinting comparison dictionary while reducing the memory requirements. The proposed method outperforms fingerprinting baselines by achieving sub-wavelength localization accuracy, even in NLoS environments. Remarkably, it offers an improvement by several orders of magnitude in localization accuracy, while simultaneously reducing memory requirements by an order of magnitude compared to classical fingerprinting methods.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06387","Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 5 Jun 2025]\r\nTitle:Model-based Neural Data Augmentation for sub-wavelength Radio Localization\r\nView PDFAbstract:The increasing deployment of large antenna arrays at base stations has significantly improved the spatial resolution and localization accuracy of radio-localization methods. However, traditional signal processing techniques struggle in complex radio environments, particularly in scenarios dominated by non line of sight (NLoS) propagation paths, resulting in degraded localization accuracy. Recent developments in machine learning have facilitated the development of machine learning-assisted localization techniques, enhancing localization accuracy in complex radio environments. However, these methods often involve substantial computational complexity during both the training and inference phases. This work extends the well-established fingerprinting-based localization framework by simultaneously reducing its memory requirements and improving its accuracy. Specifically, a model-based neural network is used to learn the location-to-channel mapping, and then serves as a generative neural channel model. This generative model augments the fingerprinting comparison dictionary while reducing the memory requirements. The proposed method outperforms fingerprinting baselines by achieving sub-wavelength localization accuracy, even in NLoS environments. Remarkably, it offers an improvement by several orders of magnitude in localization accuracy, while simultaneously reducing memory requirements by an order of magnitude compared to classical fingerprinting methods.\r\nSubmission history\r\nFrom: Baptiste CHATELIER [view email] [via CCSD proxy][v1] Thu, 5 Jun 2025 08:20:51 UTC (10,326 KB)\r\nCurrent browse context:\r\neess.SP\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-model-based-neural-data-augmentation-for-sub-wavelength-radio-localization-4262e2.md","32b4b77a866354a8",{"html":2233,"metadata":2234},"\u003Cp>Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 5 Jun 2025]\r\nTitle:Model-based Neural Data Augmentation for sub-wavelength Radio Localization\r\nView PDFAbstract:The increasing deployment of large antenna arrays at base stations has significantly improved the spatial resolution and localization accuracy of radio-localization methods. However, traditional signal processing techniques struggle in complex radio environments, particularly in scenarios dominated by non line of sight (NLoS) propagation paths, resulting in degraded localization accuracy. Recent developments in machine learning have facilitated the development of machine learning-assisted localization techniques, enhancing localization accuracy in complex radio environments. However, these methods often involve substantial computational complexity during both the training and inference phases. This work extends the well-established fingerprinting-based localization framework by simultaneously reducing its memory requirements and improving its accuracy. Specifically, a model-based neural network is used to learn the location-to-channel mapping, and then serves as a generative neural channel model. This generative model augments the fingerprinting comparison dictionary while reducing the memory requirements. The proposed method outperforms fingerprinting baselines by achieving sub-wavelength localization accuracy, even in NLoS environments. Remarkably, it offers an improvement by several orders of magnitude in localization accuracy, while simultaneously reducing memory requirements by an order of magnitude compared to classical fingerprinting methods.\r\nSubmission history\r\nFrom: Baptiste CHATELIER [view email] [via CCSD proxy][v1] Thu, 5 Jun 2025 08:20:51 UTC (10,326 KB)\r\nCurrent browse context:\r\neess.SP\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2235,"localImagePaths":2236,"remoteImagePaths":2237,"frontmatter":2238,"imagePaths":2240},[],[],[],{"title":2224,"description":2225,"pubDate":33,"source":17,"tags":2239,"url":2228},[19,20,21],[],"2025-06-10-model-based-neural-data-augmentation-for-sub-wavelength-radio-localization-4262e2.md","2025-06-10-moe-gyro-self-supervised-over-range-reconstruction-and-denoising-for-mems-gyrosc-b9f22f",{"id":2242,"data":2244,"body":2250,"filePath":2251,"digest":2252,"rendered":2253,"legacyId":2262},{"title":2245,"description":2246,"pubDate":2247,"source":17,"tags":2248,"url":2249},"MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes","arXiv:2506.06318v1 Announce Type: cross \nAbstract: MEMS gyroscopes play a critical role in inertial navigation and motion control applications but typically suffer from a fundamental trade-off between measurement range and noise performance. Existing hardware-based solutions aimed at mitigating this issue introduce additional complexity, cost, and scalability challenges. Deep-learning methods primarily focus on noise reduction and typically require precisely aligned ground-truth signals, making them difficult to deploy in practical scenarios and leaving the fundamental trade-off unresolved. To address these challenges, we introduce Mixture of Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework specifically designed for simultaneous over-range signal reconstruction and noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing saturated segments; and a Denoise Expert (DE), utilizing dual-branch complementary masking combined with FFT-guided augmentation for robust noise reduction. A lightweight gating module dynamically routes input segments to the appropriate expert. Furthermore, existing evaluation lack a comprehensive standard for assessing multi-dimensional signal enhancement. To bridge this gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source benchmarking platform comprising the GyroPeak-100 dataset and a unified evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our proposed ISEBench, demonstrating that our framework significantly extends the measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by 98.4%, and achieves state-of-the-art performance, effectively addressing the long-standing trade-off in inertial sensing.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06318","Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 27 May 2025]\r\nTitle:MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes\r\nView PDF HTML (experimental)Abstract:MEMS gyroscopes play a critical role in inertial navigation and motion control applications but typically suffer from a fundamental trade-off between measurement range and noise performance. Existing hardware-based solutions aimed at mitigating this issue introduce additional complexity, cost, and scalability challenges. Deep-learning methods primarily focus on noise reduction and typically require precisely aligned ground-truth signals, making them difficult to deploy in practical scenarios and leaving the fundamental trade-off unresolved. To address these challenges, we introduce Mixture of Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework specifically designed for simultaneous over-range signal reconstruction and noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing saturated segments; and a Denoise Expert (DE), utilizing dual-branch complementary masking combined with FFT-guided augmentation for robust noise reduction. A lightweight gating module dynamically routes input segments to the appropriate expert. Furthermore, existing evaluation lack a comprehensive standard for assessing multi-dimensional signal enhancement. To bridge this gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source benchmarking platform comprising the GyroPeak-100 dataset and a unified evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our proposed ISEBench, demonstrating that our framework significantly extends the measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by 98.4%, and achieves state-of-the-art performance, effectively addressing the long-standing trade-off in inertial sensing.\r\nCurrent browse context:\r\neess.SP\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-moe-gyro-self-supervised-over-range-reconstruction-and-denoising-for-mems-gyrosc-b9f22f.md","d6c56110f026744f",{"html":2254,"metadata":2255},"\u003Cp>Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 27 May 2025]\r\nTitle:MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes\r\nView PDF HTML (experimental)Abstract:MEMS gyroscopes play a critical role in inertial navigation and motion control applications but typically suffer from a fundamental trade-off between measurement range and noise performance. Existing hardware-based solutions aimed at mitigating this issue introduce additional complexity, cost, and scalability challenges. Deep-learning methods primarily focus on noise reduction and typically require precisely aligned ground-truth signals, making them difficult to deploy in practical scenarios and leaving the fundamental trade-off unresolved. To address these challenges, we introduce Mixture of Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework specifically designed for simultaneous over-range signal reconstruction and noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing saturated segments; and a Denoise Expert (DE), utilizing dual-branch complementary masking combined with FFT-guided augmentation for robust noise reduction. A lightweight gating module dynamically routes input segments to the appropriate expert. Furthermore, existing evaluation lack a comprehensive standard for assessing multi-dimensional signal enhancement. To bridge this gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source benchmarking platform comprising the GyroPeak-100 dataset and a unified evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our proposed ISEBench, demonstrating that our framework significantly extends the measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by 98.4%, and achieves state-of-the-art performance, effectively addressing the long-standing trade-off in inertial sensing.\r\nCurrent browse context:\r\neess.SP\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2256,"localImagePaths":2257,"remoteImagePaths":2258,"frontmatter":2259,"imagePaths":2261},[],[],[],{"title":2245,"description":2246,"pubDate":33,"source":17,"tags":2260,"url":2249},[19,20,21],[],"2025-06-10-moe-gyro-self-supervised-over-range-reconstruction-and-denoising-for-mems-gyrosc-b9f22f.md","2025-06-10-mutual-taught-for-co-adapting-policy-and-reward-models-125d08",{"id":2263,"data":2265,"body":2271,"filePath":2272,"digest":2273,"rendered":2274,"legacyId":2283},{"title":2266,"description":2267,"pubDate":2268,"source":17,"tags":2269,"url":2270},"Mutual-Taught for Co-adapting Policy and Reward Models","arXiv:2506.06292v1 Announce Type: cross \nAbstract: During the preference optimization of large language models (LLMs), distribution shifts may arise between newly generated model samples and the data used to train the reward model (RM). This shift reduces the efficacy of the RM, which in turn negatively impacts the performance of the policy model (PM). To address this challenge, we propose Mutual-Taught, a self-training method that iteratively improves both the PM and RM without requiring additional human annotation. Our approach mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM, guiding the PM toward a better approximation of the latent optimal preference distribution. In the M-step, we update the RM by constructing training data from the outputs of the PM before and after the E-step update. This process ensures that the RM adapts to the evolving policy distribution. Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, our 8B policy model, LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\\% on AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par with GPT-4o-2024-08-06 on RewardBench.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06292","Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Mutual-Taught for Co-adapting Policy and Reward Models\r\nView PDF HTML (experimental)Abstract:During the preference optimization of large language models (LLMs), distribution shifts may arise between newly generated model samples and the data used to train the reward model (RM). This shift reduces the efficacy of the RM, which in turn negatively impacts the performance of the policy model (PM). To address this challenge, we propose Mutual-Taught, a self-training method that iteratively improves both the PM and RM without requiring additional human annotation. Our approach mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM, guiding the PM toward a better approximation of the latent optimal preference distribution. In the M-step, we update the RM by constructing training data from the outputs of the PM before and after the E-step update. This process ensures that the RM adapts to the evolving policy distribution. Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, our 8B policy model, LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\\% on AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par with GPT-4o-2024-08-06 on RewardBench.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-mutual-taught-for-co-adapting-policy-and-reward-models-125d08.md","37fe37840222eff1",{"html":2275,"metadata":2276},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Mutual-Taught for Co-adapting Policy and Reward Models\r\nView PDF HTML (experimental)Abstract:During the preference optimization of large language models (LLMs), distribution shifts may arise between newly generated model samples and the data used to train the reward model (RM). This shift reduces the efficacy of the RM, which in turn negatively impacts the performance of the policy model (PM). To address this challenge, we propose Mutual-Taught, a self-training method that iteratively improves both the PM and RM without requiring additional human annotation. Our approach mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM, guiding the PM toward a better approximation of the latent optimal preference distribution. In the M-step, we update the RM by constructing training data from the outputs of the PM before and after the E-step update. This process ensures that the RM adapts to the evolving policy distribution. Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, our 8B policy model, LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1% on AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par with GPT-4o-2024-08-06 on RewardBench.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2277,"localImagePaths":2278,"remoteImagePaths":2279,"frontmatter":2280,"imagePaths":2282},[],[],[],{"title":2266,"description":2267,"pubDate":33,"source":17,"tags":2281,"url":2270},[19,20,21],[],"2025-06-10-mutual-taught-for-co-adapting-policy-and-reward-models-125d08.md","2025-06-10-natural-language-interaction-with-databases-on-edge-devices-in-the-internet-of-b-2ee427",{"id":2284,"data":2286,"body":2292,"filePath":2293,"digest":2294,"rendered":2295,"legacyId":2304},{"title":2287,"description":2288,"pubDate":2289,"source":17,"tags":2290,"url":2291},"Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things","arXiv:2506.06396v1 Announce Type: cross \nAbstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of Battlefield Things (IoBT), gives rise to new opportunities for enhancing situational awareness. To increase the potential of IoBT for situational awareness in critical decision making, the data from these devices must be processed into consumer-ready information objects, and made available to consumers on demand. To address this challenge we propose a workflow that makes use of natural language processing (NLP) to query a database technology and return a response in natural language. Our solution utilizes Large Language Models (LLMs) that are sized for edge devices to perform NLP as well as graphical databases which are well suited for dynamic connected networks which are pervasive in the IoBT. Our architecture employs LLMs for both mapping questions in natural language to Cypher database queries as well as to summarize the database output back to the user in natural language. We evaluate several medium sized LLMs for both of these tasks on a database representing publicly available data from the US Army's Multipurpose Sensing Area (MSA) at the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion parameters) outperforms the other models across all the considered metrics. Most importantly, we note that, unlike current methods, our two step approach allows the relaxation of the Exact Match (EM) requirement of the produced Cypher queries with ground truth code and, in this way, it achieves a 19.4% increase in accuracy. Our workflow lays the ground work for deploying LLMs on edge devices to enable natural language interactions with databases containing information objects for critical decision making.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06396","Computer Science > Computation and Language\r\n[Submitted on 5 Jun 2025]\r\nTitle:Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things\r\nView PDF HTML (experimental)Abstract:The expansion of the Internet of Things (IoT) in the battlefield, Internet of Battlefield Things (IoBT), gives rise to new opportunities for enhancing situational awareness. To increase the potential of IoBT for situational awareness in critical decision making, the data from these devices must be processed into consumer-ready information objects, and made available to consumers on demand. To address this challenge we propose a workflow that makes use of natural language processing (NLP) to query a database technology and return a response in natural language. Our solution utilizes Large Language Models (LLMs) that are sized for edge devices to perform NLP as well as graphical databases which are well suited for dynamic connected networks which are pervasive in the IoBT. Our architecture employs LLMs for both mapping questions in natural language to Cypher database queries as well as to summarize the database output back to the user in natural language. We evaluate several medium sized LLMs for both of these tasks on a database representing publicly available data from the US Army's Multipurpose Sensing Area (MSA) at the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion parameters) outperforms the other models across all the considered metrics. Most importantly, we note that, unlike current methods, our two step approach allows the relaxation of the Exact Match (EM) requirement of the produced Cypher queries with ground truth code and, in this way, it achieves a 19.4% increase in accuracy. Our workflow lays the ground work for deploying LLMs on edge devices to enable natural language interactions with databases containing information objects for critical decision making.\r\nCurrent browse context:\r\ncs.CL\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-natural-language-interaction-with-databases-on-edge-devices-in-the-internet-of-b-2ee427.md","6a734cf2fcff64a4",{"html":2296,"metadata":2297},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 5 Jun 2025]\r\nTitle:Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things\r\nView PDF HTML (experimental)Abstract:The expansion of the Internet of Things (IoT) in the battlefield, Internet of Battlefield Things (IoBT), gives rise to new opportunities for enhancing situational awareness. To increase the potential of IoBT for situational awareness in critical decision making, the data from these devices must be processed into consumer-ready information objects, and made available to consumers on demand. To address this challenge we propose a workflow that makes use of natural language processing (NLP) to query a database technology and return a response in natural language. Our solution utilizes Large Language Models (LLMs) that are sized for edge devices to perform NLP as well as graphical databases which are well suited for dynamic connected networks which are pervasive in the IoBT. Our architecture employs LLMs for both mapping questions in natural language to Cypher database queries as well as to summarize the database output back to the user in natural language. We evaluate several medium sized LLMs for both of these tasks on a database representing publicly available data from the US Army’s Multipurpose Sensing Area (MSA) at the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion parameters) outperforms the other models across all the considered metrics. Most importantly, we note that, unlike current methods, our two step approach allows the relaxation of the Exact Match (EM) requirement of the produced Cypher queries with ground truth code and, in this way, it achieves a 19.4% increase in accuracy. Our workflow lays the ground work for deploying LLMs on edge devices to enable natural language interactions with databases containing information objects for critical decision making.\r\nCurrent browse context:\r\ncs.CL\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2298,"localImagePaths":2299,"remoteImagePaths":2300,"frontmatter":2301,"imagePaths":2303},[],[],[],{"title":2287,"description":2288,"pubDate":33,"source":17,"tags":2302,"url":2291},[19,20,21],[],"2025-06-10-natural-language-interaction-with-databases-on-edge-devices-in-the-internet-of-b-2ee427.md","2025-06-10-neural-networks-with-image-recognition-by-pairs-cc38da",{"id":2305,"data":2307,"body":2313,"filePath":2314,"digest":2315,"rendered":2316,"legacyId":2325},{"title":2308,"description":2309,"pubDate":2310,"source":17,"tags":2311,"url":2312},"Neural networks with image recognition by pairs","arXiv:2506.06322v1 Announce Type: cross \nAbstract: Neural networks based on metric recognition methods have a strictly determined architecture. Number of neurons, connections, as well as weights and thresholds values are calculated analytically, based on the initial conditions of tasks: number of recognizable classes, number of samples, metric expressions used. This paper discusses the possibility of transforming these networks in order to apply classical learning algorithms to them without using analytical expressions that calculate weight values. In the received network, training is carried out by recognizing images in pairs. This approach simplifies the learning process and easily allows to expand the neural network by adding new images to the recognition task. The advantages of these networks, including such as: 1) network architecture simplicity and transparency; 2) training simplicity and reliability; 3) the possibility of using a large number of images in the recognition problem using a neural network; 4) a consistent increase in the number of recognizable classes without changing the previous values of weights and thresholds.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06322","Computer Science > Neural and Evolutionary Computing\r\n[Submitted on 29 May 2025]\r\nTitle:Neural networks with image recognition by pairs\r\nView PDFAbstract:Neural networks based on metric recognition methods have a strictly determined architecture. Number of neurons, connections, as well as weights and thresholds values are calculated analytically, based on the initial conditions of tasks: number of recognizable classes, number of samples, metric expressions used. This paper discusses the possibility of transforming these networks in order to apply classical learning algorithms to them without using analytical expressions that calculate weight values. In the received network, training is carried out by recognizing images in pairs. This approach simplifies the learning process and easily allows to expand the neural network by adding new images to the recognition task. The advantages of these networks, including such as: 1) network architecture simplicity and transparency; 2) training simplicity and reliability; 3) the possibility of using a large number of images in the recognition problem using a neural network; 4) a consistent increase in the number of recognizable classes without changing the previous values of weights and thresholds.\r\nSubmission history\r\nFrom: Polad Geidarov P.Sh. [view email][v1] Thu, 29 May 2025 15:20:14 UTC (1,125 KB)\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-neural-networks-with-image-recognition-by-pairs-cc38da.md","4731118b1b5b1a42",{"html":2317,"metadata":2318},"\u003Cp>Computer Science > Neural and Evolutionary Computing\r\n[Submitted on 29 May 2025]\r\nTitle:Neural networks with image recognition by pairs\r\nView PDFAbstract:Neural networks based on metric recognition methods have a strictly determined architecture. Number of neurons, connections, as well as weights and thresholds values are calculated analytically, based on the initial conditions of tasks: number of recognizable classes, number of samples, metric expressions used. This paper discusses the possibility of transforming these networks in order to apply classical learning algorithms to them without using analytical expressions that calculate weight values. In the received network, training is carried out by recognizing images in pairs. This approach simplifies the learning process and easily allows to expand the neural network by adding new images to the recognition task. The advantages of these networks, including such as: 1) network architecture simplicity and transparency; 2) training simplicity and reliability; 3) the possibility of using a large number of images in the recognition problem using a neural network; 4) a consistent increase in the number of recognizable classes without changing the previous values of weights and thresholds.\r\nSubmission history\r\nFrom: Polad Geidarov P.Sh. [view email][v1] Thu, 29 May 2025 15:20:14 UTC (1,125 KB)\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2319,"localImagePaths":2320,"remoteImagePaths":2321,"frontmatter":2322,"imagePaths":2324},[],[],[],{"title":2308,"description":2309,"pubDate":33,"source":17,"tags":2323,"url":2312},[19,20,21],[],"2025-06-10-neural-networks-with-image-recognition-by-pairs-cc38da.md","2025-06-10-neurips-2025-e2lm-competition--early-training-evaluation-of-language-models-8b5260",{"id":2326,"data":2328,"body":2334,"filePath":2335,"digest":2336,"rendered":2337,"legacyId":2346},{"title":2329,"description":2330,"pubDate":2331,"source":17,"tags":2332,"url":2333},"NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models","arXiv:2506.07731v1 Announce Type: new \nAbstract: Existing benchmarks have proven effective for assessing the performance of fully trained large language models. However, we find striking differences in the early training stages of small models, where benchmarks often fail to provide meaningful or discriminative signals. To explore how these differences arise, this competition tackles the challenge of designing scientific knowledge evaluation tasks specifically tailored for measuring early training progress of language models. Participants are invited to develop novel evaluation methodologies or adapt existing benchmarks to better capture performance differences among language models. To support this effort, we provide three pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate checkpoints sampled during training up to 200B tokens. All experiments and development work can be run on widely available free cloud-based GPU platforms, making participation accessible to researchers with limited computational resources. Submissions will be evaluated based on three criteria: the quality of the performance signal they produce, the consistency of model rankings at 1 trillion tokens of training, and their relevance to the scientific knowledge domain. By promoting the design of tailored evaluation strategies for early training, this competition aims to attract a broad range of participants from various disciplines, including those who may not be machine learning experts or have access to dedicated GPU resources. Ultimately, this initiative seeks to make foundational LLM research more systematic and benchmark-informed from the earliest phases of model development.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07731","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models\r\nView PDF HTML (experimental)Abstract:Existing benchmarks have proven effective for assessing the performance of fully trained large language models. However, we find striking differences in the early training stages of small models, where benchmarks often fail to provide meaningful or discriminative signals. To explore how these differences arise, this competition tackles the challenge of designing scientific knowledge evaluation tasks specifically tailored for measuring early training progress of language models. Participants are invited to develop novel evaluation methodologies or adapt existing benchmarks to better capture performance differences among language models. To support this effort, we provide three pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate checkpoints sampled during training up to 200B tokens. All experiments and development work can be run on widely available free cloud-based GPU platforms, making participation accessible to researchers with limited computational resources. Submissions will be evaluated based on three criteria: the quality of the performance signal they produce, the consistency of model rankings at 1 trillion tokens of training, and their relevance to the scientific knowledge domain. By promoting the design of tailored evaluation strategies for early training, this competition aims to attract a broad range of participants from various disciplines, including those who may not be machine learning experts or have access to dedicated GPU resources. Ultimately, this initiative seeks to make foundational LLM research more systematic and benchmark-informed from the earliest phases of model development.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-neurips-2025-e2lm-competition--early-training-evaluation-of-language-models-8b5260.md","dada21cd3ca69a10",{"html":2338,"metadata":2339},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models\r\nView PDF HTML (experimental)Abstract:Existing benchmarks have proven effective for assessing the performance of fully trained large language models. However, we find striking differences in the early training stages of small models, where benchmarks often fail to provide meaningful or discriminative signals. To explore how these differences arise, this competition tackles the challenge of designing scientific knowledge evaluation tasks specifically tailored for measuring early training progress of language models. Participants are invited to develop novel evaluation methodologies or adapt existing benchmarks to better capture performance differences among language models. To support this effort, we provide three pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate checkpoints sampled during training up to 200B tokens. All experiments and development work can be run on widely available free cloud-based GPU platforms, making participation accessible to researchers with limited computational resources. Submissions will be evaluated based on three criteria: the quality of the performance signal they produce, the consistency of model rankings at 1 trillion tokens of training, and their relevance to the scientific knowledge domain. By promoting the design of tailored evaluation strategies for early training, this competition aims to attract a broad range of participants from various disciplines, including those who may not be machine learning experts or have access to dedicated GPU resources. Ultimately, this initiative seeks to make foundational LLM research more systematic and benchmark-informed from the earliest phases of model development.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2340,"localImagePaths":2341,"remoteImagePaths":2342,"frontmatter":2343,"imagePaths":2345},[],[],[],{"title":2329,"description":2330,"pubDate":33,"source":17,"tags":2344,"url":2333},[19,20,21],[],"2025-06-10-neurips-2025-e2lm-competition--early-training-evaluation-of-language-models-8b5260.md","2025-06-10-non-intrusive-load-monitoring-based-on-image-load-signatures-and-continual-learn-feea47",{"id":2347,"data":2349,"body":2355,"filePath":2356,"digest":2357,"rendered":2358,"legacyId":2367},{"title":2350,"description":2351,"pubDate":2352,"source":17,"tags":2353,"url":2354},"Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning","arXiv:2506.06637v1 Announce Type: cross \nAbstract: Non-Intrusive Load Monitoring (NILM) identifies the operating status and energy consumption of each electrical device in the circuit by analyzing the electrical signals at the bus, which is of great significance for smart power management. However, the complex and changeable load combinations and application environments lead to the challenges of poor feature robustness and insufficient model generalization of traditional NILM methods. To this end, this paper proposes a new non-intrusive load monitoring method that integrates \"image load signature\" and continual learning. This method converts multi-dimensional power signals such as current, voltage, and power factor into visual image load feature signatures, and combines deep convolutional neural networks to realize the identification and classification of multiple devices; at the same time, self-supervised pre-training is introduced to improve feature generalization, and continual online learning strategies are used to overcome model forgetting to adapt to the emergence of new loads. This paper conducts a large number of experiments on high-sampling rate load datasets, and compares a variety of existing methods and model variants. The results show that the proposed method has achieved significant improvements in recognition accuracy.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06637","Computer Science > Machine Learning\r\n[Submitted on 7 Jun 2025]\r\nTitle:Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning\r\nView PDFAbstract:Non-Intrusive Load Monitoring (NILM) identifies the operating status and energy consumption of each electrical device in the circuit by analyzing the electrical signals at the bus, which is of great significance for smart power management. However, the complex and changeable load combinations and application environments lead to the challenges of poor feature robustness and insufficient model generalization of traditional NILM methods. To this end, this paper proposes a new non-intrusive load monitoring method that integrates \"image load signature\" and continual learning. This method converts multi-dimensional power signals such as current, voltage, and power factor into visual image load feature signatures, and combines deep convolutional neural networks to realize the identification and classification of multiple devices; at the same time, self-supervised pre-training is introduced to improve feature generalization, and continual online learning strategies are used to overcome model forgetting to adapt to the emergence of new loads. This paper conducts a large number of experiments on high-sampling rate load datasets, and compares a variety of existing methods and model variants. The results show that the proposed method has achieved significant improvements in recognition accuracy.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-non-intrusive-load-monitoring-based-on-image-load-signatures-and-continual-learn-feea47.md","bcfe7cd4d8f407d8",{"html":2359,"metadata":2360},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 7 Jun 2025]\r\nTitle:Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning\r\nView PDFAbstract:Non-Intrusive Load Monitoring (NILM) identifies the operating status and energy consumption of each electrical device in the circuit by analyzing the electrical signals at the bus, which is of great significance for smart power management. However, the complex and changeable load combinations and application environments lead to the challenges of poor feature robustness and insufficient model generalization of traditional NILM methods. To this end, this paper proposes a new non-intrusive load monitoring method that integrates “image load signature” and continual learning. This method converts multi-dimensional power signals such as current, voltage, and power factor into visual image load feature signatures, and combines deep convolutional neural networks to realize the identification and classification of multiple devices; at the same time, self-supervised pre-training is introduced to improve feature generalization, and continual online learning strategies are used to overcome model forgetting to adapt to the emergence of new loads. This paper conducts a large number of experiments on high-sampling rate load datasets, and compares a variety of existing methods and model variants. The results show that the proposed method has achieved significant improvements in recognition accuracy.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2361,"localImagePaths":2362,"remoteImagePaths":2363,"frontmatter":2364,"imagePaths":2366},[],[],[],{"title":2350,"description":2351,"pubDate":33,"source":17,"tags":2365,"url":2354},[19,20,21],[],"2025-06-10-non-intrusive-load-monitoring-based-on-image-load-signatures-and-continual-learn-feea47.md","2025-06-10-noise-consistency-regularization-for-improved-subject-driven-image-synthesis-566791",{"id":2368,"data":2370,"body":2376,"filePath":2377,"digest":2378,"rendered":2379,"legacyId":2388},{"title":2371,"description":2372,"pubDate":2373,"source":17,"tags":2374,"url":2375},"Noise Consistency Regularization for Improved Subject-Driven Image Synthesis","arXiv:2506.06483v1 Announce Type: cross \nAbstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by adapting the model to generate images containing specific subjects. However, existing fine-tuning methods suffer from two key issues: underfitting, where the model fails to reliably capture subject identity, and overfitting, where it memorizes the subject image and reduces background diversity. To address these challenges, we propose two auxiliary consistency losses for diffusion fine-tuning. First, a prior consistency regularization loss ensures that the predicted diffusion noise for prior (non-subject) images remains consistent with that of the pretrained model, improving fidelity. Second, a subject consistency regularization loss enhances the fine-tuned model's robustness to multiplicative noise modulated latent code, helping to preserve subject identity while improving diversity. Our experimental results demonstrate that incorporating these losses into fine-tuning not only preserves subject identity but also enhances image diversity, outperforming DreamBooth in terms of CLIP scores, background variation, and overall visual quality.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06483","Computer Science > Graphics\r\n[Submitted on 6 Jun 2025]\r\nTitle:Noise Consistency Regularization for Improved Subject-Driven Image Synthesis\r\nView PDF HTML (experimental)Abstract:Fine-tuning Stable Diffusion enables subject-driven image synthesis by adapting the model to generate images containing specific subjects. However, existing fine-tuning methods suffer from two key issues: underfitting, where the model fails to reliably capture subject identity, and overfitting, where it memorizes the subject image and reduces background diversity. To address these challenges, we propose two auxiliary consistency losses for diffusion fine-tuning. First, a prior consistency regularization loss ensures that the predicted diffusion noise for prior (non-subject) images remains consistent with that of the pretrained model, improving fidelity. Second, a subject consistency regularization loss enhances the fine-tuned model's robustness to multiplicative noise modulated latent code, helping to preserve subject identity while improving diversity. Our experimental results demonstrate that incorporating these losses into fine-tuning not only preserves subject identity but also enhances image diversity, outperforming DreamBooth in terms of CLIP scores, background variation, and overall visual quality.\r\nCurrent browse context:\r\ncs.GR\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-noise-consistency-regularization-for-improved-subject-driven-image-synthesis-566791.md","d2640991019de3c1",{"html":2380,"metadata":2381},"\u003Cp>Computer Science > Graphics\r\n[Submitted on 6 Jun 2025]\r\nTitle:Noise Consistency Regularization for Improved Subject-Driven Image Synthesis\r\nView PDF HTML (experimental)Abstract:Fine-tuning Stable Diffusion enables subject-driven image synthesis by adapting the model to generate images containing specific subjects. However, existing fine-tuning methods suffer from two key issues: underfitting, where the model fails to reliably capture subject identity, and overfitting, where it memorizes the subject image and reduces background diversity. To address these challenges, we propose two auxiliary consistency losses for diffusion fine-tuning. First, a prior consistency regularization loss ensures that the predicted diffusion noise for prior (non-subject) images remains consistent with that of the pretrained model, improving fidelity. Second, a subject consistency regularization loss enhances the fine-tuned model’s robustness to multiplicative noise modulated latent code, helping to preserve subject identity while improving diversity. Our experimental results demonstrate that incorporating these losses into fine-tuning not only preserves subject identity but also enhances image diversity, outperforming DreamBooth in terms of CLIP scores, background variation, and overall visual quality.\r\nCurrent browse context:\r\ncs.GR\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2382,"localImagePaths":2383,"remoteImagePaths":2384,"frontmatter":2385,"imagePaths":2387},[],[],[],{"title":2371,"description":2372,"pubDate":33,"source":17,"tags":2386,"url":2375},[19,20,21],[],"2025-06-10-noise-consistency-regularization-for-improved-subject-driven-image-synthesis-566791.md","2025-06-10-nfisis-new-perspectives-on-fuzzy-inference-systems-for-renewable-energy-forecast-f72f28",{"id":2389,"data":2391,"body":2397,"filePath":2398,"digest":2399,"rendered":2400,"legacyId":2409},{"title":2392,"description":2393,"pubDate":2394,"source":17,"tags":2395,"url":2396},"NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting","arXiv:2506.06285v1 Announce Type: new \nAbstract: Evolving Fuzzy Systems (eFS) have gained significant attention due to their ability to adaptively update their structure in response to data dynamics while maintaining interpretability. However, the lack of publicly available implementations of these models limits their accessibility and widespread adoption. To address this gap, we present evolvingfuzzysystems, a Python library that provides implementations of several well-established eFS models, including ePL-KRLS-DISCO, ePL+, eMG, ePL, exTS, Simpl\\_eTS, and eTS. The library facilitates model evaluation and comparison by offering built-in tools for training, visualization, and performance assessment. The models are evaluated using the fetch\\_california\\_housing dataset, with performance measured in terms of normalized root-mean-square error (NRMSE), non-dimensional error index (NDEI), and mean absolute percentage error (MAPE). Additionally, computational complexity is analyzed by measuring execution times and rule evolution during training and testing phases. The results highlight ePL as a simple yet efficient model that balances accuracy and computational cost, making it particularly suitable for real-world applications. By making these models publicly available, evolvingfuzzysystems aims to foster research and practical applications in adaptive and interpretable machine learning.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06285","Computer Science > Artificial Intelligence\r\n[Submitted on 28 Apr 2025]\r\nTitle:NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting\r\nView PDF HTML (experimental)Abstract:Evolving Fuzzy Systems (eFS) have gained significant attention due to their ability to adaptively update their structure in response to data dynamics while maintaining interpretability. However, the lack of publicly available implementations of these models limits their accessibility and widespread adoption. To address this gap, we present evolvingfuzzysystems, a Python library that provides implementations of several well-established eFS models, including ePL-KRLS-DISCO, ePL+, eMG, ePL, exTS, Simpl\\_eTS, and eTS. The library facilitates model evaluation and comparison by offering built-in tools for training, visualization, and performance assessment. The models are evaluated using the fetch\\_california\\_housing dataset, with performance measured in terms of normalized root-mean-square error (NRMSE), non-dimensional error index (NDEI), and mean absolute percentage error (MAPE). Additionally, computational complexity is analyzed by measuring execution times and rule evolution during training and testing phases. The results highlight ePL as a simple yet efficient model that balances accuracy and computational cost, making it particularly suitable for real-world applications. By making these models publicly available, evolvingfuzzysystems aims to foster research and practical applications in adaptive and interpretable machine learning.\r\nSubmission history\r\nFrom: Kaike Sa Teles Rocha Alves Mr. [view email][v1] Mon, 28 Apr 2025 20:18:46 UTC (301 KB)\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-nfisis-new-perspectives-on-fuzzy-inference-systems-for-renewable-energy-forecast-f72f28.md","b57c2bc14f916873",{"html":2401,"metadata":2402},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 28 Apr 2025]\r\nTitle:NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting\r\nView PDF HTML (experimental)Abstract:Evolving Fuzzy Systems (eFS) have gained significant attention due to their ability to adaptively update their structure in response to data dynamics while maintaining interpretability. However, the lack of publicly available implementations of these models limits their accessibility and widespread adoption. To address this gap, we present evolvingfuzzysystems, a Python library that provides implementations of several well-established eFS models, including ePL-KRLS-DISCO, ePL+, eMG, ePL, exTS, Simpl_eTS, and eTS. The library facilitates model evaluation and comparison by offering built-in tools for training, visualization, and performance assessment. The models are evaluated using the fetch_california_housing dataset, with performance measured in terms of normalized root-mean-square error (NRMSE), non-dimensional error index (NDEI), and mean absolute percentage error (MAPE). Additionally, computational complexity is analyzed by measuring execution times and rule evolution during training and testing phases. The results highlight ePL as a simple yet efficient model that balances accuracy and computational cost, making it particularly suitable for real-world applications. By making these models publicly available, evolvingfuzzysystems aims to foster research and practical applications in adaptive and interpretable machine learning.\r\nSubmission history\r\nFrom: Kaike Sa Teles Rocha Alves Mr. [view email][v1] Mon, 28 Apr 2025 20:18:46 UTC (301 KB)\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2403,"localImagePaths":2404,"remoteImagePaths":2405,"frontmatter":2406,"imagePaths":2408},[],[],[],{"title":2392,"description":2393,"pubDate":33,"source":17,"tags":2407,"url":2396},[19,20,21],[],"2025-06-10-nfisis-new-perspectives-on-fuzzy-inference-systems-for-renewable-energy-forecast-f72f28.md","2025-06-10-nr4der-neural-re-ranking-for-diversified-exercise-recommendation-d3aab0",{"id":2410,"data":2412,"body":2418,"filePath":2419,"digest":2420,"rendered":2421,"legacyId":2430},{"title":2413,"description":2414,"pubDate":2415,"source":17,"tags":2416,"url":2417},"NR4DER: Neural Re-ranking for Diversified Exercise Recommendation","arXiv:2506.06341v1 Announce Type: cross \nAbstract: With the widespread adoption of online education platforms, an increasing number of students are gaining new knowledge through Massive Open Online Courses (MOOCs). Exercise recommendation have made strides toward improving student learning outcomes. However, existing methods not only struggle with high dropout rates but also fail to match the diverse learning pace of students. They frequently face difficulties in adjusting to inactive students' learning patterns and in accommodating individualized learning paces, resulting in limited accuracy and diversity in recommendations. To tackle these challenges, we propose Neural Re-ranking for Diversified Exercise Recommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to improve the effectiveness of the exercise filter module. It then employs a sequence enhancement method to enhance the representation of inactive students, accurately matches students with exercises of appropriate difficulty. Finally, it utilizes neural re-ranking to generate diverse recommendation lists based on individual students' learning histories. Extensive experimental results indicate that NR4DER significantly outperforms existing methods across multiple real-world datasets and effectively caters to the diverse learning pace of students.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06341","Computer Science > Information Retrieval\r\n[Submitted on 1 Jun 2025]\r\nTitle:NR4DER: Neural Re-ranking for Diversified Exercise Recommendation\r\nView PDF HTML (experimental)Abstract:With the widespread adoption of online education platforms, an increasing number of students are gaining new knowledge through Massive Open Online Courses (MOOCs). Exercise recommendation have made strides toward improving student learning outcomes. However, existing methods not only struggle with high dropout rates but also fail to match the diverse learning pace of students. They frequently face difficulties in adjusting to inactive students' learning patterns and in accommodating individualized learning paces, resulting in limited accuracy and diversity in recommendations. To tackle these challenges, we propose Neural Re-ranking for Diversified Exercise Recommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to improve the effectiveness of the exercise filter module. It then employs a sequence enhancement method to enhance the representation of inactive students, accurately matches students with exercises of appropriate difficulty. Finally, it utilizes neural re-ranking to generate diverse recommendation lists based on individual students' learning histories. Extensive experimental results indicate that NR4DER significantly outperforms existing methods across multiple real-world datasets and effectively caters to the diverse learning pace of students.\r\nCurrent browse context:\r\ncs.IR\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-nr4der-neural-re-ranking-for-diversified-exercise-recommendation-d3aab0.md","9298910da619e18f",{"html":2422,"metadata":2423},"\u003Cp>Computer Science > Information Retrieval\r\n[Submitted on 1 Jun 2025]\r\nTitle:NR4DER: Neural Re-ranking for Diversified Exercise Recommendation\r\nView PDF HTML (experimental)Abstract:With the widespread adoption of online education platforms, an increasing number of students are gaining new knowledge through Massive Open Online Courses (MOOCs). Exercise recommendation have made strides toward improving student learning outcomes. However, existing methods not only struggle with high dropout rates but also fail to match the diverse learning pace of students. They frequently face difficulties in adjusting to inactive students’ learning patterns and in accommodating individualized learning paces, resulting in limited accuracy and diversity in recommendations. To tackle these challenges, we propose Neural Re-ranking for Diversified Exercise Recommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to improve the effectiveness of the exercise filter module. It then employs a sequence enhancement method to enhance the representation of inactive students, accurately matches students with exercises of appropriate difficulty. Finally, it utilizes neural re-ranking to generate diverse recommendation lists based on individual students’ learning histories. Extensive experimental results indicate that NR4DER significantly outperforms existing methods across multiple real-world datasets and effectively caters to the diverse learning pace of students.\r\nCurrent browse context:\r\ncs.IR\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2424,"localImagePaths":2425,"remoteImagePaths":2426,"frontmatter":2427,"imagePaths":2429},[],[],[],{"title":2413,"description":2414,"pubDate":33,"source":17,"tags":2428,"url":2417},[19,20,21],[],"2025-06-10-nr4der-neural-re-ranking-for-diversified-exercise-recommendation-d3aab0.md","2025-06-10-on-the-fundamental-impossibility-of-hallucination-control-in-large-language-mode-175b60",{"id":2431,"data":2433,"body":2439,"filePath":2440,"digest":2441,"rendered":2442,"legacyId":2451},{"title":2434,"description":2435,"pubDate":2436,"source":17,"tags":2437,"url":2438},"On the Fundamental Impossibility of Hallucination Control in Large Language Models","arXiv:2506.06382v1 Announce Type: cross \nAbstract: This paper explains \\textbf{why it is impossible to create large language models that do not hallucinate and what are the trade-offs we should be looking for}. It presents a formal \\textbf{impossibility theorem} demonstrating that no inference mechanism can simultaneously satisfy four fundamental properties: \\textbf{truthful (non-hallucinatory) generation, semantic information conservation, relevant knowledge revelation, and knowledge-constrained optimality}. By modeling LLM inference as an \\textbf{auction of ideas} where neural components compete to contribute to responses, we prove the impossibility using the Green-Laffont theorem. That mathematical framework provides a rigorous foundation for understanding the nature of inference process, with implications for model architecture, training objectives, and evaluation methods.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06382","Statistics > Machine Learning\r\n[Submitted on 4 Jun 2025]\r\nTitle:On the Fundamental Impossibility of Hallucination Control in Large Language Models\r\nView PDF HTML (experimental)Abstract:This paper explains \\textbf{why it is impossible to create large language models that do not hallucinate and what are the trade-offs we should be looking for}. It presents a formal \\textbf{impossibility theorem} demonstrating that no inference mechanism can simultaneously satisfy four fundamental properties: \\textbf{truthful (non-hallucinatory) generation, semantic information conservation, relevant knowledge revelation, and knowledge-constrained optimality}. By modeling LLM inference as an \\textbf{auction of ideas} where neural components compete to contribute to responses, we prove the impossibility using the Green-Laffont theorem. That mathematical framework provides a rigorous foundation for understanding the nature of inference process, with implications for model architecture, training objectives, and evaluation methods.\r\nCurrent browse context:\r\nstat.ML\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-on-the-fundamental-impossibility-of-hallucination-control-in-large-language-mode-175b60.md","daea0ae033de1f2e",{"html":2443,"metadata":2444},"\u003Cp>Statistics > Machine Learning\r\n[Submitted on 4 Jun 2025]\r\nTitle:On the Fundamental Impossibility of Hallucination Control in Large Language Models\r\nView PDF HTML (experimental)Abstract:This paper explains \\textbf{why it is impossible to create large language models that do not hallucinate and what are the trade-offs we should be looking for}. It presents a formal \\textbf{impossibility theorem} demonstrating that no inference mechanism can simultaneously satisfy four fundamental properties: \\textbf{truthful (non-hallucinatory) generation, semantic information conservation, relevant knowledge revelation, and knowledge-constrained optimality}. By modeling LLM inference as an \\textbf{auction of ideas} where neural components compete to contribute to responses, we prove the impossibility using the Green-Laffont theorem. That mathematical framework provides a rigorous foundation for understanding the nature of inference process, with implications for model architecture, training objectives, and evaluation methods.\r\nCurrent browse context:\r\nstat.ML\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2445,"localImagePaths":2446,"remoteImagePaths":2447,"frontmatter":2448,"imagePaths":2450},[],[],[],{"title":2434,"description":2435,"pubDate":33,"source":17,"tags":2449,"url":2438},[19,20,21],[],"2025-06-10-on-the-fundamental-impossibility-of-hallucination-control-in-large-language-mode-175b60.md","2025-06-10-optimal-patient-allocation-for-echocardiographic-assessments-6fddd6",{"id":2452,"data":2454,"body":2460,"filePath":2461,"digest":2462,"rendered":2463,"legacyId":2472},{"title":2455,"description":2456,"pubDate":2457,"source":17,"tags":2458,"url":2459},"Optimal patient allocation for echocardiographic assessments","arXiv:2506.06297v1 Announce Type: cross \nAbstract: Scheduling echocardiographic exams in a hospital presents significant challenges due to non-deterministic factors (e.g., patient no-shows, patient arrival times, diverse exam durations, etc.) and asymmetric resource constraints between fetal and non-fetal patient streams. To address these challenges, we first conducted extensive pre-processing on one week of operational data from the Echo Laboratory at Stanford University's Lucile Packard Children's Hospital, to estimate patient no-show probabilities and derive empirical distributions of arrival times and exam durations. Based on these inputs, we developed a discrete-event stochastic simulation model using SimPy, and integrate it with the open source Gymnasium Python library. As a baseline for policy optimization, we developed a comparative framework to evaluate on-the-fly versus reservation-based allocation strategies, in which different proportions of resources are reserved in advance. Considering a hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2 ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation generally yields better performance, more effectively adapting to patient variability and resource constraints. Building on this foundation, we apply reinforcement learning (RL) to derive an approximated optimal dynamic allocation policy. This RL-based policy is benchmarked against the best-performing rule-based strategies, allowing us to quantify their differences and provide actionable insights for improving echo lab efficiency through intelligent, data-driven resource management.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06297","Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Optimal patient allocation for echocardiographic assessments\r\nView PDF HTML (experimental)Abstract:Scheduling echocardiographic exams in a hospital presents significant challenges due to non-deterministic factors (e.g., patient no-shows, patient arrival times, diverse exam durations, etc.) and asymmetric resource constraints between fetal and non-fetal patient streams. To address these challenges, we first conducted extensive pre-processing on one week of operational data from the Echo Laboratory at Stanford University's Lucile Packard Children's Hospital, to estimate patient no-show probabilities and derive empirical distributions of arrival times and exam durations. Based on these inputs, we developed a discrete-event stochastic simulation model using SimPy, and integrate it with the open source Gymnasium Python library. As a baseline for policy optimization, we developed a comparative framework to evaluate on-the-fly versus reservation-based allocation strategies, in which different proportions of resources are reserved in advance. Considering a hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2 ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation generally yields better performance, more effectively adapting to patient variability and resource constraints. Building on this foundation, we apply reinforcement learning (RL) to derive an approximated optimal dynamic allocation policy. This RL-based policy is benchmarked against the best-performing rule-based strategies, allowing us to quantify their differences and provide actionable insights for improving echo lab efficiency through intelligent, data-driven resource management.\r\nSubmission history\r\nFrom: Daniele Schiavazzi [view email][v1] Sat, 17 May 2025 17:51:23 UTC (9,150 KB)\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-optimal-patient-allocation-for-echocardiographic-assessments-6fddd6.md","230d49f4db1240f9",{"html":2464,"metadata":2465},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Optimal patient allocation for echocardiographic assessments\r\nView PDF HTML (experimental)Abstract:Scheduling echocardiographic exams in a hospital presents significant challenges due to non-deterministic factors (e.g., patient no-shows, patient arrival times, diverse exam durations, etc.) and asymmetric resource constraints between fetal and non-fetal patient streams. To address these challenges, we first conducted extensive pre-processing on one week of operational data from the Echo Laboratory at Stanford University’s Lucile Packard Children’s Hospital, to estimate patient no-show probabilities and derive empirical distributions of arrival times and exam durations. Based on these inputs, we developed a discrete-event stochastic simulation model using SimPy, and integrate it with the open source Gymnasium Python library. As a baseline for policy optimization, we developed a comparative framework to evaluate on-the-fly versus reservation-based allocation strategies, in which different proportions of resources are reserved in advance. Considering a hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2 ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation generally yields better performance, more effectively adapting to patient variability and resource constraints. Building on this foundation, we apply reinforcement learning (RL) to derive an approximated optimal dynamic allocation policy. This RL-based policy is benchmarked against the best-performing rule-based strategies, allowing us to quantify their differences and provide actionable insights for improving echo lab efficiency through intelligent, data-driven resource management.\r\nSubmission history\r\nFrom: Daniele Schiavazzi [view email][v1] Sat, 17 May 2025 17:51:23 UTC (9,150 KB)\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2466,"localImagePaths":2467,"remoteImagePaths":2468,"frontmatter":2469,"imagePaths":2471},[],[],[],{"title":2455,"description":2456,"pubDate":33,"source":17,"tags":2470,"url":2459},[19,20,21],[],"2025-06-10-optimal-patient-allocation-for-echocardiographic-assessments-6fddd6.md","2025-06-10-optimizing-rag-pipelines-for-arabic-a-systematic-analysis-of-core-components-f744b6",{"id":2473,"data":2475,"body":2481,"filePath":2482,"digest":2483,"rendered":2484,"legacyId":2493},{"title":2476,"description":2477,"pubDate":2478,"source":17,"tags":2479,"url":2480},"Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components","arXiv:2506.06339v1 Announce Type: cross \nAbstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture for combining the precision of retrieval systems with the fluency of large language models. While several studies have investigated RAG pipelines for high-resource languages, the optimization of RAG components for Arabic remains underexplored. This study presents a comprehensive empirical evaluation of state-of-the-art RAG components-including chunking strategies, embedding models, rerankers, and language models-across a diverse set of Arabic datasets. Using the RAGAS framework, we systematically compare performance across four core metrics: context precision, context recall, answer faithfulness, and answer relevancy. Our experiments demonstrate that sentence-aware chunking outperforms all other segmentation methods, while BGE-M3 and Multilingual-E5-large emerge as the most effective embedding models. The inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness in complex datasets, and Aya-8B surpasses StableLM in generation quality. These findings provide critical insights for building high-quality Arabic RAG pipelines and offer practical guidelines for selecting optimal components across different document types.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06339","Computer Science > Information Retrieval\r\n[Submitted on 1 Jun 2025]\r\nTitle:Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components\r\nView PDF HTML (experimental)Abstract:Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture for combining the precision of retrieval systems with the fluency of large language models. While several studies have investigated RAG pipelines for high-resource languages, the optimization of RAG components for Arabic remains underexplored. This study presents a comprehensive empirical evaluation of state-of-the-art RAG components-including chunking strategies, embedding models, rerankers, and language models-across a diverse set of Arabic datasets. Using the RAGAS framework, we systematically compare performance across four core metrics: context precision, context recall, answer faithfulness, and answer relevancy. Our experiments demonstrate that sentence-aware chunking outperforms all other segmentation methods, while BGE-M3 and Multilingual-E5-large emerge as the most effective embedding models. The inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness in complex datasets, and Aya-8B surpasses StableLM in generation quality. These findings provide critical insights for building high-quality Arabic RAG pipelines and offer practical guidelines for selecting optimal components across different document types.\r\nCurrent browse context:\r\ncs.IR\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-optimizing-rag-pipelines-for-arabic-a-systematic-analysis-of-core-components-f744b6.md","5398c3e2304e9e93",{"html":2485,"metadata":2486},"\u003Cp>Computer Science > Information Retrieval\r\n[Submitted on 1 Jun 2025]\r\nTitle:Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components\r\nView PDF HTML (experimental)Abstract:Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture for combining the precision of retrieval systems with the fluency of large language models. While several studies have investigated RAG pipelines for high-resource languages, the optimization of RAG components for Arabic remains underexplored. This study presents a comprehensive empirical evaluation of state-of-the-art RAG components-including chunking strategies, embedding models, rerankers, and language models-across a diverse set of Arabic datasets. Using the RAGAS framework, we systematically compare performance across four core metrics: context precision, context recall, answer faithfulness, and answer relevancy. Our experiments demonstrate that sentence-aware chunking outperforms all other segmentation methods, while BGE-M3 and Multilingual-E5-large emerge as the most effective embedding models. The inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness in complex datasets, and Aya-8B surpasses StableLM in generation quality. These findings provide critical insights for building high-quality Arabic RAG pipelines and offer practical guidelines for selecting optimal components across different document types.\r\nCurrent browse context:\r\ncs.IR\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2487,"localImagePaths":2488,"remoteImagePaths":2489,"frontmatter":2490,"imagePaths":2492},[],[],[],{"title":2476,"description":2477,"pubDate":33,"source":17,"tags":2491,"url":2480},[19,20,21],[],"2025-06-10-optimizing-rag-pipelines-for-arabic-a-systematic-analysis-of-core-components-f744b6.md","2025-06-10-pairwise-calibrated-rewards-for-pluralistic-alignment-7fcb3b",{"id":2494,"data":2496,"body":2502,"filePath":2503,"digest":2504,"rendered":2505,"legacyId":2514},{"title":2497,"description":2498,"pubDate":2499,"source":17,"tags":2500,"url":2501},"Pairwise Calibrated Rewards for Pluralistic Alignment","arXiv:2506.06298v1 Announce Type: cross \nAbstract: Current alignment pipelines presume a single, universal notion of desirable behavior. However, human preferences often diverge across users, contexts, and cultures. As a result, disagreement collapses into the majority signal and minority perspectives are discounted. To address this, we propose reflecting diverse human preferences through a distribution over multiple reward functions, each inducing a distinct aligned policy. The distribution is learned directly from pairwise preference without annotator identifiers or predefined groups. Instead, annotator disagreements are treated as informative soft labels. Our central criterion is pairwise calibration: for every pair of candidate responses, the proportion of reward functions preferring one response matches the fraction of annotators with that preference. We prove that even a small outlier-free ensemble can accurately represent diverse preference distributions. Empirically, we introduce and validate a practical training heuristic to learn such ensembles, and demonstrate its effectiveness through improved calibration, implying a more faithful representation of pluralistic values.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06298","Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Pairwise Calibrated Rewards for Pluralistic Alignment\r\nView PDF HTML (experimental)Abstract:Current alignment pipelines presume a single, universal notion of desirable behavior. However, human preferences often diverge across users, contexts, and cultures. As a result, disagreement collapses into the majority signal and minority perspectives are discounted. To address this, we propose reflecting diverse human preferences through a distribution over multiple reward functions, each inducing a distinct aligned policy. The distribution is learned directly from pairwise preference without annotator identifiers or predefined groups. Instead, annotator disagreements are treated as informative soft labels. Our central criterion is pairwise calibration: for every pair of candidate responses, the proportion of reward functions preferring one response matches the fraction of annotators with that preference. We prove that even a small outlier-free ensemble can accurately represent diverse preference distributions. Empirically, we introduce and validate a practical training heuristic to learn such ensembles, and demonstrate its effectiveness through improved calibration, implying a more faithful representation of pluralistic values.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-pairwise-calibrated-rewards-for-pluralistic-alignment-7fcb3b.md","5b37ac92ad964505",{"html":2506,"metadata":2507},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Pairwise Calibrated Rewards for Pluralistic Alignment\r\nView PDF HTML (experimental)Abstract:Current alignment pipelines presume a single, universal notion of desirable behavior. However, human preferences often diverge across users, contexts, and cultures. As a result, disagreement collapses into the majority signal and minority perspectives are discounted. To address this, we propose reflecting diverse human preferences through a distribution over multiple reward functions, each inducing a distinct aligned policy. The distribution is learned directly from pairwise preference without annotator identifiers or predefined groups. Instead, annotator disagreements are treated as informative soft labels. Our central criterion is pairwise calibration: for every pair of candidate responses, the proportion of reward functions preferring one response matches the fraction of annotators with that preference. We prove that even a small outlier-free ensemble can accurately represent diverse preference distributions. Empirically, we introduce and validate a practical training heuristic to learn such ensembles, and demonstrate its effectiveness through improved calibration, implying a more faithful representation of pluralistic values.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2508,"localImagePaths":2509,"remoteImagePaths":2510,"frontmatter":2511,"imagePaths":2513},[],[],[],{"title":2497,"description":2498,"pubDate":33,"source":17,"tags":2512,"url":2501},[19,20,21],[],"2025-06-10-pairwise-calibrated-rewards-for-pluralistic-alignment-7fcb3b.md","2025-06-10-private-gpts-for-llm-driven-testing-in-software-development-and-machine-learning-f9a717",{"id":2515,"data":2517,"body":2523,"filePath":2524,"digest":2525,"rendered":2526,"legacyId":2535},{"title":2518,"description":2519,"pubDate":2520,"source":17,"tags":2521,"url":2522},"Private GPTs for LLM-driven testing in software development and machine learning","arXiv:2506.06509v1 Announce Type: cross \nAbstract: In this contribution, we examine the capability of private GPTs to automatically generate executable test code based on requirements. More specifically, we use acceptance criteria as input, formulated as part of epics, or stories, which are typically used in modern development processes. This gives product owners, or business intelligence, respectively, a way to directly produce testable criteria through the use of LLMs. We explore the quality of the so-produced tests in two ways: i) directly by letting the LLM generate code from requirements, ii) through an intermediate step using Gherkin syntax. As a result, it turns out that the two-step procedure yields better results -where we define better in terms of human readability and best coding practices, i.e. lines of code and use of additional libraries typically used in testing. Concretely, we evaluate prompt effectiveness across two scenarios: a simple \"Hello World\" program and a digit classification model, showing that structured prompts lead to higher-quality test outputs.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06509","Computer Science > Software Engineering\r\n[Submitted on 6 Jun 2025]\r\nTitle:Private GPTs for LLM-driven testing in software development and machine learning\r\nView PDFAbstract:In this contribution, we examine the capability of private GPTs to automatically generate executable test code based on requirements. More specifically, we use acceptance criteria as input, formulated as part of epics, or stories, which are typically used in modern development processes. This gives product owners, or business intelligence, respectively, a way to directly produce testable criteria through the use of LLMs. We explore the quality of the so-produced tests in two ways: i) directly by letting the LLM generate code from requirements, ii) through an intermediate step using Gherkin syntax. As a result, it turns out that the two-step procedure yields better results -where we define better in terms of human readability and best coding practices, i.e. lines of code and use of additional libraries typically used in testing. Concretely, we evaluate prompt effectiveness across two scenarios: a simple \"Hello World\" program and a digit classification model, showing that structured prompts lead to higher-quality test outputs.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-private-gpts-for-llm-driven-testing-in-software-development-and-machine-learning-f9a717.md","d0d5a99b3b82fd68",{"html":2527,"metadata":2528},"\u003Cp>Computer Science > Software Engineering\r\n[Submitted on 6 Jun 2025]\r\nTitle:Private GPTs for LLM-driven testing in software development and machine learning\r\nView PDFAbstract:In this contribution, we examine the capability of private GPTs to automatically generate executable test code based on requirements. More specifically, we use acceptance criteria as input, formulated as part of epics, or stories, which are typically used in modern development processes. This gives product owners, or business intelligence, respectively, a way to directly produce testable criteria through the use of LLMs. We explore the quality of the so-produced tests in two ways: i) directly by letting the LLM generate code from requirements, ii) through an intermediate step using Gherkin syntax. As a result, it turns out that the two-step procedure yields better results -where we define better in terms of human readability and best coding practices, i.e. lines of code and use of additional libraries typically used in testing. Concretely, we evaluate prompt effectiveness across two scenarios: a simple “Hello World” program and a digit classification model, showing that structured prompts lead to higher-quality test outputs.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2529,"localImagePaths":2530,"remoteImagePaths":2531,"frontmatter":2532,"imagePaths":2534},[],[],[],{"title":2518,"description":2519,"pubDate":33,"source":17,"tags":2533,"url":2522},[19,20,21],[],"2025-06-10-private-gpts-for-llm-driven-testing-in-software-development-and-machine-learning-f9a717.md","2025-06-10-prediction-of-bank-credit-ratings-using-heterogeneous-topological-graph-neural-n-4d5ea7",{"id":2536,"data":2538,"body":2544,"filePath":2545,"digest":2546,"rendered":2547,"legacyId":2556},{"title":2539,"description":2540,"pubDate":2541,"source":17,"tags":2542,"url":2543},"Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks","arXiv:2506.06293v1 Announce Type: cross \nAbstract: Agencies such as Standard & Poor's and Moody's provide bank credit ratings that influence economic stability and decision-making by stakeholders. Accurate and timely predictions support informed decision-making, regulatory actions, and investor protection. However, a complete interbank connection graph is often unavailable due to privacy concerns, complicating the direct application of Graph Neural Networks (GNNs) for rating prediction. our research utilizes persistent homology to construct a network that captures relationships among banks and combines this with a traditional lending network to create a heterogeneous network that integrates information from both sources, leading to improved predictions. Experiments on a global, real-world dataset validate the effectiveness of HTGNN. This research has implications for investors and regulatory bodies in enhancing proactive risk mitigation and the implementation of effective market interventions.The code can be find at https://github.com/Liu-Jun-Yi/HTGNN.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06293","Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks\r\nView PDF HTML (experimental)Abstract:Agencies such as Standard & Poor's and Moody's provide bank credit ratings that influence economic stability and decision-making by stakeholders. Accurate and timely predictions support informed decision-making, regulatory actions, and investor protection. However, a complete interbank connection graph is often unavailable due to privacy concerns, complicating the direct application of Graph Neural Networks (GNNs) for rating prediction. our research utilizes persistent homology to construct a network that captures relationships among banks and combines this with a traditional lending network to create a heterogeneous network that integrates information from both sources, leading to improved predictions. Experiments on a global, real-world dataset validate the effectiveness of HTGNN. This research has implications for investors and regulatory bodies in enhancing proactive risk mitigation and the implementation of effective market this http URL code can be find at this https URL.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-prediction-of-bank-credit-ratings-using-heterogeneous-topological-graph-neural-n-4d5ea7.md","98847382768757fb",{"html":2548,"metadata":2549},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 17 May 2025]\r\nTitle:Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks\r\nView PDF HTML (experimental)Abstract:Agencies such as Standard &#x26; Poor’s and Moody’s provide bank credit ratings that influence economic stability and decision-making by stakeholders. Accurate and timely predictions support informed decision-making, regulatory actions, and investor protection. However, a complete interbank connection graph is often unavailable due to privacy concerns, complicating the direct application of Graph Neural Networks (GNNs) for rating prediction. our research utilizes persistent homology to construct a network that captures relationships among banks and combines this with a traditional lending network to create a heterogeneous network that integrates information from both sources, leading to improved predictions. Experiments on a global, real-world dataset validate the effectiveness of HTGNN. This research has implications for investors and regulatory bodies in enhancing proactive risk mitigation and the implementation of effective market this http URL code can be find at this https URL.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2550,"localImagePaths":2551,"remoteImagePaths":2552,"frontmatter":2553,"imagePaths":2555},[],[],[],{"title":2539,"description":2540,"pubDate":33,"source":17,"tags":2554,"url":2543},[19,20,21],[],"2025-06-10-prediction-of-bank-credit-ratings-using-heterogeneous-topological-graph-neural-n-4d5ea7.md","2025-06-10-reasoning-multimodal-large-language-model-data-contamination-and-dynamic-evaluat-c2141c",{"id":2557,"data":2559,"body":2565,"filePath":2566,"digest":2567,"rendered":2568,"legacyId":2577},{"title":2560,"description":2561,"pubDate":2562,"source":17,"tags":2563,"url":2564},"Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation","arXiv:2506.07202v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) show impressive vision-language benchmark performance, yet growing concerns about data contamination (test set exposure during training) risk masking true generalization. This concern extends to reasoning MLLMs, often fine-tuned via reinforcement learning from potentially contaminated base models. We propose a novel dynamic evaluation framework to rigorously assess MLLM generalization, moving beyond static benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the same visual input, models are evaluated across a family of tasks (e.g., QA, captioning, question posing, verification) to probe diverse capabilities. This task perturbation reveals whether model performance is robust or reliant on superficial task-specific cues. Our approach is analogous to loss landscape sharpness: models overfit or contaminated for a single task (sharp minima) falter under task shifts, unlike models with generalizable solutions (flatter minima). We developed an automated pipeline with a calibrated judge scoring open-ended generations (captions, questions) using paraphrase and corruption sampling. Applying this framework to leading image/video MLLMs on benchmarks including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task \"ability vector.\" We demonstrate that fine-tuning on simulated test data (extreme contamination) drastically sharpens task-specific performance but harms overall generalization. Our dynamic task perturbation offers deeper insights into MLLM generalization, distinguishing genuine understanding from spurious leakage or overfitting.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07202","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation\r\nView PDF HTML (experimental)Abstract:Multimodal Large Language Models (MLLMs) show impressive vision-language benchmark performance, yet growing concerns about data contamination (test set exposure during training) risk masking true generalization. This concern extends to reasoning MLLMs, often fine-tuned via reinforcement learning from potentially contaminated base models. We propose a novel dynamic evaluation framework to rigorously assess MLLM generalization, moving beyond static benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the same visual input, models are evaluated across a family of tasks (e.g., QA, captioning, question posing, verification) to probe diverse capabilities. This task perturbation reveals whether model performance is robust or reliant on superficial task-specific cues. Our approach is analogous to loss landscape sharpness: models overfit or contaminated for a single task (sharp minima) falter under task shifts, unlike models with generalizable solutions (flatter minima). We developed an automated pipeline with a calibrated judge scoring open-ended generations (captions, questions) using paraphrase and corruption sampling. Applying this framework to leading image/video MLLMs on benchmarks including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task \"ability vector.\" We demonstrate that fine-tuning on simulated test data (extreme contamination) drastically sharpens task-specific performance but harms overall generalization. Our dynamic task perturbation offers deeper insights into MLLM generalization, distinguishing genuine understanding from spurious leakage or overfitting.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-reasoning-multimodal-large-language-model-data-contamination-and-dynamic-evaluat-c2141c.md","9655a00e34734f29",{"html":2569,"metadata":2570},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation\r\nView PDF HTML (experimental)Abstract:Multimodal Large Language Models (MLLMs) show impressive vision-language benchmark performance, yet growing concerns about data contamination (test set exposure during training) risk masking true generalization. This concern extends to reasoning MLLMs, often fine-tuned via reinforcement learning from potentially contaminated base models. We propose a novel dynamic evaluation framework to rigorously assess MLLM generalization, moving beyond static benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the same visual input, models are evaluated across a family of tasks (e.g., QA, captioning, question posing, verification) to probe diverse capabilities. This task perturbation reveals whether model performance is robust or reliant on superficial task-specific cues. Our approach is analogous to loss landscape sharpness: models overfit or contaminated for a single task (sharp minima) falter under task shifts, unlike models with generalizable solutions (flatter minima). We developed an automated pipeline with a calibrated judge scoring open-ended generations (captions, questions) using paraphrase and corruption sampling. Applying this framework to leading image/video MLLMs on benchmarks including MME, RealWorldQA, and CVRR-ES, we analyze each model’s cross-task “ability vector.” We demonstrate that fine-tuning on simulated test data (extreme contamination) drastically sharpens task-specific performance but harms overall generalization. Our dynamic task perturbation offers deeper insights into MLLM generalization, distinguishing genuine understanding from spurious leakage or overfitting.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2571,"localImagePaths":2572,"remoteImagePaths":2573,"frontmatter":2574,"imagePaths":2576},[],[],[],{"title":2560,"description":2561,"pubDate":33,"source":17,"tags":2575,"url":2564},[19,20,21],[],"2025-06-10-reasoning-multimodal-large-language-model-data-contamination-and-dynamic-evaluat-c2141c.md","2025-06-10-reasoning-paths-as-signals-augmenting-multi-hop-fact-verification-through-struct-6444b4",{"id":2578,"data":2580,"body":2586,"filePath":2587,"digest":2588,"rendered":2589,"legacyId":2598},{"title":2581,"description":2582,"pubDate":2583,"source":17,"tags":2584,"url":2585},"Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression","arXiv:2506.07075v1 Announce Type: new \nAbstract: The growing complexity of factual claims in real-world scenarios presents significant challenges for automated fact verification systems, particularly in accurately aggregating and reasoning over multi-hop evidence. Existing approaches often rely on static or shallow models that fail to capture the evolving structure of reasoning paths, leading to fragmented retrieval and limited interpretability. To address these issues, we propose a Structural Reasoning framework for Multi-hop Fact Verification that explicitly models reasoning paths as structured graphs throughout both evidence retrieval and claim verification stages. Our method comprises two key modules: a structure-enhanced retrieval mechanism that constructs reasoning graphs to guide evidence collection, and a reasoning-path-guided verification module that incrementally builds subgraphs to represent evolving inference trajectories. We further incorporate a structure-aware reasoning mechanism that captures long-range dependencies across multi-hop evidence chains, enabling more precise verification. Extensive experiments on the FEVER and HoVer datasets demonstrate that our approach consistently outperforms strong baselines, highlighting the effectiveness of reasoning-path modeling in enhancing retrieval precision and verification accuracy.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07075","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression\r\nView PDF HTML (experimental)Abstract:The growing complexity of factual claims in real-world scenarios presents significant challenges for automated fact verification systems, particularly in accurately aggregating and reasoning over multi-hop evidence. Existing approaches often rely on static or shallow models that fail to capture the evolving structure of reasoning paths, leading to fragmented retrieval and limited interpretability. To address these issues, we propose a Structural Reasoning framework for Multi-hop Fact Verification that explicitly models reasoning paths as structured graphs throughout both evidence retrieval and claim verification stages. Our method comprises two key modules: a structure-enhanced retrieval mechanism that constructs reasoning graphs to guide evidence collection, and a reasoning-path-guided verification module that incrementally builds subgraphs to represent evolving inference trajectories. We further incorporate a structure-aware reasoning mechanism that captures long-range dependencies across multi-hop evidence chains, enabling more precise verification. Extensive experiments on the FEVER and HoVer datasets demonstrate that our approach consistently outperforms strong baselines, highlighting the effectiveness of reasoning-path modeling in enhancing retrieval precision and verification accuracy.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-reasoning-paths-as-signals-augmenting-multi-hop-fact-verification-through-struct-6444b4.md","a83932c826b91325",{"html":2590,"metadata":2591},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression\r\nView PDF HTML (experimental)Abstract:The growing complexity of factual claims in real-world scenarios presents significant challenges for automated fact verification systems, particularly in accurately aggregating and reasoning over multi-hop evidence. Existing approaches often rely on static or shallow models that fail to capture the evolving structure of reasoning paths, leading to fragmented retrieval and limited interpretability. To address these issues, we propose a Structural Reasoning framework for Multi-hop Fact Verification that explicitly models reasoning paths as structured graphs throughout both evidence retrieval and claim verification stages. Our method comprises two key modules: a structure-enhanced retrieval mechanism that constructs reasoning graphs to guide evidence collection, and a reasoning-path-guided verification module that incrementally builds subgraphs to represent evolving inference trajectories. We further incorporate a structure-aware reasoning mechanism that captures long-range dependencies across multi-hop evidence chains, enabling more precise verification. Extensive experiments on the FEVER and HoVer datasets demonstrate that our approach consistently outperforms strong baselines, highlighting the effectiveness of reasoning-path modeling in enhancing retrieval precision and verification accuracy.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2592,"localImagePaths":2593,"remoteImagePaths":2594,"frontmatter":2595,"imagePaths":2597},[],[],[],{"title":2581,"description":2582,"pubDate":33,"source":17,"tags":2596,"url":2585},[19,20,21],[],"2025-06-10-reasoning-paths-as-signals-augmenting-multi-hop-fact-verification-through-struct-6444b4.md","2025-06-10-quantile-regression-with-large-language-models-for-price-prediction-7ec821",{"id":2599,"data":2601,"body":2607,"filePath":2608,"digest":2609,"rendered":2610,"legacyId":2619},{"title":2602,"description":2603,"pubDate":2604,"source":17,"tags":2605,"url":2606},"Quantile Regression with Large Language Models for Price Prediction","arXiv:2506.06657v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) have shown promise in structured prediction tasks, including regression, but existing approaches primarily focus on point estimates and lack systematic comparison across different methods. We investigate probabilistic regression using LLMs for unstructured inputs, addressing challenging text-to-distribution prediction tasks such as price estimation where both nuanced text understanding and uncertainty quantification are critical. We propose a novel quantile regression approach that enables LLMs to produce full predictive distributions, improving upon traditional point estimates. Through extensive experiments across three diverse price prediction datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads significantly outperforms traditional approaches for both point and distributional estimations, as measured by three established metrics each for prediction accuracy and distributional calibration. Our systematic comparison of LLM approaches, model architectures, training approaches, and data scaling reveals that Mistral-7B consistently outperforms encoder architectures, embedding-based methods, and few-shot learning methods. Our experiments also reveal the effectiveness of LLM-assisted label correction in achieving human-level accuracy without systematic bias. Our curated datasets are made available at https://github.com/vnik18/llm-price-quantile-reg/ to support future research.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06657","Computer Science > Computation and Language\r\n[Submitted on 7 Jun 2025]\r\nTitle:Quantile Regression with Large Language Models for Price Prediction\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) have shown promise in structured prediction tasks, including regression, but existing approaches primarily focus on point estimates and lack systematic comparison across different methods. We investigate probabilistic regression using LLMs for unstructured inputs, addressing challenging text-to-distribution prediction tasks such as price estimation where both nuanced text understanding and uncertainty quantification are critical. We propose a novel quantile regression approach that enables LLMs to produce full predictive distributions, improving upon traditional point estimates. Through extensive experiments across three diverse price prediction datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads significantly outperforms traditional approaches for both point and distributional estimations, as measured by three established metrics each for prediction accuracy and distributional calibration. Our systematic comparison of LLM approaches, model architectures, training approaches, and data scaling reveals that Mistral-7B consistently outperforms encoder architectures, embedding-based methods, and few-shot learning methods. Our experiments also reveal the effectiveness of LLM-assisted label correction in achieving human-level accuracy without systematic bias. Our curated datasets are made available at this https URL to support future research.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-quantile-regression-with-large-language-models-for-price-prediction-7ec821.md","5482104e5c0acc27",{"html":2611,"metadata":2612},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 7 Jun 2025]\r\nTitle:Quantile Regression with Large Language Models for Price Prediction\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) have shown promise in structured prediction tasks, including regression, but existing approaches primarily focus on point estimates and lack systematic comparison across different methods. We investigate probabilistic regression using LLMs for unstructured inputs, addressing challenging text-to-distribution prediction tasks such as price estimation where both nuanced text understanding and uncertainty quantification are critical. We propose a novel quantile regression approach that enables LLMs to produce full predictive distributions, improving upon traditional point estimates. Through extensive experiments across three diverse price prediction datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads significantly outperforms traditional approaches for both point and distributional estimations, as measured by three established metrics each for prediction accuracy and distributional calibration. Our systematic comparison of LLM approaches, model architectures, training approaches, and data scaling reveals that Mistral-7B consistently outperforms encoder architectures, embedding-based methods, and few-shot learning methods. Our experiments also reveal the effectiveness of LLM-assisted label correction in achieving human-level accuracy without systematic bias. Our curated datasets are made available at this https URL to support future research.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2613,"localImagePaths":2614,"remoteImagePaths":2615,"frontmatter":2616,"imagePaths":2618},[],[],[],{"title":2602,"description":2603,"pubDate":33,"source":17,"tags":2617,"url":2606},[19,20,21],[],"2025-06-10-quantile-regression-with-large-language-models-for-price-prediction-7ec821.md","2025-06-10-reinforcement-learning-for-autonomous-warehouse-orchestration-in-sap-logistics-e-3e7f2a",{"id":2620,"data":2622,"body":2628,"filePath":2629,"digest":2630,"rendered":2631,"legacyId":2640},{"title":2623,"description":2624,"pubDate":2625,"source":17,"tags":2626,"url":2627},"Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility","arXiv:2506.06523v1 Announce Type: new \nAbstract: In an era of escalating supply chain demands, SAP Logistics Execution (LE) is pivotal for managing warehouse operations, transportation, and delivery. This research introduces a pioneering framework leveraging reinforcement learning (RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing operational agility and efficiency. By modeling warehouse processes as dynamic environments, the framework optimizes task allocation, inventory movement, and order picking in real-time. A synthetic dataset of 300,000 LE transactions simulates real-world warehouse scenarios, including multilingual data and operational disruptions. The analysis achieves 95% task optimization accuracy, reducing processing times by 60% compared to traditional methods. Visualizations, including efficiency heatmaps and performance graphs, guide agile warehouse strategies. This approach tackles data privacy, scalability, and SAP integration, offering a transformative solution for modern supply chains.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06523","Computer Science > Artificial Intelligence\r\n[Submitted on 6 Jun 2025]\r\nTitle:Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility\r\nView PDFAbstract:In an era of escalating supply chain demands, SAP Logistics Execution (LE) is pivotal for managing warehouse operations, transportation, and delivery. This research introduces a pioneering framework leveraging reinforcement learning (RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing operational agility and efficiency. By modeling warehouse processes as dynamic environments, the framework optimizes task allocation, inventory movement, and order picking in real-time. A synthetic dataset of 300,000 LE transactions simulates real-world warehouse scenarios, including multilingual data and operational disruptions. The analysis achieves 95% task optimization accuracy, reducing processing times by 60% compared to traditional methods. Visualizations, including efficiency heatmaps and performance graphs, guide agile warehouse strategies. This approach tackles data privacy, scalability, and SAP integration, offering a transformative solution for modern supply chains.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-reinforcement-learning-for-autonomous-warehouse-orchestration-in-sap-logistics-e-3e7f2a.md","5eda5dfe6f68bed3",{"html":2632,"metadata":2633},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 6 Jun 2025]\r\nTitle:Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility\r\nView PDFAbstract:In an era of escalating supply chain demands, SAP Logistics Execution (LE) is pivotal for managing warehouse operations, transportation, and delivery. This research introduces a pioneering framework leveraging reinforcement learning (RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing operational agility and efficiency. By modeling warehouse processes as dynamic environments, the framework optimizes task allocation, inventory movement, and order picking in real-time. A synthetic dataset of 300,000 LE transactions simulates real-world warehouse scenarios, including multilingual data and operational disruptions. The analysis achieves 95% task optimization accuracy, reducing processing times by 60% compared to traditional methods. Visualizations, including efficiency heatmaps and performance graphs, guide agile warehouse strategies. This approach tackles data privacy, scalability, and SAP integration, offering a transformative solution for modern supply chains.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2634,"localImagePaths":2635,"remoteImagePaths":2636,"frontmatter":2637,"imagePaths":2639},[],[],[],{"title":2623,"description":2624,"pubDate":33,"source":17,"tags":2638,"url":2627},[19,20,21],[],"2025-06-10-reinforcement-learning-for-autonomous-warehouse-orchestration-in-sap-logistics-e-3e7f2a.md","2025-06-10-reinforcing-multimodal-understanding-and-generation-with-dual-self-rewards-a68b62",{"id":2641,"data":2643,"body":2649,"filePath":2650,"digest":2651,"rendered":2652,"legacyId":2661},{"title":2644,"description":2645,"pubDate":2646,"source":17,"tags":2647,"url":2648},"Reinforcing Multimodal Understanding and Generation with Dual Self-rewards","arXiv:2506.07963v1 Announce Type: new \nAbstract: Building upon large language models (LLMs), recent large multimodal models (LMMs) unify cross-model understanding and generation into a single framework. However, LMMs still struggle to achieve accurate image-text alignment, prone to generating text responses contradicting the visual input or failing to follow the text-to-image prompts. Current solutions require external supervision (e.g., human feedback or reward models) and only address unidirectional tasks-either understanding or generation. In this work, based on the observation that understanding and generation are inverse dual tasks, we introduce a self-supervised dual reward mechanism to reinforce the understanding and generation capabilities of LMMs. Specifically, we sample multiple outputs for a given input in one task domain, then reverse the input-output pairs to compute the dual likelihood of the model as self-rewards for optimization. Extensive experimental results on visual understanding and generation benchmarks demonstrate that our method can effectively enhance the performance of the model without any external supervision, especially achieving remarkable improvements in text-to-image tasks.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07963","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Reinforcing Multimodal Understanding and Generation with Dual Self-rewards\r\nView PDF HTML (experimental)Abstract:Building upon large language models (LLMs), recent large multimodal models (LMMs) unify cross-model understanding and generation into a single framework. However, LMMs still struggle to achieve accurate image-text alignment, prone to generating text responses contradicting the visual input or failing to follow the text-to-image prompts. Current solutions require external supervision (e.g., human feedback or reward models) and only address unidirectional tasks-either understanding or generation. In this work, based on the observation that understanding and generation are inverse dual tasks, we introduce a self-supervised dual reward mechanism to reinforce the understanding and generation capabilities of LMMs. Specifically, we sample multiple outputs for a given input in one task domain, then reverse the input-output pairs to compute the dual likelihood of the model as self-rewards for optimization. Extensive experimental results on visual understanding and generation benchmarks demonstrate that our method can effectively enhance the performance of the model without any external supervision, especially achieving remarkable improvements in text-to-image tasks.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-reinforcing-multimodal-understanding-and-generation-with-dual-self-rewards-a68b62.md","7404d9bee020aa7b",{"html":2653,"metadata":2654},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Reinforcing Multimodal Understanding and Generation with Dual Self-rewards\r\nView PDF HTML (experimental)Abstract:Building upon large language models (LLMs), recent large multimodal models (LMMs) unify cross-model understanding and generation into a single framework. However, LMMs still struggle to achieve accurate image-text alignment, prone to generating text responses contradicting the visual input or failing to follow the text-to-image prompts. Current solutions require external supervision (e.g., human feedback or reward models) and only address unidirectional tasks-either understanding or generation. In this work, based on the observation that understanding and generation are inverse dual tasks, we introduce a self-supervised dual reward mechanism to reinforce the understanding and generation capabilities of LMMs. Specifically, we sample multiple outputs for a given input in one task domain, then reverse the input-output pairs to compute the dual likelihood of the model as self-rewards for optimization. Extensive experimental results on visual understanding and generation benchmarks demonstrate that our method can effectively enhance the performance of the model without any external supervision, especially achieving remarkable improvements in text-to-image tasks.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2655,"localImagePaths":2656,"remoteImagePaths":2657,"frontmatter":2658,"imagePaths":2660},[],[],[],{"title":2644,"description":2645,"pubDate":33,"source":17,"tags":2659,"url":2648},[19,20,21],[],"2025-06-10-reinforcing-multimodal-understanding-and-generation-with-dual-self-rewards-a68b62.md","2025-06-10-remoh-a-reflective-evolution-of-multi-objective-heuristics-approach-via-large-la-ceb2f4",{"id":2662,"data":2664,"body":2670,"filePath":2671,"digest":2672,"rendered":2673,"legacyId":2682},{"title":2665,"description":2666,"pubDate":2667,"source":17,"tags":2668,"url":2669},"REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models","arXiv:2506.07759v1 Announce Type: new \nAbstract: Multi-objective optimization is fundamental in complex decision-making tasks. Traditional algorithms, while effective, often demand extensive problem-specific modeling and struggle to adapt to nonlinear structures. Recent advances in Large Language Models (LLMs) offer enhanced explainability, adaptability, and reasoning. This work proposes Reflective Evolution of Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with LLM-based heuristic generation. A key innovation is a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics, improving convergence and maintaining solution diversity. The approach is evaluated on the Flexible Job Shop Scheduling Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate that REMoH achieves competitive results compared to state-of-the-art approaches with reduced modeling effort and enhanced adaptability. These findings underscore the potential of LLMs to augment traditional optimization, offering greater flexibility, interpretability, and robustness in multi-objective scenarios.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07759","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models\r\nView PDF HTML (experimental)Abstract:Multi-objective optimization is fundamental in complex decision-making tasks. Traditional algorithms, while effective, often demand extensive problem-specific modeling and struggle to adapt to nonlinear structures. Recent advances in Large Language Models (LLMs) offer enhanced explainability, adaptability, and reasoning. This work proposes Reflective Evolution of Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with LLM-based heuristic generation. A key innovation is a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics, improving convergence and maintaining solution diversity. The approach is evaluated on the Flexible Job Shop Scheduling Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate that REMoH achieves competitive results compared to state-of-the-art approaches with reduced modeling effort and enhanced adaptability. These findings underscore the potential of LLMs to augment traditional optimization, offering greater flexibility, interpretability, and robustness in multi-objective scenarios.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-remoh-a-reflective-evolution-of-multi-objective-heuristics-approach-via-large-la-ceb2f4.md","6185aef86d0c6e8a",{"html":2674,"metadata":2675},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models\r\nView PDF HTML (experimental)Abstract:Multi-objective optimization is fundamental in complex decision-making tasks. Traditional algorithms, while effective, often demand extensive problem-specific modeling and struggle to adapt to nonlinear structures. Recent advances in Large Language Models (LLMs) offer enhanced explainability, adaptability, and reasoning. This work proposes Reflective Evolution of Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with LLM-based heuristic generation. A key innovation is a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics, improving convergence and maintaining solution diversity. The approach is evaluated on the Flexible Job Shop Scheduling Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate that REMoH achieves competitive results compared to state-of-the-art approaches with reduced modeling effort and enhanced adaptability. These findings underscore the potential of LLMs to augment traditional optimization, offering greater flexibility, interpretability, and robustness in multi-objective scenarios.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2676,"localImagePaths":2677,"remoteImagePaths":2678,"frontmatter":2679,"imagePaths":2681},[],[],[],{"title":2665,"description":2666,"pubDate":33,"source":17,"tags":2680,"url":2669},[19,20,21],[],"2025-06-10-remoh-a-reflective-evolution-of-multi-objective-heuristics-approach-via-large-la-ceb2f4.md","2025-06-10-reward-is-enough-llms-are-in-context-reinforcement-learners-c74e41",{"id":2683,"data":2685,"body":2691,"filePath":2692,"digest":2693,"rendered":2694,"legacyId":2703},{"title":2686,"description":2687,"pubDate":2688,"source":17,"tags":2689,"url":2690},"Reward Is Enough: LLMs Are In-Context Reinforcement Learners","arXiv:2506.06303v1 Announce Type: cross \nAbstract: Reinforcement learning (RL) is a human-designed framework for solving sequential decision making problems. In this work, we demonstrate that, surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a phenomenon known as in-context RL (ICRL). Specifically, we propose a novel multi-round prompting framework called ICRL prompting. The goal is to prompt the LLM to complete a task. After the LLM generates a response at the current round, we give numerical scalar feedbacks for the response, called the rewards. At the next round, we prompt the LLM again with the same task and a context consisting of all previous responses and rewards. We observe that the quality of the LLM's response increases as the context grows. In other words, the LLM is able to maximize the scalar reward signal in the inference time, just like an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24, creative writing, and ScienceWorld) and demonstrate significant performance improvements over baseline methods such as Self-Refine and Reflexion. Surprisingly, in some experiments the reward signals are generated by the LLM itself, yet performance improvements are still observed from ICRL prompting, offering a promising paradigm for scaling test-time compute.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06303","Computer Science > Machine Learning\r\n[Submitted on 21 May 2025]\r\nTitle:Reward Is Enough: LLMs Are In-Context Reinforcement Learners\r\nView PDF HTML (experimental)Abstract:Reinforcement learning (RL) is a human-designed framework for solving sequential decision making problems. In this work, we demonstrate that, surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a phenomenon known as in-context RL (ICRL). Specifically, we propose a novel multi-round prompting framework called ICRL prompting. The goal is to prompt the LLM to complete a task. After the LLM generates a response at the current round, we give numerical scalar feedbacks for the response, called the rewards. At the next round, we prompt the LLM again with the same task and a context consisting of all previous responses and rewards. We observe that the quality of the LLM's response increases as the context grows. In other words, the LLM is able to maximize the scalar reward signal in the inference time, just like an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24, creative writing, and ScienceWorld) and demonstrate significant performance improvements over baseline methods such as Self-Refine and Reflexion. Surprisingly, in some experiments the reward signals are generated by the LLM itself, yet performance improvements are still observed from ICRL prompting, offering a promising paradigm for scaling test-time compute.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-reward-is-enough-llms-are-in-context-reinforcement-learners-c74e41.md","d06c686c8898c210",{"html":2695,"metadata":2696},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 21 May 2025]\r\nTitle:Reward Is Enough: LLMs Are In-Context Reinforcement Learners\r\nView PDF HTML (experimental)Abstract:Reinforcement learning (RL) is a human-designed framework for solving sequential decision making problems. In this work, we demonstrate that, surprisingly, RL emerges in LLM’s (Large Language Model) inference time — a phenomenon known as in-context RL (ICRL). Specifically, we propose a novel multi-round prompting framework called ICRL prompting. The goal is to prompt the LLM to complete a task. After the LLM generates a response at the current round, we give numerical scalar feedbacks for the response, called the rewards. At the next round, we prompt the LLM again with the same task and a context consisting of all previous responses and rewards. We observe that the quality of the LLM’s response increases as the context grows. In other words, the LLM is able to maximize the scalar reward signal in the inference time, just like an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24, creative writing, and ScienceWorld) and demonstrate significant performance improvements over baseline methods such as Self-Refine and Reflexion. Surprisingly, in some experiments the reward signals are generated by the LLM itself, yet performance improvements are still observed from ICRL prompting, offering a promising paradigm for scaling test-time compute.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2697,"localImagePaths":2698,"remoteImagePaths":2699,"frontmatter":2700,"imagePaths":2702},[],[],[],{"title":2686,"description":2687,"pubDate":33,"source":17,"tags":2701,"url":2690},[19,20,21],[],"2025-06-10-reward-is-enough-llms-are-in-context-reinforcement-learners-c74e41.md","2025-06-10-rsafe-incentivizing-proactive-reasoning-to-build-robust-and-adaptive-llm-safegua-fd9403",{"id":2704,"data":2706,"body":2712,"filePath":2713,"digest":2714,"rendered":2715,"legacyId":2724},{"title":2707,"description":2708,"pubDate":2709,"source":17,"tags":2710,"url":2711},"RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards","arXiv:2506.07736v1 Announce Type: new \nAbstract: Large Language Models (LLMs) continue to exhibit vulnerabilities despite deliberate safety alignment efforts, posing significant risks to users and society. To safeguard against the risk of policy-violating content, system-level moderation via external guard models-designed to monitor LLM inputs and outputs and block potentially harmful content-has emerged as a prevalent mitigation strategy. Existing approaches of training guard models rely heavily on extensive human curated datasets and struggle with out-of-distribution threats, such as emerging harmful categories or jailbreak attacks. To address these limitations, we propose RSafe, an adaptive reasoning-based safeguard that conducts guided safety reasoning to provide robust protection within the scope of specified safety policies. RSafe operates in two stages: 1) guided reasoning, where it analyzes safety risks of input content through policy-guided step-by-step reasoning, and 2) reinforced alignment, where rule-based RL optimizes its reasoning paths to align with accurate safety prediction. This two-stage training paradigm enables RSafe to internalize safety principles to generalize safety protection capability over unseen or adversarial safety violation scenarios. During inference, RSafe accepts user-specified safety policies to provide enhanced safeguards tailored to specific safety requirements.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07736","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) continue to exhibit vulnerabilities despite deliberate safety alignment efforts, posing significant risks to users and society. To safeguard against the risk of policy-violating content, system-level moderation via external guard models-designed to monitor LLM inputs and outputs and block potentially harmful content-has emerged as a prevalent mitigation strategy. Existing approaches of training guard models rely heavily on extensive human curated datasets and struggle with out-of-distribution threats, such as emerging harmful categories or jailbreak attacks. To address these limitations, we propose RSafe, an adaptive reasoning-based safeguard that conducts guided safety reasoning to provide robust protection within the scope of specified safety policies. RSafe operates in two stages: 1) guided reasoning, where it analyzes safety risks of input content through policy-guided step-by-step reasoning, and 2) reinforced alignment, where rule-based RL optimizes its reasoning paths to align with accurate safety prediction. This two-stage training paradigm enables RSafe to internalize safety principles to generalize safety protection capability over unseen or adversarial safety violation scenarios. During inference, RSafe accepts user-specified safety policies to provide enhanced safeguards tailored to specific safety requirements.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-rsafe-incentivizing-proactive-reasoning-to-build-robust-and-adaptive-llm-safegua-fd9403.md","a27f61c67482e4df",{"html":2716,"metadata":2717},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) continue to exhibit vulnerabilities despite deliberate safety alignment efforts, posing significant risks to users and society. To safeguard against the risk of policy-violating content, system-level moderation via external guard models-designed to monitor LLM inputs and outputs and block potentially harmful content-has emerged as a prevalent mitigation strategy. Existing approaches of training guard models rely heavily on extensive human curated datasets and struggle with out-of-distribution threats, such as emerging harmful categories or jailbreak attacks. To address these limitations, we propose RSafe, an adaptive reasoning-based safeguard that conducts guided safety reasoning to provide robust protection within the scope of specified safety policies. RSafe operates in two stages: 1) guided reasoning, where it analyzes safety risks of input content through policy-guided step-by-step reasoning, and 2) reinforced alignment, where rule-based RL optimizes its reasoning paths to align with accurate safety prediction. This two-stage training paradigm enables RSafe to internalize safety principles to generalize safety protection capability over unseen or adversarial safety violation scenarios. During inference, RSafe accepts user-specified safety policies to provide enhanced safeguards tailored to specific safety requirements.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2718,"localImagePaths":2719,"remoteImagePaths":2720,"frontmatter":2721,"imagePaths":2723},[],[],[],{"title":2707,"description":2708,"pubDate":33,"source":17,"tags":2722,"url":2711},[19,20,21],[],"2025-06-10-rsafe-incentivizing-proactive-reasoning-to-build-robust-and-adaptive-llm-safegua-fd9403.md","2025-06-10-saffron-1-towards-an-inference-scaling-paradigm-for-llm-safety-assurance-7259aa",{"id":2725,"data":2727,"body":2733,"filePath":2734,"digest":2735,"rendered":2736,"legacyId":2745},{"title":2728,"description":2729,"pubDate":2730,"source":17,"tags":2731,"url":2732},"Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance","arXiv:2506.06444v1 Announce Type: cross \nAbstract: Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06444","Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance\r\nView PDF HTML (experimental)Abstract:Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at this https URL , and our project homepage is at this https URL .\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-saffron-1-towards-an-inference-scaling-paradigm-for-llm-safety-assurance-7259aa.md","64e32518de14a476",{"html":2737,"metadata":2738},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance\r\nView PDF HTML (experimental)Abstract:Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods’ susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration—efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key—value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at this https URL , and our project homepage is at this https URL .\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2739,"localImagePaths":2740,"remoteImagePaths":2741,"frontmatter":2742,"imagePaths":2744},[],[],[],{"title":2728,"description":2729,"pubDate":33,"source":17,"tags":2743,"url":2732},[19,20,21],[],"2025-06-10-saffron-1-towards-an-inference-scaling-paradigm-for-llm-safety-assurance-7259aa.md","2025-06-10-self-adapting-improvement-loops-for-robotic-learning-ace409",{"id":2746,"data":2748,"body":2754,"filePath":2755,"digest":2756,"rendered":2757,"legacyId":2766},{"title":2749,"description":2750,"pubDate":2751,"source":17,"tags":2752,"url":2753},"Self-Adapting Improvement Loops for Robotic Learning","arXiv:2506.06658v1 Announce Type: cross \nAbstract: Video generative models trained on expert demonstrations have been utilized as performant text-conditioned visual planners for solving robotic tasks. However, generalization to unseen tasks remains a challenge. Whereas improved generalization may be facilitated by leveraging learned prior knowledge from additional pre-collected offline data sources, such as web-scale video datasets, in the era of experience we aim to design agents that can continuously improve in an online manner from self-collected behaviors. In this work we thus propose the Self-Adapting Improvement Loop (SAIL), where an in-domain video model iteratively updates itself on self-produced trajectories, collected through adaptation with an internet-scale pretrained video model, and steadily improves its performance for a specified task of interest. We apply SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks on a real robot arm, and find that performance improvements continuously emerge over multiple iterations for novel tasks initially unseen during original in-domain video model training. Furthermore, we discover that SAIL is surprisingly robust regarding if and how the self-collected experience is filtered, and the quality of the initial in-domain demonstrations. Through adaptation with summarized internet-scale data, and learning through online experience, we thus demonstrate a way to iteratively bootstrap a high-performance video model for solving novel robotic tasks through self-improvement.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06658","Computer Science > Robotics\r\n[Submitted on 7 Jun 2025]\r\nTitle:Self-Adapting Improvement Loops for Robotic Learning\r\nView PDF HTML (experimental)Abstract:Video generative models trained on expert demonstrations have been utilized as performant text-conditioned visual planners for solving robotic tasks. However, generalization to unseen tasks remains a challenge. Whereas improved generalization may be facilitated by leveraging learned prior knowledge from additional pre-collected offline data sources, such as web-scale video datasets, in the era of experience we aim to design agents that can continuously improve in an online manner from self-collected behaviors. In this work we thus propose the Self-Adapting Improvement Loop (SAIL), where an in-domain video model iteratively updates itself on self-produced trajectories, collected through adaptation with an internet-scale pretrained video model, and steadily improves its performance for a specified task of interest. We apply SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks on a real robot arm, and find that performance improvements continuously emerge over multiple iterations for novel tasks initially unseen during original in-domain video model training. Furthermore, we discover that SAIL is surprisingly robust regarding if and how the self-collected experience is filtered, and the quality of the initial in-domain demonstrations. Through adaptation with summarized internet-scale data, and learning through online experience, we thus demonstrate a way to iteratively bootstrap a high-performance video model for solving novel robotic tasks through self-improvement.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-self-adapting-improvement-loops-for-robotic-learning-ace409.md","ee94db3c2da84536",{"html":2758,"metadata":2759},"\u003Cp>Computer Science > Robotics\r\n[Submitted on 7 Jun 2025]\r\nTitle:Self-Adapting Improvement Loops for Robotic Learning\r\nView PDF HTML (experimental)Abstract:Video generative models trained on expert demonstrations have been utilized as performant text-conditioned visual planners for solving robotic tasks. However, generalization to unseen tasks remains a challenge. Whereas improved generalization may be facilitated by leveraging learned prior knowledge from additional pre-collected offline data sources, such as web-scale video datasets, in the era of experience we aim to design agents that can continuously improve in an online manner from self-collected behaviors. In this work we thus propose the Self-Adapting Improvement Loop (SAIL), where an in-domain video model iteratively updates itself on self-produced trajectories, collected through adaptation with an internet-scale pretrained video model, and steadily improves its performance for a specified task of interest. We apply SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks on a real robot arm, and find that performance improvements continuously emerge over multiple iterations for novel tasks initially unseen during original in-domain video model training. Furthermore, we discover that SAIL is surprisingly robust regarding if and how the self-collected experience is filtered, and the quality of the initial in-domain demonstrations. Through adaptation with summarized internet-scale data, and learning through online experience, we thus demonstrate a way to iteratively bootstrap a high-performance video model for solving novel robotic tasks through self-improvement.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2760,"localImagePaths":2761,"remoteImagePaths":2762,"frontmatter":2763,"imagePaths":2765},[],[],[],{"title":2749,"description":2750,"pubDate":33,"source":17,"tags":2764,"url":2753},[19,20,21],[],"2025-06-10-self-adapting-improvement-loops-for-robotic-learning-ace409.md","2025-06-10-scriptdoctor-automatic-generation-of-puzzlescript-games-via-large-language-model-746ba4",{"id":2767,"data":2769,"body":2775,"filePath":2776,"digest":2777,"rendered":2778,"legacyId":2787},{"title":2770,"description":2771,"pubDate":2772,"source":17,"tags":2773,"url":2774},"ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search","arXiv:2506.06524v1 Announce Type: new \nAbstract: There is much interest in using large pre-trained models in Automatic Game Design (AGD), whether via the generation of code, assets, or more abstract conceptualization of design ideas. But so far this interest largely stems from the ad hoc use of such generative models under persistent human supervision. Much work remains to show how these tools can be integrated into longer-time-horizon AGD pipelines, in which systems interface with game engines to test generated content autonomously. To this end, we introduce ScriptDoctor, a Large Language Model (LLM)-driven system for automatically generating and testing games in PuzzleScript, an expressive but highly constrained description language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates and tests game design ideas in an iterative loop, where human-authored examples are used to ground the system's output, compilation errors from the PuzzleScript engine are used to elicit functional code, and search-based agents play-test generated games. ScriptDoctor serves as a concrete example of the potential of automated, open-ended LLM-based workflows in generating novel game content.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06524","Computer Science > Artificial Intelligence\r\n[Submitted on 6 Jun 2025]\r\nTitle:ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search\r\nView PDF HTML (experimental)Abstract:There is much interest in using large pre-trained models in Automatic Game Design (AGD), whether via the generation of code, assets, or more abstract conceptualization of design ideas. But so far this interest largely stems from the ad hoc use of such generative models under persistent human supervision. Much work remains to show how these tools can be integrated into longer-time-horizon AGD pipelines, in which systems interface with game engines to test generated content autonomously. To this end, we introduce ScriptDoctor, a Large Language Model (LLM)-driven system for automatically generating and testing games in PuzzleScript, an expressive but highly constrained description language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates and tests game design ideas in an iterative loop, where human-authored examples are used to ground the system's output, compilation errors from the PuzzleScript engine are used to elicit functional code, and search-based agents play-test generated games. ScriptDoctor serves as a concrete example of the potential of automated, open-ended LLM-based workflows in generating novel game content.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-scriptdoctor-automatic-generation-of-puzzlescript-games-via-large-language-model-746ba4.md","300952b011fa69fc",{"html":2779,"metadata":2780},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 6 Jun 2025]\r\nTitle:ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search\r\nView PDF HTML (experimental)Abstract:There is much interest in using large pre-trained models in Automatic Game Design (AGD), whether via the generation of code, assets, or more abstract conceptualization of design ideas. But so far this interest largely stems from the ad hoc use of such generative models under persistent human supervision. Much work remains to show how these tools can be integrated into longer-time-horizon AGD pipelines, in which systems interface with game engines to test generated content autonomously. To this end, we introduce ScriptDoctor, a Large Language Model (LLM)-driven system for automatically generating and testing games in PuzzleScript, an expressive but highly constrained description language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates and tests game design ideas in an iterative loop, where human-authored examples are used to ground the system’s output, compilation errors from the PuzzleScript engine are used to elicit functional code, and search-based agents play-test generated games. ScriptDoctor serves as a concrete example of the potential of automated, open-ended LLM-based workflows in generating novel game content.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2781,"localImagePaths":2782,"remoteImagePaths":2783,"frontmatter":2784,"imagePaths":2786},[],[],[],{"title":2770,"description":2771,"pubDate":33,"source":17,"tags":2785,"url":2774},[19,20,21],[],"2025-06-10-scriptdoctor-automatic-generation-of-puzzlescript-games-via-large-language-model-746ba4.md","2025-06-10-sigma-refining-large-language-model-reasoning-via-sibling-guided-monte-carlo-aug-1a8d3b",{"id":2788,"data":2790,"body":2796,"filePath":2797,"digest":2798,"rendered":2799,"legacyId":2808},{"title":2791,"description":2792,"pubDate":2793,"source":17,"tags":2794,"url":2795},"SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation","arXiv:2506.06470v1 Announce Type: new \nAbstract: Enhancing large language models by simply scaling up datasets has begun to yield diminishing returns, shifting the spotlight to data quality. Monte Carlo Tree Search (MCTS) has emerged as a powerful technique for generating high-quality chain-of-thought data, yet conventional approaches typically retain only the top-scoring trajectory from the search tree, discarding sibling nodes that often contain valuable partial insights, recurrent error patterns, and alternative reasoning strategies. This unconditional rejection of non-optimal reasoning branches may waste vast amounts of informative data in the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo Augmentation), a novel framework that reintegrates these discarded sibling nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes along each search path and applies a two-stage refinement: a critique model identifies overlooked strengths and weaknesses across the sibling set, and a revision model conducts text-based backpropagation to refine the top-scoring trajectory in light of this comparative feedback. By recovering and amplifying the underutilized but valuable signals from non-optimal reasoning branches, SIGMA substantially improves reasoning trajectories. On the challenging MATH benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K samples, outperforming state-of-the-art models trained on 590K samples. This result highlights that our sibling-guided optimization not only significantly reduces data usage but also significantly boosts LLM reasoning.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06470","Computer Science > Artificial Intelligence\r\n[Submitted on 6 Jun 2025]\r\nTitle:SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation\r\nView PDFAbstract:Enhancing large language models by simply scaling up datasets has begun to yield diminishing returns, shifting the spotlight to data quality. Monte Carlo Tree Search (MCTS) has emerged as a powerful technique for generating high-quality chain-of-thought data, yet conventional approaches typically retain only the top-scoring trajectory from the search tree, discarding sibling nodes that often contain valuable partial insights, recurrent error patterns, and alternative reasoning strategies. This unconditional rejection of non-optimal reasoning branches may waste vast amounts of informative data in the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo Augmentation), a novel framework that reintegrates these discarded sibling nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes along each search path and applies a two-stage refinement: a critique model identifies overlooked strengths and weaknesses across the sibling set, and a revision model conducts text-based backpropagation to refine the top-scoring trajectory in light of this comparative feedback. By recovering and amplifying the underutilized but valuable signals from non-optimal reasoning branches, SIGMA substantially improves reasoning trajectories. On the challenging MATH benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K samples, outperforming state-of-the-art models trained on 590K samples. This result highlights that our sibling-guided optimization not only significantly reduces data usage but also significantly boosts LLM reasoning.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-sigma-refining-large-language-model-reasoning-via-sibling-guided-monte-carlo-aug-1a8d3b.md","45da438cb140c5c4",{"html":2800,"metadata":2801},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 6 Jun 2025]\r\nTitle:SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation\r\nView PDFAbstract:Enhancing large language models by simply scaling up datasets has begun to yield diminishing returns, shifting the spotlight to data quality. Monte Carlo Tree Search (MCTS) has emerged as a powerful technique for generating high-quality chain-of-thought data, yet conventional approaches typically retain only the top-scoring trajectory from the search tree, discarding sibling nodes that often contain valuable partial insights, recurrent error patterns, and alternative reasoning strategies. This unconditional rejection of non-optimal reasoning branches may waste vast amounts of informative data in the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo Augmentation), a novel framework that reintegrates these discarded sibling nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes along each search path and applies a two-stage refinement: a critique model identifies overlooked strengths and weaknesses across the sibling set, and a revision model conducts text-based backpropagation to refine the top-scoring trajectory in light of this comparative feedback. By recovering and amplifying the underutilized but valuable signals from non-optimal reasoning branches, SIGMA substantially improves reasoning trajectories. On the challenging MATH benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K samples, outperforming state-of-the-art models trained on 590K samples. This result highlights that our sibling-guided optimization not only significantly reduces data usage but also significantly boosts LLM reasoning.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2802,"localImagePaths":2803,"remoteImagePaths":2804,"frontmatter":2805,"imagePaths":2807},[],[],[],{"title":2791,"description":2792,"pubDate":33,"source":17,"tags":2806,"url":2795},[19,20,21],[],"2025-06-10-sigma-refining-large-language-model-reasoning-via-sibling-guided-monte-carlo-aug-1a8d3b.md","2025-06-10-safeflow-a-principled-protocol-for-trustworthy-and-transactional-autonomous-agen-fba340",{"id":2809,"data":2811,"body":2817,"filePath":2818,"digest":2819,"rendered":2820,"legacyId":2829},{"title":2812,"description":2813,"pubDate":2814,"source":17,"tags":2815,"url":2816},"SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems","arXiv:2506.07564v1 Announce Type: new \nAbstract: Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today's agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07564","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems\r\nView PDFAbstract:Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today's agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-safeflow-a-principled-protocol-for-trustworthy-and-transactional-autonomous-agen-fba340.md","ccd90ac01c64b21d",{"html":2821,"metadata":2822},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems\r\nView PDFAbstract:Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today’s agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2823,"localImagePaths":2824,"remoteImagePaths":2825,"frontmatter":2826,"imagePaths":2828},[],[],[],{"title":2812,"description":2813,"pubDate":33,"source":17,"tags":2827,"url":2816},[19,20,21],[],"2025-06-10-safeflow-a-principled-protocol-for-trustworthy-and-transactional-autonomous-agen-fba340.md","2025-06-10-smar-soft-modality-aware-routing-strategy-for-moe-based-multimodal-large-languag-aa844f",{"id":2830,"data":2832,"body":2838,"filePath":2839,"digest":2840,"rendered":2841,"legacyId":2850},{"title":2833,"description":2834,"pubDate":2835,"source":17,"tags":2836,"url":2837},"SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities","arXiv:2506.06406v1 Announce Type: cross \nAbstract: Mixture of Experts (MoE) architectures have become a key approach for scaling large language models, with growing interest in extending them to multimodal tasks. Existing methods to build multimodal MoE models either incur high training costs or suffer from degraded language capabilities when adapting pretrained models. To address this, we propose Soft ModalityAware Routing (SMAR), a novel regularization technique that uses Kullback Leibler divergence to control routing probability distributions across modalities, encouraging expert specialization without modifying model architecture or heavily relying on textual data. Experiments on visual instruction tuning show that SMAR preserves language ability at 86.6% retention with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance. Our approach offers a practical and efficient solution to balance modality differentiation and language capabilities in multimodal MoE models.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06406","Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities\r\nView PDF HTML (experimental)Abstract:Mixture of Experts (MoE) architectures have become a key approach for scaling large language models, with growing interest in extending them to multimodal tasks. Existing methods to build multimodal MoE models either incur high training costs or suffer from degraded language capabilities when adapting pretrained models. To address this, we propose Soft ModalityAware Routing (SMAR), a novel regularization technique that uses Kullback Leibler divergence to control routing probability distributions across modalities, encouraging expert specialization without modifying model architecture or heavily relying on textual data. Experiments on visual instruction tuning show that SMAR preserves language ability at 86.6% retention with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance. Our approach offers a practical and efficient solution to balance modality differentiation and language capabilities in multimodal MoE models.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-smar-soft-modality-aware-routing-strategy-for-moe-based-multimodal-large-languag-aa844f.md","25be92c950438773",{"html":2842,"metadata":2843},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities\r\nView PDF HTML (experimental)Abstract:Mixture of Experts (MoE) architectures have become a key approach for scaling large language models, with growing interest in extending them to multimodal tasks. Existing methods to build multimodal MoE models either incur high training costs or suffer from degraded language capabilities when adapting pretrained models. To address this, we propose Soft ModalityAware Routing (SMAR), a novel regularization technique that uses Kullback Leibler divergence to control routing probability distributions across modalities, encouraging expert specialization without modifying model architecture or heavily relying on textual data. Experiments on visual instruction tuning show that SMAR preserves language ability at 86.6% retention with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance. Our approach offers a practical and efficient solution to balance modality differentiation and language capabilities in multimodal MoE models.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2844,"localImagePaths":2845,"remoteImagePaths":2846,"frontmatter":2847,"imagePaths":2849},[],[],[],{"title":2833,"description":2834,"pubDate":33,"source":17,"tags":2848,"url":2837},[19,20,21],[],"2025-06-10-smar-soft-modality-aware-routing-strategy-for-moe-based-multimodal-large-languag-aa844f.md","2025-06-10-solving-inequality-proofs-with-large-language-models-6be55c",{"id":2851,"data":2853,"body":2859,"filePath":2860,"digest":2861,"rendered":2862,"legacyId":2871},{"title":2854,"description":2855,"pubDate":2856,"source":17,"tags":2857,"url":2858},"Solving Inequality Proofs with Large Language Models","arXiv:2506.07927v1 Announce Type: new \nAbstract: Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07927","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Solving Inequality Proofs with Large Language Models\r\nView PDF HTML (experimental)Abstract:Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at this https URL.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-solving-inequality-proofs-with-large-language-models-6be55c.md","f97b12ac30ea3134",{"html":2863,"metadata":2864},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:Solving Inequality Proofs with Large Language Models\r\nView PDF HTML (experimental)Abstract:Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at this https URL.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2865,"localImagePaths":2866,"remoteImagePaths":2867,"frontmatter":2868,"imagePaths":2870},[],[],[],{"title":2854,"description":2855,"pubDate":33,"source":17,"tags":2869,"url":2858},[19,20,21],[],"2025-06-10-solving-inequality-proofs-with-large-language-models-6be55c.md","2025-06-10-starflow-scaling-latent-normalizing-flows-for-high-resolution-image-synthesis-22f920",{"id":2872,"data":2874,"body":2880,"filePath":2881,"digest":2882,"rendered":2883,"legacyId":2892},{"title":2875,"description":2876,"pubDate":2877,"source":17,"tags":2878,"url":2879},"STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis","arXiv:2506.06276v1 Announce Type: cross \nAbstract: We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06276","Computer Science > Computer Vision and Pattern Recognition\r\n[Submitted on 6 Jun 2025]\r\nTitle:STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis\r\nView PDFAbstract:We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.\r\nCurrent browse context:\r\ncs.CV\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-starflow-scaling-latent-normalizing-flows-for-high-resolution-image-synthesis-22f920.md","081d9f14c284e499",{"html":2884,"metadata":2885},"\u003Cp>Computer Science > Computer Vision and Pattern Recognition\r\n[Submitted on 6 Jun 2025]\r\nTitle:STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis\r\nView PDFAbstract:We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.\r\nCurrent browse context:\r\ncs.CV\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2886,"localImagePaths":2887,"remoteImagePaths":2888,"frontmatter":2889,"imagePaths":2891},[],[],[],{"title":2875,"description":2876,"pubDate":33,"source":17,"tags":2890,"url":2879},[19,20,21],[],"2025-06-10-starflow-scaling-latent-normalizing-flows-for-high-resolution-image-synthesis-22f920.md","2025-06-10-structured-semantics-from-unstructured-notes-language-model-approaches-to-ehr-ba-3656d5",{"id":2893,"data":2895,"body":2901,"filePath":2902,"digest":2903,"rendered":2904,"legacyId":2913},{"title":2896,"description":2897,"pubDate":2898,"source":17,"tags":2899,"url":2900},"Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support","arXiv:2506.06340v1 Announce Type: cross \nAbstract: The advent of large language models (LLMs) has opened new avenues for analyzing complex, unstructured data, particularly within the medical domain. Electronic Health Records (EHRs) contain a wealth of information in various formats, including free text clinical notes, structured lab results, and diagnostic codes. This paper explores the application of advanced language models to leverage these diverse data sources for improved clinical decision support. We will discuss how text-based features, often overlooked in traditional high dimensional EHR analysis, can provide semantically rich representations and aid in harmonizing data across different institutions. Furthermore, we delve into the challenges and opportunities of incorporating medical codes and ensuring the generalizability and fairness of AI models in healthcare.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06340","Computer Science > Information Retrieval\r\n[Submitted on 1 Jun 2025]\r\nTitle:Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support\r\nView PDF HTML (experimental)Abstract:The advent of large language models (LLMs) has opened new avenues for analyzing complex, unstructured data, particularly within the medical domain. Electronic Health Records (EHRs) contain a wealth of information in various formats, including free text clinical notes, structured lab results, and diagnostic codes. This paper explores the application of advanced language models to leverage these diverse data sources for improved clinical decision support. We will discuss how text-based features, often overlooked in traditional high dimensional EHR analysis, can provide semantically rich representations and aid in harmonizing data across different institutions. Furthermore, we delve into the challenges and opportunities of incorporating medical codes and ensuring the generalizability and fairness of AI models in healthcare.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-structured-semantics-from-unstructured-notes-language-model-approaches-to-ehr-ba-3656d5.md","2b74481bccc64b3e",{"html":2905,"metadata":2906},"\u003Cp>Computer Science > Information Retrieval\r\n[Submitted on 1 Jun 2025]\r\nTitle:Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support\r\nView PDF HTML (experimental)Abstract:The advent of large language models (LLMs) has opened new avenues for analyzing complex, unstructured data, particularly within the medical domain. Electronic Health Records (EHRs) contain a wealth of information in various formats, including free text clinical notes, structured lab results, and diagnostic codes. This paper explores the application of advanced language models to leverage these diverse data sources for improved clinical decision support. We will discuss how text-based features, often overlooked in traditional high dimensional EHR analysis, can provide semantically rich representations and aid in harmonizing data across different institutions. Furthermore, we delve into the challenges and opportunities of incorporating medical codes and ensuring the generalizability and fairness of AI models in healthcare.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2907,"localImagePaths":2908,"remoteImagePaths":2909,"frontmatter":2910,"imagePaths":2912},[],[],[],{"title":2896,"description":2897,"pubDate":33,"source":17,"tags":2911,"url":2900},[19,20,21],[],"2025-06-10-structured-semantics-from-unstructured-notes-language-model-approaches-to-ehr-ba-3656d5.md","2025-06-10-subgoal-guided-policy-heuristic-search-with-learned-subgoals-d8d903",{"id":2914,"data":2916,"body":2922,"filePath":2923,"digest":2924,"rendered":2925,"legacyId":2934},{"title":2917,"description":2918,"pubDate":2919,"source":17,"tags":2920,"url":2921},"Subgoal-Guided Policy Heuristic Search with Learned Subgoals","arXiv:2506.07255v1 Announce Type: new \nAbstract: Policy tree search is a family of tree search algorithms that use a policy to guide the search. These algorithms provide guarantees on the number of expansions required to solve a given problem that are based on the quality of the policy. While these algorithms have shown promising results, the process in which they are trained requires complete solution trajectories to train the policy. Search trajectories are obtained during a trial-and-error search process. When the training problem instances are hard, learning can be prohibitively costly, especially when starting from a randomly initialized policy. As a result, search samples are wasted in failed attempts to solve these hard instances. This paper introduces a novel method for learning subgoal-based policies for policy tree search algorithms. The subgoals and policies conditioned on subgoals are learned from the trees that the search expands while attempting to solve problems, including the search trees of failed attempts. We empirically show that our policy formulation and training method improve the sample efficiency of learning a policy and heuristic function in this online setting.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07255","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Subgoal-Guided Policy Heuristic Search with Learned Subgoals\r\nView PDF HTML (experimental)Abstract:Policy tree search is a family of tree search algorithms that use a policy to guide the search. These algorithms provide guarantees on the number of expansions required to solve a given problem that are based on the quality of the policy. While these algorithms have shown promising results, the process in which they are trained requires complete solution trajectories to train the policy. Search trajectories are obtained during a trial-and-error search process. When the training problem instances are hard, learning can be prohibitively costly, especially when starting from a randomly initialized policy. As a result, search samples are wasted in failed attempts to solve these hard instances. This paper introduces a novel method for learning subgoal-based policies for policy tree search algorithms. The subgoals and policies conditioned on subgoals are learned from the trees that the search expands while attempting to solve problems, including the search trees of failed attempts. We empirically show that our policy formulation and training method improve the sample efficiency of learning a policy and heuristic function in this online setting.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-subgoal-guided-policy-heuristic-search-with-learned-subgoals-d8d903.md","1c9a7f251392aaac",{"html":2926,"metadata":2927},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Subgoal-Guided Policy Heuristic Search with Learned Subgoals\r\nView PDF HTML (experimental)Abstract:Policy tree search is a family of tree search algorithms that use a policy to guide the search. These algorithms provide guarantees on the number of expansions required to solve a given problem that are based on the quality of the policy. While these algorithms have shown promising results, the process in which they are trained requires complete solution trajectories to train the policy. Search trajectories are obtained during a trial-and-error search process. When the training problem instances are hard, learning can be prohibitively costly, especially when starting from a randomly initialized policy. As a result, search samples are wasted in failed attempts to solve these hard instances. This paper introduces a novel method for learning subgoal-based policies for policy tree search algorithms. The subgoals and policies conditioned on subgoals are learned from the trees that the search expands while attempting to solve problems, including the search trees of failed attempts. We empirically show that our policy formulation and training method improve the sample efficiency of learning a policy and heuristic function in this online setting.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2928,"localImagePaths":2929,"remoteImagePaths":2930,"frontmatter":2931,"imagePaths":2933},[],[],[],{"title":2917,"description":2918,"pubDate":33,"source":17,"tags":2932,"url":2921},[19,20,21],[],"2025-06-10-subgoal-guided-policy-heuristic-search-with-learned-subgoals-d8d903.md","2025-06-10-swe-dev-building-software-engineering-agents-with-training-and-inference-scaling-89ee75",{"id":2935,"data":2937,"body":2943,"filePath":2944,"digest":2945,"rendered":2946,"legacyId":2955},{"title":2938,"description":2939,"pubDate":2940,"source":17,"tags":2941,"url":2942},"SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling","arXiv:2506.07636v1 Announce Type: new \nAbstract: Large language models (LLMs) have advanced rapidly from conversational problem solving to addressing real-world tasks involving tool use, such as software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex and Cursor, have offered end-to-end automation of the software development process. However, building effective SWE agents remains challenging due to the lack of high-quality training data and effective test cases. To address this issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we develop a robust pipeline to synthesize test cases for patch evaluation. Second, we scale up agent trajectories to construct the training data for building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the SWE-Dev models can achieve top performance among all open SWE agents. Specifically, the success rates of the SWE-Dev 7B and 32B parameter models reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source models. All code, models, and datasets are publicly available at https://github.com/THUDM/SWE-Dev.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07636","Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling\r\nView PDFAbstract:Large language models (LLMs) have advanced rapidly from conversational problem solving to addressing real-world tasks involving tool use, such as software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex and Cursor, have offered end-to-end automation of the software development process. However, building effective SWE agents remains challenging due to the lack of high-quality training data and effective test cases. To address this issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we develop a robust pipeline to synthesize test cases for patch evaluation. Second, we scale up agent trajectories to construct the training data for building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the SWE-Dev models can achieve top performance among all open SWE agents. Specifically, the success rates of the SWE-Dev 7B and 32B parameter models reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source models. All code, models, and datasets are publicly available at this https URL.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-swe-dev-building-software-engineering-agents-with-training-and-inference-scaling-89ee75.md","d7d6e863dac03f19",{"html":2947,"metadata":2948},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 9 Jun 2025]\r\nTitle:SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling\r\nView PDFAbstract:Large language models (LLMs) have advanced rapidly from conversational problem solving to addressing real-world tasks involving tool use, such as software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex and Cursor, have offered end-to-end automation of the software development process. However, building effective SWE agents remains challenging due to the lack of high-quality training data and effective test cases. To address this issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we develop a robust pipeline to synthesize test cases for patch evaluation. Second, we scale up agent trajectories to construct the training data for building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the SWE-Dev models can achieve top performance among all open SWE agents. Specifically, the success rates of the SWE-Dev 7B and 32B parameter models reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source models. All code, models, and datasets are publicly available at this https URL.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2949,"localImagePaths":2950,"remoteImagePaths":2951,"frontmatter":2952,"imagePaths":2954},[],[],[],{"title":2938,"description":2939,"pubDate":33,"source":17,"tags":2953,"url":2942},[19,20,21],[],"2025-06-10-swe-dev-building-software-engineering-agents-with-training-and-inference-scaling-89ee75.md","2025-06-10-synthetic-problem-generation-for-reasoning-via-quality-diversity-algorithms-f3e54c",{"id":2956,"data":2958,"body":2964,"filePath":2965,"digest":2966,"rendered":2967,"legacyId":2976},{"title":2959,"description":2960,"pubDate":2961,"source":17,"tags":2962,"url":2963},"Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms","arXiv:2506.06499v1 Announce Type: cross \nAbstract: Large language model (LLM) driven synthetic data generation has emerged as a powerful method for improving model reasoning capabilities. However, most methods either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality. This limits the scalability of these approaches to more complex and diverse problem domains. To address this, we present SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for generating high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate: a proxy for problem difficulty. Starting from a seed dataset of 7.5K samples, we generate over 20 million new problem-solution pairs. We show that filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24\\%. Additionally, we conduct ablations studying the impact of synthetic data quantity, quality and diversity on model generalization. We find that higher quality, as measured by problem difficulty, facilitates better in-distribution performance. Further, while generating diverse synthetic data does not as strongly benefit in-distribution performance, filtering for more diverse data facilitates more robust OOD generalization. We also confirm the existence of model and data scaling laws for synthetically generated problems, which positively benefit downstream model generalization.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06499","Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms\r\nView PDF HTML (experimental)Abstract:Large language model (LLM) driven synthetic data generation has emerged as a powerful method for improving model reasoning capabilities. However, most methods either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality. This limits the scalability of these approaches to more complex and diverse problem domains. To address this, we present SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for generating high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate: a proxy for problem difficulty. Starting from a seed dataset of 7.5K samples, we generate over 20 million new problem-solution pairs. We show that filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24\\%. Additionally, we conduct ablations studying the impact of synthetic data quantity, quality and diversity on model generalization. We find that higher quality, as measured by problem difficulty, facilitates better in-distribution performance. Further, while generating diverse synthetic data does not as strongly benefit in-distribution performance, filtering for more diverse data facilitates more robust OOD generalization. We also confirm the existence of model and data scaling laws for synthetically generated problems, which positively benefit downstream model generalization.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-synthetic-problem-generation-for-reasoning-via-quality-diversity-algorithms-f3e54c.md","4769ac3098fee518",{"html":2968,"metadata":2969},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms\r\nView PDF HTML (experimental)Abstract:Large language model (LLM) driven synthetic data generation has emerged as a powerful method for improving model reasoning capabilities. However, most methods either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality. This limits the scalability of these approaches to more complex and diverse problem domains. To address this, we present SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for generating high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem’s solve-rate: a proxy for problem difficulty. Starting from a seed dataset of 7.5K samples, we generate over 20 million new problem-solution pairs. We show that filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24%. Additionally, we conduct ablations studying the impact of synthetic data quantity, quality and diversity on model generalization. We find that higher quality, as measured by problem difficulty, facilitates better in-distribution performance. Further, while generating diverse synthetic data does not as strongly benefit in-distribution performance, filtering for more diverse data facilitates more robust OOD generalization. We also confirm the existence of model and data scaling laws for synthetically generated problems, which positively benefit downstream model generalization.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2970,"localImagePaths":2971,"remoteImagePaths":2972,"frontmatter":2973,"imagePaths":2975},[],[],[],{"title":2959,"description":2960,"pubDate":33,"source":17,"tags":2974,"url":2963},[19,20,21],[],"2025-06-10-synthetic-problem-generation-for-reasoning-via-quality-diversity-algorithms-f3e54c.md","2025-06-10-tactile-mnist-benchmarking-active-tactile-perception-b99ec2",{"id":2977,"data":2979,"body":2985,"filePath":2986,"digest":2987,"rendered":2988,"legacyId":2997},{"title":2980,"description":2981,"pubDate":2982,"source":17,"tags":2983,"url":2984},"Tactile MNIST: Benchmarking Active Tactile Perception","arXiv:2506.06361v1 Announce Type: cross \nAbstract: Tactile perception has the potential to significantly enhance dexterous robotic manipulation by providing rich local information that can complement or substitute for other sensory modalities such as vision. However, because tactile sensing is inherently local, it is not well-suited for tasks that require broad spatial awareness or global scene understanding on its own. A human-inspired strategy to address this issue is to consider active perception techniques instead. That is, to actively guide sensors toward regions with more informative or significant features and integrate such information over time in order to understand a scene or complete a task. Both active perception and different methods for tactile sensing have received significant attention recently. Yet, despite advancements, both fields lack standardized benchmarks. To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an open-source, Gymnasium-compatible benchmark specifically designed for active tactile perception tasks, including localization, classification, and volume estimation. Our benchmark suite offers diverse simulation scenarios, from simple toy environments all the way to complex tactile perception tasks using vision-based tactile sensors. Furthermore, we also offer a comprehensive dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600 real-world tactile samples collected from 600 3D printed digits. Using this dataset, we train a CycleGAN for realistic tactile simulation rendering. By providing standardized protocols and reproducible evaluation frameworks, our benchmark suite facilitates systematic progress in the fields of tactile sensing and active perception.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06361","Computer Science > Robotics\r\n[Submitted on 3 Jun 2025]\r\nTitle:Tactile MNIST: Benchmarking Active Tactile Perception\r\nView PDFAbstract:Tactile perception has the potential to significantly enhance dexterous robotic manipulation by providing rich local information that can complement or substitute for other sensory modalities such as vision. However, because tactile sensing is inherently local, it is not well-suited for tasks that require broad spatial awareness or global scene understanding on its own. A human-inspired strategy to address this issue is to consider active perception techniques instead. That is, to actively guide sensors toward regions with more informative or significant features and integrate such information over time in order to understand a scene or complete a task. Both active perception and different methods for tactile sensing have received significant attention recently. Yet, despite advancements, both fields lack standardized benchmarks. To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an open-source, Gymnasium-compatible benchmark specifically designed for active tactile perception tasks, including localization, classification, and volume estimation. Our benchmark suite offers diverse simulation scenarios, from simple toy environments all the way to complex tactile perception tasks using vision-based tactile sensors. Furthermore, we also offer a comprehensive dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600 real-world tactile samples collected from 600 3D printed digits. Using this dataset, we train a CycleGAN for realistic tactile simulation rendering. By providing standardized protocols and reproducible evaluation frameworks, our benchmark suite facilitates systematic progress in the fields of tactile sensing and active perception.\r\nCurrent browse context:\r\ncs.RO\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-tactile-mnist-benchmarking-active-tactile-perception-b99ec2.md","10461d37ebee8e4d",{"html":2989,"metadata":2990},"\u003Cp>Computer Science > Robotics\r\n[Submitted on 3 Jun 2025]\r\nTitle:Tactile MNIST: Benchmarking Active Tactile Perception\r\nView PDFAbstract:Tactile perception has the potential to significantly enhance dexterous robotic manipulation by providing rich local information that can complement or substitute for other sensory modalities such as vision. However, because tactile sensing is inherently local, it is not well-suited for tasks that require broad spatial awareness or global scene understanding on its own. A human-inspired strategy to address this issue is to consider active perception techniques instead. That is, to actively guide sensors toward regions with more informative or significant features and integrate such information over time in order to understand a scene or complete a task. Both active perception and different methods for tactile sensing have received significant attention recently. Yet, despite advancements, both fields lack standardized benchmarks. To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an open-source, Gymnasium-compatible benchmark specifically designed for active tactile perception tasks, including localization, classification, and volume estimation. Our benchmark suite offers diverse simulation scenarios, from simple toy environments all the way to complex tactile perception tasks using vision-based tactile sensors. Furthermore, we also offer a comprehensive dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600 real-world tactile samples collected from 600 3D printed digits. Using this dataset, we train a CycleGAN for realistic tactile simulation rendering. By providing standardized protocols and reproducible evaluation frameworks, our benchmark suite facilitates systematic progress in the fields of tactile sensing and active perception.\r\nCurrent browse context:\r\ncs.RO\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":2991,"localImagePaths":2992,"remoteImagePaths":2993,"frontmatter":2994,"imagePaths":2996},[],[],[],{"title":2980,"description":2981,"pubDate":33,"source":17,"tags":2995,"url":2984},[19,20,21],[],"2025-06-10-tactile-mnist-benchmarking-active-tactile-perception-b99ec2.md","2025-06-10-textile-analysis-for-recycling-automation-using-transfer-learning-and-zero-shot--e495e0",{"id":2998,"data":3000,"body":3006,"filePath":3007,"digest":3008,"rendered":3009,"legacyId":3018},{"title":3001,"description":3002,"pubDate":3003,"source":17,"tags":3004,"url":3005},"Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models","arXiv:2506.06569v1 Announce Type: cross \nAbstract: Automated sorting is crucial for improving the efficiency and scalability of textile recycling, but accurately identifying material composition and detecting contaminants from sensor data remains challenging. This paper investigates the use of standard RGB imagery, a cost-effective sensing modality, for key pre-processing tasks in an automated system. We present computer vision components designed for a conveyor belt setup to perform (a) classification of four common textile types and (b) segmentation of non-textile features such as buttons and zippers. For classification, several pre-trained architectures were evaluated using transfer learning and cross-validation, with EfficientNetB0 achieving the best performance on a held-out test set with 81.25\\% accuracy. For feature segmentation, a zero-shot approach combining the Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM) was employed, demonstrating excellent performance with a mIoU of 0.90 for the generated masks against ground truth. This study demonstrates the feasibility of using RGB images coupled with modern deep learning techniques, including transfer learning for classification and foundation models for zero-shot segmentation, to enable essential analysis steps for automated textile recycling pipelines.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06569","Computer Science > Computer Vision and Pattern Recognition\r\n[Submitted on 6 Jun 2025]\r\nTitle:Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models\r\nView PDF HTML (experimental)Abstract:Automated sorting is crucial for improving the efficiency and scalability of textile recycling, but accurately identifying material composition and detecting contaminants from sensor data remains challenging. This paper investigates the use of standard RGB imagery, a cost-effective sensing modality, for key pre-processing tasks in an automated system. We present computer vision components designed for a conveyor belt setup to perform (a) classification of four common textile types and (b) segmentation of non-textile features such as buttons and zippers. For classification, several pre-trained architectures were evaluated using transfer learning and cross-validation, with EfficientNetB0 achieving the best performance on a held-out test set with 81.25\\% accuracy. For feature segmentation, a zero-shot approach combining the Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM) was employed, demonstrating excellent performance with a mIoU of 0.90 for the generated masks against ground truth. This study demonstrates the feasibility of using RGB images coupled with modern deep learning techniques, including transfer learning for classification and foundation models for zero-shot segmentation, to enable essential analysis steps for automated textile recycling pipelines.\r\nSubmission history\r\nFrom: Vasileios Argyriou [view email][v1] Fri, 6 Jun 2025 22:49:53 UTC (2,187 KB)\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-textile-analysis-for-recycling-automation-using-transfer-learning-and-zero-shot--e495e0.md","8c12532e082970fc",{"html":3010,"metadata":3011},"\u003Cp>Computer Science > Computer Vision and Pattern Recognition\r\n[Submitted on 6 Jun 2025]\r\nTitle:Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models\r\nView PDF HTML (experimental)Abstract:Automated sorting is crucial for improving the efficiency and scalability of textile recycling, but accurately identifying material composition and detecting contaminants from sensor data remains challenging. This paper investigates the use of standard RGB imagery, a cost-effective sensing modality, for key pre-processing tasks in an automated system. We present computer vision components designed for a conveyor belt setup to perform (a) classification of four common textile types and (b) segmentation of non-textile features such as buttons and zippers. For classification, several pre-trained architectures were evaluated using transfer learning and cross-validation, with EfficientNetB0 achieving the best performance on a held-out test set with 81.25% accuracy. For feature segmentation, a zero-shot approach combining the Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM) was employed, demonstrating excellent performance with a mIoU of 0.90 for the generated masks against ground truth. This study demonstrates the feasibility of using RGB images coupled with modern deep learning techniques, including transfer learning for classification and foundation models for zero-shot segmentation, to enable essential analysis steps for automated textile recycling pipelines.\r\nSubmission history\r\nFrom: Vasileios Argyriou [view email][v1] Fri, 6 Jun 2025 22:49:53 UTC (2,187 KB)\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3012,"localImagePaths":3013,"remoteImagePaths":3014,"frontmatter":3015,"imagePaths":3017},[],[],[],{"title":3001,"description":3002,"pubDate":33,"source":17,"tags":3016,"url":3005},[19,20,21],[],"2025-06-10-textile-analysis-for-recycling-automation-using-transfer-learning-and-zero-shot--e495e0.md","2025-06-10-tesu-llm-training-speech-llms-without-speech-via-unified-encoder-alignment-723096",{"id":3019,"data":3021,"body":3027,"filePath":3028,"digest":3029,"rendered":3030,"legacyId":3039},{"title":3022,"description":3023,"pubDate":3024,"source":17,"tags":3025,"url":3026},"TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment","arXiv:2506.06343v1 Announce Type: cross \nAbstract: Recent advances in speech-enabled language models have shown promising results in building intelligent voice assistants. However, most existing approaches rely on large-scale paired speech-text data and extensive computational resources, which pose challenges in terms of scalability and accessibility. In this paper, we present \\textbf{TESU-LLM}, a novel framework that enables training speech-capable language models using only text data. Our key insight is to leverage a unified encoder that maps semantically equivalent text and speech inputs to a shared latent space. By aligning the encoder output with the embedding space of a LLM via a lightweight projection network, we enable the model to generalize from text-only supervision to speech-based inference. Despite being trained exclusively on text, TESU-LLM achieves strong performance on various speech-related benchmarks, comparable to baseline methods trained with large-scale multimodal datasets and substantial computational resources. These results highlight the effectiveness and efficiency of our approach, offering a scalable path toward building speech LLMs without speech data.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06343","Computer Science > Computation and Language\r\n[Submitted on 1 Jun 2025]\r\nTitle:TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment\r\nView PDF HTML (experimental)Abstract:Recent advances in speech-enabled language models have shown promising results in building intelligent voice assistants. However, most existing approaches rely on large-scale paired speech-text data and extensive computational resources, which pose challenges in terms of scalability and accessibility. In this paper, we present \\textbf{TESU-LLM}, a novel framework that enables training speech-capable language models using only text data. Our key insight is to leverage a unified encoder that maps semantically equivalent text and speech inputs to a shared latent space. By aligning the encoder output with the embedding space of a LLM via a lightweight projection network, we enable the model to generalize from text-only supervision to speech-based inference. Despite being trained exclusively on text, TESU-LLM achieves strong performance on various speech-related benchmarks, comparable to baseline methods trained with large-scale multimodal datasets and substantial computational resources. These results highlight the effectiveness and efficiency of our approach, offering a scalable path toward building speech LLMs without speech data.\r\nCurrent browse context:\r\ncs.CL\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-tesu-llm-training-speech-llms-without-speech-via-unified-encoder-alignment-723096.md","fdf1758e61af9bc5",{"html":3031,"metadata":3032},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 1 Jun 2025]\r\nTitle:TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment\r\nView PDF HTML (experimental)Abstract:Recent advances in speech-enabled language models have shown promising results in building intelligent voice assistants. However, most existing approaches rely on large-scale paired speech-text data and extensive computational resources, which pose challenges in terms of scalability and accessibility. In this paper, we present \\textbf{TESU-LLM}, a novel framework that enables training speech-capable language models using only text data. Our key insight is to leverage a unified encoder that maps semantically equivalent text and speech inputs to a shared latent space. By aligning the encoder output with the embedding space of a LLM via a lightweight projection network, we enable the model to generalize from text-only supervision to speech-based inference. Despite being trained exclusively on text, TESU-LLM achieves strong performance on various speech-related benchmarks, comparable to baseline methods trained with large-scale multimodal datasets and substantial computational resources. These results highlight the effectiveness and efficiency of our approach, offering a scalable path toward building speech LLMs without speech data.\r\nCurrent browse context:\r\ncs.CL\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3033,"localImagePaths":3034,"remoteImagePaths":3035,"frontmatter":3036,"imagePaths":3038},[],[],[],{"title":3022,"description":3023,"pubDate":33,"source":17,"tags":3037,"url":3026},[19,20,21],[],"2025-06-10-tesu-llm-training-speech-llms-without-speech-via-unified-encoder-alignment-723096.md","2025-06-10-textitquantmcp-grounding-large-language-models-in-verifiable-financial-reality-5ca06c",{"id":3040,"data":3042,"body":3048,"filePath":3049,"digest":3050,"rendered":3051,"legacyId":3060},{"title":3043,"description":3044,"pubDate":3045,"source":17,"tags":3046,"url":3047},"\\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality","arXiv:2506.06622v1 Announce Type: cross \nAbstract: Large Language Models (LLMs) hold immense promise for revolutionizing financial analysis and decision-making, yet their direct application is often hampered by issues of data hallucination and lack of access to real-time, verifiable financial information. This paper introduces QuantMCP, a novel framework designed to rigorously ground LLMs in financial reality. By leveraging the Model Context Protocol (MCP) for standardized and secure tool invocation, QuantMCP enables LLMs to accurately interface with a diverse array of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can interact via natural language to precisely retrieve up-to-date financial data, thereby overcoming LLM's inherent limitations in factual data recall. More critically, once furnished with this verified, structured data, the LLM's analytical capabilities are unlocked, empowering it to perform sophisticated data interpretation, generate insights, and ultimately support more informed financial decision-making processes. QuantMCP provides a robust, extensible, and secure bridge between conversational AI and the complex world of financial data, aiming to enhance both the reliability and the analytical depth of LLM applications in finance.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06622","Computer Science > Computational Engineering, Finance, and Science\r\n[Submitted on 7 Jun 2025]\r\nTitle:\\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) hold immense promise for revolutionizing financial analysis and decision-making, yet their direct application is often hampered by issues of data hallucination and lack of access to real-time, verifiable financial information. This paper introduces QuantMCP, a novel framework designed to rigorously ground LLMs in financial reality. By leveraging the Model Context Protocol (MCP) for standardized and secure tool invocation, QuantMCP enables LLMs to accurately interface with a diverse array of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can interact via natural language to precisely retrieve up-to-date financial data, thereby overcoming LLM's inherent limitations in factual data recall. More critically, once furnished with this verified, structured data, the LLM's analytical capabilities are unlocked, empowering it to perform sophisticated data interpretation, generate insights, and ultimately support more informed financial decision-making processes. QuantMCP provides a robust, extensible, and secure bridge between conversational AI and the complex world of financial data, aiming to enhance both the reliability and the analytical depth of LLM applications in finance.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-textit{quantmcp}-grounding-large-language-models-in-verifiable-financial-reality-5ca06c.md","76d618b4f0f27c01",{"html":3052,"metadata":3053},"\u003Cp>Computer Science > Computational Engineering, Finance, and Science\r\n[Submitted on 7 Jun 2025]\r\nTitle:\\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) hold immense promise for revolutionizing financial analysis and decision-making, yet their direct application is often hampered by issues of data hallucination and lack of access to real-time, verifiable financial information. This paper introduces QuantMCP, a novel framework designed to rigorously ground LLMs in financial reality. By leveraging the Model Context Protocol (MCP) for standardized and secure tool invocation, QuantMCP enables LLMs to accurately interface with a diverse array of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can interact via natural language to precisely retrieve up-to-date financial data, thereby overcoming LLM’s inherent limitations in factual data recall. More critically, once furnished with this verified, structured data, the LLM’s analytical capabilities are unlocked, empowering it to perform sophisticated data interpretation, generate insights, and ultimately support more informed financial decision-making processes. QuantMCP provides a robust, extensible, and secure bridge between conversational AI and the complex world of financial data, aiming to enhance both the reliability and the analytical depth of LLM applications in finance.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3054,"localImagePaths":3055,"remoteImagePaths":3056,"frontmatter":3057,"imagePaths":3059},[],[],[],{"title":3043,"description":3044,"pubDate":33,"source":17,"tags":3058,"url":3047},[19,20,21],[],"2025-06-10-textit{quantmcp}-grounding-large-language-models-in-verifiable-financial-reality-5ca06c.md","2025-06-10-the-economic-dispatch-of-power-to-gas-systems-with-deep-reinforcement-learningta-67c2e9",{"id":3061,"data":3063,"body":3069,"filePath":3070,"digest":3071,"rendered":3072,"legacyId":3081},{"title":3064,"description":3065,"pubDate":3066,"source":17,"tags":3067,"url":3068},"The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage","arXiv:2506.06484v1 Announce Type: cross \nAbstract: Power-to-Gas (P2G) technologies gain recognition for enabling the integration of intermittent renewables, such as wind and solar, into electricity grids. However, determining the most cost-effective operation of these systems is complex due to the volatile nature of renewable energy, electricity prices, and loads. Additionally, P2G systems are less efficient in converting and storing energy compared to battery energy storage systems (BESs), and the benefits of converting electricity into gas are not immediately apparent. Deep Reinforcement Learning (DRL) has shown promise in managing the operation of energy systems amidst these uncertainties. Yet, DRL techniques face difficulties with the delayed reward characteristic of P2G system operation. Previous research has mostly focused on short-term studies that look at the energy conversion process, neglecting the long-term storage capabilities of P2G.\n  This study presents a new method by thoroughly examining how DRL can be applied to the economic operation of P2G systems, in combination with BESs and gas turbines, over extended periods. Through three progressively more complex case studies, we assess the performance of DRL algorithms, specifically Deep Q-Networks and Proximal Policy Optimization, and introduce modifications to enhance their effectiveness. These modifications include integrating forecasts, implementing penalties on the reward function, and applying strategic cost calculations, all aimed at addressing the issue of delayed rewards. Our findings indicate that while DRL initially struggles with the complex decision-making required for P2G system operation, the adjustments we propose significantly improve its capability to devise cost-effective operation strategies, thereby unlocking the potential for long-term energy storage in P2G technologies.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06484","Electrical Engineering and Systems Science > Systems and Control\r\n[Submitted on 6 Jun 2025]\r\nTitle:The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage\r\nView PDF HTML (experimental)Abstract:Power-to-Gas (P2G) technologies gain recognition for enabling the integration of intermittent renewables, such as wind and solar, into electricity grids. However, determining the most cost-effective operation of these systems is complex due to the volatile nature of renewable energy, electricity prices, and loads. Additionally, P2G systems are less efficient in converting and storing energy compared to battery energy storage systems (BESs), and the benefits of converting electricity into gas are not immediately apparent. Deep Reinforcement Learning (DRL) has shown promise in managing the operation of energy systems amidst these uncertainties. Yet, DRL techniques face difficulties with the delayed reward characteristic of P2G system operation. Previous research has mostly focused on short-term studies that look at the energy conversion process, neglecting the long-term storage capabilities of P2G.\r\nThis study presents a new method by thoroughly examining how DRL can be applied to the economic operation of P2G systems, in combination with BESs and gas turbines, over extended periods. Through three progressively more complex case studies, we assess the performance of DRL algorithms, specifically Deep Q-Networks and Proximal Policy Optimization, and introduce modifications to enhance their effectiveness. These modifications include integrating forecasts, implementing penalties on the reward function, and applying strategic cost calculations, all aimed at addressing the issue of delayed rewards. Our findings indicate that while DRL initially struggles with the complex decision-making required for P2G system operation, the adjustments we propose significantly improve its capability to devise cost-effective operation strategies, thereby unlocking the potential for long-term energy storage in P2G technologies.\r\nCurrent browse context:\r\neess.SY\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-the-economic-dispatch-of-power-to-gas-systems-with-deep-reinforcement-learningta-67c2e9.md","5dd3a07d940cc40f",{"html":3073,"metadata":3074},"\u003Cp>Electrical Engineering and Systems Science > Systems and Control\r\n[Submitted on 6 Jun 2025]\r\nTitle:The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage\r\nView PDF HTML (experimental)Abstract:Power-to-Gas (P2G) technologies gain recognition for enabling the integration of intermittent renewables, such as wind and solar, into electricity grids. However, determining the most cost-effective operation of these systems is complex due to the volatile nature of renewable energy, electricity prices, and loads. Additionally, P2G systems are less efficient in converting and storing energy compared to battery energy storage systems (BESs), and the benefits of converting electricity into gas are not immediately apparent. Deep Reinforcement Learning (DRL) has shown promise in managing the operation of energy systems amidst these uncertainties. Yet, DRL techniques face difficulties with the delayed reward characteristic of P2G system operation. Previous research has mostly focused on short-term studies that look at the energy conversion process, neglecting the long-term storage capabilities of P2G.\r\nThis study presents a new method by thoroughly examining how DRL can be applied to the economic operation of P2G systems, in combination with BESs and gas turbines, over extended periods. Through three progressively more complex case studies, we assess the performance of DRL algorithms, specifically Deep Q-Networks and Proximal Policy Optimization, and introduce modifications to enhance their effectiveness. These modifications include integrating forecasts, implementing penalties on the reward function, and applying strategic cost calculations, all aimed at addressing the issue of delayed rewards. Our findings indicate that while DRL initially struggles with the complex decision-making required for P2G system operation, the adjustments we propose significantly improve its capability to devise cost-effective operation strategies, thereby unlocking the potential for long-term energy storage in P2G technologies.\r\nCurrent browse context:\r\neess.SY\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3075,"localImagePaths":3076,"remoteImagePaths":3077,"frontmatter":3078,"imagePaths":3080},[],[],[],{"title":3064,"description":3065,"pubDate":33,"source":17,"tags":3079,"url":3068},[19,20,21],[],"2025-06-10-the-economic-dispatch-of-power-to-gas-systems-with-deep-reinforcement-learningta-67c2e9.md","2025-06-10-the-illusion-of-thinking-understanding-the-strengths-and-limitations-of-reasonin-538e43",{"id":3082,"data":3084,"body":3090,"filePath":3091,"digest":3092,"rendered":3093,"legacyId":3102},{"title":3085,"description":3086,"pubDate":3087,"source":17,"tags":3088,"url":3089},"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity","arXiv:2506.06941v1 Announce Type: new \nAbstract: Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06941","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\r\nView PDFAbstract:Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.\r\nCurrent browse context:\r\ncs.AI\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-the-illusion-of-thinking-understanding-the-strengths-and-limitations-of-reasonin-538e43.md","93d21906f886f760",{"html":3094,"metadata":3095},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\r\nView PDFAbstract:Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models’ computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.\r\nCurrent browse context:\r\ncs.AI\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3096,"localImagePaths":3097,"remoteImagePaths":3098,"frontmatter":3099,"imagePaths":3101},[],[],[],{"title":3085,"description":3086,"pubDate":33,"source":17,"tags":3100,"url":3089},[19,20,21],[],"2025-06-10-the-illusion-of-thinking-understanding-the-strengths-and-limitations-of-reasonin-538e43.md","2025-06-10-the-optimization-paradox-in-clinical-ai-multi-agent-systems-0c165f",{"id":3103,"data":3105,"body":3111,"filePath":3112,"digest":3113,"rendered":3114,"legacyId":3123},{"title":3106,"description":3107,"pubDate":3108,"source":17,"tags":3109,"url":3110},"The Optimization Paradox in Clinical AI Multi-Agent Systems","arXiv:2506.06574v1 Announce Type: new \nAbstract: Multi-agent artificial intelligence systems are increasingly deployed in clinical settings, yet the relationship between component-level optimization and system-wide performance remains poorly understood. We evaluated this relationship using 2,400 real patient cases from the MIMIC-CDM dataset across four abdominal pathologies (appendicitis, pancreatitis, cholecystitis, diverticulitis), decomposing clinical diagnosis into information gathering, interpretation, and differential diagnosis. We evaluated single agent systems (one model performing all tasks) against multi-agent systems (specialized models for each task) using comprehensive metrics spanning diagnostic outcomes, process adherence, and cost efficiency. Our results reveal a paradox: while multi-agent systems generally outperformed single agents, the component-optimized or Best of Breed system with superior components and excellent process metrics (85.5% information accuracy) significantly underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent system). This finding underscores that successful integration of AI in healthcare requires not just component level optimization but also attention to information flow and compatibility between agents. Our findings highlight the need for end to end system validation rather than relying on component metrics alone.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06574","Computer Science > Artificial Intelligence\r\n[Submitted on 6 Jun 2025]\r\nTitle:The Optimization Paradox in Clinical AI Multi-Agent Systems\r\nView PDF HTML (experimental)Abstract:Multi-agent artificial intelligence systems are increasingly deployed in clinical settings, yet the relationship between component-level optimization and system-wide performance remains poorly understood. We evaluated this relationship using 2,400 real patient cases from the MIMIC-CDM dataset across four abdominal pathologies (appendicitis, pancreatitis, cholecystitis, diverticulitis), decomposing clinical diagnosis into information gathering, interpretation, and differential diagnosis. We evaluated single agent systems (one model performing all tasks) against multi-agent systems (specialized models for each task) using comprehensive metrics spanning diagnostic outcomes, process adherence, and cost efficiency. Our results reveal a paradox: while multi-agent systems generally outperformed single agents, the component-optimized or Best of Breed system with superior components and excellent process metrics (85.5% information accuracy) significantly underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent system). This finding underscores that successful integration of AI in healthcare requires not just component level optimization but also attention to information flow and compatibility between agents. Our findings highlight the need for end to end system validation rather than relying on component metrics alone.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-the-optimization-paradox-in-clinical-ai-multi-agent-systems-0c165f.md","2ea2e07607abcdab",{"html":3115,"metadata":3116},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 6 Jun 2025]\r\nTitle:The Optimization Paradox in Clinical AI Multi-Agent Systems\r\nView PDF HTML (experimental)Abstract:Multi-agent artificial intelligence systems are increasingly deployed in clinical settings, yet the relationship between component-level optimization and system-wide performance remains poorly understood. We evaluated this relationship using 2,400 real patient cases from the MIMIC-CDM dataset across four abdominal pathologies (appendicitis, pancreatitis, cholecystitis, diverticulitis), decomposing clinical diagnosis into information gathering, interpretation, and differential diagnosis. We evaluated single agent systems (one model performing all tasks) against multi-agent systems (specialized models for each task) using comprehensive metrics spanning diagnostic outcomes, process adherence, and cost efficiency. Our results reveal a paradox: while multi-agent systems generally outperformed single agents, the component-optimized or Best of Breed system with superior components and excellent process metrics (85.5% information accuracy) significantly underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent system). This finding underscores that successful integration of AI in healthcare requires not just component level optimization but also attention to information flow and compatibility between agents. Our findings highlight the need for end to end system validation rather than relying on component metrics alone.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3117,"localImagePaths":3118,"remoteImagePaths":3119,"frontmatter":3120,"imagePaths":3122},[],[],[],{"title":3106,"description":3107,"pubDate":33,"source":17,"tags":3121,"url":3110},[19,20,21],[],"2025-06-10-the-optimization-paradox-in-clinical-ai-multi-agent-systems-0c165f.md","2025-06-10-theoretical-analysis-of-positional-encodings-in-transformer-models-impact-on-exp-0dbef8",{"id":3124,"data":3126,"body":3132,"filePath":3133,"digest":3134,"rendered":3135,"legacyId":3144},{"title":3127,"description":3128,"pubDate":3129,"source":17,"tags":3130,"url":3131},"Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization","arXiv:2506.06398v1 Announce Type: cross \nAbstract: Positional encodings are a core part of transformer-based models, enabling processing of sequential data without recurrence. This paper presents a theoretical framework to analyze how various positional encoding methods, including sinusoidal, learned, relative, and bias-based methods like Attention with Linear Biases (ALiBi), impact a transformer's expressiveness, generalization ability, and extrapolation to longer sequences. Expressiveness is defined via function approximation, generalization bounds are established using Rademacher complexity, and new encoding methods based on orthogonal functions, such as wavelets and Legendre polynomials, are proposed. The extrapolation capacity of existing and proposed encodings is analyzed, extending ALiBi's biasing approach to a unified theoretical context. Experimental evaluation on synthetic sequence-to-sequence tasks shows that orthogonal transform-based encodings outperform traditional sinusoidal encodings in generalization and extrapolation. This work addresses a critical gap in transformer theory, providing insights for design choices in natural language processing, computer vision, and other transformer applications.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06398","Computer Science > Machine Learning\r\n[Submitted on 5 Jun 2025]\r\nTitle:Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization\r\nView PDF HTML (experimental)Abstract:Positional encodings are a core part of transformer-based models, enabling processing of sequential data without recurrence. This paper presents a theoretical framework to analyze how various positional encoding methods, including sinusoidal, learned, relative, and bias-based methods like Attention with Linear Biases (ALiBi), impact a transformer's expressiveness, generalization ability, and extrapolation to longer sequences. Expressiveness is defined via function approximation, generalization bounds are established using Rademacher complexity, and new encoding methods based on orthogonal functions, such as wavelets and Legendre polynomials, are proposed. The extrapolation capacity of existing and proposed encodings is analyzed, extending ALiBi's biasing approach to a unified theoretical context. Experimental evaluation on synthetic sequence-to-sequence tasks shows that orthogonal transform-based encodings outperform traditional sinusoidal encodings in generalization and extrapolation. This work addresses a critical gap in transformer theory, providing insights for design choices in natural language processing, computer vision, and other transformer applications.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-theoretical-analysis-of-positional-encodings-in-transformer-models-impact-on-exp-0dbef8.md","56c6ab90fcba4edf",{"html":3136,"metadata":3137},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 5 Jun 2025]\r\nTitle:Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization\r\nView PDF HTML (experimental)Abstract:Positional encodings are a core part of transformer-based models, enabling processing of sequential data without recurrence. This paper presents a theoretical framework to analyze how various positional encoding methods, including sinusoidal, learned, relative, and bias-based methods like Attention with Linear Biases (ALiBi), impact a transformer’s expressiveness, generalization ability, and extrapolation to longer sequences. Expressiveness is defined via function approximation, generalization bounds are established using Rademacher complexity, and new encoding methods based on orthogonal functions, such as wavelets and Legendre polynomials, are proposed. The extrapolation capacity of existing and proposed encodings is analyzed, extending ALiBi’s biasing approach to a unified theoretical context. Experimental evaluation on synthetic sequence-to-sequence tasks shows that orthogonal transform-based encodings outperform traditional sinusoidal encodings in generalization and extrapolation. This work addresses a critical gap in transformer theory, providing insights for design choices in natural language processing, computer vision, and other transformer applications.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3138,"localImagePaths":3139,"remoteImagePaths":3140,"frontmatter":3141,"imagePaths":3143},[],[],[],{"title":3127,"description":3128,"pubDate":33,"source":17,"tags":3142,"url":3131},[19,20,21],[],"2025-06-10-theoretical-analysis-of-positional-encodings-in-transformer-models-impact-on-exp-0dbef8.md","2025-06-10-towards-efficient-multi-llm-inference-characterization-and-analysis-of-llm-routi-b435d8",{"id":3145,"data":3147,"body":3153,"filePath":3154,"digest":3155,"rendered":3156,"legacyId":3165},{"title":3148,"description":3149,"pubDate":3150,"source":17,"tags":3151,"url":3152},"Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques","arXiv:2506.06579v1 Announce Type: cross \nAbstract: Recent progress in Language Models (LMs) has dramatically advanced the field of natural language processing (NLP), excelling at tasks like text generation, summarization, and question answering. However, their inference remains computationally expensive and energy intensive, especially in settings with limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in mobile, edge, or cost sensitive environments. To address these challenges, recent approaches have introduced multi LLM intelligent model selection strategies that dynamically allocate computational resources based on query complexity -- using lightweight models for simpler queries and escalating to larger models only when necessary. This survey explores two complementary strategies for efficient LLM inference: (i) routing, which selects the most suitable model based on the query, and (ii) cascading or hierarchical inference (HI), which escalates queries through a sequence of models until a confident response is found. Both approaches aim to reduce computation by using lightweight models for simpler tasks while offloading only when needed. We provide a comparative analysis of these techniques across key performance metrics, discuss benchmarking efforts, and outline open challenges. Finally, we outline future research directions to enable faster response times, adaptive model selection based on task complexity, and scalable deployment across heterogeneous environments, making LLM based systems more efficient and accessible for real world applications.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06579","Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques\r\nView PDF HTML (experimental)Abstract:Recent progress in Language Models (LMs) has dramatically advanced the field of natural language processing (NLP), excelling at tasks like text generation, summarization, and question answering. However, their inference remains computationally expensive and energy intensive, especially in settings with limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in mobile, edge, or cost sensitive environments. To address these challenges, recent approaches have introduced multi LLM intelligent model selection strategies that dynamically allocate computational resources based on query complexity -- using lightweight models for simpler queries and escalating to larger models only when necessary. This survey explores two complementary strategies for efficient LLM inference: (i) routing, which selects the most suitable model based on the query, and (ii) cascading or hierarchical inference (HI), which escalates queries through a sequence of models until a confident response is found. Both approaches aim to reduce computation by using lightweight models for simpler tasks while offloading only when needed. We provide a comparative analysis of these techniques across key performance metrics, discuss benchmarking efforts, and outline open challenges. Finally, we outline future research directions to enable faster response times, adaptive model selection based on task complexity, and scalable deployment across heterogeneous environments, making LLM based systems more efficient and accessible for real world applications.\r\nSubmission history\r\nFrom: Adarsh Prasad Behera [view email][v1] Fri, 6 Jun 2025 23:13:08 UTC (1,387 KB)\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-towards-efficient-multi-llm-inference-characterization-and-analysis-of-llm-routi-b435d8.md","435e51f48edcdbfa",{"html":3157,"metadata":3158},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques\r\nView PDF HTML (experimental)Abstract:Recent progress in Language Models (LMs) has dramatically advanced the field of natural language processing (NLP), excelling at tasks like text generation, summarization, and question answering. However, their inference remains computationally expensive and energy intensive, especially in settings with limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in mobile, edge, or cost sensitive environments. To address these challenges, recent approaches have introduced multi LLM intelligent model selection strategies that dynamically allocate computational resources based on query complexity — using lightweight models for simpler queries and escalating to larger models only when necessary. This survey explores two complementary strategies for efficient LLM inference: (i) routing, which selects the most suitable model based on the query, and (ii) cascading or hierarchical inference (HI), which escalates queries through a sequence of models until a confident response is found. Both approaches aim to reduce computation by using lightweight models for simpler tasks while offloading only when needed. We provide a comparative analysis of these techniques across key performance metrics, discuss benchmarking efforts, and outline open challenges. Finally, we outline future research directions to enable faster response times, adaptive model selection based on task complexity, and scalable deployment across heterogeneous environments, making LLM based systems more efficient and accessible for real world applications.\r\nSubmission history\r\nFrom: Adarsh Prasad Behera [view email][v1] Fri, 6 Jun 2025 23:13:08 UTC (1,387 KB)\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3159,"localImagePaths":3160,"remoteImagePaths":3161,"frontmatter":3162,"imagePaths":3164},[],[],[],{"title":3148,"description":3149,"pubDate":33,"source":17,"tags":3163,"url":3152},[19,20,21],[],"2025-06-10-towards-efficient-multi-llm-inference-characterization-and-analysis-of-llm-routi-b435d8.md","2025-06-10-timewak-temporal-chained-hashing-watermark-for-time-series-data-740e83",{"id":3166,"data":3168,"body":3174,"filePath":3175,"digest":3176,"rendered":3177,"legacyId":3186},{"title":3169,"description":3170,"pubDate":3171,"source":17,"tags":3172,"url":3173},"TimeWak: Temporal Chained-Hashing Watermark for Time Series Data","arXiv:2506.06407v1 Announce Type: cross \nAbstract: Synthetic time series generated by diffusion models enable sharing privacy-sensitive datasets, such as patients' functional MRI records. Key criteria for synthetic data include high data utility and traceability to verify the data source. Recent watermarking methods embed in homogeneous latent spaces, but state-of-the-art time series generators operate in real space, making latent-based watermarking incompatible. This creates the challenge of watermarking directly in real space while handling feature heterogeneity and temporal dependencies. We propose TimeWak, the first watermarking algorithm for multivariate time series diffusion models. To handle temporal dependence and spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark directly within the real temporal-feature space. The other unique feature is the $\\epsilon$-exact inversion, which addresses the non-uniform reconstruction error distribution across features from inverting the diffusion process to detect watermarks. We derive the error bound of inverting multivariate time series and further maintain high watermark detectability. We extensively evaluate TimeWak on its impact on synthetic data quality, watermark detectability, and robustness under various post-editing attacks, against 5 datasets and baselines of different temporal lengths. Our results show that TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in correlational scores against the state-of-the-art baseline, while remaining consistently detectable.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06407","Computer Science > Cryptography and Security\r\n[Submitted on 6 Jun 2025]\r\nTitle:TimeWak: Temporal Chained-Hashing Watermark for Time Series Data\r\nView PDF HTML (experimental)Abstract:Synthetic time series generated by diffusion models enable sharing privacy-sensitive datasets, such as patients' functional MRI records. Key criteria for synthetic data include high data utility and traceability to verify the data source. Recent watermarking methods embed in homogeneous latent spaces, but state-of-the-art time series generators operate in real space, making latent-based watermarking incompatible. This creates the challenge of watermarking directly in real space while handling feature heterogeneity and temporal dependencies. We propose TimeWak, the first watermarking algorithm for multivariate time series diffusion models. To handle temporal dependence and spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark directly within the real temporal-feature space. The other unique feature is the $\\epsilon$-exact inversion, which addresses the non-uniform reconstruction error distribution across features from inverting the diffusion process to detect watermarks. We derive the error bound of inverting multivariate time series and further maintain high watermark detectability. We extensively evaluate TimeWak on its impact on synthetic data quality, watermark detectability, and robustness under various post-editing attacks, against 5 datasets and baselines of different temporal lengths. Our results show that TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in correlational scores against the state-of-the-art baseline, while remaining consistently detectable.\r\nCurrent browse context:\r\ncs.CR\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-timewak-temporal-chained-hashing-watermark-for-time-series-data-740e83.md","731efb9d5c3442e5",{"html":3178,"metadata":3179},"\u003Cp>Computer Science > Cryptography and Security\r\n[Submitted on 6 Jun 2025]\r\nTitle:TimeWak: Temporal Chained-Hashing Watermark for Time Series Data\r\nView PDF HTML (experimental)Abstract:Synthetic time series generated by diffusion models enable sharing privacy-sensitive datasets, such as patients’ functional MRI records. Key criteria for synthetic data include high data utility and traceability to verify the data source. Recent watermarking methods embed in homogeneous latent spaces, but state-of-the-art time series generators operate in real space, making latent-based watermarking incompatible. This creates the challenge of watermarking directly in real space while handling feature heterogeneity and temporal dependencies. We propose TimeWak, the first watermarking algorithm for multivariate time series diffusion models. To handle temporal dependence and spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark directly within the real temporal-feature space. The other unique feature is the $\\epsilon$-exact inversion, which addresses the non-uniform reconstruction error distribution across features from inverting the diffusion process to detect watermarks. We derive the error bound of inverting multivariate time series and further maintain high watermark detectability. We extensively evaluate TimeWak on its impact on synthetic data quality, watermark detectability, and robustness under various post-editing attacks, against 5 datasets and baselines of different temporal lengths. Our results show that TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in correlational scores against the state-of-the-art baseline, while remaining consistently detectable.\r\nCurrent browse context:\r\ncs.CR\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3180,"localImagePaths":3181,"remoteImagePaths":3182,"frontmatter":3183,"imagePaths":3185},[],[],[],{"title":3169,"description":3170,"pubDate":33,"source":17,"tags":3184,"url":3173},[19,20,21],[],"2025-06-10-timewak-temporal-chained-hashing-watermark-for-time-series-data-740e83.md","2025-06-10-towards-real-time-assessment-of-infrasound-event-detection-capability-using-deep-003f48",{"id":3187,"data":3189,"body":3195,"filePath":3196,"digest":3197,"rendered":3198,"legacyId":3207},{"title":3190,"description":3191,"pubDate":3192,"source":17,"tags":3193,"url":3194},"Towards real-time assessment of infrasound event detection capability using deep learning-based transmission loss estimation","arXiv:2506.06358v1 Announce Type: cross \nAbstract: Accurate modeling of infrasound transmission loss is essential for evaluating the performance of the International Monitoring System, enabling the effective design and maintenance of infrasound stations to support compliance of the Comprehensive Nuclear-Test-Ban Treaty. State-of-the-art propagation modeling tools enable transmission loss to be finely simulated using atmospheric models. However, the computational cost prohibits the exploration of a large parameter space in operational monitoring applications. To address this, recent studies made use of a deep learning algorithm capable of making transmission loss predictions almost instantaneously. However, the use of nudged atmospheric models leads to an incomplete representation of the medium, and the absence of temperature as an input makes the algorithm incompatible with long range propagation. In this study, we address these limitations by using both wind and temperature fields as inputs to a neural network, simulated up to 130 km altitude and 4,000 km distance. We also optimize several aspects of the neural network architecture. We exploit convolutional and recurrent layers to capture spatially and range-dependent features embedded in realistic atmospheric models, improving the overall performance. The neural network reaches an average error of 4 dB compared to full parabolic equation simulations and provides epistemic and data-related uncertainty estimates. Its evaluation on the 2022 Hunga Tonga-Hunga Ha'apai volcanic eruption demonstrates its prediction capability using atmospheric conditions and frequencies not included in the training. This represents a significant step towards near real-time assessment of International Monitoring System detection thresholds of explosive sources.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06358","Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 3 Jun 2025]\r\nTitle:Towards real-time assessment of infrasound event detection capability using deep learning-based transmission loss estimation\r\nView PDF HTML (experimental)Abstract:Accurate modeling of infrasound transmission loss is essential for evaluating the performance of the International Monitoring System, enabling the effective design and maintenance of infrasound stations to support compliance of the Comprehensive Nuclear-Test-Ban Treaty. State-of-the-art propagation modeling tools enable transmission loss to be finely simulated using atmospheric models. However, the computational cost prohibits the exploration of a large parameter space in operational monitoring applications. To address this, recent studies made use of a deep learning algorithm capable of making transmission loss predictions almost instantaneously. However, the use of nudged atmospheric models leads to an incomplete representation of the medium, and the absence of temperature as an input makes the algorithm incompatible with long range propagation. In this study, we address these limitations by using both wind and temperature fields as inputs to a neural network, simulated up to 130 km altitude and 4,000 km distance. We also optimize several aspects of the neural network architecture. We exploit convolutional and recurrent layers to capture spatially and range-dependent features embedded in realistic atmospheric models, improving the overall performance. The neural network reaches an average error of 4 dB compared to full parabolic equation simulations and provides epistemic and data-related uncertainty estimates. Its evaluation on the 2022 Hunga Tonga-Hunga Ha'apai volcanic eruption demonstrates its prediction capability using atmospheric conditions and frequencies not included in the training. This represents a significant step towards near real-time assessment of International Monitoring System detection thresholds of explosive sources.\r\nCurrent browse context:\r\neess.SP\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-towards-real-time-assessment-of-infrasound-event-detection-capability-using-deep-003f48.md","b48f171e445bef73",{"html":3199,"metadata":3200},"\u003Cp>Electrical Engineering and Systems Science > Signal Processing\r\n[Submitted on 3 Jun 2025]\r\nTitle:Towards real-time assessment of infrasound event detection capability using deep learning-based transmission loss estimation\r\nView PDF HTML (experimental)Abstract:Accurate modeling of infrasound transmission loss is essential for evaluating the performance of the International Monitoring System, enabling the effective design and maintenance of infrasound stations to support compliance of the Comprehensive Nuclear-Test-Ban Treaty. State-of-the-art propagation modeling tools enable transmission loss to be finely simulated using atmospheric models. However, the computational cost prohibits the exploration of a large parameter space in operational monitoring applications. To address this, recent studies made use of a deep learning algorithm capable of making transmission loss predictions almost instantaneously. However, the use of nudged atmospheric models leads to an incomplete representation of the medium, and the absence of temperature as an input makes the algorithm incompatible with long range propagation. In this study, we address these limitations by using both wind and temperature fields as inputs to a neural network, simulated up to 130 km altitude and 4,000 km distance. We also optimize several aspects of the neural network architecture. We exploit convolutional and recurrent layers to capture spatially and range-dependent features embedded in realistic atmospheric models, improving the overall performance. The neural network reaches an average error of 4 dB compared to full parabolic equation simulations and provides epistemic and data-related uncertainty estimates. Its evaluation on the 2022 Hunga Tonga-Hunga Ha’apai volcanic eruption demonstrates its prediction capability using atmospheric conditions and frequencies not included in the training. This represents a significant step towards near real-time assessment of International Monitoring System detection thresholds of explosive sources.\r\nCurrent browse context:\r\neess.SP\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3201,"localImagePaths":3202,"remoteImagePaths":3203,"frontmatter":3204,"imagePaths":3206},[],[],[],{"title":3190,"description":3191,"pubDate":33,"source":17,"tags":3205,"url":3194},[19,20,21],[],"2025-06-10-towards-real-time-assessment-of-infrasound-event-detection-capability-using-deep-003f48.md","2025-06-10-towards-foundation-model-on-temporal-knowledge-graph-reasoning-460b51",{"id":3208,"data":3210,"body":3216,"filePath":3217,"digest":3218,"rendered":3219,"legacyId":3228},{"title":3211,"description":3212,"pubDate":3213,"source":17,"tags":3214,"url":3215},"Towards Foundation Model on Temporal Knowledge Graph Reasoning","arXiv:2506.06367v1 Announce Type: new \nAbstract: Temporal Knowledge Graphs (TKGs) store temporal facts with quadruple formats (s, p, o, t). Existing Temporal Knowledge Graph Embedding (TKGE) models perform link prediction tasks in transductive or semi-inductive settings, which means the entities, relations, and temporal information in the test graph are fully or partially observed during training. Such reliance on seen elements during inference limits the models' ability to transfer to new domains and generalize to real-world scenarios. A central limitation is the difficulty in learning representations for entities, relations, and timestamps that are transferable and not tied to dataset-specific vocabularies. To overcome these limitations, we introduce the first fully-inductive approach to temporal knowledge graph link prediction. Our model employs sinusoidal positional encodings to capture fine-grained temporal patterns and generates adaptive entity and relation representations using message passing conditioned on both local and global temporal contexts. Our model design is agnostic to temporal granularity and time span, effectively addressing temporal discrepancies across TKGs and facilitating time-aware structural information transfer. As a pretrained, scalable, and transferable model, POSTRA demonstrates strong zero-shot performance on unseen temporal knowledge graphs, effectively generalizing to novel entities, relations, and timestamps. Extensive theoretical analysis and empirical results show that a single pretrained model can improve zero-shot performance on various inductive temporal reasoning scenarios, marking a significant step toward a foundation model for temporal KGs.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06367","Computer Science > Artificial Intelligence\r\n[Submitted on 4 Jun 2025]\r\nTitle:Towards Foundation Model on Temporal Knowledge Graph Reasoning\r\nView PDF HTML (experimental)Abstract:Temporal Knowledge Graphs (TKGs) store temporal facts with quadruple formats (s, p, o, t). Existing Temporal Knowledge Graph Embedding (TKGE) models perform link prediction tasks in transductive or semi-inductive settings, which means the entities, relations, and temporal information in the test graph are fully or partially observed during training. Such reliance on seen elements during inference limits the models' ability to transfer to new domains and generalize to real-world scenarios. A central limitation is the difficulty in learning representations for entities, relations, and timestamps that are transferable and not tied to dataset-specific vocabularies. To overcome these limitations, we introduce the first fully-inductive approach to temporal knowledge graph link prediction. Our model employs sinusoidal positional encodings to capture fine-grained temporal patterns and generates adaptive entity and relation representations using message passing conditioned on both local and global temporal contexts. Our model design is agnostic to temporal granularity and time span, effectively addressing temporal discrepancies across TKGs and facilitating time-aware structural information transfer. As a pretrained, scalable, and transferable model, POSTRA demonstrates strong zero-shot performance on unseen temporal knowledge graphs, effectively generalizing to novel entities, relations, and timestamps. Extensive theoretical analysis and empirical results show that a single pretrained model can improve zero-shot performance on various inductive temporal reasoning scenarios, marking a significant step toward a foundation model for temporal KGs.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-towards-foundation-model-on-temporal-knowledge-graph-reasoning-460b51.md","9da50cb98a797425",{"html":3220,"metadata":3221},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 4 Jun 2025]\r\nTitle:Towards Foundation Model on Temporal Knowledge Graph Reasoning\r\nView PDF HTML (experimental)Abstract:Temporal Knowledge Graphs (TKGs) store temporal facts with quadruple formats (s, p, o, t). Existing Temporal Knowledge Graph Embedding (TKGE) models perform link prediction tasks in transductive or semi-inductive settings, which means the entities, relations, and temporal information in the test graph are fully or partially observed during training. Such reliance on seen elements during inference limits the models’ ability to transfer to new domains and generalize to real-world scenarios. A central limitation is the difficulty in learning representations for entities, relations, and timestamps that are transferable and not tied to dataset-specific vocabularies. To overcome these limitations, we introduce the first fully-inductive approach to temporal knowledge graph link prediction. Our model employs sinusoidal positional encodings to capture fine-grained temporal patterns and generates adaptive entity and relation representations using message passing conditioned on both local and global temporal contexts. Our model design is agnostic to temporal granularity and time span, effectively addressing temporal discrepancies across TKGs and facilitating time-aware structural information transfer. As a pretrained, scalable, and transferable model, POSTRA demonstrates strong zero-shot performance on unseen temporal knowledge graphs, effectively generalizing to novel entities, relations, and timestamps. Extensive theoretical analysis and empirical results show that a single pretrained model can improve zero-shot performance on various inductive temporal reasoning scenarios, marking a significant step toward a foundation model for temporal KGs.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3222,"localImagePaths":3223,"remoteImagePaths":3224,"frontmatter":3225,"imagePaths":3227},[],[],[],{"title":3211,"description":3212,"pubDate":33,"source":17,"tags":3226,"url":3215},[19,20,21],[],"2025-06-10-towards-foundation-model-on-temporal-knowledge-graph-reasoning-460b51.md","2025-06-10-training-free-tokenizer-transplantation-via-orthogonal-matching-pursuit-401a8b",{"id":3229,"data":3231,"body":3237,"filePath":3238,"digest":3239,"rendered":3240,"legacyId":3249},{"title":3232,"description":3233,"pubDate":3234,"source":17,"tags":3235,"url":3236},"Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit","arXiv:2506.06607v1 Announce Type: cross \nAbstract: We present a training-free method to transplant tokenizers in pretrained large language models (LLMs) by reconstructing unseen token embeddings via Orthogonal Matching Pursuit (OMP). Specifically, we approximate each out-of-vocabulary token as a sparse linear combination of shared tokens, in two phases: first, compute each new token's representation in the donor embedding space with a small dictionary of shared anchor tokens, then transfer these same sparse coefficients back into the base model's embedding space.\n  On two challenging cross-tokenizer tasks--Llama$\\to$Mistral NeMo (12B) and Qwen$\\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of the base model's performance across multiple benchmarks, while other zero-shot approaches degrade significantly. Compared to baselines (zero-init, mean-init, and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves the best overall performance, effectively bridging large tokenizer discrepancies without gradient updates. Our analysis further identifies mismatched numerical tokenization schemes as a critical challenge for preserving mathematical reasoning capabilities. This technique enables direct reuse of pretrained model weights with new tokenizers, facilitating cross-tokenizer knowledge distillation, speculative decoding, ensembling, merging, and domain-specific vocabulary adaptations. We integrate our method into the open-source mergekit-tokensurgeon tool for post hoc vocabulary realignment.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06607","Computer Science > Computation and Language\r\n[Submitted on 7 Jun 2025]\r\nTitle:Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit\r\nView PDF HTML (experimental)Abstract:We present a training-free method to transplant tokenizers in pretrained large language models (LLMs) by reconstructing unseen token embeddings via Orthogonal Matching Pursuit (OMP). Specifically, we approximate each out-of-vocabulary token as a sparse linear combination of shared tokens, in two phases: first, compute each new token's representation in the donor embedding space with a small dictionary of shared anchor tokens, then transfer these same sparse coefficients back into the base model's embedding space.\r\nOn two challenging cross-tokenizer tasks--Llama$\\to$Mistral NeMo (12B) and Qwen$\\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of the base model's performance across multiple benchmarks, while other zero-shot approaches degrade significantly. Compared to baselines (zero-init, mean-init, and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves the best overall performance, effectively bridging large tokenizer discrepancies without gradient updates. Our analysis further identifies mismatched numerical tokenization schemes as a critical challenge for preserving mathematical reasoning capabilities. This technique enables direct reuse of pretrained model weights with new tokenizers, facilitating cross-tokenizer knowledge distillation, speculative decoding, ensembling, merging, and domain-specific vocabulary adaptations. We integrate our method into the open-source mergekit-tokensurgeon tool for post hoc vocabulary realignment.\r\nCurrent browse context:\r\ncs.CL\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-training-free-tokenizer-transplantation-via-orthogonal-matching-pursuit-401a8b.md","69932de7e2ddfb6e",{"html":3241,"metadata":3242},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 7 Jun 2025]\r\nTitle:Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit\r\nView PDF HTML (experimental)Abstract:We present a training-free method to transplant tokenizers in pretrained large language models (LLMs) by reconstructing unseen token embeddings via Orthogonal Matching Pursuit (OMP). Specifically, we approximate each out-of-vocabulary token as a sparse linear combination of shared tokens, in two phases: first, compute each new token’s representation in the donor embedding space with a small dictionary of shared anchor tokens, then transfer these same sparse coefficients back into the base model’s embedding space.\r\nOn two challenging cross-tokenizer tasks—Llama$\\to$Mistral NeMo (12B) and Qwen$\\to$Llama (1B)—we show that OMP achieves best zero-shot preservation of the base model’s performance across multiple benchmarks, while other zero-shot approaches degrade significantly. Compared to baselines (zero-init, mean-init, and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves the best overall performance, effectively bridging large tokenizer discrepancies without gradient updates. Our analysis further identifies mismatched numerical tokenization schemes as a critical challenge for preserving mathematical reasoning capabilities. This technique enables direct reuse of pretrained model weights with new tokenizers, facilitating cross-tokenizer knowledge distillation, speculative decoding, ensembling, merging, and domain-specific vocabulary adaptations. We integrate our method into the open-source mergekit-tokensurgeon tool for post hoc vocabulary realignment.\r\nCurrent browse context:\r\ncs.CL\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3243,"localImagePaths":3244,"remoteImagePaths":3245,"frontmatter":3246,"imagePaths":3248},[],[],[],{"title":3232,"description":3233,"pubDate":33,"source":17,"tags":3247,"url":3236},[19,20,21],[],"2025-06-10-training-free-tokenizer-transplantation-via-orthogonal-matching-pursuit-401a8b.md","2025-06-10-translating-federated-learning-algorithms-in-python-into-csp-processes-using-cha-5682ca",{"id":3250,"data":3252,"body":3258,"filePath":3259,"digest":3260,"rendered":3261,"legacyId":3270},{"title":3253,"description":3254,"pubDate":3255,"source":17,"tags":3256,"url":3257},"Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT","arXiv:2506.07173v1 Announce Type: new \nAbstract: The Python Testbed for Federated Learning Algorithms is a simple Python FL framework that is easy to use by ML&amp;AI developers who do not need to be professional programmers and is also amenable to LLMs. In the previous research, generic federated learning algorithms provided by this framework were manually translated into the CSP processes and algorithms' safety and liveness properties were automatically verified by the model checker PAT. In this paper, a simple translation process is introduced wherein the ChatGPT is used to automate the translation of the mentioned federated learning algorithms in Python into the corresponding CSP processes. Within the process, the minimality of the used context is estimated based on the feedback from ChatGPT. The proposed translation process was experimentally validated by successful translation (verified by the model checker PAT) of both generic centralized and decentralized federated learning algorithms.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.07173","Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT\r\nView PDFAbstract:The Python Testbed for Federated Learning Algorithms is a simple Python FL framework that is easy to use by ML&AI developers who do not need to be professional programmers and is also amenable to LLMs. In the previous research, generic federated learning algorithms provided by this framework were manually translated into the CSP processes and algorithms' safety and liveness properties were automatically verified by the model checker PAT. In this paper, a simple translation process is introduced wherein the ChatGPT is used to automate the translation of the mentioned federated learning algorithms in Python into the corresponding CSP processes. Within the process, the minimality of the used context is estimated based on the feedback from ChatGPT. The proposed translation process was experimentally validated by successful translation (verified by the model checker PAT) of both generic centralized and decentralized federated learning algorithms.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-translating-federated-learning-algorithms-in-python-into-csp-processes-using-cha-5682ca.md","ab5c39d11dcbd2ff",{"html":3262,"metadata":3263},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 8 Jun 2025]\r\nTitle:Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT\r\nView PDFAbstract:The Python Testbed for Federated Learning Algorithms is a simple Python FL framework that is easy to use by ML&#x26;AI developers who do not need to be professional programmers and is also amenable to LLMs. In the previous research, generic federated learning algorithms provided by this framework were manually translated into the CSP processes and algorithms’ safety and liveness properties were automatically verified by the model checker PAT. In this paper, a simple translation process is introduced wherein the ChatGPT is used to automate the translation of the mentioned federated learning algorithms in Python into the corresponding CSP processes. Within the process, the minimality of the used context is estimated based on the feedback from ChatGPT. The proposed translation process was experimentally validated by successful translation (verified by the model checker PAT) of both generic centralized and decentralized federated learning algorithms.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3264,"localImagePaths":3265,"remoteImagePaths":3266,"frontmatter":3267,"imagePaths":3269},[],[],[],{"title":3253,"description":3254,"pubDate":33,"source":17,"tags":3268,"url":3257},[19,20,21],[],"2025-06-10-translating-federated-learning-algorithms-in-python-into-csp-processes-using-cha-5682ca.md","2025-06-10-understanding-financial-reasoning-in-ai-a-multimodal-benchmark-and-error-learnin-d53606",{"id":3271,"data":3273,"body":3279,"filePath":3280,"digest":3281,"rendered":3282,"legacyId":3291},{"title":3274,"description":3275,"pubDate":3276,"source":17,"tags":3277,"url":3278},"Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach","arXiv:2506.06282v1 Announce Type: new \nAbstract: Effective financial reasoning demands not only textual understanding but also the ability to interpret complex visual data such as charts, tables, and trend graphs. This paper introduces a new benchmark designed to evaluate how well AI models - especially large language and multimodal models - reason in finance-specific contexts. Covering 3,200 expert-level question-answer pairs across 15 core financial topics, the benchmark integrates both textual and visual modalities to reflect authentic analytical challenges in finance. To address limitations in current reasoning approaches, we propose an error-aware learning framework that leverages historical model mistakes and feedback to guide inference, without requiring fine-tuning. Our experiments across state-of-the-art models show that multimodal inputs significantly enhance performance and that incorporating error feedback leads to consistent and measurable improvements. The results highlight persistent challenges in visual understanding and mathematical logic, while also demonstrating the promise of self-reflective reasoning in financial AI systems. Our code and data can be found at https://anonymous/FinMR/CodeData.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06282","Computer Science > Artificial Intelligence\r\n[Submitted on 22 Apr 2025]\r\nTitle:Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach\r\nView PDF HTML (experimental)Abstract:Effective financial reasoning demands not only textual understanding but also the ability to interpret complex visual data such as charts, tables, and trend graphs. This paper introduces a new benchmark designed to evaluate how well AI models - especially large language and multimodal models - reason in finance-specific contexts. Covering 3,200 expert-level question-answer pairs across 15 core financial topics, the benchmark integrates both textual and visual modalities to reflect authentic analytical challenges in finance. To address limitations in current reasoning approaches, we propose an error-aware learning framework that leverages historical model mistakes and feedback to guide inference, without requiring fine-tuning. Our experiments across state-of-the-art models show that multimodal inputs significantly enhance performance and that incorporating error feedback leads to consistent and measurable improvements. The results highlight persistent challenges in visual understanding and mathematical logic, while also demonstrating the promise of self-reflective reasoning in financial AI systems. Our code and data can be found at https://anonymous/FinMR/CodeData.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-understanding-financial-reasoning-in-ai-a-multimodal-benchmark-and-error-learnin-d53606.md","fd26ad19d0ffa80e",{"html":3283,"metadata":3284},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 22 Apr 2025]\r\nTitle:Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach\r\nView PDF HTML (experimental)Abstract:Effective financial reasoning demands not only textual understanding but also the ability to interpret complex visual data such as charts, tables, and trend graphs. This paper introduces a new benchmark designed to evaluate how well AI models - especially large language and multimodal models - reason in finance-specific contexts. Covering 3,200 expert-level question-answer pairs across 15 core financial topics, the benchmark integrates both textual and visual modalities to reflect authentic analytical challenges in finance. To address limitations in current reasoning approaches, we propose an error-aware learning framework that leverages historical model mistakes and feedback to guide inference, without requiring fine-tuning. Our experiments across state-of-the-art models show that multimodal inputs significantly enhance performance and that incorporating error feedback leads to consistent and measurable improvements. The results highlight persistent challenges in visual understanding and mathematical logic, while also demonstrating the promise of self-reflective reasoning in financial AI systems. Our code and data can be found at \u003Ca href=\"https://anonymous/FinMR/CodeData\">https://anonymous/FinMR/CodeData\u003C/a>.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3285,"localImagePaths":3286,"remoteImagePaths":3287,"frontmatter":3288,"imagePaths":3290},[],[],[],{"title":3274,"description":3275,"pubDate":33,"source":17,"tags":3289,"url":3278},[19,20,21],[],"2025-06-10-understanding-financial-reasoning-in-ai-a-multimodal-benchmark-and-error-learnin-d53606.md","2025-06-10-unintended-harms-of-value-aligned-llms-psychological-and-empirical-insights-939366",{"id":3292,"data":3294,"body":3300,"filePath":3301,"digest":3302,"rendered":3303,"legacyId":3312},{"title":3295,"description":3296,"pubDate":3297,"source":17,"tags":3298,"url":3299},"Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights","arXiv:2506.06404v1 Announce Type: cross \nAbstract: The application scope of Large Language Models (LLMs) continues to expand, leading to increasing interest in personalized LLMs that align with human values. However, aligning these models with individual values raises significant safety concerns, as certain values may correlate with harmful information. In this paper, we identify specific safety risks associated with value-aligned LLMs and investigate the psychological principles behind these challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models and exhibit slightly higher risks in traditional safety evaluations than other fine-tuned models. (2) These safety issues arise because value-aligned LLMs genuinely generate text according to the aligned values, which can amplify harmful outcomes. Using a dataset with detailed safety categories, we find significant correlations between value alignment and safety risks, supported by psychological hypotheses. This study offers insights into the \"black box\" of value alignment and proposes in-context alignment methods to enhance the safety of value-aligned LLMs.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06404","Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights\r\nView PDF HTML (experimental)Abstract:The application scope of Large Language Models (LLMs) continues to expand, leading to increasing interest in personalized LLMs that align with human values. However, aligning these models with individual values raises significant safety concerns, as certain values may correlate with harmful information. In this paper, we identify specific safety risks associated with value-aligned LLMs and investigate the psychological principles behind these challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models and exhibit slightly higher risks in traditional safety evaluations than other fine-tuned models. (2) These safety issues arise because value-aligned LLMs genuinely generate text according to the aligned values, which can amplify harmful outcomes. Using a dataset with detailed safety categories, we find significant correlations between value alignment and safety risks, supported by psychological hypotheses. This study offers insights into the \"black box\" of value alignment and proposes in-context alignment methods to enhance the safety of value-aligned LLMs.\r\nCurrent browse context:\r\ncs.CL\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-unintended-harms-of-value-aligned-llms-psychological-and-empirical-insights-939366.md","220b039ec141abf8",{"html":3304,"metadata":3305},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights\r\nView PDF HTML (experimental)Abstract:The application scope of Large Language Models (LLMs) continues to expand, leading to increasing interest in personalized LLMs that align with human values. However, aligning these models with individual values raises significant safety concerns, as certain values may correlate with harmful information. In this paper, we identify specific safety risks associated with value-aligned LLMs and investigate the psychological principles behind these challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models and exhibit slightly higher risks in traditional safety evaluations than other fine-tuned models. (2) These safety issues arise because value-aligned LLMs genuinely generate text according to the aligned values, which can amplify harmful outcomes. Using a dataset with detailed safety categories, we find significant correlations between value alignment and safety risks, supported by psychological hypotheses. This study offers insights into the “black box” of value alignment and proposes in-context alignment methods to enhance the safety of value-aligned LLMs.\r\nCurrent browse context:\r\ncs.CL\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3306,"localImagePaths":3307,"remoteImagePaths":3308,"frontmatter":3309,"imagePaths":3311},[],[],[],{"title":3295,"description":3296,"pubDate":33,"source":17,"tags":3310,"url":3299},[19,20,21],[],"2025-06-10-unintended-harms-of-value-aligned-llms-psychological-and-empirical-insights-939366.md","2025-06-10-united-minds-or-isolated-agents-exploring-coordination-of-llms-under-cognitive-l-5bf08b",{"id":3313,"data":3315,"body":3321,"filePath":3322,"digest":3323,"rendered":3324,"legacyId":3333},{"title":3316,"description":3317,"pubDate":3318,"source":17,"tags":3319,"url":3320},"United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory","arXiv:2506.06843v1 Announce Type: new \nAbstract: Large Language Models (LLMs) exhibit a notable performance ceiling on complex, multi-faceted tasks, as they often fail to integrate diverse information or adhere to multiple constraints. We posit that such limitation arises when the demands of a task exceed the LLM's effective cognitive load capacity. This interpretation draws a strong analogy to Cognitive Load Theory (CLT) in cognitive science, which explains similar performance boundaries in the human mind, and is further supported by emerging evidence that reveals LLMs have bounded working memory characteristics. Building upon this CLT-grounded understanding, we introduce CoThinker, a novel LLM-based multi-agent framework designed to mitigate cognitive overload and enhance collaborative problem-solving abilities. CoThinker operationalizes CLT principles by distributing intrinsic cognitive load through agent specialization and managing transactional load via structured communication and a collective working memory. We empirically validate CoThinker on complex problem-solving tasks and fabricated high cognitive load scenarios, demonstrating improvements over existing multi-agent baselines in solution quality and efficiency. Our analysis reveals characteristic interaction patterns, providing insights into the emergence of collective cognition and effective load management, thus offering a principled approach to overcoming LLM performance ceilings.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06843","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) exhibit a notable performance ceiling on complex, multi-faceted tasks, as they often fail to integrate diverse information or adhere to multiple constraints. We posit that such limitation arises when the demands of a task exceed the LLM's effective cognitive load capacity. This interpretation draws a strong analogy to Cognitive Load Theory (CLT) in cognitive science, which explains similar performance boundaries in the human mind, and is further supported by emerging evidence that reveals LLMs have bounded working memory characteristics. Building upon this CLT-grounded understanding, we introduce CoThinker, a novel LLM-based multi-agent framework designed to mitigate cognitive overload and enhance collaborative problem-solving abilities. CoThinker operationalizes CLT principles by distributing intrinsic cognitive load through agent specialization and managing transactional load via structured communication and a collective working memory. We empirically validate CoThinker on complex problem-solving tasks and fabricated high cognitive load scenarios, demonstrating improvements over existing multi-agent baselines in solution quality and efficiency. Our analysis reveals characteristic interaction patterns, providing insights into the emergence of collective cognition and effective load management, thus offering a principled approach to overcoming LLM performance ceilings.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-united-minds-or-isolated-agents-exploring-coordination-of-llms-under-cognitive-l-5bf08b.md","bbfdd33a97ed9b8f",{"html":3325,"metadata":3326},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) exhibit a notable performance ceiling on complex, multi-faceted tasks, as they often fail to integrate diverse information or adhere to multiple constraints. We posit that such limitation arises when the demands of a task exceed the LLM’s effective cognitive load capacity. This interpretation draws a strong analogy to Cognitive Load Theory (CLT) in cognitive science, which explains similar performance boundaries in the human mind, and is further supported by emerging evidence that reveals LLMs have bounded working memory characteristics. Building upon this CLT-grounded understanding, we introduce CoThinker, a novel LLM-based multi-agent framework designed to mitigate cognitive overload and enhance collaborative problem-solving abilities. CoThinker operationalizes CLT principles by distributing intrinsic cognitive load through agent specialization and managing transactional load via structured communication and a collective working memory. We empirically validate CoThinker on complex problem-solving tasks and fabricated high cognitive load scenarios, demonstrating improvements over existing multi-agent baselines in solution quality and efficiency. Our analysis reveals characteristic interaction patterns, providing insights into the emergence of collective cognition and effective load management, thus offering a principled approach to overcoming LLM performance ceilings.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3327,"localImagePaths":3328,"remoteImagePaths":3329,"frontmatter":3330,"imagePaths":3332},[],[],[],{"title":3316,"description":3317,"pubDate":33,"source":17,"tags":3331,"url":3320},[19,20,21],[],"2025-06-10-united-minds-or-isolated-agents-exploring-coordination-of-llms-under-cognitive-l-5bf08b.md","2025-06-10-unified-game-moderation-soft-prompting-and-llm-assisted-label-transfer-for-resou-446921",{"id":3334,"data":3336,"body":3342,"filePath":3343,"digest":3344,"rendered":3345,"legacyId":3354},{"title":3337,"description":3338,"pubDate":3339,"source":17,"tags":3340,"url":3341},"Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection","arXiv:2506.06347v1 Announce Type: cross \nAbstract: Toxicity detection in gaming communities faces significant scaling challenges when expanding across multiple games and languages, particularly in real-time environments where computational efficiency is crucial. We present two key findings to address these challenges while building upon our previous work on ToxBuster, a BERT-based real-time toxicity detection system. First, we introduce a soft-prompting approach that enables a single model to effectively handle multiple games by incorporating game-context tokens, matching the performance of more complex methods like curriculum learning while offering superior scalability. Second, we develop an LLM-assisted label transfer framework using GPT-4o-mini to extend support to seven additional languages. Evaluations on real game chat data across French, German, Portuguese, and Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with particularly strong performance in German, surpassing the English benchmark of 45.39%. In production, this unified approach significantly reduces computational resources and maintenance overhead compared to maintaining separate models for each game and language combination. At Ubisoft, this model successfully identifies an average of 50 players, per game, per day engaging in sanctionable behavior.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06347","Computer Science > Computation and Language\r\n[Submitted on 1 Jun 2025]\r\nTitle:Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection\r\nView PDF HTML (experimental)Abstract:Toxicity detection in gaming communities faces significant scaling challenges when expanding across multiple games and languages, particularly in real-time environments where computational efficiency is crucial. We present two key findings to address these challenges while building upon our previous work on ToxBuster, a BERT-based real-time toxicity detection system. First, we introduce a soft-prompting approach that enables a single model to effectively handle multiple games by incorporating game-context tokens, matching the performance of more complex methods like curriculum learning while offering superior scalability. Second, we develop an LLM-assisted label transfer framework using GPT-4o-mini to extend support to seven additional languages. Evaluations on real game chat data across French, German, Portuguese, and Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with particularly strong performance in German, surpassing the English benchmark of 45.39%. In production, this unified approach significantly reduces computational resources and maintenance overhead compared to maintaining separate models for each game and language combination. At Ubisoft, this model successfully identifies an average of 50 players, per game, per day engaging in sanctionable behavior.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-unified-game-moderation-soft-prompting-and-llm-assisted-label-transfer-for-resou-446921.md","552813f6d563a064",{"html":3346,"metadata":3347},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 1 Jun 2025]\r\nTitle:Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection\r\nView PDF HTML (experimental)Abstract:Toxicity detection in gaming communities faces significant scaling challenges when expanding across multiple games and languages, particularly in real-time environments where computational efficiency is crucial. We present two key findings to address these challenges while building upon our previous work on ToxBuster, a BERT-based real-time toxicity detection system. First, we introduce a soft-prompting approach that enables a single model to effectively handle multiple games by incorporating game-context tokens, matching the performance of more complex methods like curriculum learning while offering superior scalability. Second, we develop an LLM-assisted label transfer framework using GPT-4o-mini to extend support to seven additional languages. Evaluations on real game chat data across French, German, Portuguese, and Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with particularly strong performance in German, surpassing the English benchmark of 45.39%. In production, this unified approach significantly reduces computational resources and maintenance overhead compared to maintaining separate models for each game and language combination. At Ubisoft, this model successfully identifies an average of 50 players, per game, per day engaging in sanctionable behavior.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3348,"localImagePaths":3349,"remoteImagePaths":3350,"frontmatter":3351,"imagePaths":3353},[],[],[],{"title":3337,"description":3338,"pubDate":33,"source":17,"tags":3352,"url":3341},[19,20,21],[],"2025-06-10-unified-game-moderation-soft-prompting-and-llm-assisted-label-transfer-for-resou-446921.md","2025-06-10-unlocking-chemical-insights-superior-molecular-representations-from-intermediate-b952dc",{"id":3355,"data":3357,"body":3363,"filePath":3364,"digest":3365,"rendered":3366,"legacyId":3375},{"title":3358,"description":3359,"pubDate":3360,"source":17,"tags":3361,"url":3362},"Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers","arXiv:2506.06443v1 Announce Type: cross \nAbstract: Pretrained molecular encoders have become indispensable in computational chemistry for tasks such as property prediction and molecular generation. However, the standard practice of relying solely on final-layer embeddings for downstream tasks may discard valuable information. In this work, we challenge this convention by conducting a comprehensive layer-wise analysis of five diverse molecular encoders across 22 ADMET property prediction tasks. Our results demonstrate that embeddings from intermediate layers consistently outperform final-layer representations. Specifically, using fixed embeddings from the optimal intermediate layers improved downstream performance by an average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to these intermediate layers yielded even greater average improvements of 8.5%, with performance increases as high as 40.8%, achieving new state-of-the-art results on several benchmarks. Additionally, a strong positive correlation between fixed embedding performance and finetuning outcomes supports an efficient evaluate-then-finetune approach, enabling identification of optimal layers with reduced computational cost. These findings highlight the importance of exploring the full representational depth of molecular encoders to achieve substantial performance improvements and computational efficiency. The code is made publicly available at https://github.com/luispintoc/Unlocking-Chemical-Insights.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06443","Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers\r\nView PDF HTML (experimental)Abstract:Pretrained molecular encoders have become indispensable in computational chemistry for tasks such as property prediction and molecular generation. However, the standard practice of relying solely on final-layer embeddings for downstream tasks may discard valuable information. In this work, we challenge this convention by conducting a comprehensive layer-wise analysis of five diverse molecular encoders across 22 ADMET property prediction tasks. Our results demonstrate that embeddings from intermediate layers consistently outperform final-layer representations. Specifically, using fixed embeddings from the optimal intermediate layers improved downstream performance by an average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to these intermediate layers yielded even greater average improvements of 8.5%, with performance increases as high as 40.8%, achieving new state-of-the-art results on several benchmarks. Additionally, a strong positive correlation between fixed embedding performance and finetuning outcomes supports an efficient evaluate-then-finetune approach, enabling identification of optimal layers with reduced computational cost. These findings highlight the importance of exploring the full representational depth of molecular encoders to achieve substantial performance improvements and computational efficiency. The code is made publicly available at this https URL.\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-unlocking-chemical-insights-superior-molecular-representations-from-intermediate-b952dc.md","327cbc92cc5e0500",{"html":3367,"metadata":3368},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers\r\nView PDF HTML (experimental)Abstract:Pretrained molecular encoders have become indispensable in computational chemistry for tasks such as property prediction and molecular generation. However, the standard practice of relying solely on final-layer embeddings for downstream tasks may discard valuable information. In this work, we challenge this convention by conducting a comprehensive layer-wise analysis of five diverse molecular encoders across 22 ADMET property prediction tasks. Our results demonstrate that embeddings from intermediate layers consistently outperform final-layer representations. Specifically, using fixed embeddings from the optimal intermediate layers improved downstream performance by an average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to these intermediate layers yielded even greater average improvements of 8.5%, with performance increases as high as 40.8%, achieving new state-of-the-art results on several benchmarks. Additionally, a strong positive correlation between fixed embedding performance and finetuning outcomes supports an efficient evaluate-then-finetune approach, enabling identification of optimal layers with reduced computational cost. These findings highlight the importance of exploring the full representational depth of molecular encoders to achieve substantial performance improvements and computational efficiency. The code is made publicly available at this https URL.\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3369,"localImagePaths":3370,"remoteImagePaths":3371,"frontmatter":3372,"imagePaths":3374},[],[],[],{"title":3358,"description":3359,"pubDate":33,"source":17,"tags":3373,"url":3362},[19,20,21],[],"2025-06-10-unlocking-chemical-insights-superior-molecular-representations-from-intermediate-b952dc.md","2025-06-10-unreal-patterns-3c498f",{"id":3376,"data":3378,"body":3384,"filePath":3385,"digest":3386,"rendered":3387,"legacyId":3396},{"title":3379,"description":3380,"pubDate":3381,"source":17,"tags":3382,"url":3383},"Unreal Patterns","arXiv:2506.06284v1 Announce Type: new \nAbstract: This paper introduces a framework for representing information about entities that do not exist or may never exist, such as those involving fictional entities, blueprints, simulations, and future scenarios. Traditional approaches that introduce \"dummy instances\" or rely on modal logic are criticized, and a proposal is defended in which such cases are modeled using the intersections of actual types rather than specific non existent tokens. The paper positions itself within the Basic Formal Ontology and its realist commitments, emphasizing the importance of practical, implementable solutions over purely metaphysical or philosophical proposals, arguing that existing approaches to non existent entities either overcommit to metaphysical assumptions or introduce computational inefficiencies that hinder applications. By developing a structured ontology driven approach to unreal patterns, the paper aims to provide a useful and computationally viable means of handling references to hypothetical or non existent entities.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06284","Computer Science > Artificial Intelligence\r\n[Submitted on 28 Apr 2025]\r\nTitle:Unreal Patterns\r\nView PDFAbstract:This paper introduces a framework for representing information about entities that do not exist or may never exist, such as those involving fictional entities, blueprints, simulations, and future scenarios. Traditional approaches that introduce \"dummy instances\" or rely on modal logic are criticized, and a proposal is defended in which such cases are modeled using the intersections of actual types rather than specific non existent tokens. The paper positions itself within the Basic Formal Ontology and its realist commitments, emphasizing the importance of practical, implementable solutions over purely metaphysical or philosophical proposals, arguing that existing approaches to non existent entities either overcommit to metaphysical assumptions or introduce computational inefficiencies that hinder applications. By developing a structured ontology driven approach to unreal patterns, the paper aims to provide a useful and computationally viable means of handling references to hypothetical or non existent entities.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-unreal-patterns-3c498f.md","88e5c1b20ccaa429",{"html":3388,"metadata":3389},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 28 Apr 2025]\r\nTitle:Unreal Patterns\r\nView PDFAbstract:This paper introduces a framework for representing information about entities that do not exist or may never exist, such as those involving fictional entities, blueprints, simulations, and future scenarios. Traditional approaches that introduce “dummy instances” or rely on modal logic are criticized, and a proposal is defended in which such cases are modeled using the intersections of actual types rather than specific non existent tokens. The paper positions itself within the Basic Formal Ontology and its realist commitments, emphasizing the importance of practical, implementable solutions over purely metaphysical or philosophical proposals, arguing that existing approaches to non existent entities either overcommit to metaphysical assumptions or introduce computational inefficiencies that hinder applications. By developing a structured ontology driven approach to unreal patterns, the paper aims to provide a useful and computationally viable means of handling references to hypothetical or non existent entities.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3390,"localImagePaths":3391,"remoteImagePaths":3392,"frontmatter":3393,"imagePaths":3395},[],[],[],{"title":3379,"description":3380,"pubDate":33,"source":17,"tags":3394,"url":3383},[19,20,21],[],"2025-06-10-unreal-patterns-3c498f.md","2025-06-10-visiomath-benchmarking-figure-based-mathematical-reasoning-in-lmms-e407ac",{"id":3397,"data":3399,"body":3405,"filePath":3406,"digest":3407,"rendered":3408,"legacyId":3417},{"title":3400,"description":3401,"pubDate":3402,"source":17,"tags":3403,"url":3404},"VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs","arXiv:2506.06727v1 Announce Type: new \nAbstract: Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving capabilities across various domains. However, their ability to perform mathematical reasoning when answer options are represented as images--an essential aspect of multi-image comprehension--remains underexplored. To bridge this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical reasoning in multimodal contexts involving image-based answer choices. VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where each answer option is an image, presenting unique challenges to existing LMMs. To the best of our knowledge, VisioMath is the first dataset specifically tailored for mathematical reasoning in image-based-option scenarios, where fine-grained distinctions between answer choices are critical for accurate problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath and find that even the most advanced models struggle with this task. Notably, GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current models in reasoning over visually similar answer choices. By addressing a crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed for future research, driving advancements in multimodal reasoning.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06727","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs\r\nView PDF HTML (experimental)Abstract:Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving capabilities across various domains. However, their ability to perform mathematical reasoning when answer options are represented as images--an essential aspect of multi-image comprehension--remains underexplored. To bridge this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical reasoning in multimodal contexts involving image-based answer choices. VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where each answer option is an image, presenting unique challenges to existing LMMs. To the best of our knowledge, VisioMath is the first dataset specifically tailored for mathematical reasoning in image-based-option scenarios, where fine-grained distinctions between answer choices are critical for accurate problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath and find that even the most advanced models struggle with this task. Notably, GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current models in reasoning over visually similar answer choices. By addressing a crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed for future research, driving advancements in multimodal reasoning.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-visiomath-benchmarking-figure-based-mathematical-reasoning-in-lmms-e407ac.md","b940583ca42db116",{"html":3409,"metadata":3410},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs\r\nView PDF HTML (experimental)Abstract:Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving capabilities across various domains. However, their ability to perform mathematical reasoning when answer options are represented as images—an essential aspect of multi-image comprehension—remains underexplored. To bridge this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical reasoning in multimodal contexts involving image-based answer choices. VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where each answer option is an image, presenting unique challenges to existing LMMs. To the best of our knowledge, VisioMath is the first dataset specifically tailored for mathematical reasoning in image-based-option scenarios, where fine-grained distinctions between answer choices are critical for accurate problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath and find that even the most advanced models struggle with this task. Notably, GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current models in reasoning over visually similar answer choices. By addressing a crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed for future research, driving advancements in multimodal reasoning.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3411,"localImagePaths":3412,"remoteImagePaths":3413,"frontmatter":3414,"imagePaths":3416},[],[],[],{"title":3400,"description":3401,"pubDate":33,"source":17,"tags":3415,"url":3404},[19,20,21],[],"2025-06-10-visiomath-benchmarking-figure-based-mathematical-reasoning-in-lmms-e407ac.md","2025-06-10-what-is-seen-cannot-be-unseen-the-disruptive-effect-of-knowledge-conflict-on-lar-b1d89e",{"id":3418,"data":3420,"body":3426,"filePath":3427,"digest":3428,"rendered":3429,"legacyId":3438},{"title":3421,"description":3422,"pubDate":3423,"source":17,"tags":3424,"url":3425},"What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models","arXiv:2506.06485v1 Announce Type: cross \nAbstract: Large language models frequently rely on both contextual input and parametric knowledge to perform tasks. However, these sources can come into conflict, especially when retrieved documents contradict the model's parametric knowledge. We propose a diagnostic framework to systematically evaluate LLM behavior under context-memory conflict, where the contextual information diverges from their parametric beliefs. We construct diagnostic data that elicit these conflicts and analyze model performance across multiple task types. Our findings reveal that (1) knowledge conflict has minimal impact on tasks that do not require knowledge utilization, (2) model performance is consistently higher when contextual and parametric knowledge are aligned, (3) models are unable to fully suppress their internal knowledge even when instructed, and (4) providing rationales that explain the conflict increases reliance on contexts. These insights raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06485","Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models\r\nView PDF HTML (experimental)Abstract:Large language models frequently rely on both contextual input and parametric knowledge to perform tasks. However, these sources can come into conflict, especially when retrieved documents contradict the model's parametric knowledge. We propose a diagnostic framework to systematically evaluate LLM behavior under context-memory conflict, where the contextual information diverges from their parametric beliefs. We construct diagnostic data that elicit these conflicts and analyze model performance across multiple task types. Our findings reveal that (1) knowledge conflict has minimal impact on tasks that do not require knowledge utilization, (2) model performance is consistently higher when contextual and parametric knowledge are aligned, (3) models are unable to fully suppress their internal knowledge even when instructed, and (4) providing rationales that explain the conflict increases reliance on contexts. These insights raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-what-is-seen-cannot-be-unseen-the-disruptive-effect-of-knowledge-conflict-on-lar-b1d89e.md","a1d87f494b8ef991",{"html":3430,"metadata":3431},"\u003Cp>Computer Science > Computation and Language\r\n[Submitted on 6 Jun 2025]\r\nTitle:What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models\r\nView PDF HTML (experimental)Abstract:Large language models frequently rely on both contextual input and parametric knowledge to perform tasks. However, these sources can come into conflict, especially when retrieved documents contradict the model’s parametric knowledge. We propose a diagnostic framework to systematically evaluate LLM behavior under context-memory conflict, where the contextual information diverges from their parametric beliefs. We construct diagnostic data that elicit these conflicts and analyze model performance across multiple task types. Our findings reveal that (1) knowledge conflict has minimal impact on tasks that do not require knowledge utilization, (2) model performance is consistently higher when contextual and parametric knowledge are aligned, (3) models are unable to fully suppress their internal knowledge even when instructed, and (4) providing rationales that explain the conflict increases reliance on contexts. These insights raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3432,"localImagePaths":3433,"remoteImagePaths":3434,"frontmatter":3435,"imagePaths":3437},[],[],[],{"title":3421,"description":3422,"pubDate":33,"source":17,"tags":3436,"url":3425},[19,20,21],[],"2025-06-10-what-is-seen-cannot-be-unseen-the-disruptive-effect-of-knowledge-conflict-on-lar-b1d89e.md","2025-06-10-will-artificial-agents-pursue-power-by-default-4b4ab9",{"id":3439,"data":3441,"body":3447,"filePath":3448,"digest":3449,"rendered":3450,"legacyId":3459},{"title":3442,"description":3443,"pubDate":3444,"source":17,"tags":3445,"url":3446},"Will artificial agents pursue power by default?","arXiv:2506.06352v1 Announce Type: new \nAbstract: Researchers worried about catastrophic risks from advanced AI have argued that we should expect sufficiently capable AI agents to pursue power over humanity because power is a convergent instrumental goal, something that is useful for a wide range of final goals. Others have recently expressed skepticism of these claims. This paper aims to formalize the concepts of instrumental convergence and power-seeking in an abstract, decision-theoretic framework, and to assess the claim that power is a convergent instrumental goal. I conclude that this claim contains at least an element of truth, but might turn out to have limited predictive utility, since an agent's options cannot always be ranked in terms of power in the absence of substantive information about the agent's final goals. However, the fact of instrumental convergence is more predictive for agents who have a good shot at attaining absolute or near-absolute power.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06352","Computer Science > Artificial Intelligence\r\n[Submitted on 2 Jun 2025]\r\nTitle:Will artificial agents pursue power by default?\r\nView PDF HTML (experimental)Abstract:Researchers worried about catastrophic risks from advanced AI have argued that we should expect sufficiently capable AI agents to pursue power over humanity because power is a convergent instrumental goal, something that is useful for a wide range of final goals. Others have recently expressed skepticism of these claims. This paper aims to formalize the concepts of instrumental convergence and power-seeking in an abstract, decision-theoretic framework, and to assess the claim that power is a convergent instrumental goal. I conclude that this claim contains at least an element of truth, but might turn out to have limited predictive utility, since an agent's options cannot always be ranked in terms of power in the absence of substantive information about the agent's final goals. However, the fact of instrumental convergence is more predictive for agents who have a good shot at attaining absolute or near-absolute power.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-will-artificial-agents-pursue-power-by-default-4b4ab9.md","967e19ab062f64f4",{"html":3451,"metadata":3452},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 2 Jun 2025]\r\nTitle:Will artificial agents pursue power by default?\r\nView PDF HTML (experimental)Abstract:Researchers worried about catastrophic risks from advanced AI have argued that we should expect sufficiently capable AI agents to pursue power over humanity because power is a convergent instrumental goal, something that is useful for a wide range of final goals. Others have recently expressed skepticism of these claims. This paper aims to formalize the concepts of instrumental convergence and power-seeking in an abstract, decision-theoretic framework, and to assess the claim that power is a convergent instrumental goal. I conclude that this claim contains at least an element of truth, but might turn out to have limited predictive utility, since an agent’s options cannot always be ranked in terms of power in the absence of substantive information about the agent’s final goals. However, the fact of instrumental convergence is more predictive for agents who have a good shot at attaining absolute or near-absolute power.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3453,"localImagePaths":3454,"remoteImagePaths":3455,"frontmatter":3456,"imagePaths":3458},[],[],[],{"title":3442,"description":3443,"pubDate":33,"source":17,"tags":3457,"url":3446},[19,20,21],[],"2025-06-10-will-artificial-agents-pursue-power-by-default-4b4ab9.md","2025-06-10-wisca-a-consensus-based-approach-to-harmonizing-interpretability-in-tabular-data-d42b8c",{"id":3460,"data":3462,"body":3468,"filePath":3469,"digest":3470,"rendered":3471,"legacyId":3480},{"title":3463,"description":3464,"pubDate":3465,"source":17,"tags":3466,"url":3467},"WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets","arXiv:2506.06455v1 Announce Type: cross \nAbstract: While predictive accuracy is often prioritized in machine learning (ML) models, interpretability remains essential in scientific and high-stakes domains. However, diverse interpretability algorithms frequently yield conflicting explanations, highlighting the need for consensus to harmonize results. In this study, six ML models were trained on six synthetic datasets with known ground truths, utilizing various model-agnostic interpretability techniques. Consensus explanations were generated using established methods and a novel approach: WISCA (Weighted Scaled Consensus Attributions), which integrates class probability and normalized attributions. WISCA consistently aligned with the most reliable individual method, underscoring the value of robust consensus strategies in improving explanation reliability.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06455","Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets\r\nView PDF HTML (experimental)Abstract:While predictive accuracy is often prioritized in machine learning (ML) models, interpretability remains essential in scientific and high-stakes domains. However, diverse interpretability algorithms frequently yield conflicting explanations, highlighting the need for consensus to harmonize results. In this study, six ML models were trained on six synthetic datasets with known ground truths, utilizing various model-agnostic interpretability techniques. Consensus explanations were generated using established methods and a novel approach: WISCA (Weighted Scaled Consensus Attributions), which integrates class probability and normalized attributions. WISCA consistently aligned with the most reliable individual method, underscoring the value of robust consensus strategies in improving explanation reliability.\r\nSubmission history\r\nFrom: Antonio Jesus Banegas-Luna [view email][v1] Fri, 6 Jun 2025 18:26:25 UTC (1,759 KB)\r\nCurrent browse context:\r\ncs.LG\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-wisca-a-consensus-based-approach-to-harmonizing-interpretability-in-tabular-data-d42b8c.md","92b607e53cecfe4a",{"html":3472,"metadata":3473},"\u003Cp>Computer Science > Machine Learning\r\n[Submitted on 6 Jun 2025]\r\nTitle:WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets\r\nView PDF HTML (experimental)Abstract:While predictive accuracy is often prioritized in machine learning (ML) models, interpretability remains essential in scientific and high-stakes domains. However, diverse interpretability algorithms frequently yield conflicting explanations, highlighting the need for consensus to harmonize results. In this study, six ML models were trained on six synthetic datasets with known ground truths, utilizing various model-agnostic interpretability techniques. Consensus explanations were generated using established methods and a novel approach: WISCA (Weighted Scaled Consensus Attributions), which integrates class probability and normalized attributions. WISCA consistently aligned with the most reliable individual method, underscoring the value of robust consensus strategies in improving explanation reliability.\r\nSubmission history\r\nFrom: Antonio Jesus Banegas-Luna [view email][v1] Fri, 6 Jun 2025 18:26:25 UTC (1,759 KB)\r\nCurrent browse context:\r\ncs.LG\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\nIArxiv Recommender\r\n(What is IArxiv?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3474,"localImagePaths":3475,"remoteImagePaths":3476,"frontmatter":3477,"imagePaths":3479},[],[],[],{"title":3463,"description":3464,"pubDate":33,"source":17,"tags":3478,"url":3467},[19,20,21],[],"2025-06-10-wisca-a-consensus-based-approach-to-harmonizing-interpretability-in-tabular-data-d42b8c.md","2025-06-10-worldllm-improving-llms-world-modeling-using-curiosity-driven-theory-making-bd4324",{"id":3481,"data":3483,"body":3489,"filePath":3490,"digest":3491,"rendered":3492,"legacyId":3501},{"title":3484,"description":3485,"pubDate":3486,"source":17,"tags":3487,"url":3488},"WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making","arXiv:2506.06725v1 Announce Type: new \nAbstract: Large Language Models (LLMs) possess general world knowledge but often struggle to generate precise predictions in structured, domain-specific contexts such as simulations. These limitations arise from their inability to ground their broad, unstructured understanding in specific environments. To address this, we present WorldLLM, a framework that enhances LLM-based world modeling by combining Bayesian inference and autonomous active exploration with reinforcement learning. WorldLLM leverages the in-context learning abilities of LLMs to guide an LLM-based world model's predictions using natural language hypotheses given in its prompt. These hypotheses are iteratively refined through a Bayesian inference framework that leverages a second LLM as the proposal distribution given collected evidence. This evidence is collected using a curiosity-driven reinforcement learning policy that explores the environment to find transitions with a low log-likelihood under our LLM-based predictive model using the current hypotheses. By alternating between refining hypotheses and collecting new evidence, our framework autonomously drives continual improvement of the predictions. Our experiments demonstrate the effectiveness of WorldLLM in a textual game environment that requires agents to manipulate and combine objects. The framework not only enhances predictive accuracy, but also generates human-interpretable theories of environment dynamics.",["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"https://arxiv.org/abs/2506.06725","Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) possess general world knowledge but often struggle to generate precise predictions in structured, domain-specific contexts such as simulations. These limitations arise from their inability to ground their broad, unstructured understanding in specific environments. To address this, we present WorldLLM, a framework that enhances LLM-based world modeling by combining Bayesian inference and autonomous active exploration with reinforcement learning. WorldLLM leverages the in-context learning abilities of LLMs to guide an LLM-based world model's predictions using natural language hypotheses given in its prompt. These hypotheses are iteratively refined through a Bayesian inference framework that leverages a second LLM as the proposal distribution given collected evidence. This evidence is collected using a curiosity-driven reinforcement learning policy that explores the environment to find transitions with a low log-likelihood under our LLM-based predictive model using the current hypotheses. By alternating between refining hypotheses and collecting new evidence, our framework autonomously drives continual improvement of the predictions. Our experiments demonstrate the effectiveness of WorldLLM in a textual game environment that requires agents to manipulate and combine objects. The framework not only enhances predictive accuracy, but also generates human-interpretable theories of environment dynamics.\r\nReferences & Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.","src/content/posts/2025-06-10-worldllm-improving-llms'-world-modeling-using-curiosity-driven-theory-making-bd4324.md","3a5b62f229a1f8eb",{"html":3493,"metadata":3494},"\u003Cp>Computer Science > Artificial Intelligence\r\n[Submitted on 7 Jun 2025]\r\nTitle:WorldLLM: Improving LLMs’ world modeling using curiosity-driven theory-making\r\nView PDF HTML (experimental)Abstract:Large Language Models (LLMs) possess general world knowledge but often struggle to generate precise predictions in structured, domain-specific contexts such as simulations. These limitations arise from their inability to ground their broad, unstructured understanding in specific environments. To address this, we present WorldLLM, a framework that enhances LLM-based world modeling by combining Bayesian inference and autonomous active exploration with reinforcement learning. WorldLLM leverages the in-context learning abilities of LLMs to guide an LLM-based world model’s predictions using natural language hypotheses given in its prompt. These hypotheses are iteratively refined through a Bayesian inference framework that leverages a second LLM as the proposal distribution given collected evidence. This evidence is collected using a curiosity-driven reinforcement learning policy that explores the environment to find transitions with a low log-likelihood under our LLM-based predictive model using the current hypotheses. By alternating between refining hypotheses and collecting new evidence, our framework autonomously drives continual improvement of the predictions. Our experiments demonstrate the effectiveness of WorldLLM in a textual game environment that requires agents to manipulate and combine objects. The framework not only enhances predictive accuracy, but also generates human-interpretable theories of environment dynamics.\r\nReferences &#x26; Citations\r\nBibliographic and Citation Tools\r\nBibliographic Explorer (What is the Explorer?)\r\nConnected Papers (What is Connected Papers?)\r\nLitmaps (What is Litmaps?)\r\nscite Smart Citations (What are Smart Citations?)\r\nCode, Data and Media Associated with this Article\r\nalphaXiv (What is alphaXiv?)\r\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\r\nDagsHub (What is DagsHub?)\r\nGotit.pub (What is GotitPub?)\r\nHugging Face (What is Huggingface?)\r\nPapers with Code (What is Papers with Code?)\r\nScienceCast (What is ScienceCast?)\r\nDemos\r\nRecommenders and Search Tools\r\nInfluence Flower (What are Influence Flowers?)\r\nCORE Recommender (What is CORE?)\r\narXivLabs: experimental projects with community collaborators\r\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\r\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\r\nHave an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.\u003C/p>",{"headings":3495,"localImagePaths":3496,"remoteImagePaths":3497,"frontmatter":3498,"imagePaths":3500},[],[],[],{"title":3484,"description":3485,"pubDate":33,"source":17,"tags":3499,"url":3488},[19,20,21],[],"2025-06-10-worldllm-improving-llms'-world-modeling-using-curiosity-driven-theory-making-bd4324.md","2025-06-11-ai-psyroom-artificial-intelligence-platform-for-segmented-yearning-and-reactive--ad0200",{"id":3502,"data":3504,"body":171,"filePath":3507,"digest":173,"rendered":3508,"legacyId":3516},{"title":166,"description":167,"pubDate":3505,"source":17,"tags":3506,"url":170},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-ai-psyroom-artificial-intelligence-platform-for-segmented-yearning-and-reactive--ad0200.md",{"html":175,"metadata":3509},{"headings":3510,"localImagePaths":3511,"remoteImagePaths":3512,"frontmatter":3513,"imagePaths":3515},[],[],[],{"title":166,"description":167,"pubDate":33,"source":17,"tags":3514,"url":170},[19,20,21],[],"2025-06-11-ai-psyroom-artificial-intelligence-platform-for-segmented-yearning-and-reactive--ad0200.md","2025-06-11-an-agentic-framework-for-autonomous-metamaterial-modeling-and-inverse-design-1e3057",{"id":3517,"data":3519,"body":234,"filePath":3522,"digest":236,"rendered":3523,"legacyId":3531},{"title":229,"description":230,"pubDate":3520,"source":17,"tags":3521,"url":233},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-an-agentic-framework-for-autonomous-metamaterial-modeling-and-inverse-design-1e3057.md",{"html":238,"metadata":3524},{"headings":3525,"localImagePaths":3526,"remoteImagePaths":3527,"frontmatter":3528,"imagePaths":3530},[],[],[],{"title":229,"description":230,"pubDate":33,"source":17,"tags":3529,"url":233},[19,20,21],[],"2025-06-11-an-agentic-framework-for-autonomous-metamaterial-modeling-and-inverse-design-1e3057.md","2025-06-11-ai-simulation-by-digital-twins-systematic-survey-reference-framework-and-mappi-c726f1",{"id":3532,"data":3534,"body":150,"filePath":3537,"digest":152,"rendered":3538,"legacyId":3546},{"title":145,"description":146,"pubDate":3535,"source":17,"tags":3536,"url":149},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-ai-simulation-by-digital-twins-systematic-survey,-reference-framework,-and-mappi-c726f1.md",{"html":154,"metadata":3539},{"headings":3540,"localImagePaths":3541,"remoteImagePaths":3542,"frontmatter":3543,"imagePaths":3545},[],[],[],{"title":145,"description":146,"pubDate":33,"source":17,"tags":3544,"url":149},[19,20,21],[],"2025-06-11-ai-simulation-by-digital-twins-systematic-survey,-reference-framework,-and-mappi-c726f1.md","2025-06-11-causal-graph-based-event-reasoning-using-semantic-relation-experts-418702",{"id":3547,"data":3549,"body":528,"filePath":3552,"digest":530,"rendered":3553,"legacyId":3561},{"title":523,"description":524,"pubDate":3550,"source":17,"tags":3551,"url":527},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-causal-graph-based-event-reasoning-using-semantic-relation-experts-418702.md",{"html":532,"metadata":3554},{"headings":3555,"localImagePaths":3556,"remoteImagePaths":3557,"frontmatter":3558,"imagePaths":3560},[],[],[],{"title":523,"description":524,"pubDate":33,"source":17,"tags":3559,"url":527},[19,20,21],[],"2025-06-11-causal-graph-based-event-reasoning-using-semantic-relation-experts-418702.md","2025-06-11-boosting-llm-reasoning-via-spontaneous-self-correction-ddd1a7",{"id":3562,"data":3564,"body":444,"filePath":3567,"digest":446,"rendered":3568,"legacyId":3576},{"title":439,"description":440,"pubDate":3565,"source":17,"tags":3566,"url":443},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-boosting-llm-reasoning-via-spontaneous-self-correction-ddd1a7.md",{"html":448,"metadata":3569},{"headings":3570,"localImagePaths":3571,"remoteImagePaths":3572,"frontmatter":3573,"imagePaths":3575},[],[],[],{"title":439,"description":440,"pubDate":33,"source":17,"tags":3574,"url":443},[19,20,21],[],"2025-06-11-boosting-llm-reasoning-via-spontaneous-self-correction-ddd1a7.md","2025-06-11-bio-inspired-classification-combining-information-theory-and-spiking-neural-netw-b49b52",{"id":3577,"data":3579,"body":423,"filePath":3582,"digest":425,"rendered":3583,"legacyId":3591},{"title":418,"description":419,"pubDate":3580,"source":17,"tags":3581,"url":422},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-bio-inspired-classification-combining-information-theory-and-spiking-neural-netw-b49b52.md",{"html":427,"metadata":3584},{"headings":3585,"localImagePaths":3586,"remoteImagePaths":3587,"frontmatter":3588,"imagePaths":3590},[],[],[],{"title":418,"description":419,"pubDate":33,"source":17,"tags":3589,"url":422},[19,20,21],[],"2025-06-11-bio-inspired-classification-combining-information-theory-and-spiking-neural-netw-b49b52.md","2025-06-11-contextual-experience-replay-for-self-improvement-of-language-agents-2d70b8",{"id":3592,"data":3594,"body":549,"filePath":3597,"digest":551,"rendered":3598,"legacyId":3606},{"title":544,"description":545,"pubDate":3595,"source":17,"tags":3596,"url":548},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-contextual-experience-replay-for-self-improvement-of-language-agents-2d70b8.md",{"html":553,"metadata":3599},{"headings":3600,"localImagePaths":3601,"remoteImagePaths":3602,"frontmatter":3603,"imagePaths":3605},[],[],[],{"title":544,"description":545,"pubDate":33,"source":17,"tags":3604,"url":548},[19,20,21],[],"2025-06-11-contextual-experience-replay-for-self-improvement-of-language-agents-2d70b8.md","2025-06-11-cross-entropy-games-for-language-models-from-implicit-knowledge-to-general-capab-e842b3",{"id":3607,"data":3609,"body":738,"filePath":3612,"digest":740,"rendered":3613,"legacyId":3621},{"title":733,"description":734,"pubDate":3610,"source":17,"tags":3611,"url":737},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-cross-entropy-games-for-language-models-from-implicit-knowledge-to-general-capab-e842b3.md",{"html":742,"metadata":3614},{"headings":3615,"localImagePaths":3616,"remoteImagePaths":3617,"frontmatter":3618,"imagePaths":3620},[],[],[],{"title":733,"description":734,"pubDate":33,"source":17,"tags":3619,"url":737},[19,20,21],[],"2025-06-11-cross-entropy-games-for-language-models-from-implicit-knowledge-to-general-capab-e842b3.md","2025-06-11-deep-research-bench-evaluating-ai-web-research-agents-a1630f",{"id":3622,"data":3624,"body":780,"filePath":3627,"digest":782,"rendered":3628,"legacyId":3636},{"title":775,"description":776,"pubDate":3625,"source":17,"tags":3626,"url":779},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-deep-research-bench-evaluating-ai-web-research-agents-a1630f.md",{"html":784,"metadata":3629},{"headings":3630,"localImagePaths":3631,"remoteImagePaths":3632,"frontmatter":3633,"imagePaths":3635},[],[],[],{"title":775,"description":776,"pubDate":33,"source":17,"tags":3634,"url":779},[19,20,21],[],"2025-06-11-deep-research-bench-evaluating-ai-web-research-agents-a1630f.md","2025-06-11-deontically-constrained-policy-improvement-in-reinforcement-learning-agents-6ab530",{"id":3637,"data":3639,"body":843,"filePath":3642,"digest":845,"rendered":3643,"legacyId":3651},{"title":838,"description":839,"pubDate":3640,"source":17,"tags":3641,"url":842},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-deontically-constrained-policy-improvement-in-reinforcement-learning-agents-6ab530.md",{"html":847,"metadata":3644},{"headings":3645,"localImagePaths":3646,"remoteImagePaths":3647,"frontmatter":3648,"imagePaths":3650},[],[],[],{"title":838,"description":839,"pubDate":33,"source":17,"tags":3649,"url":842},[19,20,21],[],"2025-06-11-deontically-constrained-policy-improvement-in-reinforcement-learning-agents-6ab530.md","2025-06-11-geld-a-unified-neural-model-for-efficiently-solving-traveling-salesman-problems--e24948",{"id":3652,"data":3654,"body":1347,"filePath":3657,"digest":1349,"rendered":3658,"legacyId":3666},{"title":1342,"description":1343,"pubDate":3655,"source":17,"tags":3656,"url":1346},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-geld-a-unified-neural-model-for-efficiently-solving-traveling-salesman-problems--e24948.md",{"html":1351,"metadata":3659},{"headings":3660,"localImagePaths":3661,"remoteImagePaths":3662,"frontmatter":3663,"imagePaths":3665},[],[],[],{"title":1342,"description":1343,"pubDate":33,"source":17,"tags":3664,"url":1346},[19,20,21],[],"2025-06-11-geld-a-unified-neural-model-for-efficiently-solving-traveling-salesman-problems--e24948.md","2025-06-11-honey-i-shrunk-the-hypothesis-space-through-logical-preprocessing-17274e",{"id":3667,"data":3669,"body":1620,"filePath":3672,"digest":1622,"rendered":3673,"legacyId":3681},{"title":1615,"description":1616,"pubDate":3670,"source":17,"tags":3671,"url":1619},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-honey,-i-shrunk-the-hypothesis-space-(through-logical-preprocessing)-17274e.md",{"html":1624,"metadata":3674},{"headings":3675,"localImagePaths":3676,"remoteImagePaths":3677,"frontmatter":3678,"imagePaths":3680},[],[],[],{"title":1615,"description":1616,"pubDate":33,"source":17,"tags":3679,"url":1619},[19,20,21],[],"2025-06-11-honey,-i-shrunk-the-hypothesis-space-(through-logical-preprocessing)-17274e.md","2025-06-11-incorporating-failure-of-machine-learning-in-dynamic-probabilistic-safety-assura-1b8298",{"id":3682,"data":3684,"body":1767,"filePath":3687,"digest":1769,"rendered":3688,"legacyId":3696},{"title":1762,"description":1763,"pubDate":3685,"source":17,"tags":3686,"url":1766},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-incorporating-failure-of-machine-learning-in-dynamic-probabilistic-safety-assura-1b8298.md",{"html":1771,"metadata":3689},{"headings":3690,"localImagePaths":3691,"remoteImagePaths":3692,"frontmatter":3693,"imagePaths":3695},[],[],[],{"title":1762,"description":1763,"pubDate":33,"source":17,"tags":3694,"url":1766},[19,20,21],[],"2025-06-11-incorporating-failure-of-machine-learning-in-dynamic-probabilistic-safety-assura-1b8298.md","2025-06-11-integrating-ai-planning-semantics-into-sysml-system-models-for-automated-pddl-fi-01340a",{"id":3697,"data":3699,"body":1746,"filePath":3702,"digest":1748,"rendered":3703,"legacyId":3711},{"title":1741,"description":1742,"pubDate":3700,"source":17,"tags":3701,"url":1745},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-integrating-ai-planning-semantics-into-sysml-system-models-for-automated-pddl-fi-01340a.md",{"html":1750,"metadata":3704},{"headings":3705,"localImagePaths":3706,"remoteImagePaths":3707,"frontmatter":3708,"imagePaths":3710},[],[],[],{"title":1741,"description":1742,"pubDate":33,"source":17,"tags":3709,"url":1745},[19,20,21],[],"2025-06-11-integrating-ai-planning-semantics-into-sysml-system-models-for-automated-pddl-fi-01340a.md","2025-06-11-large-language-models-and-their-applications-in-roadway-safety-and-mobility-enha-610ee6",{"id":3712,"data":3714,"body":1830,"filePath":3717,"digest":1832,"rendered":3718,"legacyId":3726},{"title":1825,"description":1826,"pubDate":3715,"source":17,"tags":3716,"url":1829},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-large-language-models-and-their-applications-in-roadway-safety-and-mobility-enha-610ee6.md",{"html":1834,"metadata":3719},{"headings":3720,"localImagePaths":3721,"remoteImagePaths":3722,"frontmatter":3723,"imagePaths":3725},[],[],[],{"title":1825,"description":1826,"pubDate":33,"source":17,"tags":3724,"url":1829},[19,20,21],[],"2025-06-11-large-language-models-and-their-applications-in-roadway-safety-and-mobility-enha-610ee6.md","2025-06-11-knowcoder-v2-deep-knowledge-analysis-fef1e5",{"id":3727,"data":3729,"body":1788,"filePath":3732,"digest":1790,"rendered":3733,"legacyId":3741},{"title":1783,"description":1784,"pubDate":3730,"source":17,"tags":3731,"url":1787},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-knowcoder-v2-deep-knowledge-analysis-fef1e5.md",{"html":1792,"metadata":3734},{"headings":3735,"localImagePaths":3736,"remoteImagePaths":3737,"frontmatter":3738,"imagePaths":3740},[],[],[],{"title":1783,"description":1784,"pubDate":33,"source":17,"tags":3739,"url":1787},[19,20,21],[],"2025-06-11-knowcoder-v2-deep-knowledge-analysis-fef1e5.md","2025-06-11-mapping-human-agent-co-learning-and-co-adaptation-a-scoping-review-6574e7",{"id":3742,"data":3744,"body":2061,"filePath":3747,"digest":2063,"rendered":3748,"legacyId":3756},{"title":2056,"description":2057,"pubDate":3745,"source":17,"tags":3746,"url":2060},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-mapping-human-agent-co-learning-and-co-adaptation-a-scoping-review-6574e7.md",{"html":2065,"metadata":3749},{"headings":3750,"localImagePaths":3751,"remoteImagePaths":3752,"frontmatter":3753,"imagePaths":3755},[],[],[],{"title":2056,"description":2057,"pubDate":33,"source":17,"tags":3754,"url":2060},[19,20,21],[],"2025-06-11-mapping-human-agent-co-learning-and-co-adaptation-a-scoping-review-6574e7.md","2025-06-11-learning-what-matters-now-a-dual-critic-context-aware-rl-framework-for-priority--42f7f3",{"id":3757,"data":3759,"body":1914,"filePath":3762,"digest":1916,"rendered":3763,"legacyId":3771},{"title":1909,"description":1910,"pubDate":3760,"source":17,"tags":3761,"url":1913},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-learning-what-matters-now-a-dual-critic-context-aware-rl-framework-for-priority--42f7f3.md",{"html":1918,"metadata":3764},{"headings":3765,"localImagePaths":3766,"remoteImagePaths":3767,"frontmatter":3768,"imagePaths":3770},[],[],[],{"title":1909,"description":1910,"pubDate":33,"source":17,"tags":3769,"url":1913},[19,20,21],[],"2025-06-11-learning-what-matters-now-a-dual-critic-context-aware-rl-framework-for-priority--42f7f3.md","2025-06-11-meta-adaptive-prompt-distillation-for-few-shot-visual-question-answering-a50265",{"id":3772,"data":3774,"body":2166,"filePath":3777,"digest":2168,"rendered":3778,"legacyId":3786},{"title":2161,"description":2162,"pubDate":3775,"source":17,"tags":3776,"url":2165},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-meta-adaptive-prompt-distillation-for-few-shot-visual-question-answering-a50265.md",{"html":2170,"metadata":3779},{"headings":3780,"localImagePaths":3781,"remoteImagePaths":3782,"frontmatter":3783,"imagePaths":3785},[],[],[],{"title":2161,"description":2162,"pubDate":33,"source":17,"tags":3784,"url":2165},[19,20,21],[],"2025-06-11-meta-adaptive-prompt-distillation-for-few-shot-visual-question-answering-a50265.md","2025-06-11-memory-os-of-ai-agent-041cf0",{"id":3787,"data":3789,"body":2124,"filePath":3792,"digest":2126,"rendered":3793,"legacyId":3801},{"title":2119,"description":2120,"pubDate":3790,"source":17,"tags":3791,"url":2123},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-memory-os-of-ai-agent-041cf0.md",{"html":2128,"metadata":3794},{"headings":3795,"localImagePaths":3796,"remoteImagePaths":3797,"frontmatter":3798,"imagePaths":3800},[],[],[],{"title":2119,"description":2120,"pubDate":33,"source":17,"tags":3799,"url":2123},[19,20,21],[],"2025-06-11-memory-os-of-ai-agent-041cf0.md","2025-06-11-reinforcement-learning-for-autonomous-warehouse-orchestration-in-sap-logistics-e-3e7f2a",{"id":3802,"data":3804,"body":2628,"filePath":3807,"digest":2630,"rendered":3808,"legacyId":3816},{"title":2623,"description":2624,"pubDate":3805,"source":17,"tags":3806,"url":2627},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-reinforcement-learning-for-autonomous-warehouse-orchestration-in-sap-logistics-e-3e7f2a.md",{"html":2632,"metadata":3809},{"headings":3810,"localImagePaths":3811,"remoteImagePaths":3812,"frontmatter":3813,"imagePaths":3815},[],[],[],{"title":2623,"description":2624,"pubDate":33,"source":17,"tags":3814,"url":2627},[19,20,21],[],"2025-06-11-reinforcement-learning-for-autonomous-warehouse-orchestration-in-sap-logistics-e-3e7f2a.md","2025-06-11-nfisis-new-perspectives-on-fuzzy-inference-systems-for-renewable-energy-forecast-f72f28",{"id":3817,"data":3819,"body":2397,"filePath":3822,"digest":2399,"rendered":3823,"legacyId":3831},{"title":2392,"description":2393,"pubDate":3820,"source":17,"tags":3821,"url":2396},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-nfisis-new-perspectives-on-fuzzy-inference-systems-for-renewable-energy-forecast-f72f28.md",{"html":2401,"metadata":3824},{"headings":3825,"localImagePaths":3826,"remoteImagePaths":3827,"frontmatter":3828,"imagePaths":3830},[],[],[],{"title":2392,"description":2393,"pubDate":33,"source":17,"tags":3829,"url":2396},[19,20,21],[],"2025-06-11-nfisis-new-perspectives-on-fuzzy-inference-systems-for-renewable-energy-forecast-f72f28.md","2025-06-11-scriptdoctor-automatic-generation-of-puzzlescript-games-via-large-language-model-746ba4",{"id":3832,"data":3834,"body":2775,"filePath":3837,"digest":2777,"rendered":3838,"legacyId":3846},{"title":2770,"description":2771,"pubDate":3835,"source":17,"tags":3836,"url":2774},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-scriptdoctor-automatic-generation-of-puzzlescript-games-via-large-language-model-746ba4.md",{"html":2779,"metadata":3839},{"headings":3840,"localImagePaths":3841,"remoteImagePaths":3842,"frontmatter":3843,"imagePaths":3845},[],[],[],{"title":2770,"description":2771,"pubDate":33,"source":17,"tags":3844,"url":2774},[19,20,21],[],"2025-06-11-scriptdoctor-automatic-generation-of-puzzlescript-games-via-large-language-model-746ba4.md","2025-06-11-sigma-refining-large-language-model-reasoning-via-sibling-guided-monte-carlo-aug-1a8d3b",{"id":3847,"data":3849,"body":2796,"filePath":3852,"digest":2798,"rendered":3853,"legacyId":3861},{"title":2791,"description":2792,"pubDate":3850,"source":17,"tags":3851,"url":2795},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-sigma-refining-large-language-model-reasoning-via-sibling-guided-monte-carlo-aug-1a8d3b.md",{"html":2800,"metadata":3854},{"headings":3855,"localImagePaths":3856,"remoteImagePaths":3857,"frontmatter":3858,"imagePaths":3860},[],[],[],{"title":2791,"description":2792,"pubDate":33,"source":17,"tags":3859,"url":2795},[19,20,21],[],"2025-06-11-sigma-refining-large-language-model-reasoning-via-sibling-guided-monte-carlo-aug-1a8d3b.md","2025-06-11-the-illusion-of-thinking-understanding-the-strengths-and-limitations-of-reasonin-538e43",{"id":3862,"data":3864,"body":3090,"filePath":3867,"digest":3092,"rendered":3868,"legacyId":3876},{"title":3085,"description":3086,"pubDate":3865,"source":17,"tags":3866,"url":3089},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-the-illusion-of-thinking-understanding-the-strengths-and-limitations-of-reasonin-538e43.md",{"html":3094,"metadata":3869},{"headings":3870,"localImagePaths":3871,"remoteImagePaths":3872,"frontmatter":3873,"imagePaths":3875},[],[],[],{"title":3085,"description":3086,"pubDate":33,"source":17,"tags":3874,"url":3089},[19,20,21],[],"2025-06-11-the-illusion-of-thinking-understanding-the-strengths-and-limitations-of-reasonin-538e43.md","2025-06-11-understanding-financial-reasoning-in-ai-a-multimodal-benchmark-and-error-learnin-d53606",{"id":3877,"data":3879,"body":3279,"filePath":3882,"digest":3281,"rendered":3883,"legacyId":3891},{"title":3274,"description":3275,"pubDate":3880,"source":17,"tags":3881,"url":3278},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-understanding-financial-reasoning-in-ai-a-multimodal-benchmark-and-error-learnin-d53606.md",{"html":3283,"metadata":3884},{"headings":3885,"localImagePaths":3886,"remoteImagePaths":3887,"frontmatter":3888,"imagePaths":3890},[],[],[],{"title":3274,"description":3275,"pubDate":33,"source":17,"tags":3889,"url":3278},[19,20,21],[],"2025-06-11-understanding-financial-reasoning-in-ai-a-multimodal-benchmark-and-error-learnin-d53606.md","2025-06-11-the-optimization-paradox-in-clinical-ai-multi-agent-systems-0c165f",{"id":3892,"data":3894,"body":3111,"filePath":3897,"digest":3113,"rendered":3898,"legacyId":3906},{"title":3106,"description":3107,"pubDate":3895,"source":17,"tags":3896,"url":3110},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-the-optimization-paradox-in-clinical-ai-multi-agent-systems-0c165f.md",{"html":3115,"metadata":3899},{"headings":3900,"localImagePaths":3901,"remoteImagePaths":3902,"frontmatter":3903,"imagePaths":3905},[],[],[],{"title":3106,"description":3107,"pubDate":33,"source":17,"tags":3904,"url":3110},[19,20,21],[],"2025-06-11-the-optimization-paradox-in-clinical-ai-multi-agent-systems-0c165f.md","2025-06-11-towards-foundation-model-on-temporal-knowledge-graph-reasoning-460b51",{"id":3907,"data":3909,"body":3216,"filePath":3912,"digest":3218,"rendered":3913,"legacyId":3921},{"title":3211,"description":3212,"pubDate":3910,"source":17,"tags":3911,"url":3215},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-towards-foundation-model-on-temporal-knowledge-graph-reasoning-460b51.md",{"html":3220,"metadata":3914},{"headings":3915,"localImagePaths":3916,"remoteImagePaths":3917,"frontmatter":3918,"imagePaths":3920},[],[],[],{"title":3211,"description":3212,"pubDate":33,"source":17,"tags":3919,"url":3215},[19,20,21],[],"2025-06-11-towards-foundation-model-on-temporal-knowledge-graph-reasoning-460b51.md","2025-06-11-united-minds-or-isolated-agents-exploring-coordination-of-llms-under-cognitive-l-5bf08b",{"id":3922,"data":3924,"body":3321,"filePath":3927,"digest":3323,"rendered":3928,"legacyId":3936},{"title":3316,"description":3317,"pubDate":3925,"source":17,"tags":3926,"url":3320},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-united-minds-or-isolated-agents-exploring-coordination-of-llms-under-cognitive-l-5bf08b.md",{"html":3325,"metadata":3929},{"headings":3930,"localImagePaths":3931,"remoteImagePaths":3932,"frontmatter":3933,"imagePaths":3935},[],[],[],{"title":3316,"description":3317,"pubDate":33,"source":17,"tags":3934,"url":3320},[19,20,21],[],"2025-06-11-united-minds-or-isolated-agents-exploring-coordination-of-llms-under-cognitive-l-5bf08b.md","2025-06-11-unreal-patterns-3c498f",{"id":3937,"data":3939,"body":3384,"filePath":3942,"digest":3386,"rendered":3943,"legacyId":3951},{"title":3379,"description":3380,"pubDate":3940,"source":17,"tags":3941,"url":3383},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-unreal-patterns-3c498f.md",{"html":3388,"metadata":3944},{"headings":3945,"localImagePaths":3946,"remoteImagePaths":3947,"frontmatter":3948,"imagePaths":3950},[],[],[],{"title":3379,"description":3380,"pubDate":33,"source":17,"tags":3949,"url":3383},[19,20,21],[],"2025-06-11-unreal-patterns-3c498f.md","2025-06-11-will-artificial-agents-pursue-power-by-default-4b4ab9",{"id":3952,"data":3954,"body":3447,"filePath":3957,"digest":3449,"rendered":3958,"legacyId":3966},{"title":3442,"description":3443,"pubDate":3955,"source":17,"tags":3956,"url":3446},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-will-artificial-agents-pursue-power-by-default-4b4ab9.md",{"html":3451,"metadata":3959},{"headings":3960,"localImagePaths":3961,"remoteImagePaths":3962,"frontmatter":3963,"imagePaths":3965},[],[],[],{"title":3442,"description":3443,"pubDate":33,"source":17,"tags":3964,"url":3446},[19,20,21],[],"2025-06-11-will-artificial-agents-pursue-power-by-default-4b4ab9.md","2025-06-11-visiomath-benchmarking-figure-based-mathematical-reasoning-in-lmms-e407ac",{"id":3967,"data":3969,"body":3405,"filePath":3972,"digest":3407,"rendered":3973,"legacyId":3981},{"title":3400,"description":3401,"pubDate":3970,"source":17,"tags":3971,"url":3404},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-visiomath-benchmarking-figure-based-mathematical-reasoning-in-lmms-e407ac.md",{"html":3409,"metadata":3974},{"headings":3975,"localImagePaths":3976,"remoteImagePaths":3977,"frontmatter":3978,"imagePaths":3980},[],[],[],{"title":3400,"description":3401,"pubDate":33,"source":17,"tags":3979,"url":3404},[19,20,21],[],"2025-06-11-visiomath-benchmarking-figure-based-mathematical-reasoning-in-lmms-e407ac.md","2025-06-11-worldllm-improving-llms-world-modeling-using-curiosity-driven-theory-making-bd4324",{"id":3982,"data":3984,"body":3489,"filePath":3987,"digest":3491,"rendered":3988,"legacyId":3996},{"title":3484,"description":3485,"pubDate":3985,"source":17,"tags":3986,"url":3488},["Date","2025-06-10T04:00:00.000Z"],[19,20,21],"src/content/posts/2025-06-11-worldllm-improving-llms'-world-modeling-using-curiosity-driven-theory-making-bd4324.md",{"html":3493,"metadata":3989},{"headings":3990,"localImagePaths":3991,"remoteImagePaths":3992,"frontmatter":3993,"imagePaths":3995},[],[],[],{"title":3484,"description":3485,"pubDate":33,"source":17,"tags":3994,"url":3488},[19,20,21],[],"2025-06-11-worldllm-improving-llms'-world-modeling-using-curiosity-driven-theory-making-bd4324.md"]
[["Map",1,2,9,10,11,12],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.9.2","content-config-digest","df7ab7c44759a549","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://raisex-llc.github.io/ai-news-curation-site/\",\"compressHTML\":true,\"base\":\"/ai-news-curation-site/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"experimentalDefaultStyles\":true},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"prism\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","posts.bak",["Map"],"posts",["Map",13,14,34,35,52,53,69,70,87,88,104,105,122,123,140,141,157,158,175,176,193,194,211,212,228,229,246,247,264,265,281,282,299,300,316,317,333,334,350,351,367,368,384,385,401,402,418,419,435,436,453,454,471,472,488,489,505,506,522,523,540,541,557,558,574,575,591,592,609,610,627,628,644,645,662,663,680,681,698,699,715,716,732,733,750,751,768,769,786,787,803,804,821,822,839,840,857,858,874,875,891,892,909,910,927,928,945,946,963,964,981,982,999,1000,1017,1018,1034,1035,1052,1053,1069,1070,1087,1088,1104,1105,1121,1122,1138,1139,1156,1157,1172,1173,1190,1191,1207,1208,1224,1225,1242,1243,1260,1261,1277,1278,1295,1296,1313,1314,1331,1332,1348,1349,1366,1367,1384,1385,1400,1401,1417,1418,1435,1436,1453,1454,1470,1471,1487,1488,1504,1505,1521,1522,1538,1539,1556,1557,1574,1575,1592,1593,1610,1611,1628,1629,1646,1647,1663,1664,1680,1681,1698,1699,1716,1717,1734,1735,1752,1753,1770,1771,1788,1789,1806,1807,1823,1824,1841,1842,1859,1860,1876,1877,1894,1895,1912,1913,1929,1930,1947,1948,1965,1966,1983,1984,2000,2001,2018,2019,2036,2037,2054,2055,2072,2073,2090,2091,2108,2109,2125,2126,2143,2144,2161,2162,2179,2180,2197,2198,2215,2216,2233,2234,2251,2252,2269,2270,2287,2288,2305,2306,2323,2324,2341,2342,2358,2359,2376,2377,2394,2395,2412,2413,2430,2431,2448,2449,2466,2467,2484,2485,2502,2503,2520,2521,2538,2539,2556,2557,2574,2575,2592,2593,2609,2610,2627,2628,2645,2646,2662,2663,2679,2680,2696,2697,2714,2715,2734,2735,2753,2754,2771,2772,2789,2790,2807,2808,2825,2826,2842,2843,2860,2861,2878,2879,2896,2897,2915,2916,2933,2934,2951,2952,2968,2969,2987,2988,3005,3006,3024,3025,3043,3044,3062,3063,3081,3082,3099,3100,3117,3118,3136,3137,3155,3156,3173,3174,3190,3191,3209,3210,3226,3227,3245,3246,3264,3265,3283,3284,3301,3302,3320,3321,3339,3340,3358,3359,3376,3377,3395,3396,3414,3415,3433,3434,3452,3453,3470,3471,3488,3489,3507,3508,3526,3527,3545,3546,3563,3564,3581,3582,3598,3599,3616,3617,3635,3636,3654,3655,3672,3673,3690,3691,3708,3709,3727,3728,3744,3745,3760,3761,3778,3779,3797,3798,3816,3817,3834,3835,3853,3854,3871,3872,3890,3891,3909,3910,3927,3928,3945,3946,3963,3964,3982,3983,4001,4002,4019,4020,4038,4039,4057,4058,4075,4076,4094,4095,4112,4113,4131,4132,4150,4151,4168,4169,4187,4188,4206,4207,4225,4226,4244,4245,4263,4264,4282,4283,4299,4300,4317,4318,4335,4336,4354,4355,4373,4374,4392,4393,4410,4411,4429,4430,4448,4449,4465,4466,4482,4483,4499,4500,4518,4519,4536,4537,4555,4556,4573,4574,4592,4593,4611,4612,4630,4631,4649,4650,4668,4669,4687,4688,4704,4705,4723,4724,4740,4741,4759,4760,4778,4779,4796,4797,4815,4816,4834,4835,4853,4854,4872,4873,4890,4891,4909,4910,4928,4929,4947,4948,4966,4967,4985,4986,5003,5004,5022,5023,5041,5042,5059,5060,5078,5079,5096,5097,5115,5116,5133,5134,5152,5153,5170,5171,5189,5190,5208,5209,5227,5228,5244,5245,5262,5263,5281,5282,5299,5300,5317,5318,5335,5336,5354,5355,5373,5374,5390,5391,5409,5410,5428,5429,5446,5447,5465,5466,5483,5484,5501,5502,5520,5521,5539,5540,5558,5559,5577,5578,5595,5596,5614,5615,5633,5634,5651,5652,5670,5671,5688,5689,5707,5708,5724,5725,5743,5744,5760,5761,5779,5780,5798,5799,5817,5818,5836,5837,5855,5856,5874,5875,5892,5893,5911,5912,5930,5931,5949,5950,5967,5968,5986,5987,6005,6006,6024,6025,6043,6044,6061,6062,6079,6080,6098,6099,6116,6117,6135,6136,6154,6155,6173,6174,6192,6193,6211,6212,6229,6230,6248,6249,6267,6268,6286,6287,6304,6305,6323,6324,6342,6343,6361,6362,6379,6380,6398,6399,6417,6418,6437,6438,6456,6457,6475,6476,6492,6493,6511,6512,6530,6531,6549,6550,6567,6568,6586,6587,6605,6606,6624,6625,6642,6643,6661,6662,6680,6681,6698,6699,6717,6718,6735,6736,6754,6755,6773,6774,6792,6793,6811,6812,6830,6831,6848,6849,6867,6868,6886,6887,6905,6906,6924,6925,6942,6943,6961,6962,6979,6980,6998,6999,7016,7017,7035,7036,7053,7054,7072,7073,7091,7092,7109,7110,7127,7128,7144,7145,7162,7163,7181,7182,7200,7201,7218,7219,7236,7237,7255,7256,7273,7274,7291,7292,7310,7311,7328,7329,7347,7348,7366,7367,7385,7386,7404,7405,7422,7423,7441,7442,7460,7461,7478,7479,7497,7498,7515,7516,7533,7534,7552,7553,7570,7571,7589,7590,7608,7609,7627,7628,7645,7646,7664,7665,7683,7684,7702,7703,7720,7721,7738,7739,7757,7758,7776,7777,7794,7795,7813,7814,7831,7832,7850,7851,7869,7870,7888,7889,7906,7907,7925,7926,7944,7945,7963,7964,7981,7982,7998,7999,8015,8016,8032,8033,8049,8050,8066,8067,8083,8084,8101,8102,8120,8121,8138,8139,8157,8158,8176,8177,8195,8196,8213,8214,8232,8233,8250,8251,8269,8270,8288,8289,8306,8307,8325,8326,8344,8345,8363,8364,8382,8383,8401,8402,8420,8421,8439,8440,8457,8458,8476,8477,8495,8496,8514,8515,8532,8533,8551,8552,8570,8571,8589,8590,8608,8609,8627,8628,8645,8646,8664,8665,8682,8683,8700,8701,8719,8720,8737,8738,8755,8756,8773,8774,8792,8793,8810,8811,8828,8829,8846,8847,8864,8865,8883,8884,8901,8902,8920,8921,8938,8939,8957,8958,8975,8976,8994,8995,9012,9013,9031,9032,9050,9051,9069,9070,9088,9089,9107,9108,9126,9127,9143,9144,9162,9163,9180,9181,9198,9199,9217,9218,9235,9236,9254,9255,9272,9273,9290,9291,9309,9310,9328,9329,9347,9348,9366,9367,9385,9386,9403,9404,9421,9422,9438,9439,9457,9458,9475,9476,9494,9495,9513,9514,9531,9532,9550,9551,9569,9570,9588,9589,9607,9608,9624,9625,9642,9643,9661,9662,9680,9681,9699,9700,9718,9719,9736,9737,9755,9756,9773,9774,9791,9792,9810,9811,9828,9829,9847,9848,9866,9867,9885,9886,9904,9905,9923,9924,9941,9942,9959,9960,9978,9979,9995,9996,10014,10015,10032,10033,10051,10052,10069,10070,10088,10089,10107,10108,10125,10126,10144,10145,10162,10163,10181,10182,10199,10200,10217,10218,10235,10236,10254,10255,10272,10273,10291,10292,10310,10311,10328,10329,10346,10347,10364,10365,10383,10384,10402,10403,10420,10421,10439,10440,10457,10458,10476,10477,10495,10496,10514,10515,10533,10534,10552,10553,10570,10571,10588,10589,10607,10608,10626,10627,10644,10645,10663,10664,10681,10682,10700,10701,10719,10720,10737,10738,10753,10754,10772,10773,10791,10792,10810,10811,10828,10829,10847,10848,10866,10867,10884,10885,10901,10902,10920,10921,10939,10940,10957,10958,10974,10975,10990,10991,11007,11008,11025,11026,11044,11045,11063,11064,11082,11083,11100,11101,11118,11119,11136,11137,11155,11156,11173,11174,11190,11191,11209,11210,11228,11229,11247,11248,11265,11266,11282,11283,11301,11302,11319,11320,11337,11338,11355,11356,11374,11375,11393,11394,11412,11413,11431,11432,11448,11449,11467,11468,11485,11486,11503,11504,11522,11523,11540,11541,11558,11559,11577,11578,11596,11597,11615,11616,11634,11635,11652,11653,11670,11671,11689,11690,11707,11708,11723,11724,11740,11741,11758,11759,11776,11777,11795,11796,11814,11815,11832,11833,11851,11852,11869,11870,11888,11889,11907,11908,11925,11926,11944,11945,11962,11963,11979,11980,11998,11999,12016,12017,12035,12036,12053,12054,12071,12072,12089,12090,12108,12109,12127,12128,12146,12147,12164,12165,12183,12184,12202,12203,12220,12221,12239,12240,12257,12258,12276,12277,12294,12295,12313,12314,12332,12333,12350,12351,12368,12369,12387,12388,12404,12405,12423,12424,12442,12443,12461,12462,12479,12480,12498,12499,12516,12517,12535,12536,12553,12554,12572,12573,12591,12592,12610,12611,12629,12630,12648,12649,12667,12668,12685,12686,12703,12704,12722,12723,12741,12742,12760,12761,12779,12780,12798,12799,12817,12818,12835,12836,12853,12854,12870,12871,12887,12888,12905,12906,12922,12923,12941,12942,12959,12960,12976,12977,12994,12995,13012,13013,13030,13031,13049,13050,13066,13067,13084,13085,13101,13102,13120,13121,13139,13140,13157,13158,13174,13175,13192,13193,13211,13212,13230,13231,13248,13249,13267,13268,13284,13285,13302,13303,13320,13321,13338,13339,13356,13357,13373,13374,13392,13393,13409,13410,13427,13428,13446,13447,13463,13464,13481,13482,13499,13500,13518,13519,13537,13538,13555,13556,13574,13575,13592,13593,13610,13611,13629,13630,13646,13647,13664,13665,13682,13683,13701,13702,13720,13721,13739,13740,13757,13758,13776,13777,13794,13795,13812,13813,13828,13829,13846,13847,13863,13864,13880,13881,13899,13900,13916,13917,13934,13935,13953,13954,13972,13973,13991,13992,14008,14009,14027,14028,14045,14046,14063,14064,14080,14081,14099,14100,14116,14117,14135,14136,14152,14153,14170,14171,14188,14189,14207,14208,14224,14225,14241,14242,14260,14261,14279,14280,14296,14297,14314,14315,14333,14334,14351,14352,14370,14371,14388,14389,14406,14407,14424,14425,14442,14443,14461,14462,14479,14480,14498,14499,14517,14518,14535,14536,14552,14553,14570,14571,14588,14589,14606,14607,14625,14626,14644,14645,14662,14663,14679,14680,14698,14699,14716,14717,14734,14735,14753,14754,14771,14772,14789,14790,14807,14808,14825,14826,14842,14843,14861,14862,14880,14881,14899,14900,14917,14918,14935,14936,14952,14953,14969,14970,14987,14988,15006,15007,15024,15025,15042,15043,15059,15060,15078,15079,15096,15097,15115,15116,15133,15134,15152,15153,15170,15171,15187,15188,15206,15207,15224,15225,15243,15244,15261,15262,15278,15279,15296,15297,15314,15315,15331,15332,15349,15350,15368,15369,15386,15387,15405,15406,15423,15424,15441,15442,15460,15461,15479,15480,15498,15499,15517,15518,15536,15537,15554,15555,15573,15574,15591,15592,15609,15610,15627,15628,15646,15647,15665,15666,15683,15684,15701,15702,15720,15721,15738,15739,15755,15756,15773,15774,15792,15793,15811,15812,15830,15831,15848,15849,15867,15868,15886,15887,15903,15904,15921,15922,15939,15940,15957,15958,15976,15977,15995,15996,16014,16015,16033,16034,16051,16052,16069,16070,16087,16088,16105,16106,16123,16124,16140,16141,16158,16159,16176,16177,16195,16196,16213,16214,16231,16232,16250,16251,16269,16270,16287,16288,16305,16306,16323,16324,16342,16343,16360,16361,16378,16379,16395,16396,16414,16415,16433,16434,16452,16453,16469,16470,16487,16488,16506,16507,16525,16526,16544,16545,16562,16563,16579,16580,16597,16598,16614,16615,16631,16632,16648,16649,16665,16666,16683,16684,16700,16701,16718,16719,16735,16736,16754,16755,16773,16774,16790,16791,16809,16810,16828,16829,16846,16847,16865,16866,16884,16885,16902,16903,16920,16921,16939,16940,16957,16958,16975,16976,16993,16994,17012,17013,17031,17032,17050,17051,17067,17068,17085,17086,17103,17104,17122,17123,17138,17139,17157,17158,17175,17176,17193,17194,17211,17212,17229,17230,17247,17248,17265,17266,17284,17285,17302,17303,17320,17321,17339,17340,17358,17359,17376,17377,17394,17395,17413,17414,17432,17433,17450,17451,17468,17469,17487,17488,17504,17505,17522,17523,17540,17541,17559,17560,17577,17578,17595,17596,17614,17615,17633,17634,17651,17652,17668,17669,17686,17687,17703,17704,17720,17721,17738,17739,17756,17757,17775,17776,17793,17794,17811,17812,17830,17831,17848,17849,17867,17868,17884,17885,17903,17904,17921,17922,17939,17940,17957,17958,17976,17977,17994,17995,18012,18013,18029,18030,18048,18049,18066,18067,18085,18086,18104,18105,18121,18122,18139,18140,18157,18158,18176,18177,18195,18196,18213,18214,18232,18233,18249,18250,18268,18269,18286,18287,18304,18305,18321,18322,18338,18339,18355,18356,18374,18375,18393,18394,18411,18412,18430,18431,18449,18450,18468,18469,18487,18488,18505,18506,18524,18525,18542,18543,18561,18562,18580,18581,18598,18599,18616,18617,18634,18635,18652,18653,18670,18671,18687,18688,18705,18706,18721,18722,18738,18739,18755,18756,18773,18774,18792,18793,18810,18811,18829,18830,18847,18848,18866,18867,18885,18886,18903,18904,18922,18923,18939,18940,18957,18958,18976,18977,18995,18996,19013,19014,19032,19033,19051,19052,19070,19071,19088,19089,19107,19108,19125,19126,19144,19145,19163,19164,19180,19181,19198,19199,19216,19217,19234,19235,19253,19254,19271,19272,19288,19289,19305,19306,19322,19323,19340,19341,19359,19360,19377,19378,19395,19396,19412,19413,19429,19430,19448,19449,19467,19468,19486,19487,19504,19505,19523,19524,19541,19542,19560,19561,19578,19579,19596,19597,19613,19614,19632,19633,19650,19651,19667,19668,19685,19686,19702,19703,19719,19720,19737,19738,19755,19756,19773,19774,19792,19793,19810,19811,19829,19830,19847,19848,19865,19866,19883,19884,19901,19902,19919,19920,19938,19939,19956,19957,19974,19975,19993,19994,20011,20012,20029,20030,20047,20048,20066,20067,20084,20085,20102,20103,20120,20121,20139,20140,20158,20159,20176,20177,20195,20196,20213,20214,20231,20232,20250,20251,20269,20270,20287,20288,20305,20306,20324,20325,20343,20344,20361,20362,20380,20381,20398,20399,20416,20417,20434,20435,20452,20453,20471,20472,20490,20491,20508,20509,20526,20527,20544,20545,20562,20563,20581,20582,20598,20599,20616,20617,20634,20635,20653,20654,20672,20673,20691,20692,20710,20711,20728,20729,20747,20748,20765,20766,20783,20784,20803,20804,20822,20823,20840,20841,20858,20859,20876,20877,20894,20895,20912,20913,20930,20931,20949,20950,20967,20968,20985,20986,21004,21005,21023,21024,21041,21042,21059,21060,21078,21079,21096,21097,21115,21116,21133,21134,21151,21152,21169,21170,21187,21188,21205,21206,21224,21225,21243,21244,21261,21262,21280,21281,21298,21299,21316,21317,21334,21335,21354,21355,21373,21374,21391,21392,21410,21411,21429,21430,21448,21449,21467,21468,21486,21487,21504,21505,21521,21522,21539,21540,21558,21559,21577,21578,21595,21596,21613,21614,21632,21633,21651,21652,21670,21671,21688,21689,21706,21707,21725,21726,21744,21745,21762,21763,21780,21781,21799,21800,21818,21819,21836,21837,21855,21856,21873,21874,21891,21892,21908,21909,21925,21926,21944,21945,21963,21964,21982,21983,22001,22002,22020,22021,22039,22040,22057,22058,22075,22076,22093,22094,22112,22113,22131,22132,22150,22151,22169,22170,22187,22188,22206,22207,22225,22226,22244,22245,22263,22264,22282,22283,22300,22301,22318,22319,22337,22338,22355,22356,22374,22375,22393,22394,22412,22413,22430,22431,22449,22450,22467,22468,22486,22487,22504,22505,22523,22524,22541,22542,22560,22561,22579,22580,22598,22599,22616,22617,22633,22634,22651,22652,22669,22670,22687,22688,22705,22706,22723,22724,22742,22743,22760,22761,22779,22780,22798,22799,22817,22818,22835,22836,22854,22855,22873,22874,22892,22893,22911,22912,22930,22931,22948,22949,22965,22966,22984,22985,23003,23004,23022,23023,23041,23042,23059,23060,23079,23080,23097,23098,23115,23116,23133,23134,23151,23152,23169,23170,23188,23189,23207,23208,23225,23226,23243,23244,23259,23260,23277,23278,23296,23297,23314,23315,23332,23333,23350,23351,23368,23369,23386,23387,23405,23406,23423,23424,23442,23443,23460,23461,23479,23480,23499,23500,23518,23519,23537,23538,23556,23557,23574,23575,23592,23593,23611,23612,23630,23631,23649,23650,23668,23669,23687,23688,23706,23707,23725,23726,23743,23744,23762,23763,23780,23781,23799,23800,23818,23819,23837,23838,23856,23857,23874,23875,23892,23893,23910,23911,23929,23930,23947,23948,23966,23967,23984,23985,24003,24004,24022,24023,24041,24042,24060,24061,24079,24080,24098,24099,24115,24116,24133,24134,24152,24153,24171,24172,24190,24191,24209,24210,24227,24228,24246,24247,24266,24267,24285,24286,24304,24305,24323,24324,24342,24343,24361,24362,24380,24381,24399,24400,24418,24419,24436,24437,24454,24455,24473,24474,24491,24492,24509,24510,24528,24529,24547,24548,24566,24567,24585,24586,24604,24605,24623,24624,24642,24643,24661,24662,24678,24679,24697,24698,24716,24717,24735,24736,24753,24754,24771,24772,24790,24791,24809,24810,24828,24829,24847,24848,24865,24866,24884,24885,24901,24902,24920,24921,24938,24939,24958,24959,24977,24978,24996,24997,25015,25016,25034,25035,25053,25054,25072,25073,25091,25092,25110,25111,25129,25130,25148,25149,25167,25168,25185,25186,25205,25206,25222,25223,25239,25240,25256,25257,25273,25274,25290,25291,25307,25308,25324,25325,25341,25342,25358,25359,25375,25376,25392,25393,25409,25410,25426,25427,25443,25444,25460,25461,25477,25478,25494,25495,25511,25512,25528,25529,25545,25546,25562,25563,25579,25580,25596,25597,25614,25615,25631,25632,25650,25651,25669,25670,25688,25689,25705,25706,25722,25723,25739,25740,25756,25757,25773,25774,25790,25791,25807,25808,25824,25825,25841,25842,25858,25859,25877,25878,25894,25895,25911,25912,25928,25929,25945,25946,25962,25963,25979,25980,25996,25997,26013,26014,26030,26031,26047,26048,26064,26065,26081,26082,26098,26099,26115,26116,26132,26133,26149,26150,26166,26167,26183,26184,26200,26201,26217,26218,26234,26235,26251,26252,26268,26269,26285,26286,26302,26303,26319,26320,26336,26337,26353,26354,26370,26371,26387,26388,26404,26405,26421,26422,26438,26439,26457,26458,26474,26475,26491,26492,26508,26509,26525,26526,26542,26543,26559,26560,26576,26577,26593,26594,26610,26611,26627,26628,26644,26645,26661,26662,26678,26679,26695,26696,26712,26713,26729,26730,26746,26747,26763,26764,26780,26781,26797,26798,26814,26815,26831,26832,26848,26849,26865,26866,26882,26883,26899,26900,26916,26917,26933,26934,26950,26951,26967,26968,26984,26985,27001,27002,27018,27019,27037,27038,27054,27055,27071,27072,27088,27089,27105,27106,27122,27123,27139,27140,27156,27157,27173,27174,27190,27191,27207,27208,27224,27225,27241,27242,27258,27259,27275,27276,27292,27293,27309,27310,27326,27327,27343,27344,27360,27361,27377,27378,27394,27395,27411,27412,27428,27429,27445,27446,27462,27463,27479,27480,27496,27497,27513,27514,27530,27531,27547,27548,27564,27565,27581,27582,27598,27599,27615,27616,27632,27633,27649,27650,27666,27667,27683,27684,27700,27701,27717,27718,27734,27735,27751,27752,27768,27769,27785,27786,27802,27803,27819,27820,27836,27837,27853,27854,27870,27871,27887,27888,27904,27905,27921,27922,27938,27939,27955,27956,27972,27973,27989,27990,28006,28007,28023,28024,28040,28041,28057,28058,28074,28075,28091,28092,28108,28109,28125,28126,28142,28143,28159,28160,28176,28177,28195,28196,28212,28213,28229,28230,28244,28245,28261,28262,28278,28279,28295,28296,28312,28313,28329,28330,28346,28347,28363,28364,28380,28381,28397,28398,28414,28415,28431,28432,28448,28449,28465,28466,28482,28483,28499,28500,28516,28517,28533,28534,28550,28551,28567,28568,28584,28585,28602,28603,28619,28620,28636,28637,28653,28654,28670,28671,28687,28688,28704,28705,28721,28722,28738,28739,28755,28756,28772,28773,28789,28790,28806,28807,28824,28825,28841,28842,28858,28859,28875,28876,28892,28893,28909,28910,28926,28927,28943,28944,28960,28961,28977,28978,28994,28995,29011,29012,29028,29029,29045,29046,29062,29063,29079,29080,29096,29097,29113,29114,29130,29131,29147,29148,29164,29165,29181,29182,29198,29199,29215,29216,29232,29233,29249,29250,29266,29267,29283,29284,29300,29301,29317,29318,29334,29335,29351,29352,29368,29369,29385,29386,29402,29403,29419,29420,29436,29437,29453,29454,29470,29471,29487,29488,29504,29505,29523,29524,29540,29541,29557,29558,29574,29575,29591,29592,29608,29609,29625,29626,29642,29643,29659,29660,29676,29677,29693,29694,29710,29711,29727,29728,29744,29745,29761,29762,29778,29779,29795,29796,29812,29813,29829,29830,29846,29847,29863,29864,29880,29881,29897,29898,29914,29915,29931,29932,29948,29949,29965,29966,29982,29983,29999,30000,30016,30017,30033,30034,30050,30051,30067,30068,30084,30085,30101,30102,30118,30119,30135,30136,30152,30153,30169,30170,30186,30187,30203,30204,30220,30221,30237,30238,30254,30255,30271,30272,30288,30289,30305,30306,30322,30323,30339,30340,30356,30357,30373,30374,30390,30391,30407,30408,30424,30425,30441,30442,30458,30459,30475,30476,30492,30493,30509,30510,30526,30527,30543,30544,30560,30561,30577,30578,30594,30595,30611,30612,30628,30629,30645,30646,30662,30663,30679,30680,30696,30697,30713,30714,30730,30731,30747,30748,30764,30765,30781,30782,30798,30799,30815,30816,30832,30833,30849,30850,30866,30867,30883,30884,30900,30901,30917,30918,30934,30935,30951,30952,30968,30969,30985,30986,31002,31003,31019,31020,31036,31037,31053,31054,31070,31071,31087,31088,31104,31105,31121,31122,31138,31139,31155,31156,31172,31173,31189,31190,31206,31207,31223,31224,31240,31241,31257,31258,31274,31275,31291,31292,31308,31309,31325,31326,31342,31343,31359,31360,31378,31379,31395,31396,31412,31413,31429,31430,31446,31447,31463,31464,31480,31481,31497,31498,31514,31515,31531,31532,31548,31549,31565,31566,31582,31583,31599,31600,31616,31617,31633,31634,31650,31651,31667,31668,31684,31685,31701,31702,31718,31719,31735,31736,31752,31753,31769,31770,31786,31787,31805,31806,31824,31825,31843,31844,31862,31863,31880,31881,31899,31900,31918,31919,31937,31938,31956,31957,31974,31975,31993,31994,32012,32013,32031,32032,32050,32051,32069,32070,32087,32088,32104,32105,32121,32122,32138,32139,32155,32156,32172,32173,32189,32190,32206,32207,32223,32224,32240,32241,32257,32258,32274,32275,32291,32292,32310,32311,32327,32328,32344,32345,32363,32364,32380,32381,32397,32398,32414,32415,32431,32432,32448,32449,32465,32466,32482,32483,32499,32500,32516,32517,32533,32534,32550,32551,32567,32568,32584,32585,32601,32602,32618,32619,32635,32636,32652,32653,32669,32670,32686,32687,32703,32704,32720,32721,32737,32738,32754,32755,32771,32772,32788,32789,32805,32806,32822,32823,32839,32840,32856,32857,32873,32874,32890,32891,32907,32908,32924,32925,32941,32942,32958,32959,32975,32976,32992,32993,33009,33010,33026,33027,33043,33044,33060,33061,33077,33078,33094,33095,33111,33112,33128,33129,33145,33146,33162,33163,33179,33180,33196,33197,33213,33214,33230,33231,33247,33248,33264,33265,33281,33282,33298,33299,33315,33316,33332,33333,33349,33350,33366,33367,33383,33384,33400,33401,33417,33418,33434,33435,33451,33452,33468,33469,33485,33486,33502,33503,33521,33522,33538,33539,33555,33556,33572,33573,33591,33592,33608,33609,33625,33626,33642,33643,33659,33660,33676,33677,33693,33694,33710,33711,33727,33728,33744,33745,33761,33762,33778,33779,33795,33796,33812,33813,33829,33830,33846,33847,33863,33864,33880,33881,33897,33898,33914,33915,33931,33932,33948,33949,33965,33966,33982,33983,33999,34000,34016,34017,34033,34034,34050,34051,34067,34068,34084,34085,34101,34102,34118,34119,34135,34136,34152,34153,34169,34170,34184,34185,34201,34202,34218,34219,34235,34236,34252,34253,34269,34270,34286,34287,34303,34304,34322,34323,34339,34340,34356,34357,34373,34374,34390,34391,34407,34408,34424,34425,34441,34442,34458,34459,34475,34476,34492,34493,34509,34510,34526,34527,34543,34544,34560,34561,34577,34578,34594,34595,34611,34612,34628,34629,34645,34646,34662,34663,34679,34680,34696,34697,34713,34714,34730,34731,34747,34748,34764,34765,34781,34782,34798,34799,34815,34816,34832,34833,34849,34850,34866,34867,34883,34884,34900,34901,34917,34918,34934,34935,34951,34952,34968,34969,34985,34986,35002,35003,35019,35020,35036,35037,35053,35054,35070,35071,35087,35088,35106,35107,35123,35124,35140,35141,35157,35158,35174,35175,35191,35192,35208,35209,35225,35226,35242,35243,35259,35260,35276,35277,35293,35294,35310,35311,35327,35328,35344,35345,35361,35362,35378,35379,35395,35396,35412,35413,35427,35428,35444,35445,35461,35462,35478,35479,35495,35496,35512,35513,35529,35530,35546,35547,35563,35564,35580,35581,35597,35598,35614,35615,35631,35632,35648,35649,35663,35664,35680,35681,35697,35698,35714,35715,35731,35732,35748,35749,35765,35766,35782,35783,35799,35800,35816,35817,35833,35834,35850,35851,35867,35868,35884,35885,35901,35902,35918,35919,35935,35936,35954,35955,35972,35973,35991,35992,36010,36011,36029,36030,36048,36049,36067,36068,36086,36087,36105,36106],"2016-03-31-team",{"id":13,"data":15,"filePath":22,"digest":23,"rendered":24,"legacyId":33},{"title":16,"description":17,"summary":17,"pubDate":18,"source":19,"url":20,"thumbnail":21},"Team++","We've had some fantastic people join over the past few months (and we're still hiring). Welcome, everyone!",["Date","2016-03-31T07:00:00.000Z"],"OpenAI Blog","https://openai.com/blog/team-plus-plus","https://raisex-llc.github.io/ai-news-curation-site/assets/openai_logo.png","src/content/posts/2016-03-31-team.md","319a4b11f2137ebc",{"html":25,"metadata":26},"",{"headings":27,"localImagePaths":28,"remoteImagePaths":29,"frontmatter":30,"imagePaths":32},[],[],[],{"title":16,"description":17,"summary":17,"pubDate":31,"source":19,"url":20,"thumbnail":21},"Thu, 31 Mar 2016 07:00:00 GMT",[],"2016-03-31-team.md","2015-12-11-introducing-openai",{"id":34,"data":36,"filePath":41,"digest":42,"rendered":43,"legacyId":51},{"title":37,"description":38,"summary":38,"pubDate":39,"source":19,"url":40,"thumbnail":21},"Introducing OpenAI","OpenAI is a non-profit artificial intelligence research company. Our goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return. Since our research is free from financial obligations, we can better focus on a positive human impact.",["Date","2015-12-11T08:00:00.000Z"],"https://openai.com/blog/introducing-openai","src/content/posts/2015-12-11-introducing-openai.md","8bf78a6752f96736",{"html":25,"metadata":44},{"headings":45,"localImagePaths":46,"remoteImagePaths":47,"frontmatter":48,"imagePaths":50},[],[],[],{"title":37,"description":38,"summary":38,"pubDate":49,"source":19,"url":40,"thumbnail":21},"Fri, 11 Dec 2015 08:00:00 GMT",[],"2015-12-11-introducing-openai.md","2016-02-25-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks",{"id":52,"data":54,"filePath":58,"digest":59,"rendered":60,"legacyId":68},{"title":55,"description":25,"summary":25,"pubDate":56,"source":19,"url":57,"thumbnail":21},"Weight normalization: A simple reparameterization to accelerate training of deep neural networks",["Date","2016-02-25T08:00:00.000Z"],"https://openai.com/blog/weight-normalization","src/content/posts/2016-02-25-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.md","ad5c65709f939509",{"html":25,"metadata":61},{"headings":62,"localImagePaths":63,"remoteImagePaths":64,"frontmatter":65,"imagePaths":67},[],[],[],{"title":55,"description":25,"summary":25,"pubDate":66,"source":19,"url":57,"thumbnail":21},"Thu, 25 Feb 2016 08:00:00 GMT",[],"2016-02-25-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.md","2016-04-27-openai-gym-beta",{"id":69,"data":71,"filePath":76,"digest":77,"rendered":78,"legacyId":86},{"title":72,"description":73,"summary":73,"pubDate":74,"source":19,"url":75,"thumbnail":21},"OpenAI Gym Beta","We’re releasing the public beta of OpenAI Gym, a toolkit for developing and comparing reinforcement learning (RL) algorithms. It consists of a growing suite of environments (from simulated robots to Atari games), and a site for comparing and reproducing results.",["Date","2016-04-27T07:00:00.000Z"],"https://openai.com/blog/openai-gym-beta","src/content/posts/2016-04-27-openai-gym-beta.md","77866a37b3f41642",{"html":25,"metadata":79},{"headings":80,"localImagePaths":81,"remoteImagePaths":82,"frontmatter":83,"imagePaths":85},[],[],[],{"title":72,"description":73,"summary":73,"pubDate":84,"source":19,"url":75,"thumbnail":21},"Wed, 27 Apr 2016 07:00:00 GMT",[],"2016-04-27-openai-gym-beta.md","2016-05-25-adversarial-training-methods-for-semi-supervised-text-classification",{"id":87,"data":89,"filePath":93,"digest":94,"rendered":95,"legacyId":103},{"title":90,"description":25,"summary":25,"pubDate":91,"source":19,"url":92,"thumbnail":21},"Adversarial training methods for semi-supervised text classification",["Date","2016-05-25T07:00:00.000Z"],"https://openai.com/blog/adversarial-training-methods-for-semi-supervised-text-classification","src/content/posts/2016-05-25-adversarial-training-methods-for-semi-supervised-text-classification.md","3ddf6dc613d78e40",{"html":25,"metadata":96},{"headings":97,"localImagePaths":98,"remoteImagePaths":99,"frontmatter":100,"imagePaths":102},[],[],[],{"title":90,"description":25,"summary":25,"pubDate":101,"source":19,"url":92,"thumbnail":21},"Wed, 25 May 2016 07:00:00 GMT",[],"2016-05-25-adversarial-training-methods-for-semi-supervised-text-classification.md","2016-04-26-welcome-pieter-and-shivon",{"id":104,"data":106,"filePath":111,"digest":112,"rendered":113,"legacyId":121},{"title":107,"description":108,"summary":108,"pubDate":109,"source":19,"url":110,"thumbnail":21},"Welcome, Pieter and Shivon!","We have two more team updates.",["Date","2016-04-26T07:00:00.000Z"],"https://openai.com/blog/welcome-pieter-and-shivon","src/content/posts/2016-04-26-welcome-pieter-and-shivon.md","894778275d88c2a4",{"html":25,"metadata":114},{"headings":115,"localImagePaths":116,"remoteImagePaths":117,"frontmatter":118,"imagePaths":120},[],[],[],{"title":107,"description":108,"summary":108,"pubDate":119,"source":19,"url":110,"thumbnail":21},"Tue, 26 Apr 2016 07:00:00 GMT",[],"2016-04-26-welcome-pieter-and-shivon.md","2016-06-16-generative-models",{"id":122,"data":124,"filePath":129,"digest":130,"rendered":131,"legacyId":139},{"title":125,"description":126,"summary":126,"pubDate":127,"source":19,"url":128,"thumbnail":21},"Generative models","This post describes four projects that share a common theme of enhancing or using generative models, a branch of unsupervised learning techniques in machine learning. In addition to describing our work, this post will tell you a bit more about generative models: what they are, why they are important, and where they might be going.",["Date","2016-06-16T07:00:00.000Z"],"https://openai.com/blog/generative-models","src/content/posts/2016-06-16-generative-models.md","7443923777ed5d4d",{"html":25,"metadata":132},{"headings":133,"localImagePaths":134,"remoteImagePaths":135,"frontmatter":136,"imagePaths":138},[],[],[],{"title":125,"description":126,"summary":126,"pubDate":137,"source":19,"url":128,"thumbnail":21},"Thu, 16 Jun 2016 07:00:00 GMT",[],"2016-06-16-generative-models.md","2016-05-25-team-update",{"id":140,"data":142,"filePath":147,"digest":148,"rendered":149,"legacyId":156},{"title":143,"description":144,"summary":144,"pubDate":145,"source":19,"url":146,"thumbnail":21},"Team update","We’d like to welcome the latest set of team members to OpenAI (and we’re still hiring!)",["Date","2016-05-25T07:00:00.000Z"],"https://openai.com/blog/team-update","src/content/posts/2016-05-25-team-update.md","84dd9d5733627674",{"html":25,"metadata":150},{"headings":151,"localImagePaths":152,"remoteImagePaths":153,"frontmatter":154,"imagePaths":155},[],[],[],{"title":143,"description":144,"summary":144,"pubDate":101,"source":19,"url":146,"thumbnail":21},[],"2016-05-25-team-update.md","2016-06-20-openai-technical-goals",{"id":157,"data":159,"filePath":164,"digest":165,"rendered":166,"legacyId":174},{"title":160,"description":161,"summary":161,"pubDate":162,"source":19,"url":163,"thumbnail":21},"OpenAI technical goals","OpenAI’s mission is to build safe AI, and ensure AI’s benefits are as widely and evenly distributed as possible.",["Date","2016-06-20T07:00:00.000Z"],"https://openai.com/blog/openai-technical-goals","src/content/posts/2016-06-20-openai-technical-goals.md","bc97c1ce0c2fafbb",{"html":25,"metadata":167},{"headings":168,"localImagePaths":169,"remoteImagePaths":170,"frontmatter":171,"imagePaths":173},[],[],[],{"title":160,"description":161,"summary":161,"pubDate":172,"source":19,"url":163,"thumbnail":21},"Mon, 20 Jun 2016 07:00:00 GMT",[],"2016-06-20-openai-technical-goals.md","2016-06-21-concrete-ai-safety-problems",{"id":175,"data":177,"filePath":182,"digest":183,"rendered":184,"legacyId":192},{"title":178,"description":179,"summary":179,"pubDate":180,"source":19,"url":181,"thumbnail":21},"Concrete AI safety problems","We (along with researchers from Berkeley and Stanford) are co-authors on today’s paper led by Google Brain researchers, Concrete Problems in AI Safety. The paper explores many research problems around ensuring that modern machine learning systems operate as intended.",["Date","2016-06-21T07:00:00.000Z"],"https://openai.com/blog/concrete-ai-safety-problems","src/content/posts/2016-06-21-concrete-ai-safety-problems.md","0aab137d88cd6ffe",{"html":25,"metadata":185},{"headings":186,"localImagePaths":187,"remoteImagePaths":188,"frontmatter":189,"imagePaths":191},[],[],[],{"title":178,"description":179,"summary":179,"pubDate":190,"source":19,"url":181,"thumbnail":21},"Tue, 21 Jun 2016 07:00:00 GMT",[],"2016-06-21-concrete-ai-safety-problems.md","2016-07-28-special-projects",{"id":193,"data":195,"filePath":200,"digest":201,"rendered":202,"legacyId":210},{"title":196,"description":197,"summary":197,"pubDate":198,"source":19,"url":199,"thumbnail":21},"Special projects","Impactful scientific work requires working on the right problems—problems which are not just interesting, but whose solutions matter.",["Date","2016-07-28T07:00:00.000Z"],"https://openai.com/blog/special-projects","src/content/posts/2016-07-28-special-projects.md","6d84bb6ad3d0a464",{"html":25,"metadata":203},{"headings":204,"localImagePaths":205,"remoteImagePaths":206,"frontmatter":207,"imagePaths":209},[],[],[],{"title":196,"description":197,"summary":197,"pubDate":208,"source":19,"url":199,"thumbnail":21},"Thu, 28 Jul 2016 07:00:00 GMT",[],"2016-07-28-special-projects.md","2016-08-16-team-update",{"id":211,"data":213,"filePath":217,"digest":218,"rendered":219,"legacyId":227},{"title":143,"description":214,"summary":214,"pubDate":215,"source":19,"url":216,"thumbnail":21},"We’ve hired more great people to help us achieve our goals. Welcome, everyone!",["Date","2016-08-16T07:00:00.000Z"],"https://openai.com/blog/team-update-august","src/content/posts/2016-08-16-team-update.md","551abb51a4c04d09",{"html":25,"metadata":220},{"headings":221,"localImagePaths":222,"remoteImagePaths":223,"frontmatter":224,"imagePaths":226},[],[],[],{"title":143,"description":214,"summary":214,"pubDate":225,"source":19,"url":216,"thumbnail":21},"Tue, 16 Aug 2016 07:00:00 GMT",[],"2016-08-16-team-update.md","2016-08-18-machine-learning-unconference",{"id":228,"data":230,"filePath":235,"digest":236,"rendered":237,"legacyId":245},{"title":231,"description":232,"summary":232,"pubDate":233,"source":19,"url":234,"thumbnail":21},"Machine Learning Unconference","The latest information about the Unconference is now available at the Unconference wiki, which will be periodically updated with more information for attendees.",["Date","2016-08-18T07:00:00.000Z"],"https://openai.com/blog/machine-learning-unconference","src/content/posts/2016-08-18-machine-learning-unconference.md","7d684909d8d55c3c",{"html":25,"metadata":238},{"headings":239,"localImagePaths":240,"remoteImagePaths":241,"frontmatter":242,"imagePaths":244},[],[],[],{"title":231,"description":232,"summary":232,"pubDate":243,"source":19,"url":234,"thumbnail":21},"Thu, 18 Aug 2016 07:00:00 GMT",[],"2016-08-18-machine-learning-unconference.md","2016-08-29-infrastructure-for-deep-learning",{"id":246,"data":248,"filePath":253,"digest":254,"rendered":255,"legacyId":263},{"title":249,"description":250,"summary":250,"pubDate":251,"source":19,"url":252,"thumbnail":21},"Infrastructure for deep learning","Deep learning is an empirical science, and the quality of a group’s infrastructure is a multiplier on progress. Fortunately, today’s open-source ecosystem makes it possible for anyone to build great deep learning infrastructure.",["Date","2016-08-29T07:00:00.000Z"],"https://openai.com/blog/infrastructure-for-deep-learning","src/content/posts/2016-08-29-infrastructure-for-deep-learning.md","dd7827408290ae6b",{"html":25,"metadata":256},{"headings":257,"localImagePaths":258,"remoteImagePaths":259,"frontmatter":260,"imagePaths":262},[],[],[],{"title":249,"description":250,"summary":250,"pubDate":261,"source":19,"url":252,"thumbnail":21},"Mon, 29 Aug 2016 07:00:00 GMT",[],"2016-08-29-infrastructure-for-deep-learning.md","2016-10-11-transfer-from-simulation-to-real-world-through-learning-deep-inverse-dynamics-model",{"id":264,"data":266,"filePath":270,"digest":271,"rendered":272,"legacyId":280},{"title":267,"description":25,"summary":25,"pubDate":268,"source":19,"url":269,"thumbnail":21},"Transfer from simulation to real world through learning deep inverse dynamics model",["Date","2016-10-11T07:00:00.000Z"],"https://openai.com/blog/transfer-from-simulation-to-real-world-through-learning-deep-inverse-dynamics-model","src/content/posts/2016-10-11-transfer-from-simulation-to-real-world-through-learning-deep-inverse-dynamics-model.md","c9ed63317fa2be0e",{"html":25,"metadata":273},{"headings":274,"localImagePaths":275,"remoteImagePaths":276,"frontmatter":277,"imagePaths":279},[],[],[],{"title":267,"description":25,"summary":25,"pubDate":278,"source":19,"url":269,"thumbnail":21},"Tue, 11 Oct 2016 07:00:00 GMT",[],"2016-10-11-transfer-from-simulation-to-real-world-through-learning-deep-inverse-dynamics-model.md","2016-10-13-report-from-the-self-organizing-conference",{"id":281,"data":283,"filePath":288,"digest":289,"rendered":290,"legacyId":298},{"title":284,"description":285,"summary":285,"pubDate":286,"source":19,"url":287,"thumbnail":21},"Report from the self-organizing conference","Last week we hosted over a hundred and fifty AI practitioners in our offices for our first self-organizing conference on machine learning.",["Date","2016-10-13T07:00:00.000Z"],"https://openai.com/blog/report-from-the-self-organizing-conference","src/content/posts/2016-10-13-report-from-the-self-organizing-conference.md","8b5cb80525738b9f",{"html":25,"metadata":291},{"headings":292,"localImagePaths":293,"remoteImagePaths":294,"frontmatter":295,"imagePaths":297},[],[],[],{"title":284,"description":285,"summary":285,"pubDate":296,"source":19,"url":287,"thumbnail":21},"Thu, 13 Oct 2016 07:00:00 GMT",[],"2016-10-13-report-from-the-self-organizing-conference.md","2016-10-18-semi-supervised-knowledge-transfer-for-deep-learning-from-private-training-data",{"id":299,"data":301,"filePath":305,"digest":306,"rendered":307,"legacyId":315},{"title":302,"description":25,"summary":25,"pubDate":303,"source":19,"url":304,"thumbnail":21},"Semi-supervised knowledge transfer for deep learning from private training data",["Date","2016-10-18T07:00:00.000Z"],"https://openai.com/blog/semi-supervised-knowledge-transfer-for-deep-learning-from-private-training-data","src/content/posts/2016-10-18-semi-supervised-knowledge-transfer-for-deep-learning-from-private-training-data.md","f862b8ba89379e8c",{"html":25,"metadata":308},{"headings":309,"localImagePaths":310,"remoteImagePaths":311,"frontmatter":312,"imagePaths":314},[],[],[],{"title":302,"description":25,"summary":25,"pubDate":313,"source":19,"url":304,"thumbnail":21},"Tue, 18 Oct 2016 07:00:00 GMT",[],"2016-10-18-semi-supervised-knowledge-transfer-for-deep-learning-from-private-training-data.md","2016-11-02-extensions-and-limitations-of-the-neural-gpu",{"id":316,"data":318,"filePath":322,"digest":323,"rendered":324,"legacyId":332},{"title":319,"description":25,"summary":25,"pubDate":320,"source":19,"url":321,"thumbnail":21},"Extensions and limitations of the neural GPU",["Date","2016-11-02T07:00:00.000Z"],"https://openai.com/blog/extensions-and-limitations-of-the-neural-gpu","src/content/posts/2016-11-02-extensions-and-limitations-of-the-neural-gpu.md","b62f420cda12fd33",{"html":25,"metadata":325},{"headings":326,"localImagePaths":327,"remoteImagePaths":328,"frontmatter":329,"imagePaths":331},[],[],[],{"title":319,"description":25,"summary":25,"pubDate":330,"source":19,"url":321,"thumbnail":21},"Wed, 02 Nov 2016 07:00:00 GMT",[],"2016-11-02-extensions-and-limitations-of-the-neural-gpu.md","2016-11-08-variational-lossy-autoencoder",{"id":333,"data":335,"filePath":339,"digest":340,"rendered":341,"legacyId":349},{"title":336,"description":25,"summary":25,"pubDate":337,"source":19,"url":338,"thumbnail":21},"Variational lossy autoencoder",["Date","2016-11-08T08:00:00.000Z"],"https://openai.com/blog/variational-lossy-autoencoder","src/content/posts/2016-11-08-variational-lossy-autoencoder.md","01da48d83e014c76",{"html":25,"metadata":342},{"headings":343,"localImagePaths":344,"remoteImagePaths":345,"frontmatter":346,"imagePaths":348},[],[],[],{"title":336,"description":25,"summary":25,"pubDate":347,"source":19,"url":338,"thumbnail":21},"Tue, 08 Nov 2016 08:00:00 GMT",[],"2016-11-08-variational-lossy-autoencoder.md","2016-11-09-rl-fast-reinforcement-learning-via-slow-reinforcement-learning",{"id":350,"data":352,"filePath":356,"digest":357,"rendered":358,"legacyId":366},{"title":353,"description":25,"summary":25,"pubDate":354,"source":19,"url":355,"thumbnail":21},"RL²: Fast reinforcement learning via slow reinforcement learning",["Date","2016-11-09T08:00:00.000Z"],"https://openai.com/blog/rl2","src/content/posts/2016-11-09-rl²-fast-reinforcement-learning-via-slow-reinforcement-learning.md","56e80344bc1c60f9",{"html":25,"metadata":359},{"headings":360,"localImagePaths":361,"remoteImagePaths":362,"frontmatter":363,"imagePaths":365},[],[],[],{"title":353,"description":25,"summary":25,"pubDate":364,"source":19,"url":355,"thumbnail":21},"Wed, 09 Nov 2016 08:00:00 GMT",[],"2016-11-09-rl²-fast-reinforcement-learning-via-slow-reinforcement-learning.md","2016-11-11-a-connection-between-generative-adversarial-networks-inverse-reinforcement-learning-and-energy-based-models",{"id":367,"data":369,"filePath":373,"digest":374,"rendered":375,"legacyId":383},{"title":370,"description":25,"summary":25,"pubDate":371,"source":19,"url":372,"thumbnail":21},"A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models",["Date","2016-11-11T08:00:00.000Z"],"https://openai.com/blog/a-connection-between-generative-adversarial-networks-inverse-reinforcement-learning-and-energy-based-models","src/content/posts/2016-11-11-a-connection-between-generative-adversarial-networks-inverse-reinforcement-learning-and-energy-based-models.md","6364758b0b484e88",{"html":25,"metadata":376},{"headings":377,"localImagePaths":378,"remoteImagePaths":379,"frontmatter":380,"imagePaths":382},[],[],[],{"title":370,"description":25,"summary":25,"pubDate":381,"source":19,"url":372,"thumbnail":21},"Fri, 11 Nov 2016 08:00:00 GMT",[],"2016-11-11-a-connection-between-generative-adversarial-networks-inverse-reinforcement-learning-and-energy-based-models.md","2016-11-14-on-the-quantitative-analysis-of-decoder-based-generative-models",{"id":384,"data":386,"filePath":390,"digest":391,"rendered":392,"legacyId":400},{"title":387,"description":25,"summary":25,"pubDate":388,"source":19,"url":389,"thumbnail":21},"On the quantitative analysis of decoder-based generative models",["Date","2016-11-14T08:00:00.000Z"],"https://openai.com/blog/on-the-quantitative-analysis-of-decoder-based-generative-models","src/content/posts/2016-11-14-on-the-quantitative-analysis-of-decoder-based-generative-models.md","0bc1f4b566a4ecdb",{"html":25,"metadata":393},{"headings":394,"localImagePaths":395,"remoteImagePaths":396,"frontmatter":397,"imagePaths":399},[],[],[],{"title":387,"description":25,"summary":25,"pubDate":398,"source":19,"url":389,"thumbnail":21},"Mon, 14 Nov 2016 08:00:00 GMT",[],"2016-11-14-on-the-quantitative-analysis-of-decoder-based-generative-models.md","2016-11-15-exploration-a-study-of-count-based-exploration-for-deep-reinforcement-learning",{"id":401,"data":403,"filePath":407,"digest":408,"rendered":409,"legacyId":417},{"title":404,"description":25,"summary":25,"pubDate":405,"source":19,"url":406,"thumbnail":21},"#Exploration: A study of count-based exploration for deep reinforcement learning",["Date","2016-11-15T08:00:00.000Z"],"https://openai.com/blog/exploration","src/content/posts/2016-11-15-exploration-a-study-of-count-based-exploration-for-deep-reinforcement-learning.md","d3c5a70732139834",{"html":25,"metadata":410},{"headings":411,"localImagePaths":412,"remoteImagePaths":413,"frontmatter":414,"imagePaths":416},[],[],[],{"title":404,"description":25,"summary":25,"pubDate":415,"source":19,"url":406,"thumbnail":21},"Tue, 15 Nov 2016 08:00:00 GMT",[],"2016-11-15-exploration-a-study-of-count-based-exploration-for-deep-reinforcement-learning.md","2016-11-15-openai-and-microsoft",{"id":418,"data":420,"filePath":425,"digest":426,"rendered":427,"legacyId":434},{"title":421,"description":422,"summary":422,"pubDate":423,"source":19,"url":424,"thumbnail":21},"OpenAI and Microsoft","We’re working with Microsoft to start running most of our large-scale experiments on Azure.",["Date","2016-11-15T08:00:00.000Z"],"https://openai.com/blog/openai-and-microsoft","src/content/posts/2016-11-15-openai-and-microsoft.md","bdb61f5c56a60197",{"html":25,"metadata":428},{"headings":429,"localImagePaths":430,"remoteImagePaths":431,"frontmatter":432,"imagePaths":433},[],[],[],{"title":421,"description":422,"summary":422,"pubDate":415,"source":19,"url":424,"thumbnail":21},[],"2016-11-15-openai-and-microsoft.md","2016-12-05-universe",{"id":435,"data":437,"filePath":442,"digest":443,"rendered":444,"legacyId":452},{"title":438,"description":439,"summary":439,"pubDate":440,"source":19,"url":441,"thumbnail":21},"Universe","We’re releasing Universe, a software platform for measuring and training an AI’s general intelligence across the world’s supply of games, websites and other applications.",["Date","2016-12-05T08:00:00.000Z"],"https://openai.com/blog/universe","src/content/posts/2016-12-05-universe.md","e9ac8fe91b6d1430",{"html":25,"metadata":445},{"headings":446,"localImagePaths":447,"remoteImagePaths":448,"frontmatter":449,"imagePaths":451},[],[],[],{"title":438,"description":439,"summary":439,"pubDate":450,"source":19,"url":441,"thumbnail":21},"Mon, 05 Dec 2016 08:00:00 GMT",[],"2016-12-05-universe.md","2016-12-21-faulty-reward-functions-in-the-wild",{"id":453,"data":455,"filePath":460,"digest":461,"rendered":462,"legacyId":470},{"title":456,"description":457,"summary":457,"pubDate":458,"source":19,"url":459,"thumbnail":21},"Faulty reward functions in the wild","Reinforcement learning algorithms can break in surprising, counterintuitive ways. In this post we’ll explore one failure mode, which is where you misspecify your reward function.",["Date","2016-12-21T08:00:00.000Z"],"https://openai.com/blog/faulty-reward-functions","src/content/posts/2016-12-21-faulty-reward-functions-in-the-wild.md","76cd391619ba984a",{"html":25,"metadata":463},{"headings":464,"localImagePaths":465,"remoteImagePaths":466,"frontmatter":467,"imagePaths":469},[],[],[],{"title":456,"description":457,"summary":457,"pubDate":468,"source":19,"url":459,"thumbnail":21},"Wed, 21 Dec 2016 08:00:00 GMT",[],"2016-12-21-faulty-reward-functions-in-the-wild.md","2017-01-19-pixelcnn-improving-the-pixelcnn-with-discretized-logistic-mixture-likelihood-and-other-modifications",{"id":471,"data":473,"filePath":477,"digest":478,"rendered":479,"legacyId":487},{"title":474,"description":25,"summary":25,"pubDate":475,"source":19,"url":476,"thumbnail":21},"PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications",["Date","2017-01-19T08:00:00.000Z"],"https://openai.com/blog/pixelcnn-plus-plus","src/content/posts/2017-01-19-pixelcnn-improving-the-pixelcnn-with-discretized-logistic-mixture-likelihood-and-other-modifications.md","bb080692e5a181df",{"html":25,"metadata":480},{"headings":481,"localImagePaths":482,"remoteImagePaths":483,"frontmatter":484,"imagePaths":486},[],[],[],{"title":474,"description":25,"summary":25,"pubDate":485,"source":19,"url":476,"thumbnail":21},"Thu, 19 Jan 2017 08:00:00 GMT",[],"2017-01-19-pixelcnn-improving-the-pixelcnn-with-discretized-logistic-mixture-likelihood-and-other-modifications.md","2017-01-30-team-update",{"id":488,"data":490,"filePath":494,"digest":495,"rendered":496,"legacyId":504},{"title":143,"description":491,"summary":491,"pubDate":492,"source":19,"url":493,"thumbnail":21},"The OpenAI team is now 45 people. Together, we’re pushing the frontier of AI capabilities—whether by validating novel ideas, creating new software systems, or deploying machine learning on robots.",["Date","2017-01-30T08:00:00.000Z"],"https://openai.com/blog/team-update-january","src/content/posts/2017-01-30-team-update.md","7405855447e764e9",{"html":25,"metadata":497},{"headings":498,"localImagePaths":499,"remoteImagePaths":500,"frontmatter":501,"imagePaths":503},[],[],[],{"title":143,"description":491,"summary":491,"pubDate":502,"source":19,"url":493,"thumbnail":21},"Mon, 30 Jan 2017 08:00:00 GMT",[],"2017-01-30-team-update.md","2017-02-08-adversarial-attacks-on-neural-network-policies",{"id":505,"data":507,"filePath":511,"digest":512,"rendered":513,"legacyId":521},{"title":508,"description":25,"summary":25,"pubDate":509,"source":19,"url":510,"thumbnail":21},"Adversarial attacks on neural network policies",["Date","2017-02-08T08:00:00.000Z"],"https://openai.com/blog/adversarial-attacks-on-neural-network-policies","src/content/posts/2017-02-08-adversarial-attacks-on-neural-network-policies.md","4276e923c4c2f066",{"html":25,"metadata":514},{"headings":515,"localImagePaths":516,"remoteImagePaths":517,"frontmatter":518,"imagePaths":520},[],[],[],{"title":508,"description":25,"summary":25,"pubDate":519,"source":19,"url":510,"thumbnail":21},"Wed, 08 Feb 2017 08:00:00 GMT",[],"2017-02-08-adversarial-attacks-on-neural-network-policies.md","2017-02-24-attacking-machine-learning-with-adversarial-examples",{"id":522,"data":524,"filePath":529,"digest":530,"rendered":531,"legacyId":539},{"title":525,"description":526,"summary":526,"pubDate":527,"source":19,"url":528,"thumbnail":21},"Attacking machine learning with adversarial examples","Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake; they’re like optical illusions for machines. In this post we’ll show how adversarial examples work across different mediums, and will discuss why securing systems against them can be difficult.",["Date","2017-02-24T08:00:00.000Z"],"https://openai.com/blog/attacking-machine-learning-with-adversarial-examples","src/content/posts/2017-02-24-attacking-machine-learning-with-adversarial-examples.md","e65551e41d94b051",{"html":25,"metadata":532},{"headings":533,"localImagePaths":534,"remoteImagePaths":535,"frontmatter":536,"imagePaths":538},[],[],[],{"title":525,"description":526,"summary":526,"pubDate":537,"source":19,"url":528,"thumbnail":21},"Fri, 24 Feb 2017 08:00:00 GMT",[],"2017-02-24-attacking-machine-learning-with-adversarial-examples.md","2017-03-06-third-person-imitation-learning",{"id":540,"data":542,"filePath":546,"digest":547,"rendered":548,"legacyId":556},{"title":543,"description":25,"summary":25,"pubDate":544,"source":19,"url":545,"thumbnail":21},"Third-person imitation learning",["Date","2017-03-06T08:00:00.000Z"],"https://openai.com/blog/third-person-imitation-learning","src/content/posts/2017-03-06-third-person-imitation-learning.md","b456d71e7e89a6ea",{"html":25,"metadata":549},{"headings":550,"localImagePaths":551,"remoteImagePaths":552,"frontmatter":553,"imagePaths":555},[],[],[],{"title":543,"description":25,"summary":25,"pubDate":554,"source":19,"url":545,"thumbnail":21},"Mon, 06 Mar 2017 08:00:00 GMT",[],"2017-03-06-third-person-imitation-learning.md","2017-03-12-prediction-and-control-with-temporal-segment-models",{"id":557,"data":559,"filePath":563,"digest":564,"rendered":565,"legacyId":573},{"title":560,"description":25,"summary":25,"pubDate":561,"source":19,"url":562,"thumbnail":21},"Prediction and control with temporal segment models",["Date","2017-03-12T08:00:00.000Z"],"https://openai.com/blog/prediction-and-control-with-temporal-segment-models","src/content/posts/2017-03-12-prediction-and-control-with-temporal-segment-models.md","54dcc1af162c6d9c",{"html":25,"metadata":566},{"headings":567,"localImagePaths":568,"remoteImagePaths":569,"frontmatter":570,"imagePaths":572},[],[],[],{"title":560,"description":25,"summary":25,"pubDate":571,"source":19,"url":562,"thumbnail":21},"Sun, 12 Mar 2017 08:00:00 GMT",[],"2017-03-12-prediction-and-control-with-temporal-segment-models.md","2017-03-15-emergence-of-grounded-compositional-language-in-multi-agent-populations",{"id":574,"data":576,"filePath":580,"digest":581,"rendered":582,"legacyId":590},{"title":577,"description":25,"summary":25,"pubDate":578,"source":19,"url":579,"thumbnail":21},"Emergence of grounded compositional language in multi-agent populations",["Date","2017-03-15T07:00:00.000Z"],"https://openai.com/blog/emergence-of-grounded-compositional-language-in-multi-agent-populations","src/content/posts/2017-03-15-emergence-of-grounded-compositional-language-in-multi-agent-populations.md","74a929a74251f15a",{"html":25,"metadata":583},{"headings":584,"localImagePaths":585,"remoteImagePaths":586,"frontmatter":587,"imagePaths":589},[],[],[],{"title":577,"description":25,"summary":25,"pubDate":588,"source":19,"url":579,"thumbnail":21},"Wed, 15 Mar 2017 07:00:00 GMT",[],"2017-03-15-emergence-of-grounded-compositional-language-in-multi-agent-populations.md","2017-03-16-learning-to-communicate",{"id":591,"data":593,"filePath":598,"digest":599,"rendered":600,"legacyId":608},{"title":594,"description":595,"summary":595,"pubDate":596,"source":19,"url":597,"thumbnail":21},"Learning to communicate","In this post we’ll outline new OpenAI research in which agents develop their own language.",["Date","2017-03-16T07:00:00.000Z"],"https://openai.com/blog/learning-to-communicate","src/content/posts/2017-03-16-learning-to-communicate.md","96e182b75e506c10",{"html":25,"metadata":601},{"headings":602,"localImagePaths":603,"remoteImagePaths":604,"frontmatter":605,"imagePaths":607},[],[],[],{"title":594,"description":595,"summary":595,"pubDate":606,"source":19,"url":597,"thumbnail":21},"Thu, 16 Mar 2017 07:00:00 GMT",[],"2017-03-16-learning-to-communicate.md","2017-03-20-distill",{"id":609,"data":611,"filePath":616,"digest":617,"rendered":618,"legacyId":626},{"title":612,"description":613,"summary":613,"pubDate":614,"source":19,"url":615,"thumbnail":21},"Distill","We’re excited to support today’s launch of Distill, a new kind of journal aimed at excellent communication of machine learning results (novel or existing).",["Date","2017-03-20T07:00:00.000Z"],"https://openai.com/blog/distill","src/content/posts/2017-03-20-distill.md","ecc6b840d06ab1ae",{"html":25,"metadata":619},{"headings":620,"localImagePaths":621,"remoteImagePaths":622,"frontmatter":623,"imagePaths":625},[],[],[],{"title":612,"description":613,"summary":613,"pubDate":624,"source":19,"url":615,"thumbnail":21},"Mon, 20 Mar 2017 07:00:00 GMT",[],"2017-03-20-distill.md","2017-03-21-one-shot-imitation-learning",{"id":627,"data":629,"filePath":633,"digest":634,"rendered":635,"legacyId":643},{"title":630,"description":25,"summary":25,"pubDate":631,"source":19,"url":632,"thumbnail":21},"One-shot imitation learning",["Date","2017-03-21T07:00:00.000Z"],"https://openai.com/blog/one-shot-imitation-learning","src/content/posts/2017-03-21-one-shot-imitation-learning.md","4978e4c16ac8bd18",{"html":25,"metadata":636},{"headings":637,"localImagePaths":638,"remoteImagePaths":639,"frontmatter":640,"imagePaths":642},[],[],[],{"title":630,"description":25,"summary":25,"pubDate":641,"source":19,"url":632,"thumbnail":21},"Tue, 21 Mar 2017 07:00:00 GMT",[],"2017-03-21-one-shot-imitation-learning.md","2017-04-01-spam-detection-in-the-physical-world",{"id":644,"data":646,"filePath":651,"digest":652,"rendered":653,"legacyId":661},{"title":647,"description":648,"summary":648,"pubDate":649,"source":19,"url":650,"thumbnail":21},"Spam detection in the physical world","We’ve created the world’s first Spam-detecting AI trained entirely in simulation and deployed on a physical robot.",["Date","2017-04-01T07:00:00.000Z"],"https://openai.com/blog/spam-detection-in-the-physical-world","src/content/posts/2017-04-01-spam-detection-in-the-physical-world.md","cf3e057394810d74",{"html":25,"metadata":654},{"headings":655,"localImagePaths":656,"remoteImagePaths":657,"frontmatter":658,"imagePaths":660},[],[],[],{"title":647,"description":648,"summary":648,"pubDate":659,"source":19,"url":650,"thumbnail":21},"Sat, 01 Apr 2017 07:00:00 GMT",[],"2017-04-01-spam-detection-in-the-physical-world.md","2017-03-24-evolution-strategies-as-a-scalable-alternative-to-reinforcement-learning",{"id":662,"data":664,"filePath":669,"digest":670,"rendered":671,"legacyId":679},{"title":665,"description":666,"summary":666,"pubDate":667,"source":19,"url":668,"thumbnail":21},"Evolution strategies as a scalable alternative to reinforcement learning","We’ve discovered that evolution strategies (ES), an optimization technique that’s been known for decades, rivals the performance of standard reinforcement learning (RL) techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL’s inconveniences.",["Date","2017-03-24T07:00:00.000Z"],"https://openai.com/blog/evolution-strategies","src/content/posts/2017-03-24-evolution-strategies-as-a-scalable-alternative-to-reinforcement-learning.md","cc40fef111ec977c",{"html":25,"metadata":672},{"headings":673,"localImagePaths":674,"remoteImagePaths":675,"frontmatter":676,"imagePaths":678},[],[],[],{"title":665,"description":666,"summary":666,"pubDate":677,"source":19,"url":668,"thumbnail":21},"Fri, 24 Mar 2017 07:00:00 GMT",[],"2017-03-24-evolution-strategies-as-a-scalable-alternative-to-reinforcement-learning.md","2017-04-06-unsupervised-sentiment-neuron",{"id":680,"data":682,"filePath":687,"digest":688,"rendered":689,"legacyId":697},{"title":683,"description":684,"summary":684,"pubDate":685,"source":19,"url":686,"thumbnail":21},"Unsupervised sentiment neuron","We’ve developed an unsupervised system which learns an excellent representation of sentiment, despite being trained only to predict the next character in the text of Amazon reviews.",["Date","2017-04-06T07:00:00.000Z"],"https://openai.com/blog/unsupervised-sentiment-neuron","src/content/posts/2017-04-06-unsupervised-sentiment-neuron.md","57e252f28e1202a7",{"html":25,"metadata":690},{"headings":691,"localImagePaths":692,"remoteImagePaths":693,"frontmatter":694,"imagePaths":696},[],[],[],{"title":683,"description":684,"summary":684,"pubDate":695,"source":19,"url":686,"thumbnail":21},"Thu, 06 Apr 2017 07:00:00 GMT",[],"2017-04-06-unsupervised-sentiment-neuron.md","2017-04-10-stochastic-neural-networks-for-hierarchical-reinforcement-learning",{"id":698,"data":700,"filePath":704,"digest":705,"rendered":706,"legacyId":714},{"title":701,"description":25,"summary":25,"pubDate":702,"source":19,"url":703,"thumbnail":21},"Stochastic Neural Networks for hierarchical reinforcement learning",["Date","2017-04-10T07:00:00.000Z"],"https://openai.com/blog/stochastic-neural-networks-for-hierarchical-reinforcement-learning","src/content/posts/2017-04-10-stochastic-neural-networks-for-hierarchical-reinforcement-learning.md","55ab61664f08639c",{"html":25,"metadata":707},{"headings":708,"localImagePaths":709,"remoteImagePaths":710,"frontmatter":711,"imagePaths":713},[],[],[],{"title":701,"description":25,"summary":25,"pubDate":712,"source":19,"url":703,"thumbnail":21},"Mon, 10 Apr 2017 07:00:00 GMT",[],"2017-04-10-stochastic-neural-networks-for-hierarchical-reinforcement-learning.md","2017-04-21-equivalence-between-policy-gradients-and-soft-q-learning",{"id":715,"data":717,"filePath":721,"digest":722,"rendered":723,"legacyId":731},{"title":718,"description":25,"summary":25,"pubDate":719,"source":19,"url":720,"thumbnail":21},"Equivalence between policy gradients and soft Q-learning",["Date","2017-04-21T07:00:00.000Z"],"https://openai.com/blog/equivalence-between-policy-gradients-and-soft-q-learning","src/content/posts/2017-04-21-equivalence-between-policy-gradients-and-soft-q-learning.md","6864cded695c73fd",{"html":25,"metadata":724},{"headings":725,"localImagePaths":726,"remoteImagePaths":727,"frontmatter":728,"imagePaths":730},[],[],[],{"title":718,"description":25,"summary":25,"pubDate":729,"source":19,"url":720,"thumbnail":21},"Fri, 21 Apr 2017 07:00:00 GMT",[],"2017-04-21-equivalence-between-policy-gradients-and-soft-q-learning.md","2017-05-15-roboschool",{"id":732,"data":734,"filePath":739,"digest":740,"rendered":741,"legacyId":749},{"title":735,"description":736,"summary":736,"pubDate":737,"source":19,"url":738,"thumbnail":21},"Roboschool","We are releasing Roboschool: open-source software for robot simulation, integrated with OpenAI Gym.",["Date","2017-05-15T07:00:00.000Z"],"https://openai.com/blog/roboschool","src/content/posts/2017-05-15-roboschool.md","4e892b72b4506ef8",{"html":25,"metadata":742},{"headings":743,"localImagePaths":744,"remoteImagePaths":745,"frontmatter":746,"imagePaths":748},[],[],[],{"title":735,"description":736,"summary":736,"pubDate":747,"source":19,"url":738,"thumbnail":21},"Mon, 15 May 2017 07:00:00 GMT",[],"2017-05-15-roboschool.md","2017-05-24-openai-baselines-dqn",{"id":750,"data":752,"filePath":757,"digest":758,"rendered":759,"legacyId":767},{"title":753,"description":754,"summary":754,"pubDate":755,"source":19,"url":756,"thumbnail":21},"OpenAI Baselines: DQN","We’re open-sourcing OpenAI Baselines, our internal effort to reproduce reinforcement learning algorithms with performance on par with published results. We’ll release the algorithms over upcoming months; today’s release includes DQN and three of its variants.",["Date","2017-05-24T07:00:00.000Z"],"https://openai.com/blog/openai-baselines-dqn","src/content/posts/2017-05-24-openai-baselines-dqn.md","aee6dd94cde7c774",{"html":25,"metadata":760},{"headings":761,"localImagePaths":762,"remoteImagePaths":763,"frontmatter":764,"imagePaths":766},[],[],[],{"title":753,"description":754,"summary":754,"pubDate":765,"source":19,"url":756,"thumbnail":21},"Wed, 24 May 2017 07:00:00 GMT",[],"2017-05-24-openai-baselines-dqn.md","2017-05-16-robots-that-learn",{"id":768,"data":770,"filePath":775,"digest":776,"rendered":777,"legacyId":785},{"title":771,"description":772,"summary":772,"pubDate":773,"source":19,"url":774,"thumbnail":21},"Robots that learn","We’ve created a robotics system, trained entirely in simulation and deployed on a physical robot, which can learn a new task after seeing it done once.",["Date","2017-05-16T07:00:00.000Z"],"https://openai.com/blog/robots-that-learn","src/content/posts/2017-05-16-robots-that-learn.md","03be0ff29bec0382",{"html":25,"metadata":778},{"headings":779,"localImagePaths":780,"remoteImagePaths":781,"frontmatter":782,"imagePaths":784},[],[],[],{"title":771,"description":772,"summary":772,"pubDate":783,"source":19,"url":774,"thumbnail":21},"Tue, 16 May 2017 07:00:00 GMT",[],"2017-05-16-robots-that-learn.md","2017-06-05-ucb-exploration-via-q-ensembles",{"id":786,"data":788,"filePath":792,"digest":793,"rendered":794,"legacyId":802},{"title":789,"description":25,"summary":25,"pubDate":790,"source":19,"url":791,"thumbnail":21},"UCB exploration via Q-ensembles",["Date","2017-06-05T07:00:00.000Z"],"https://openai.com/blog/ucb-exploration-via-q-ensembles","src/content/posts/2017-06-05-ucb-exploration-via-q-ensembles.md","36653398ad2a6a76",{"html":25,"metadata":795},{"headings":796,"localImagePaths":797,"remoteImagePaths":798,"frontmatter":799,"imagePaths":801},[],[],[],{"title":789,"description":25,"summary":25,"pubDate":800,"source":19,"url":791,"thumbnail":21},"Mon, 05 Jun 2017 07:00:00 GMT",[],"2017-06-05-ucb-exploration-via-q-ensembles.md","2017-06-08-learning-to-cooperate-compete-and-communicate",{"id":803,"data":805,"filePath":810,"digest":811,"rendered":812,"legacyId":820},{"title":806,"description":807,"summary":807,"pubDate":808,"source":19,"url":809,"thumbnail":21},"Learning to cooperate, compete, and communicate","Multiagent environments where agents compete for resources are stepping stones on the path to AGI. Multiagent environments have two useful properties: first, there is a natural curriculum—the difficulty of the environment is determined by the skill of your competitors (and if you’re competing against clones of yourself, the environment exactly matches your skill level). Second, a multiagent environment has no stable equilibrium: no matter how smart an agent is, there’s always pressure to get smarter. These environments have a very different feel from traditional environments, and it’ll take a lot more research before we become good at them.",["Date","2017-06-08T07:00:00.000Z"],"https://openai.com/blog/learning-to-cooperate-compete-and-communicate","src/content/posts/2017-06-08-learning-to-cooperate-compete-and-communicate.md","b5f0d9b68b272f39",{"html":25,"metadata":813},{"headings":814,"localImagePaths":815,"remoteImagePaths":816,"frontmatter":817,"imagePaths":819},[],[],[],{"title":806,"description":807,"summary":807,"pubDate":818,"source":19,"url":809,"thumbnail":21},"Thu, 08 Jun 2017 07:00:00 GMT",[],"2017-06-08-learning-to-cooperate-compete-and-communicate.md","2017-06-13-learning-from-human-preferences",{"id":821,"data":823,"filePath":828,"digest":829,"rendered":830,"legacyId":838},{"title":824,"description":825,"summary":825,"pubDate":826,"source":19,"url":827,"thumbnail":21},"Learning from human preferences","One step towards building safe AI systems is to remove the need for humans to write goal functions, since using a simple proxy for a complex goal, or getting the complex goal a bit wrong, can lead to undesirable and even dangerous behavior. In collaboration with DeepMind’s safety team, we’ve developed an algorithm which can infer what humans want by being told which of two proposed behaviors is better.",["Date","2017-06-13T07:00:00.000Z"],"https://openai.com/blog/learning-from-human-preferences","src/content/posts/2017-06-13-learning-from-human-preferences.md","59c1fc45b0e1cfe6",{"html":25,"metadata":831},{"headings":832,"localImagePaths":833,"remoteImagePaths":834,"frontmatter":835,"imagePaths":837},[],[],[],{"title":824,"description":825,"summary":825,"pubDate":836,"source":19,"url":827,"thumbnail":21},"Tue, 13 Jun 2017 07:00:00 GMT",[],"2017-06-13-learning-from-human-preferences.md","2017-06-28-faster-physics-in-python",{"id":839,"data":841,"filePath":846,"digest":847,"rendered":848,"legacyId":856},{"title":842,"description":843,"summary":843,"pubDate":844,"source":19,"url":845,"thumbnail":21},"Faster physics in Python","We’re open-sourcing a high-performance Python library for robotic simulation using the MuJoCo engine, developed over our past year of robotics research.",["Date","2017-06-28T07:00:00.000Z"],"https://openai.com/blog/faster-physics-in-python","src/content/posts/2017-06-28-faster-physics-in-python.md","3ba449ea108dff61",{"html":25,"metadata":849},{"headings":850,"localImagePaths":851,"remoteImagePaths":852,"frontmatter":853,"imagePaths":855},[],[],[],{"title":842,"description":843,"summary":843,"pubDate":854,"source":19,"url":845,"thumbnail":21},"Wed, 28 Jun 2017 07:00:00 GMT",[],"2017-06-28-faster-physics-in-python.md","2017-07-01-teacherstudent-curriculum-learning",{"id":857,"data":859,"filePath":863,"digest":864,"rendered":865,"legacyId":873},{"title":860,"description":25,"summary":25,"pubDate":861,"source":19,"url":862,"thumbnail":21},"Teacher–student curriculum learning",["Date","2017-07-01T07:00:00.000Z"],"https://openai.com/blog/teacher-student-curriculum-learning","src/content/posts/2017-07-01-teacherstudent-curriculum-learning.md","59c9ea7a9be135f9",{"html":25,"metadata":866},{"headings":867,"localImagePaths":868,"remoteImagePaths":869,"frontmatter":870,"imagePaths":872},[],[],[],{"title":860,"description":25,"summary":25,"pubDate":871,"source":19,"url":862,"thumbnail":21},"Sat, 01 Jul 2017 07:00:00 GMT",[],"2017-07-01-teacherstudent-curriculum-learning.md","2017-07-05-hindsight-experience-replay",{"id":874,"data":876,"filePath":880,"digest":881,"rendered":882,"legacyId":890},{"title":877,"description":25,"summary":25,"pubDate":878,"source":19,"url":879,"thumbnail":21},"Hindsight Experience Replay",["Date","2017-07-05T07:00:00.000Z"],"https://openai.com/blog/hindsight-experience-replay","src/content/posts/2017-07-05-hindsight-experience-replay.md","1c9c2b5fe0354fa7",{"html":25,"metadata":883},{"headings":884,"localImagePaths":885,"remoteImagePaths":886,"frontmatter":887,"imagePaths":889},[],[],[],{"title":877,"description":25,"summary":25,"pubDate":888,"source":19,"url":879,"thumbnail":21},"Wed, 05 Jul 2017 07:00:00 GMT",[],"2017-07-05-hindsight-experience-replay.md","2017-07-17-robust-adversarial-inputs",{"id":891,"data":893,"filePath":898,"digest":899,"rendered":900,"legacyId":908},{"title":894,"description":895,"summary":895,"pubDate":896,"source":19,"url":897,"thumbnail":21},"Robust adversarial inputs","We’ve created images that reliably fool neural network classifiers when viewed from varied scales and perspectives. This challenges a claim from last week that self-driving cars would be hard to trick maliciously since they capture images from multiple scales, angles, perspectives, and the like.",["Date","2017-07-17T07:00:00.000Z"],"https://openai.com/blog/robust-adversarial-inputs","src/content/posts/2017-07-17-robust-adversarial-inputs.md","807bfe6d9ac773e6",{"html":25,"metadata":901},{"headings":902,"localImagePaths":903,"remoteImagePaths":904,"frontmatter":905,"imagePaths":907},[],[],[],{"title":894,"description":895,"summary":895,"pubDate":906,"source":19,"url":897,"thumbnail":21},"Mon, 17 Jul 2017 07:00:00 GMT",[],"2017-07-17-robust-adversarial-inputs.md","2017-07-20-proximal-policy-optimization",{"id":909,"data":911,"filePath":916,"digest":917,"rendered":918,"legacyId":926},{"title":912,"description":913,"summary":913,"pubDate":914,"source":19,"url":915,"thumbnail":21},"Proximal Policy Optimization","We’re releasing a new class of reinforcement learning algorithms, Proximal Policy Optimization (PPO), which perform comparably or better than state-of-the-art approaches while being much simpler to implement and tune. PPO has become the default reinforcement learning algorithm at OpenAI because of its ease of use and good performance.",["Date","2017-07-20T07:00:00.000Z"],"https://openai.com/blog/openai-baselines-ppo","src/content/posts/2017-07-20-proximal-policy-optimization.md","c5617f260c9bfab7",{"html":25,"metadata":919},{"headings":920,"localImagePaths":921,"remoteImagePaths":922,"frontmatter":923,"imagePaths":925},[],[],[],{"title":912,"description":913,"summary":913,"pubDate":924,"source":19,"url":915,"thumbnail":21},"Thu, 20 Jul 2017 07:00:00 GMT",[],"2017-07-20-proximal-policy-optimization.md","2017-07-27-better-exploration-with-parameter-noise",{"id":927,"data":929,"filePath":934,"digest":935,"rendered":936,"legacyId":944},{"title":930,"description":931,"summary":931,"pubDate":932,"source":19,"url":933,"thumbnail":21},"Better exploration with parameter noise","We’ve found that adding adaptive noise to the parameters of reinforcement learning algorithms frequently boosts performance. This exploration method is simple to implement and very rarely decreases performance, so it’s worth trying on any problem.",["Date","2017-07-27T07:00:00.000Z"],"https://openai.com/blog/better-exploration-with-parameter-noise","src/content/posts/2017-07-27-better-exploration-with-parameter-noise.md","6472a9195444f2de",{"html":25,"metadata":937},{"headings":938,"localImagePaths":939,"remoteImagePaths":940,"frontmatter":941,"imagePaths":943},[],[],[],{"title":930,"description":931,"summary":931,"pubDate":942,"source":19,"url":933,"thumbnail":21},"Thu, 27 Jul 2017 07:00:00 GMT",[],"2017-07-27-better-exploration-with-parameter-noise.md","2017-08-03-gathering-human-feedback",{"id":945,"data":947,"filePath":952,"digest":953,"rendered":954,"legacyId":962},{"title":948,"description":949,"summary":949,"pubDate":950,"source":19,"url":951,"thumbnail":21},"Gathering human feedback","RL-Teacher is an open-source implementation of our interface to train AIs via occasional human feedback rather than hand-crafted reward functions. The underlying technique was developed as a step towards safe AI systems, but also applies to reinforcement learning problems with rewards that are hard to specify.",["Date","2017-08-03T07:00:00.000Z"],"https://openai.com/blog/gathering-human-feedback","src/content/posts/2017-08-03-gathering-human-feedback.md","845df637e08cd745",{"html":25,"metadata":955},{"headings":956,"localImagePaths":957,"remoteImagePaths":958,"frontmatter":959,"imagePaths":961},[],[],[],{"title":948,"description":949,"summary":949,"pubDate":960,"source":19,"url":951,"thumbnail":21},"Thu, 03 Aug 2017 07:00:00 GMT",[],"2017-08-03-gathering-human-feedback.md","2017-08-11-dota-2",{"id":963,"data":965,"filePath":970,"digest":971,"rendered":972,"legacyId":980},{"title":966,"description":967,"summary":967,"pubDate":968,"source":19,"url":969,"thumbnail":21},"Dota 2","We’ve created a bot which beats the world’s top professionals at 1v1 matches of Dota 2 under standard tournament rules. The bot learned the game from scratch by self-play, and does not use imitation learning or tree search. This is a step towards building AI systems which accomplish well-defined goals in messy, complicated situations involving real humans.",["Date","2017-08-11T07:00:00.000Z"],"https://openai.com/blog/dota-2","src/content/posts/2017-08-11-dota-2.md","4ada7328050e255f",{"html":25,"metadata":973},{"headings":974,"localImagePaths":975,"remoteImagePaths":976,"frontmatter":977,"imagePaths":979},[],[],[],{"title":966,"description":967,"summary":967,"pubDate":978,"source":19,"url":969,"thumbnail":21},"Fri, 11 Aug 2017 07:00:00 GMT",[],"2017-08-11-dota-2.md","2017-08-16-more-on-dota-2",{"id":981,"data":983,"filePath":988,"digest":989,"rendered":990,"legacyId":998},{"title":984,"description":985,"summary":985,"pubDate":986,"source":19,"url":987,"thumbnail":21},"More on Dota 2","Our Dota 2 result shows that self-play can catapult the performance of machine learning systems from far below human level to superhuman, given sufficient compute. In the span of a month, our system went from barely matching a high-ranked player to beating the top pros and has continued to improve since then. Supervised deep learning systems can only be as good as their training datasets, but in self-play systems, the available data improves automatically as the agent gets better.",["Date","2017-08-16T07:00:00.000Z"],"https://openai.com/blog/more-on-dota-2","src/content/posts/2017-08-16-more-on-dota-2.md","48832dbabfff0c31",{"html":25,"metadata":991},{"headings":992,"localImagePaths":993,"remoteImagePaths":994,"frontmatter":995,"imagePaths":997},[],[],[],{"title":984,"description":985,"summary":985,"pubDate":996,"source":19,"url":987,"thumbnail":21},"Wed, 16 Aug 2017 07:00:00 GMT",[],"2017-08-16-more-on-dota-2.md","2017-08-18-openai-baselines-acktr-a2c",{"id":999,"data":1001,"filePath":1006,"digest":1007,"rendered":1008,"legacyId":1016},{"title":1002,"description":1003,"summary":1003,"pubDate":1004,"source":19,"url":1005,"thumbnail":21},"OpenAI Baselines: ACKTR & A2C","We’re releasing two new OpenAI Baselines implementations: ACKTR and A2C. A2C is a synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C) which we’ve found gives equal performance. ACKTR is a more sample-efficient reinforcement learning algorithm than TRPO and A2C, and requires only slightly more computation than A2C per update.",["Date","2017-08-18T07:00:00.000Z"],"https://openai.com/blog/openai-baselines-acktr-a2c","src/content/posts/2017-08-18-openai-baselines-acktr-a2c.md","c04ebe26f15dde9d",{"html":25,"metadata":1009},{"headings":1010,"localImagePaths":1011,"remoteImagePaths":1012,"frontmatter":1013,"imagePaths":1015},[],[],[],{"title":1002,"description":1003,"summary":1003,"pubDate":1014,"source":19,"url":1005,"thumbnail":21},"Fri, 18 Aug 2017 07:00:00 GMT",[],"2017-08-18-openai-baselines-acktr-a2c.md","2017-09-13-learning-with-opponent-learning-awareness",{"id":1017,"data":1019,"filePath":1023,"digest":1024,"rendered":1025,"legacyId":1033},{"title":1020,"description":25,"summary":25,"pubDate":1021,"source":19,"url":1022,"thumbnail":21},"Learning with opponent-learning awareness",["Date","2017-09-13T07:00:00.000Z"],"https://openai.com/blog/learning-with-opponent-learning-awareness","src/content/posts/2017-09-13-learning-with-opponent-learning-awareness.md","09aa4b8cf3ba1e27",{"html":25,"metadata":1026},{"headings":1027,"localImagePaths":1028,"remoteImagePaths":1029,"frontmatter":1030,"imagePaths":1032},[],[],[],{"title":1020,"description":25,"summary":25,"pubDate":1031,"source":19,"url":1022,"thumbnail":21},"Wed, 13 Sep 2017 07:00:00 GMT",[],"2017-09-13-learning-with-opponent-learning-awareness.md","2017-09-14-learning-to-model-other-minds",{"id":1034,"data":1036,"filePath":1041,"digest":1042,"rendered":1043,"legacyId":1051},{"title":1037,"description":1038,"summary":1038,"pubDate":1039,"source":19,"url":1040,"thumbnail":21},"Learning to model other minds","We’re releasing an algorithm which accounts for the fact that other agents are learning too, and discovers self-interested yet collaborative strategies like tit-for-tat in the iterated prisoner’s dilemma. This algorithm, Learning with Opponent-Learning Awareness (LOLA), is a small step towards agents that model other minds.",["Date","2017-09-14T07:00:00.000Z"],"https://openai.com/blog/learning-to-model-other-minds","src/content/posts/2017-09-14-learning-to-model-other-minds.md","6280abdee0c12def",{"html":25,"metadata":1044},{"headings":1045,"localImagePaths":1046,"remoteImagePaths":1047,"frontmatter":1048,"imagePaths":1050},[],[],[],{"title":1037,"description":1038,"summary":1038,"pubDate":1049,"source":19,"url":1040,"thumbnail":21},"Thu, 14 Sep 2017 07:00:00 GMT",[],"2017-09-14-learning-to-model-other-minds.md","2017-09-29-nonlinear-computation-in-deep-linear-networks",{"id":1052,"data":1054,"filePath":1058,"digest":1059,"rendered":1060,"legacyId":1068},{"title":1055,"description":25,"summary":25,"pubDate":1056,"source":19,"url":1057,"thumbnail":21},"Nonlinear computation in deep linear networks",["Date","2017-09-29T07:00:00.000Z"],"https://openai.com/blog/nonlinear-computation-in-deep-linear-networks","src/content/posts/2017-09-29-nonlinear-computation-in-deep-linear-networks.md","6eab3b6fef543c79",{"html":25,"metadata":1061},{"headings":1062,"localImagePaths":1063,"remoteImagePaths":1064,"frontmatter":1065,"imagePaths":1067},[],[],[],{"title":1055,"description":25,"summary":25,"pubDate":1066,"source":19,"url":1057,"thumbnail":21},"Fri, 29 Sep 2017 07:00:00 GMT",[],"2017-09-29-nonlinear-computation-in-deep-linear-networks.md","2017-10-11-competitive-self-play",{"id":1069,"data":1071,"filePath":1076,"digest":1077,"rendered":1078,"legacyId":1086},{"title":1072,"description":1073,"summary":1073,"pubDate":1074,"source":19,"url":1075,"thumbnail":21},"Competitive self-play","We’ve found that self-play allows simulated AIs to discover physical skills like tackling, ducking, faking, kicking, catching, and diving for the ball, without explicitly designing an environment with these skills in mind. Self-play ensures that the environment is always the right difficulty for an AI to improve. Taken alongside our Dota 2 self-play results, we have increasing confidence that self-play will be a core part of powerful AI systems in the future.",["Date","2017-10-11T07:00:00.000Z"],"https://openai.com/blog/competitive-self-play","src/content/posts/2017-10-11-competitive-self-play.md","f7765a20d509146b",{"html":25,"metadata":1079},{"headings":1080,"localImagePaths":1081,"remoteImagePaths":1082,"frontmatter":1083,"imagePaths":1085},[],[],[],{"title":1072,"description":1073,"summary":1073,"pubDate":1084,"source":19,"url":1075,"thumbnail":21},"Wed, 11 Oct 2017 07:00:00 GMT",[],"2017-10-11-competitive-self-play.md","2017-10-11-meta-learning-for-wrestling",{"id":1087,"data":1089,"filePath":1094,"digest":1095,"rendered":1096,"legacyId":1103},{"title":1090,"description":1091,"summary":1091,"pubDate":1092,"source":19,"url":1093,"thumbnail":21},"Meta-learning for wrestling","We show that for the task of simulated robot wrestling, a meta-learning agent can learn to quickly defeat a stronger non-meta-learning agent, and also show that the meta-learning agent can adapt to physical malfunction.",["Date","2017-10-11T07:00:00.000Z"],"https://openai.com/blog/meta-learning-for-wrestling","src/content/posts/2017-10-11-meta-learning-for-wrestling.md","3711593c4607bcf6",{"html":25,"metadata":1097},{"headings":1098,"localImagePaths":1099,"remoteImagePaths":1100,"frontmatter":1101,"imagePaths":1102},[],[],[],{"title":1090,"description":1091,"summary":1091,"pubDate":1084,"source":19,"url":1093,"thumbnail":21},[],"2017-10-11-meta-learning-for-wrestling.md","2017-10-17-domain-randomization-and-generative-models-for-robotic-grasping",{"id":1104,"data":1106,"filePath":1110,"digest":1111,"rendered":1112,"legacyId":1120},{"title":1107,"description":25,"summary":25,"pubDate":1108,"source":19,"url":1109,"thumbnail":21},"Domain randomization and generative models for robotic grasping",["Date","2017-10-17T07:00:00.000Z"],"https://openai.com/blog/domain-randomization-and-generative-models-for-robotic-grasping","src/content/posts/2017-10-17-domain-randomization-and-generative-models-for-robotic-grasping.md","a47b1d246c206136",{"html":25,"metadata":1113},{"headings":1114,"localImagePaths":1115,"remoteImagePaths":1116,"frontmatter":1117,"imagePaths":1119},[],[],[],{"title":1107,"description":25,"summary":25,"pubDate":1118,"source":19,"url":1109,"thumbnail":21},"Tue, 17 Oct 2017 07:00:00 GMT",[],"2017-10-17-domain-randomization-and-generative-models-for-robotic-grasping.md","2017-10-18-asymmetric-actor-critic-for-image-based-robot-learning",{"id":1121,"data":1123,"filePath":1127,"digest":1128,"rendered":1129,"legacyId":1137},{"title":1124,"description":25,"summary":25,"pubDate":1125,"source":19,"url":1126,"thumbnail":21},"Asymmetric actor critic for image-based robot learning",["Date","2017-10-18T07:00:00.000Z"],"https://openai.com/blog/asymmetric-actor-critic-for-image-based-robot-learning","src/content/posts/2017-10-18-asymmetric-actor-critic-for-image-based-robot-learning.md","9e378222a691abe4",{"html":25,"metadata":1130},{"headings":1131,"localImagePaths":1132,"remoteImagePaths":1133,"frontmatter":1134,"imagePaths":1136},[],[],[],{"title":1124,"description":25,"summary":25,"pubDate":1135,"source":19,"url":1126,"thumbnail":21},"Wed, 18 Oct 2017 07:00:00 GMT",[],"2017-10-18-asymmetric-actor-critic-for-image-based-robot-learning.md","2017-10-19-generalizing-from-simulation",{"id":1138,"data":1140,"filePath":1145,"digest":1146,"rendered":1147,"legacyId":1155},{"title":1141,"description":1142,"summary":1142,"pubDate":1143,"source":19,"url":1144,"thumbnail":21},"Generalizing from simulation","Our latest robotics techniques allow robot controllers, trained entirely in simulation and deployed on physical robots, to react to unplanned changes in the environment as they solve simple tasks. That is, we’ve used these techniques to build closed-loop systems rather than open-loop ones as before.",["Date","2017-10-19T07:00:00.000Z"],"https://openai.com/blog/generalizing-from-simulation","src/content/posts/2017-10-19-generalizing-from-simulation.md","3cd3d07741fee7cb",{"html":25,"metadata":1148},{"headings":1149,"localImagePaths":1150,"remoteImagePaths":1151,"frontmatter":1152,"imagePaths":1154},[],[],[],{"title":1141,"description":1142,"summary":1142,"pubDate":1153,"source":19,"url":1144,"thumbnail":21},"Thu, 19 Oct 2017 07:00:00 GMT",[],"2017-10-19-generalizing-from-simulation.md","2017-10-18-sim-to-real-transfer-of-robotic-control-with-dynamics-randomization",{"id":1156,"data":1158,"filePath":1162,"digest":1163,"rendered":1164,"legacyId":1171},{"title":1159,"description":25,"summary":25,"pubDate":1160,"source":19,"url":1161,"thumbnail":21},"Sim-to-real transfer of robotic control with dynamics randomization",["Date","2017-10-18T07:00:00.000Z"],"https://openai.com/blog/sim-to-real-transfer-of-robotic-control-with-dynamics-randomization","src/content/posts/2017-10-18-sim-to-real-transfer-of-robotic-control-with-dynamics-randomization.md","dff5cbecc62043f7",{"html":25,"metadata":1165},{"headings":1166,"localImagePaths":1167,"remoteImagePaths":1168,"frontmatter":1169,"imagePaths":1170},[],[],[],{"title":1159,"description":25,"summary":25,"pubDate":1135,"source":19,"url":1161,"thumbnail":21},[],"2017-10-18-sim-to-real-transfer-of-robotic-control-with-dynamics-randomization.md","2017-10-26-learning-a-hierarchy",{"id":1172,"data":1174,"filePath":1179,"digest":1180,"rendered":1181,"legacyId":1189},{"title":1175,"description":1176,"summary":1176,"pubDate":1177,"source":19,"url":1178,"thumbnail":21},"Learning a hierarchy","We’ve developed a hierarchical reinforcement learning algorithm that learns high-level actions useful for solving a range of tasks, allowing fast solving of tasks requiring thousands of timesteps. Our algorithm, when applied to a set of navigation problems, discovers a set of high-level actions for walking and crawling in different directions, which enables the agent to master new navigation tasks quickly.",["Date","2017-10-26T07:00:00.000Z"],"https://openai.com/blog/learning-a-hierarchy","src/content/posts/2017-10-26-learning-a-hierarchy.md","ba8106d77d819f66",{"html":25,"metadata":1182},{"headings":1183,"localImagePaths":1184,"remoteImagePaths":1185,"frontmatter":1186,"imagePaths":1188},[],[],[],{"title":1175,"description":1176,"summary":1176,"pubDate":1187,"source":19,"url":1178,"thumbnail":21},"Thu, 26 Oct 2017 07:00:00 GMT",[],"2017-10-26-learning-a-hierarchy.md","2017-11-02-interpretable-and-pedagogical-examples",{"id":1190,"data":1192,"filePath":1196,"digest":1197,"rendered":1198,"legacyId":1206},{"title":1193,"description":25,"summary":25,"pubDate":1194,"source":19,"url":1195,"thumbnail":21},"Interpretable and pedagogical examples",["Date","2017-11-02T07:00:00.000Z"],"https://openai.com/blog/interpretable-and-pedagogical-examples","src/content/posts/2017-11-02-interpretable-and-pedagogical-examples.md","506688131aafedb1",{"html":25,"metadata":1199},{"headings":1200,"localImagePaths":1201,"remoteImagePaths":1202,"frontmatter":1203,"imagePaths":1205},[],[],[],{"title":1193,"description":25,"summary":25,"pubDate":1204,"source":19,"url":1195,"thumbnail":21},"Thu, 02 Nov 2017 07:00:00 GMT",[],"2017-11-02-interpretable-and-pedagogical-examples.md","2017-12-04-learning-sparse-neural-networks-through-l-regularization",{"id":1207,"data":1209,"filePath":1213,"digest":1214,"rendered":1215,"legacyId":1223},{"title":1210,"description":25,"summary":25,"pubDate":1211,"source":19,"url":1212,"thumbnail":21},"Learning sparse neural networks through L₀ regularization",["Date","2017-12-04T08:00:00.000Z"],"https://openai.com/blog/learning-sparse-neural-networks-through-l0-regularization","src/content/posts/2017-12-04-learning-sparse-neural-networks-through-l₀-regularization.md","ee281749d5756745",{"html":25,"metadata":1216},{"headings":1217,"localImagePaths":1218,"remoteImagePaths":1219,"frontmatter":1220,"imagePaths":1222},[],[],[],{"title":1210,"description":25,"summary":25,"pubDate":1221,"source":19,"url":1212,"thumbnail":21},"Mon, 04 Dec 2017 08:00:00 GMT",[],"2017-12-04-learning-sparse-neural-networks-through-l₀-regularization.md","2017-12-06-block-sparse-gpu-kernels",{"id":1224,"data":1226,"filePath":1231,"digest":1232,"rendered":1233,"legacyId":1241},{"title":1227,"description":1228,"summary":1228,"pubDate":1229,"source":19,"url":1230,"thumbnail":21},"Block-sparse GPU kernels","We’re releasing highly-optimized GPU kernels for an underexplored class of neural network architectures: networks with block-sparse weights. Depending on the chosen sparsity, these kernels can run orders of magnitude faster than cuBLAS or cuSPARSE. We’ve used them to attain state-of-the-art results in text sentiment analysis and generative modeling of text and images.",["Date","2017-12-06T08:00:00.000Z"],"https://openai.com/blog/block-sparse-gpu-kernels","src/content/posts/2017-12-06-block-sparse-gpu-kernels.md","5f6cadd7154dadde",{"html":25,"metadata":1234},{"headings":1235,"localImagePaths":1236,"remoteImagePaths":1237,"frontmatter":1238,"imagePaths":1240},[],[],[],{"title":1227,"description":1228,"summary":1228,"pubDate":1239,"source":19,"url":1230,"thumbnail":21},"Wed, 06 Dec 2017 08:00:00 GMT",[],"2017-12-06-block-sparse-gpu-kernels.md","2018-01-31-requests-for-research-20",{"id":1242,"data":1244,"filePath":1249,"digest":1250,"rendered":1251,"legacyId":1259},{"title":1245,"description":1246,"summary":1246,"pubDate":1247,"source":19,"url":1248,"thumbnail":21},"Requests for Research 2.0","We’re releasing a new batch of seven unsolved problems which have come up in the course of our research at OpenAI.",["Date","2018-01-31T08:00:00.000Z"],"https://openai.com/blog/requests-for-research-2","src/content/posts/2018-01-31-requests-for-research-20.md","4c0c706f2d18aeaa",{"html":25,"metadata":1252},{"headings":1253,"localImagePaths":1254,"remoteImagePaths":1255,"frontmatter":1256,"imagePaths":1258},[],[],[],{"title":1245,"description":1246,"summary":1246,"pubDate":1257,"source":19,"url":1248,"thumbnail":21},"Wed, 31 Jan 2018 08:00:00 GMT",[],"2018-01-31-requests-for-research-20.md","2018-01-18-scaling-kubernetes-to-2500-nodes",{"id":1260,"data":1262,"filePath":1266,"digest":1267,"rendered":1268,"legacyId":1276},{"title":1263,"description":25,"summary":25,"pubDate":1264,"source":19,"url":1265,"thumbnail":21},"Scaling Kubernetes to 2,500 nodes",["Date","2018-01-18T08:00:00.000Z"],"https://openai.com/blog/scaling-kubernetes-to-2500-nodes","src/content/posts/2018-01-18-scaling-kubernetes-to-2500-nodes.md","67a61a0072b80d6e",{"html":25,"metadata":1269},{"headings":1270,"localImagePaths":1271,"remoteImagePaths":1272,"frontmatter":1273,"imagePaths":1275},[],[],[],{"title":1263,"description":25,"summary":25,"pubDate":1274,"source":19,"url":1265,"thumbnail":21},"Thu, 18 Jan 2018 08:00:00 GMT",[],"2018-01-18-scaling-kubernetes-to-2500-nodes.md","2018-02-07-discovering-types-for-entity-disambiguation",{"id":1277,"data":1279,"filePath":1284,"digest":1285,"rendered":1286,"legacyId":1294},{"title":1280,"description":1281,"summary":1281,"pubDate":1282,"source":19,"url":1283,"thumbnail":21},"Discovering types for entity disambiguation","We’ve built a system for automatically figuring out which object is meant by a word by having a neural network decide if the word belongs to each of about 100 automatically-discovered “types” (non-exclusive categories).",["Date","2018-02-07T08:00:00.000Z"],"https://openai.com/blog/discovering-types-for-entity-disambiguation","src/content/posts/2018-02-07-discovering-types-for-entity-disambiguation.md","95bfcc7d7cbe0b6f",{"html":25,"metadata":1287},{"headings":1288,"localImagePaths":1289,"remoteImagePaths":1290,"frontmatter":1291,"imagePaths":1293},[],[],[],{"title":1280,"description":1281,"summary":1281,"pubDate":1292,"source":19,"url":1283,"thumbnail":21},"Wed, 07 Feb 2018 08:00:00 GMT",[],"2018-02-07-discovering-types-for-entity-disambiguation.md","2018-02-15-interpretable-machine-learning-through-teaching",{"id":1295,"data":1297,"filePath":1302,"digest":1303,"rendered":1304,"legacyId":1312},{"title":1298,"description":1299,"summary":1299,"pubDate":1300,"source":19,"url":1301,"thumbnail":21},"Interpretable machine learning through teaching","We’ve designed a method that encourages AIs to teach each other with examples that also make sense to humans. Our approach automatically selects the most informative examples to teach a concept—for instance, the best images to describe the concept of dogs—and experimentally we found our approach to be effective at teaching both AIs",["Date","2018-02-15T08:00:00.000Z"],"https://openai.com/blog/interpretable-machine-learning-through-teaching","src/content/posts/2018-02-15-interpretable-machine-learning-through-teaching.md","fc8a656301d4e063",{"html":25,"metadata":1305},{"headings":1306,"localImagePaths":1307,"remoteImagePaths":1308,"frontmatter":1309,"imagePaths":1311},[],[],[],{"title":1298,"description":1299,"summary":1299,"pubDate":1310,"source":19,"url":1301,"thumbnail":21},"Thu, 15 Feb 2018 08:00:00 GMT",[],"2018-02-15-interpretable-machine-learning-through-teaching.md","2018-02-20-openai-supporters",{"id":1313,"data":1315,"filePath":1320,"digest":1321,"rendered":1322,"legacyId":1330},{"title":1316,"description":1317,"summary":1317,"pubDate":1318,"source":19,"url":1319,"thumbnail":21},"OpenAI supporters","We’re excited to welcome new donors to OpenAI.",["Date","2018-02-20T08:00:00.000Z"],"https://openai.com/blog/openai-supporters","src/content/posts/2018-02-20-openai-supporters.md","621fa2edb99c4de6",{"html":25,"metadata":1323},{"headings":1324,"localImagePaths":1325,"remoteImagePaths":1326,"frontmatter":1327,"imagePaths":1329},[],[],[],{"title":1316,"description":1317,"summary":1317,"pubDate":1328,"source":19,"url":1319,"thumbnail":21},"Tue, 20 Feb 2018 08:00:00 GMT",[],"2018-02-20-openai-supporters.md","2018-02-20-preparing-for-malicious-uses-of-ai",{"id":1331,"data":1333,"filePath":1338,"digest":1339,"rendered":1340,"legacyId":1347},{"title":1334,"description":1335,"summary":1335,"pubDate":1336,"source":19,"url":1337,"thumbnail":21},"Preparing for malicious uses of AI","We’ve co-authored a paper that forecasts how malicious actors could misuse AI technology, and potential ways we can prevent and mitigate these threats. This paper is the outcome of almost a year of sustained work with our colleagues at the Future of Humanity Institute, the Centre for the Study of Existential Risk, the Center for a New American Security, the Electronic Frontier Foundation, and others.",["Date","2018-02-20T08:00:00.000Z"],"https://openai.com/blog/preparing-for-malicious-uses-of-ai","src/content/posts/2018-02-20-preparing-for-malicious-uses-of-ai.md","96a3249ae96b8708",{"html":25,"metadata":1341},{"headings":1342,"localImagePaths":1343,"remoteImagePaths":1344,"frontmatter":1345,"imagePaths":1346},[],[],[],{"title":1334,"description":1335,"summary":1335,"pubDate":1328,"source":19,"url":1337,"thumbnail":21},[],"2018-02-20-preparing-for-malicious-uses-of-ai.md","2018-02-26-ingredients-for-robotics-research",{"id":1348,"data":1350,"filePath":1355,"digest":1356,"rendered":1357,"legacyId":1365},{"title":1351,"description":1352,"summary":1352,"pubDate":1353,"source":19,"url":1354,"thumbnail":21},"Ingredients for robotics research","We’re releasing eight simulated robotics environments and a Baselines implementation of Hindsight Experience Replay, all developed for our research over the past year. We’ve used these environments to train models which work on physical robots. We’re also releasing a set of requests for robotics research.",["Date","2018-02-26T08:00:00.000Z"],"https://openai.com/blog/ingredients-for-robotics-research","src/content/posts/2018-02-26-ingredients-for-robotics-research.md","89e61db1fcc50c33",{"html":25,"metadata":1358},{"headings":1359,"localImagePaths":1360,"remoteImagePaths":1361,"frontmatter":1362,"imagePaths":1364},[],[],[],{"title":1351,"description":1352,"summary":1352,"pubDate":1363,"source":19,"url":1354,"thumbnail":21},"Mon, 26 Feb 2018 08:00:00 GMT",[],"2018-02-26-ingredients-for-robotics-research.md","2018-02-22-openai-hackathon",{"id":1366,"data":1368,"filePath":1373,"digest":1374,"rendered":1375,"legacyId":1383},{"title":1369,"description":1370,"summary":1370,"pubDate":1371,"source":19,"url":1372,"thumbnail":21},"OpenAI hackathon","Come to OpenAI’s office in San Francisco’s Mission District for talks and a hackathon on Saturday, March 3rd.",["Date","2018-02-22T08:00:00.000Z"],"https://openai.com/blog/openai-hackathon","src/content/posts/2018-02-22-openai-hackathon.md","949b919f0b048b3d",{"html":25,"metadata":1376},{"headings":1377,"localImagePaths":1378,"remoteImagePaths":1379,"frontmatter":1380,"imagePaths":1382},[],[],[],{"title":1369,"description":1370,"summary":1370,"pubDate":1381,"source":19,"url":1372,"thumbnail":21},"Thu, 22 Feb 2018 08:00:00 GMT",[],"2018-02-22-openai-hackathon.md","2018-02-26-multi-goal-reinforcement-learning-challenging-robotics-environments-and-request-for-research",{"id":1384,"data":1386,"filePath":1390,"digest":1391,"rendered":1392,"legacyId":1399},{"title":1387,"description":25,"summary":25,"pubDate":1388,"source":19,"url":1389,"thumbnail":21},"Multi-Goal Reinforcement Learning: Challenging robotics environments and request for research",["Date","2018-02-26T08:00:00.000Z"],"https://openai.com/blog/multi-goal-reinforcement-learning","src/content/posts/2018-02-26-multi-goal-reinforcement-learning-challenging-robotics-environments-and-request-for-research.md","6f134842e314b95f",{"html":25,"metadata":1393},{"headings":1394,"localImagePaths":1395,"remoteImagePaths":1396,"frontmatter":1397,"imagePaths":1398},[],[],[],{"title":1387,"description":25,"summary":25,"pubDate":1363,"source":19,"url":1389,"thumbnail":21},[],"2018-02-26-multi-goal-reinforcement-learning-challenging-robotics-environments-and-request-for-research.md","2018-03-03-some-considerations-on-learning-to-explore-via-meta-reinforcement-learning",{"id":1400,"data":1402,"filePath":1406,"digest":1407,"rendered":1408,"legacyId":1416},{"title":1403,"description":25,"summary":25,"pubDate":1404,"source":19,"url":1405,"thumbnail":21},"Some considerations on learning to explore via meta-reinforcement learning",["Date","2018-03-03T08:00:00.000Z"],"https://openai.com/blog/some-considerations-on-learning-to-explore-via-meta-reinforcement-learning","src/content/posts/2018-03-03-some-considerations-on-learning-to-explore-via-meta-reinforcement-learning.md","bbc81d5232937f6c",{"html":25,"metadata":1409},{"headings":1410,"localImagePaths":1411,"remoteImagePaths":1412,"frontmatter":1413,"imagePaths":1415},[],[],[],{"title":1403,"description":25,"summary":25,"pubDate":1414,"source":19,"url":1405,"thumbnail":21},"Sat, 03 Mar 2018 08:00:00 GMT",[],"2018-03-03-some-considerations-on-learning-to-explore-via-meta-reinforcement-learning.md","2018-03-06-openai-scholars",{"id":1417,"data":1419,"filePath":1424,"digest":1425,"rendered":1426,"legacyId":1434},{"title":1420,"description":1421,"summary":1421,"pubDate":1422,"source":19,"url":1423,"thumbnail":21},"OpenAI Scholars","We’re providing 6–10 stipends and mentorship to individuals from underrepresented groups to study deep learning full-time for 3 months and open-source a project.",["Date","2018-03-06T08:00:00.000Z"],"https://openai.com/blog/openai-scholars","src/content/posts/2018-03-06-openai-scholars.md","b5094b1c33187963",{"html":25,"metadata":1427},{"headings":1428,"localImagePaths":1429,"remoteImagePaths":1430,"frontmatter":1431,"imagePaths":1433},[],[],[],{"title":1420,"description":1421,"summary":1421,"pubDate":1432,"source":19,"url":1423,"thumbnail":21},"Tue, 06 Mar 2018 08:00:00 GMT",[],"2018-03-06-openai-scholars.md","2018-03-07-reptile-a-scalable-meta-learning-algorithm",{"id":1435,"data":1437,"filePath":1442,"digest":1443,"rendered":1444,"legacyId":1452},{"title":1438,"description":1439,"summary":1439,"pubDate":1440,"source":19,"url":1441,"thumbnail":21},"Reptile: A scalable meta-learning algorithm","We’ve developed a simple meta-learning algorithm called Reptile which works by repeatedly sampling a task, performing stochastic gradient descent on it, and updating the initial parameters towards the final parameters learned on that task. Reptile is the application of the Shortest Descent algorithm to the meta-learning setting, and is mathematically similar to first-order MAML (which is a version of the well-known MAML algorithm) that only needs black-box access to an optimizer such as SGD or Adam, with similar computational efficiency and performance.",["Date","2018-03-07T08:00:00.000Z"],"https://openai.com/blog/reptile","src/content/posts/2018-03-07-reptile-a-scalable-meta-learning-algorithm.md","1873a9de7df30885",{"html":25,"metadata":1445},{"headings":1446,"localImagePaths":1447,"remoteImagePaths":1448,"frontmatter":1449,"imagePaths":1451},[],[],[],{"title":1438,"description":1439,"summary":1439,"pubDate":1450,"source":19,"url":1441,"thumbnail":21},"Wed, 07 Mar 2018 08:00:00 GMT",[],"2018-03-07-reptile-a-scalable-meta-learning-algorithm.md","2018-03-15-improving-gans-using-optimal-transport",{"id":1453,"data":1455,"filePath":1459,"digest":1460,"rendered":1461,"legacyId":1469},{"title":1456,"description":25,"summary":25,"pubDate":1457,"source":19,"url":1458,"thumbnail":21},"Improving GANs using optimal transport",["Date","2018-03-15T07:00:00.000Z"],"https://openai.com/blog/improving-gans-using-optimal-transport","src/content/posts/2018-03-15-improving-gans-using-optimal-transport.md","53696a55e6319628",{"html":25,"metadata":1462},{"headings":1463,"localImagePaths":1464,"remoteImagePaths":1465,"frontmatter":1466,"imagePaths":1468},[],[],[],{"title":1456,"description":25,"summary":25,"pubDate":1467,"source":19,"url":1458,"thumbnail":21},"Thu, 15 Mar 2018 07:00:00 GMT",[],"2018-03-15-improving-gans-using-optimal-transport.md","2018-03-08-on-first-order-meta-learning-algorithms",{"id":1470,"data":1472,"filePath":1476,"digest":1477,"rendered":1478,"legacyId":1486},{"title":1473,"description":25,"summary":25,"pubDate":1474,"source":19,"url":1475,"thumbnail":21},"On first-order meta-learning algorithms",["Date","2018-03-08T08:00:00.000Z"],"https://openai.com/blog/on-first-order-meta-learning-algorithms","src/content/posts/2018-03-08-on-first-order-meta-learning-algorithms.md","5bb130f664ddbe90",{"html":25,"metadata":1479},{"headings":1480,"localImagePaths":1481,"remoteImagePaths":1482,"frontmatter":1483,"imagePaths":1485},[],[],[],{"title":1473,"description":25,"summary":25,"pubDate":1484,"source":19,"url":1475,"thumbnail":21},"Thu, 08 Mar 2018 08:00:00 GMT",[],"2018-03-08-on-first-order-meta-learning-algorithms.md","2018-03-20-variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines",{"id":1487,"data":1489,"filePath":1493,"digest":1494,"rendered":1495,"legacyId":1503},{"title":1490,"description":25,"summary":25,"pubDate":1491,"source":19,"url":1492,"thumbnail":21},"Variance reduction for policy gradient with action-dependent factorized baselines",["Date","2018-03-20T07:00:00.000Z"],"https://openai.com/blog/variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines","src/content/posts/2018-03-20-variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines.md","a8b042592f80efa3",{"html":25,"metadata":1496},{"headings":1497,"localImagePaths":1498,"remoteImagePaths":1499,"frontmatter":1500,"imagePaths":1502},[],[],[],{"title":1490,"description":25,"summary":25,"pubDate":1501,"source":19,"url":1492,"thumbnail":21},"Tue, 20 Mar 2018 07:00:00 GMT",[],"2018-03-20-variance-reduction-for-policy-gradient-with-action-dependent-factorized-baselines.md","2018-03-15-report-from-the-openai-hackathon",{"id":1504,"data":1506,"filePath":1511,"digest":1512,"rendered":1513,"legacyId":1520},{"title":1507,"description":1508,"summary":1508,"pubDate":1509,"source":19,"url":1510,"thumbnail":21},"Report from the OpenAI hackathon","On March 3rd, we hosted our first hackathon with 100 members of the artificial intelligence community.",["Date","2018-03-15T07:00:00.000Z"],"https://openai.com/blog/hackathon-follow-up","src/content/posts/2018-03-15-report-from-the-openai-hackathon.md","cd0cb7c0a3a4b292",{"html":25,"metadata":1514},{"headings":1515,"localImagePaths":1516,"remoteImagePaths":1517,"frontmatter":1518,"imagePaths":1519},[],[],[],{"title":1507,"description":1508,"summary":1508,"pubDate":1467,"source":19,"url":1510,"thumbnail":21},[],"2018-03-15-report-from-the-openai-hackathon.md","2018-04-10-gotta-learn-fast-a-new-benchmark-for-generalization-in-rl",{"id":1521,"data":1523,"filePath":1527,"digest":1528,"rendered":1529,"legacyId":1537},{"title":1524,"description":25,"summary":25,"pubDate":1525,"source":19,"url":1526,"thumbnail":21},"Gotta Learn Fast: A new benchmark for generalization in RL",["Date","2018-04-10T07:00:00.000Z"],"https://openai.com/blog/gotta-learn-fast","src/content/posts/2018-04-10-gotta-learn-fast-a-new-benchmark-for-generalization-in-rl.md","e0fa1abfed130e9b",{"html":25,"metadata":1530},{"headings":1531,"localImagePaths":1532,"remoteImagePaths":1533,"frontmatter":1534,"imagePaths":1536},[],[],[],{"title":1524,"description":25,"summary":25,"pubDate":1535,"source":19,"url":1526,"thumbnail":21},"Tue, 10 Apr 2018 07:00:00 GMT",[],"2018-04-10-gotta-learn-fast-a-new-benchmark-for-generalization-in-rl.md","2018-04-05-retro-contest",{"id":1538,"data":1540,"filePath":1545,"digest":1546,"rendered":1547,"legacyId":1555},{"title":1541,"description":1542,"summary":1542,"pubDate":1543,"source":19,"url":1544,"thumbnail":21},"Retro Contest","We’re launching a transfer learning contest that measures a reinforcement learning algorithm’s ability to generalize from previous experience.",["Date","2018-04-05T07:00:00.000Z"],"https://openai.com/blog/retro-contest","src/content/posts/2018-04-05-retro-contest.md","1aa13e7fded03010",{"html":25,"metadata":1548},{"headings":1549,"localImagePaths":1550,"remoteImagePaths":1551,"frontmatter":1552,"imagePaths":1554},[],[],[],{"title":1541,"description":1542,"summary":1542,"pubDate":1553,"source":19,"url":1544,"thumbnail":21},"Thu, 05 Apr 2018 07:00:00 GMT",[],"2018-04-05-retro-contest.md","2018-04-18-evolved-policy-gradients",{"id":1556,"data":1558,"filePath":1563,"digest":1564,"rendered":1565,"legacyId":1573},{"title":1559,"description":1560,"summary":1560,"pubDate":1561,"source":19,"url":1562,"thumbnail":21},"Evolved Policy Gradients","We’re releasing an experimental metalearning approach called Evolved Policy Gradients, a method that evolves the loss function of learning agents, which can enable fast training on novel tasks. Agents trained with EPG can succeed at basic tasks at test time that were outside their training regime, like learning to navigate to an object on a different side of the room from where it was placed during training.",["Date","2018-04-18T07:00:00.000Z"],"https://openai.com/blog/evolved-policy-gradients","src/content/posts/2018-04-18-evolved-policy-gradients.md","1e1d3e1512398d8e",{"html":25,"metadata":1566},{"headings":1567,"localImagePaths":1568,"remoteImagePaths":1569,"frontmatter":1570,"imagePaths":1572},[],[],[],{"title":1559,"description":1560,"summary":1560,"pubDate":1571,"source":19,"url":1562,"thumbnail":21},"Wed, 18 Apr 2018 07:00:00 GMT",[],"2018-04-18-evolved-policy-gradients.md","2018-05-03-ai-safety-via-debate",{"id":1574,"data":1576,"filePath":1581,"digest":1582,"rendered":1583,"legacyId":1591},{"title":1577,"description":1578,"summary":1578,"pubDate":1579,"source":19,"url":1580,"thumbnail":21},"AI safety via debate","We’re proposing an AI safety technique which trains agents to debate topics with one another, using a human to judge who wins.",["Date","2018-05-03T07:00:00.000Z"],"https://openai.com/blog/debate","src/content/posts/2018-05-03-ai-safety-via-debate.md","b21e05298300145b",{"html":25,"metadata":1584},{"headings":1585,"localImagePaths":1586,"remoteImagePaths":1587,"frontmatter":1588,"imagePaths":1590},[],[],[],{"title":1577,"description":1578,"summary":1578,"pubDate":1589,"source":19,"url":1580,"thumbnail":21},"Thu, 03 May 2018 07:00:00 GMT",[],"2018-05-03-ai-safety-via-debate.md","2018-05-16-ai-and-compute",{"id":1592,"data":1594,"filePath":1599,"digest":1600,"rendered":1601,"legacyId":1609},{"title":1595,"description":1596,"summary":1596,"pubDate":1597,"source":19,"url":1598,"thumbnail":21},"AI and compute","We’re releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore’s Law had a 2-year doubling period)[^footnote-correction]. Since 2012, this metric has grown by more than 300,000x (a 2-year doubling period would yield only a 7x increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it’s worth preparing for the implications of systems far outside today’s capabilities.",["Date","2018-05-16T07:00:00.000Z"],"https://openai.com/blog/ai-and-compute","src/content/posts/2018-05-16-ai-and-compute.md","503e194c31e9b0d3",{"html":25,"metadata":1602},{"headings":1603,"localImagePaths":1604,"remoteImagePaths":1605,"frontmatter":1606,"imagePaths":1608},[],[],[],{"title":1595,"description":1596,"summary":1596,"pubDate":1607,"source":19,"url":1598,"thumbnail":21},"Wed, 16 May 2018 07:00:00 GMT",[],"2018-05-16-ai-and-compute.md","2018-05-25-gym-retro",{"id":1610,"data":1612,"filePath":1617,"digest":1618,"rendered":1619,"legacyId":1627},{"title":1613,"description":1614,"summary":1614,"pubDate":1615,"source":19,"url":1616,"thumbnail":21},"Gym Retro","We’re releasing the full version of Gym Retro, a platform for reinforcement learning research on games. This brings our publicly-released game count from around 70 Atari games and 30 Sega games to over 1,000 games across a variety of backing emulators. We’re also releasing the tool we use to add new games to the platform.",["Date","2018-05-25T07:00:00.000Z"],"https://openai.com/blog/gym-retro","src/content/posts/2018-05-25-gym-retro.md","04c2b0af477d0539",{"html":25,"metadata":1620},{"headings":1621,"localImagePaths":1622,"remoteImagePaths":1623,"frontmatter":1624,"imagePaths":1626},[],[],[],{"title":1613,"description":1614,"summary":1614,"pubDate":1625,"source":19,"url":1616,"thumbnail":21},"Fri, 25 May 2018 07:00:00 GMT",[],"2018-05-25-gym-retro.md","2018-05-30-openai-fellows-fall-2018",{"id":1628,"data":1630,"filePath":1635,"digest":1636,"rendered":1637,"legacyId":1645},{"title":1631,"description":1632,"summary":1632,"pubDate":1633,"source":19,"url":1634,"thumbnail":21},"OpenAI Fellows Fall 2018","We’re now accepting applications for the next cohort of OpenAI Fellows, a program which offers a compensated 6-month apprenticeship in AI research at OpenAI.",["Date","2018-05-30T07:00:00.000Z"],"https://openai.com/blog/openai-fellows","src/content/posts/2018-05-30-openai-fellows-fall-2018.md","cf808dd9544858e6",{"html":25,"metadata":1638},{"headings":1639,"localImagePaths":1640,"remoteImagePaths":1641,"frontmatter":1642,"imagePaths":1644},[],[],[],{"title":1631,"description":1632,"summary":1632,"pubDate":1643,"source":19,"url":1634,"thumbnail":21},"Wed, 30 May 2018 07:00:00 GMT",[],"2018-05-30-openai-fellows-fall-2018.md","2018-06-02-gamepad-a-learning-environment-for-theorem-proving",{"id":1646,"data":1648,"filePath":1652,"digest":1653,"rendered":1654,"legacyId":1662},{"title":1649,"description":25,"summary":25,"pubDate":1650,"source":19,"url":1651,"thumbnail":21},"GamePad: A learning environment for theorem proving",["Date","2018-06-02T07:00:00.000Z"],"https://openai.com/blog/gamepad","src/content/posts/2018-06-02-gamepad-a-learning-environment-for-theorem-proving.md","63a74e1444cf3545",{"html":25,"metadata":1655},{"headings":1656,"localImagePaths":1657,"remoteImagePaths":1658,"frontmatter":1659,"imagePaths":1661},[],[],[],{"title":1649,"description":25,"summary":25,"pubDate":1660,"source":19,"url":1651,"thumbnail":21},"Sat, 02 Jun 2018 07:00:00 GMT",[],"2018-06-02-gamepad-a-learning-environment-for-theorem-proving.md","2018-06-17-learning-policy-representations-in-multiagent-systems",{"id":1663,"data":1665,"filePath":1669,"digest":1670,"rendered":1671,"legacyId":1679},{"title":1666,"description":25,"summary":25,"pubDate":1667,"source":19,"url":1668,"thumbnail":21},"Learning policy representations in multiagent systems",["Date","2018-06-17T07:00:00.000Z"],"https://openai.com/blog/learning-policy-representations-in-multiagent-systems","src/content/posts/2018-06-17-learning-policy-representations-in-multiagent-systems.md","dad0477df1a72767",{"html":25,"metadata":1672},{"headings":1673,"localImagePaths":1674,"remoteImagePaths":1675,"frontmatter":1676,"imagePaths":1678},[],[],[],{"title":1666,"description":25,"summary":25,"pubDate":1677,"source":19,"url":1668,"thumbnail":21},"Sun, 17 Jun 2018 07:00:00 GMT",[],"2018-06-17-learning-policy-representations-in-multiagent-systems.md","2018-06-11-improving-language-understanding-with-unsupervised-learning",{"id":1680,"data":1682,"filePath":1687,"digest":1688,"rendered":1689,"legacyId":1697},{"title":1683,"description":1684,"summary":1684,"pubDate":1685,"source":19,"url":1686,"thumbnail":21},"Improving language understanding with unsupervised learning","We’ve obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system, which we’re also releasing. Our approach is a combination of two existing ideas: transformers and unsupervised pre-training. These results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well; this is an idea that many have explored in the past, and we hope our result motivates further research into applying this idea on larger and more diverse datasets.",["Date","2018-06-11T07:00:00.000Z"],"https://openai.com/blog/language-unsupervised","src/content/posts/2018-06-11-improving-language-understanding-with-unsupervised-learning.md","845ecdb5d38a7ae5",{"html":25,"metadata":1690},{"headings":1691,"localImagePaths":1692,"remoteImagePaths":1693,"frontmatter":1694,"imagePaths":1696},[],[],[],{"title":1683,"description":1684,"summary":1684,"pubDate":1695,"source":19,"url":1686,"thumbnail":21},"Mon, 11 Jun 2018 07:00:00 GMT",[],"2018-06-11-improving-language-understanding-with-unsupervised-learning.md","2018-06-22-retro-contest-results",{"id":1698,"data":1700,"filePath":1705,"digest":1706,"rendered":1707,"legacyId":1715},{"title":1701,"description":1702,"summary":1702,"pubDate":1703,"source":19,"url":1704,"thumbnail":21},"Retro Contest: Results","The first run of our Retro Contest—exploring the development of algorithms that can generalize from previous experience—is now complete.",["Date","2018-06-22T07:00:00.000Z"],"https://openai.com/blog/retro-contest-results","src/content/posts/2018-06-22-retro-contest-results.md","ea2e060460097ace",{"html":25,"metadata":1708},{"headings":1709,"localImagePaths":1710,"remoteImagePaths":1711,"frontmatter":1712,"imagePaths":1714},[],[],[],{"title":1701,"description":1702,"summary":1702,"pubDate":1713,"source":19,"url":1704,"thumbnail":21},"Fri, 22 Jun 2018 07:00:00 GMT",[],"2018-06-22-retro-contest-results.md","2018-06-25-openai-five",{"id":1716,"data":1718,"filePath":1723,"digest":1724,"rendered":1725,"legacyId":1733},{"title":1719,"description":1720,"summary":1720,"pubDate":1721,"source":19,"url":1722,"thumbnail":21},"OpenAI Five","Our team of five neural networks, OpenAI Five, has started to defeat amateur human teams at Dota 2.",["Date","2018-06-25T07:00:00.000Z"],"https://openai.com/blog/openai-five","src/content/posts/2018-06-25-openai-five.md","2c353b27f64cdf24",{"html":25,"metadata":1726},{"headings":1727,"localImagePaths":1728,"remoteImagePaths":1729,"frontmatter":1730,"imagePaths":1732},[],[],[],{"title":1719,"description":1720,"summary":1720,"pubDate":1731,"source":19,"url":1722,"thumbnail":21},"Mon, 25 Jun 2018 07:00:00 GMT",[],"2018-06-25-openai-five.md","2018-07-04-learning-montezumas-revenge-from-a-single-demonstration",{"id":1734,"data":1736,"filePath":1741,"digest":1742,"rendered":1743,"legacyId":1751},{"title":1737,"description":1738,"summary":1738,"pubDate":1739,"source":19,"url":1740,"thumbnail":21},"Learning Montezuma’s Revenge from a single demonstration","We’ve trained an agent to achieve a high score of 74,500 on Montezuma’s Revenge from a single human demonstration, better than any previously published result. Our algorithm is simple: the agent plays a sequence of games starting from carefully chosen states from the demonstration, and learns from them by optimizing the game score using PPO, the same reinforcement learning algorithm that underpins OpenAI Five.",["Date","2018-07-04T07:00:00.000Z"],"https://openai.com/blog/learning-montezumas-revenge-from-a-single-demonstration","src/content/posts/2018-07-04-learning-montezumas-revenge-from-a-single-demonstration.md","e4d9ce44961a960b",{"html":25,"metadata":1744},{"headings":1745,"localImagePaths":1746,"remoteImagePaths":1747,"frontmatter":1748,"imagePaths":1750},[],[],[],{"title":1737,"description":1738,"summary":1738,"pubDate":1749,"source":19,"url":1740,"thumbnail":21},"Wed, 04 Jul 2018 07:00:00 GMT",[],"2018-07-04-learning-montezumas-revenge-from-a-single-demonstration.md","2018-07-18-openai-five-benchmark",{"id":1752,"data":1754,"filePath":1759,"digest":1760,"rendered":1761,"legacyId":1769},{"title":1755,"description":1756,"summary":1756,"pubDate":1757,"source":19,"url":1758,"thumbnail":21},"OpenAI Five Benchmark","The OpenAI Five Benchmark match is now over!",["Date","2018-07-18T07:00:00.000Z"],"https://openai.com/blog/openai-five-benchmark","src/content/posts/2018-07-18-openai-five-benchmark.md","92d0a84fd18baa9c",{"html":25,"metadata":1762},{"headings":1763,"localImagePaths":1764,"remoteImagePaths":1765,"frontmatter":1766,"imagePaths":1768},[],[],[],{"title":1755,"description":1756,"summary":1756,"pubDate":1767,"source":19,"url":1758,"thumbnail":21},"Wed, 18 Jul 2018 07:00:00 GMT",[],"2018-07-18-openai-five-benchmark.md","2018-07-09-glow-better-reversible-generative-models",{"id":1770,"data":1772,"filePath":1777,"digest":1778,"rendered":1779,"legacyId":1787},{"title":1773,"description":1774,"summary":1774,"pubDate":1775,"source":19,"url":1776,"thumbnail":21},"Glow: Better reversible generative models","We introduce Glow, a reversible generative model which uses invertible 1x1 convolutions. It extends previous work on reversible generative models and simplifies the architecture. Our model can generate realistic high resolution images, supports efficient sampling, and discovers features that can be used to manipulate attributes of data. We’re releasing code for the model and an online visualization tool so people can explore and build on these results.",["Date","2018-07-09T07:00:00.000Z"],"https://openai.com/blog/glow","src/content/posts/2018-07-09-glow-better-reversible-generative-models.md","740f763b13b86ff0",{"html":25,"metadata":1780},{"headings":1781,"localImagePaths":1782,"remoteImagePaths":1783,"frontmatter":1784,"imagePaths":1786},[],[],[],{"title":1773,"description":1774,"summary":1774,"pubDate":1785,"source":19,"url":1776,"thumbnail":21},"Mon, 09 Jul 2018 07:00:00 GMT",[],"2018-07-09-glow-better-reversible-generative-models.md","2018-07-25-openai-scholars-2018-meet-our-scholars",{"id":1788,"data":1790,"filePath":1795,"digest":1796,"rendered":1797,"legacyId":1805},{"title":1791,"description":1792,"summary":1792,"pubDate":1793,"source":19,"url":1794,"thumbnail":21},"OpenAI Scholars 2018: Meet our Scholars","Our first class of OpenAI Scholars is underway, and you can now follow along as this group of experienced software developers becomes machine learning practitioners.",["Date","2018-07-25T07:00:00.000Z"],"https://openai.com/blog/openai-scholars-2018-meet-our-scholars","src/content/posts/2018-07-25-openai-scholars-2018-meet-our-scholars.md","3fa64302c09138fb",{"html":25,"metadata":1798},{"headings":1799,"localImagePaths":1800,"remoteImagePaths":1801,"frontmatter":1802,"imagePaths":1804},[],[],[],{"title":1791,"description":1792,"summary":1792,"pubDate":1803,"source":19,"url":1794,"thumbnail":21},"Wed, 25 Jul 2018 07:00:00 GMT",[],"2018-07-25-openai-scholars-2018-meet-our-scholars.md","2018-07-26-variational-option-discovery-algorithms",{"id":1806,"data":1808,"filePath":1812,"digest":1813,"rendered":1814,"legacyId":1822},{"title":1809,"description":25,"summary":25,"pubDate":1810,"source":19,"url":1811,"thumbnail":21},"Variational option discovery algorithms",["Date","2018-07-26T07:00:00.000Z"],"https://openai.com/blog/variational-option-discovery-algorithms","src/content/posts/2018-07-26-variational-option-discovery-algorithms.md","e72ec9f98cdab15d",{"html":25,"metadata":1815},{"headings":1816,"localImagePaths":1817,"remoteImagePaths":1818,"frontmatter":1819,"imagePaths":1821},[],[],[],{"title":1809,"description":25,"summary":25,"pubDate":1820,"source":19,"url":1811,"thumbnail":21},"Thu, 26 Jul 2018 07:00:00 GMT",[],"2018-07-26-variational-option-discovery-algorithms.md","2018-07-30-learning-dexterity",{"id":1823,"data":1825,"filePath":1830,"digest":1831,"rendered":1832,"legacyId":1840},{"title":1826,"description":1827,"summary":1827,"pubDate":1828,"source":19,"url":1829,"thumbnail":21},"Learning dexterity","We’ve trained a human-like robot hand to manipulate physical objects with unprecedented dexterity.",["Date","2018-07-30T07:00:00.000Z"],"https://openai.com/blog/learning-dexterity","src/content/posts/2018-07-30-learning-dexterity.md","873c14b7a7bef4c0",{"html":25,"metadata":1833},{"headings":1834,"localImagePaths":1835,"remoteImagePaths":1836,"frontmatter":1837,"imagePaths":1839},[],[],[],{"title":1826,"description":1827,"summary":1827,"pubDate":1838,"source":19,"url":1829,"thumbnail":21},"Mon, 30 Jul 2018 07:00:00 GMT",[],"2018-07-30-learning-dexterity.md","2018-08-06-openai-five-benchmark-results",{"id":1841,"data":1843,"filePath":1848,"digest":1849,"rendered":1850,"legacyId":1858},{"title":1844,"description":1845,"summary":1845,"pubDate":1846,"source":19,"url":1847,"thumbnail":21},"OpenAI Five Benchmark: Results","Yesterday, OpenAI Five won a best-of-three against a team of 99.95th percentile Dota players: Blitz, Cap, Fogged, Merlini, and MoonMeander—four of whom have played Dota professionally—in front of a live audience and 100,000 concurrent livestream viewers.",["Date","2018-08-06T07:00:00.000Z"],"https://openai.com/blog/openai-five-benchmark-results","src/content/posts/2018-08-06-openai-five-benchmark-results.md","6e2411f0c5721fae",{"html":25,"metadata":1851},{"headings":1852,"localImagePaths":1853,"remoteImagePaths":1854,"frontmatter":1855,"imagePaths":1857},[],[],[],{"title":1844,"description":1845,"summary":1845,"pubDate":1856,"source":19,"url":1847,"thumbnail":21},"Mon, 06 Aug 2018 07:00:00 GMT",[],"2018-08-06-openai-five-benchmark-results.md","2018-08-13-large-scale-study-of-curiosity-driven-learning",{"id":1859,"data":1861,"filePath":1865,"digest":1866,"rendered":1867,"legacyId":1875},{"title":1862,"description":25,"summary":25,"pubDate":1863,"source":19,"url":1864,"thumbnail":21},"Large-scale study of curiosity-driven learning",["Date","2018-08-13T07:00:00.000Z"],"https://openai.com/blog/large-scale-study-of-curiosity-driven-learning","src/content/posts/2018-08-13-large-scale-study-of-curiosity-driven-learning.md","b1f98243bdb52437",{"html":25,"metadata":1868},{"headings":1869,"localImagePaths":1870,"remoteImagePaths":1871,"frontmatter":1872,"imagePaths":1874},[],[],[],{"title":1862,"description":25,"summary":25,"pubDate":1873,"source":19,"url":1864,"thumbnail":21},"Mon, 13 Aug 2018 07:00:00 GMT",[],"2018-08-13-large-scale-study-of-curiosity-driven-learning.md","2018-08-23-the-international-2018-results",{"id":1876,"data":1878,"filePath":1883,"digest":1884,"rendered":1885,"legacyId":1893},{"title":1879,"description":1880,"summary":1880,"pubDate":1881,"source":19,"url":1882,"thumbnail":21},"The International 2018: Results","OpenAI Five lost two games against top Dota 2 players at The International in Vancouver this week, maintaining a good chance of winning for the first 20–35 minutes of both games.",["Date","2018-08-23T07:00:00.000Z"],"https://openai.com/blog/the-international-2018-results","src/content/posts/2018-08-23-the-international-2018-results.md","9537464fd8380bbf",{"html":25,"metadata":1886},{"headings":1887,"localImagePaths":1888,"remoteImagePaths":1889,"frontmatter":1890,"imagePaths":1892},[],[],[],{"title":1879,"description":1880,"summary":1880,"pubDate":1891,"source":19,"url":1882,"thumbnail":21},"Thu, 23 Aug 2018 07:00:00 GMT",[],"2018-08-23-the-international-2018-results.md","2018-09-10-openai-scholars-2018-final-projects",{"id":1894,"data":1896,"filePath":1901,"digest":1902,"rendered":1903,"legacyId":1911},{"title":1897,"description":1898,"summary":1898,"pubDate":1899,"source":19,"url":1900,"thumbnail":21},"OpenAI Scholars 2018: Final projects","Our first cohort of OpenAI Scholars has now completed the program.",["Date","2018-09-10T07:00:00.000Z"],"https://openai.com/blog/openai-scholars-2018-final-projects","src/content/posts/2018-09-10-openai-scholars-2018-final-projects.md","59d63a70a7bee41e",{"html":25,"metadata":1904},{"headings":1905,"localImagePaths":1906,"remoteImagePaths":1907,"frontmatter":1908,"imagePaths":1910},[],[],[],{"title":1897,"description":1898,"summary":1898,"pubDate":1909,"source":19,"url":1900,"thumbnail":21},"Mon, 10 Sep 2018 07:00:00 GMT",[],"2018-09-10-openai-scholars-2018-final-projects.md","2018-10-02-ffjord-free-form-continuous-dynamics-for-scalable-reversible-generative-models",{"id":1912,"data":1914,"filePath":1918,"digest":1919,"rendered":1920,"legacyId":1928},{"title":1915,"description":25,"summary":25,"pubDate":1916,"source":19,"url":1917,"thumbnail":21},"FFJORD: Free-form continuous dynamics for scalable reversible generative models",["Date","2018-10-02T07:00:00.000Z"],"https://openai.com/blog/ffjord","src/content/posts/2018-10-02-ffjord-free-form-continuous-dynamics-for-scalable-reversible-generative-models.md","71503609bba8c340",{"html":25,"metadata":1921},{"headings":1922,"localImagePaths":1923,"remoteImagePaths":1924,"frontmatter":1925,"imagePaths":1927},[],[],[],{"title":1915,"description":25,"summary":25,"pubDate":1926,"source":19,"url":1917,"thumbnail":21},"Tue, 02 Oct 2018 07:00:00 GMT",[],"2018-10-02-ffjord-free-form-continuous-dynamics-for-scalable-reversible-generative-models.md","2018-10-11-openai-scholars-2019-applications-open",{"id":1929,"data":1931,"filePath":1936,"digest":1937,"rendered":1938,"legacyId":1946},{"title":1932,"description":1933,"summary":1933,"pubDate":1934,"source":19,"url":1935,"thumbnail":21},"OpenAI Scholars 2019: Applications open","We are now accepting applications for our second cohort of OpenAI Scholars, a program where we provide 6–10 stipends and mentorship to individuals from underrepresented groups to study deep learning full-time for 3 months and open-source a project.",["Date","2018-10-11T07:00:00.000Z"],"https://openai.com/blog/openai-scholars-2019","src/content/posts/2018-10-11-openai-scholars-2019-applications-open.md","750aa187c1c46d04",{"html":25,"metadata":1939},{"headings":1940,"localImagePaths":1941,"remoteImagePaths":1942,"frontmatter":1943,"imagePaths":1945},[],[],[],{"title":1932,"description":1933,"summary":1933,"pubDate":1944,"source":19,"url":1935,"thumbnail":21},"Thu, 11 Oct 2018 07:00:00 GMT",[],"2018-10-11-openai-scholars-2019-applications-open.md","2018-10-09-openai-fellows-winter-2019-interns-summer-2019",{"id":1947,"data":1949,"filePath":1954,"digest":1955,"rendered":1956,"legacyId":1964},{"title":1950,"description":1951,"summary":1951,"pubDate":1952,"source":19,"url":1953,"thumbnail":21},"OpenAI Fellows Winter 2019 & Interns Summer 2019","We are now accepting applications for OpenAI Fellows and Interns for 2019.",["Date","2018-10-09T07:00:00.000Z"],"https://openai.com/blog/openai-fellows-interns-2019","src/content/posts/2018-10-09-openai-fellows-winter-2019-interns-summer-2019.md","a381a10a38db07a2",{"html":25,"metadata":1957},{"headings":1958,"localImagePaths":1959,"remoteImagePaths":1960,"frontmatter":1961,"imagePaths":1963},[],[],[],{"title":1950,"description":1951,"summary":1951,"pubDate":1962,"source":19,"url":1953,"thumbnail":21},"Tue, 09 Oct 2018 07:00:00 GMT",[],"2018-10-09-openai-fellows-winter-2019-interns-summer-2019.md","2018-10-22-learning-complex-goals-with-iterated-amplification",{"id":1965,"data":1967,"filePath":1972,"digest":1973,"rendered":1974,"legacyId":1982},{"title":1968,"description":1969,"summary":1969,"pubDate":1970,"source":19,"url":1971,"thumbnail":21},"Learning complex goals with iterated amplification","We’re proposing an AI safety technique called iterated amplification that lets us specify complicated behaviors and goals that are beyond human scale, by demonstrating how to decompose a task into simpler sub-tasks, rather than by providing labeled data or a reward function. Although this idea is in its very early stages and we have only completed experiments on simple toy algorithmic domains, we’ve decided to present it in its preliminary state because we think it could prove to be a scalable approach to AI safety.",["Date","2018-10-22T07:00:00.000Z"],"https://openai.com/blog/learning-complex-goals-with-iterated-amplification","src/content/posts/2018-10-22-learning-complex-goals-with-iterated-amplification.md","66182fe8ece42586",{"html":25,"metadata":1975},{"headings":1976,"localImagePaths":1977,"remoteImagePaths":1978,"frontmatter":1979,"imagePaths":1981},[],[],[],{"title":1968,"description":1969,"summary":1969,"pubDate":1980,"source":19,"url":1971,"thumbnail":21},"Mon, 22 Oct 2018 07:00:00 GMT",[],"2018-10-22-learning-complex-goals-with-iterated-amplification.md","2018-11-05-plan-online-learn-offline-efficient-learning-and-exploration-via-model-based-control",{"id":1983,"data":1985,"filePath":1989,"digest":1990,"rendered":1991,"legacyId":1999},{"title":1986,"description":25,"summary":25,"pubDate":1987,"source":19,"url":1988,"thumbnail":21},"Plan online, learn offline: Efficient learning and exploration via model-based control",["Date","2018-11-05T08:00:00.000Z"],"https://openai.com/blog/plan-online-learn-offline","src/content/posts/2018-11-05-plan-online-learn-offline-efficient-learning-and-exploration-via-model-based-control.md","a526b3622c681279",{"html":25,"metadata":1992},{"headings":1993,"localImagePaths":1994,"remoteImagePaths":1995,"frontmatter":1996,"imagePaths":1998},[],[],[],{"title":1986,"description":25,"summary":25,"pubDate":1997,"source":19,"url":1988,"thumbnail":21},"Mon, 05 Nov 2018 08:00:00 GMT",[],"2018-11-05-plan-online-learn-offline-efficient-learning-and-exploration-via-model-based-control.md","2018-10-31-reinforcement-learning-with-prediction-based-rewards",{"id":2000,"data":2002,"filePath":2007,"digest":2008,"rendered":2009,"legacyId":2017},{"title":2003,"description":2004,"summary":2004,"pubDate":2005,"source":19,"url":2006,"thumbnail":21},"Reinforcement learning with prediction-based rewards","We’ve developed Random Network Distillation (RND), a prediction-based method for encouraging reinforcement learning agents to explore their environments through curiosity, which for the first time exceeds average human performance on Montezuma’s Revenge.",["Date","2018-10-31T07:00:00.000Z"],"https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards","src/content/posts/2018-10-31-reinforcement-learning-with-prediction-based-rewards.md","6443a8633def43a4",{"html":25,"metadata":2010},{"headings":2011,"localImagePaths":2012,"remoteImagePaths":2013,"frontmatter":2014,"imagePaths":2016},[],[],[],{"title":2003,"description":2004,"summary":2004,"pubDate":2015,"source":19,"url":2006,"thumbnail":21},"Wed, 31 Oct 2018 07:00:00 GMT",[],"2018-10-31-reinforcement-learning-with-prediction-based-rewards.md","2018-11-07-learning-concepts-with-energy-functions",{"id":2018,"data":2020,"filePath":2025,"digest":2026,"rendered":2027,"legacyId":2035},{"title":2021,"description":2022,"summary":2022,"pubDate":2023,"source":19,"url":2024,"thumbnail":21},"Learning concepts with energy functions","We’ve developed an energy-based model that can quickly learn to identify and generate instances of concepts, such as near, above, between, closest, and furthest, expressed as sets of 2d points. Our model learns these concepts after only five demonstrations. We also show cross-domain transfer: we use concepts learned in a 2d particle environment to solve tasks on a 3-dimensional physics-based robot.",["Date","2018-11-07T08:00:00.000Z"],"https://openai.com/blog/learning-concepts-with-energy-functions","src/content/posts/2018-11-07-learning-concepts-with-energy-functions.md","33192b4b942a9352",{"html":25,"metadata":2028},{"headings":2029,"localImagePaths":2030,"remoteImagePaths":2031,"frontmatter":2032,"imagePaths":2034},[],[],[],{"title":2021,"description":2022,"summary":2022,"pubDate":2033,"source":19,"url":2024,"thumbnail":21},"Wed, 07 Nov 2018 08:00:00 GMT",[],"2018-11-07-learning-concepts-with-energy-functions.md","2018-12-06-quantifying-generalization-in-reinforcement-learning",{"id":2036,"data":2038,"filePath":2043,"digest":2044,"rendered":2045,"legacyId":2053},{"title":2039,"description":2040,"summary":2040,"pubDate":2041,"source":19,"url":2042,"thumbnail":21},"Quantifying generalization in reinforcement learning","We’re releasing CoinRun, a training environment which provides a metric for an agent’s ability to transfer its experience to novel situations and has already helped clarify a longstanding puzzle in reinforcement learning. CoinRun strikes a desirable balance in complexity: the environment is simpler than traditional platformer games like Sonic the Hedgehog but still poses a worthy generalization challenge for state of the art algorithms.",["Date","2018-12-06T08:00:00.000Z"],"https://openai.com/blog/quantifying-generalization-in-reinforcement-learning","src/content/posts/2018-12-06-quantifying-generalization-in-reinforcement-learning.md","79f55e7a64f06b9c",{"html":25,"metadata":2046},{"headings":2047,"localImagePaths":2048,"remoteImagePaths":2049,"frontmatter":2050,"imagePaths":2052},[],[],[],{"title":2039,"description":2040,"summary":2040,"pubDate":2051,"source":19,"url":2042,"thumbnail":21},"Thu, 06 Dec 2018 08:00:00 GMT",[],"2018-12-06-quantifying-generalization-in-reinforcement-learning.md","2018-11-08-spinning-up-in-deep-rl",{"id":2054,"data":2056,"filePath":2061,"digest":2062,"rendered":2063,"legacyId":2071},{"title":2057,"description":2058,"summary":2058,"pubDate":2059,"source":19,"url":2060,"thumbnail":21},"Spinning Up in Deep RL","We’re releasing Spinning Up in Deep RL, an educational resource designed to let anyone learn to become a skilled practitioner in deep reinforcement learning. Spinning Up consists of crystal-clear examples of RL code, educational exercises, documentation, and tutorials.",["Date","2018-11-08T08:00:00.000Z"],"https://openai.com/blog/spinning-up-in-deep-rl","src/content/posts/2018-11-08-spinning-up-in-deep-rl.md","01052762391073c1",{"html":25,"metadata":2064},{"headings":2065,"localImagePaths":2066,"remoteImagePaths":2067,"frontmatter":2068,"imagePaths":2070},[],[],[],{"title":2057,"description":2058,"summary":2058,"pubDate":2069,"source":19,"url":2060,"thumbnail":21},"Thu, 08 Nov 2018 08:00:00 GMT",[],"2018-11-08-spinning-up-in-deep-rl.md","2018-12-14-how-ai-training-scales",{"id":2072,"data":2074,"filePath":2079,"digest":2080,"rendered":2081,"legacyId":2089},{"title":2075,"description":2076,"summary":2076,"pubDate":2077,"source":19,"url":2078,"thumbnail":21},"How AI training scales","We’ve discovered that the gradient noise scale, a simple statistical metric, predicts the parallelizability of neural network training on a wide range of tasks. Since complex tasks tend to have noisier gradients, increasingly large batch sizes are likely to become useful in the future, removing one potential limit to further growth of AI systems. More broadly, these results show that neural network training need not be considered a mysterious art, but can be rigorized and systematized.",["Date","2018-12-14T08:00:00.000Z"],"https://openai.com/blog/how-ai-training-scales","src/content/posts/2018-12-14-how-ai-training-scales.md","38fd3ad34f023fe3",{"html":25,"metadata":2082},{"headings":2083,"localImagePaths":2084,"remoteImagePaths":2085,"frontmatter":2086,"imagePaths":2088},[],[],[],{"title":2075,"description":2076,"summary":2076,"pubDate":2087,"source":19,"url":2078,"thumbnail":21},"Fri, 14 Dec 2018 08:00:00 GMT",[],"2018-12-14-how-ai-training-scales.md","2018-12-19-openai-fellows-summer-2018-final-projects",{"id":2090,"data":2092,"filePath":2097,"digest":2098,"rendered":2099,"legacyId":2107},{"title":2093,"description":2094,"summary":2094,"pubDate":2095,"source":19,"url":2096,"thumbnail":21},"OpenAI Fellows Summer 2018: Final projects","Our first cohort of OpenAI Fellows has concluded, with each Fellow going from a machine learning beginner to core OpenAI contributor in the course of a 6-month apprenticeship.",["Date","2018-12-19T08:00:00.000Z"],"https://openai.com/blog/openai-summer-fellows-2018","src/content/posts/2018-12-19-openai-fellows-summer-2018-final-projects.md","2ad78b4ceec12353",{"html":25,"metadata":2100},{"headings":2101,"localImagePaths":2102,"remoteImagePaths":2103,"frontmatter":2104,"imagePaths":2106},[],[],[],{"title":2093,"description":2094,"summary":2094,"pubDate":2105,"source":19,"url":2096,"thumbnail":21},"Wed, 19 Dec 2018 08:00:00 GMT",[],"2018-12-19-openai-fellows-summer-2018-final-projects.md","2019-02-04-computational-limitations-in-robust-classification-and-win-win-results",{"id":2108,"data":2110,"filePath":2114,"digest":2115,"rendered":2116,"legacyId":2124},{"title":2111,"description":25,"summary":25,"pubDate":2112,"source":19,"url":2113,"thumbnail":21},"Computational limitations in robust classification and win-win results",["Date","2019-02-04T08:00:00.000Z"],"https://openai.com/blog/computational-limitations-in-robust-classification-and-win-win-results","src/content/posts/2019-02-04-computational-limitations-in-robust-classification-and-win-win-results.md","837703f6c1c919a8",{"html":25,"metadata":2117},{"headings":2118,"localImagePaths":2119,"remoteImagePaths":2120,"frontmatter":2121,"imagePaths":2123},[],[],[],{"title":2111,"description":25,"summary":25,"pubDate":2122,"source":19,"url":2113,"thumbnail":21},"Mon, 04 Feb 2019 08:00:00 GMT",[],"2019-02-04-computational-limitations-in-robust-classification-and-win-win-results.md","2019-02-14-better-language-models-and-their-implications",{"id":2125,"data":2127,"filePath":2132,"digest":2133,"rendered":2134,"legacyId":2142},{"title":2128,"description":2129,"summary":2129,"pubDate":2130,"source":19,"url":2131,"thumbnail":21},"Better language models and their implications","We’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training.",["Date","2019-02-14T08:00:00.000Z"],"https://openai.com/blog/better-language-models","src/content/posts/2019-02-14-better-language-models-and-their-implications.md","7b93ab86866d4402",{"html":25,"metadata":2135},{"headings":2136,"localImagePaths":2137,"remoteImagePaths":2138,"frontmatter":2139,"imagePaths":2141},[],[],[],{"title":2128,"description":2129,"summary":2129,"pubDate":2140,"source":19,"url":2131,"thumbnail":21},"Thu, 14 Feb 2019 08:00:00 GMT",[],"2019-02-14-better-language-models-and-their-implications.md","2019-02-26-spinning-up-in-deep-rl-workshop-review",{"id":2143,"data":2145,"filePath":2150,"digest":2151,"rendered":2152,"legacyId":2160},{"title":2146,"description":2147,"summary":2147,"pubDate":2148,"source":19,"url":2149,"thumbnail":21},"Spinning Up in Deep RL: Workshop review","On February 2, we held our first Spinning Up Workshop as part of our new education initiative at OpenAI.",["Date","2019-02-26T08:00:00.000Z"],"https://openai.com/blog/spinning-up-in-deep-rl-workshop-review","src/content/posts/2019-02-26-spinning-up-in-deep-rl-workshop-review.md","f3b679cff456e381",{"html":25,"metadata":2153},{"headings":2154,"localImagePaths":2155,"remoteImagePaths":2156,"frontmatter":2157,"imagePaths":2159},[],[],[],{"title":2146,"description":2147,"summary":2147,"pubDate":2158,"source":19,"url":2149,"thumbnail":21},"Tue, 26 Feb 2019 08:00:00 GMT",[],"2019-02-26-spinning-up-in-deep-rl-workshop-review.md","2019-02-19-ai-safety-needs-social-scientists",{"id":2161,"data":2163,"filePath":2168,"digest":2169,"rendered":2170,"legacyId":2178},{"title":2164,"description":2165,"summary":2165,"pubDate":2166,"source":19,"url":2167,"thumbnail":21},"AI safety needs social scientists","We’ve written a paper arguing that long-term AI safety research needs social scientists to ensure AI alignment algorithms succeed when actual humans are involved. Properly aligning advanced AI systems with human values requires resolving many uncertainties related to the psychology of human rationality, emotion, and biases. The aim of this paper is to spark further collaboration between machine learning and social science researchers, and we plan to hire social scientists to work on this full time at OpenAI.",["Date","2019-02-19T08:00:00.000Z"],"https://openai.com/blog/ai-safety-needs-social-scientists","src/content/posts/2019-02-19-ai-safety-needs-social-scientists.md","93b44d5bd3230620",{"html":25,"metadata":2171},{"headings":2172,"localImagePaths":2173,"remoteImagePaths":2174,"frontmatter":2175,"imagePaths":2177},[],[],[],{"title":2164,"description":2165,"summary":2165,"pubDate":2176,"source":19,"url":2167,"thumbnail":21},"Tue, 19 Feb 2019 08:00:00 GMT",[],"2019-02-19-ai-safety-needs-social-scientists.md","2019-03-04-neural-mmo-a-massively-multiagent-game-environment",{"id":2179,"data":2181,"filePath":2186,"digest":2187,"rendered":2188,"legacyId":2196},{"title":2182,"description":2183,"summary":2183,"pubDate":2184,"source":19,"url":2185,"thumbnail":21},"Neural MMO: A massively multiagent game environment","We’re releasing a Neural MMO, a massively multiagent game environment for reinforcement learning agents. Our platform supports a large, variable number of agents within a persistent and open-ended task. The inclusion of many agents and species leads to better exploration, divergent niche formation, and greater overall competence.",["Date","2019-03-04T08:00:00.000Z"],"https://openai.com/blog/neural-mmo","src/content/posts/2019-03-04-neural-mmo-a-massively-multiagent-game-environment.md","25c90638de4a621e",{"html":25,"metadata":2189},{"headings":2190,"localImagePaths":2191,"remoteImagePaths":2192,"frontmatter":2193,"imagePaths":2195},[],[],[],{"title":2182,"description":2183,"summary":2183,"pubDate":2194,"source":19,"url":2185,"thumbnail":21},"Mon, 04 Mar 2019 08:00:00 GMT",[],"2019-03-04-neural-mmo-a-massively-multiagent-game-environment.md","2019-03-06-introducing-activation-atlases",{"id":2197,"data":2199,"filePath":2204,"digest":2205,"rendered":2206,"legacyId":2214},{"title":2200,"description":2201,"summary":2201,"pubDate":2202,"source":19,"url":2203,"thumbnail":21},"Introducing Activation Atlases","We’ve created activation atlases (in collaboration with Google researchers), a new technique for visualizing what interactions between neurons can represent. As AI systems are deployed in increasingly sensitive contexts, having a better understanding of their internal decision-making processes will let us identify weaknesses and investigate failures.",["Date","2019-03-06T08:00:00.000Z"],"https://openai.com/blog/introducing-activation-atlases","src/content/posts/2019-03-06-introducing-activation-atlases.md","c10a8035a65ed457",{"html":25,"metadata":2207},{"headings":2208,"localImagePaths":2209,"remoteImagePaths":2210,"frontmatter":2211,"imagePaths":2213},[],[],[],{"title":2200,"description":2201,"summary":2201,"pubDate":2212,"source":19,"url":2203,"thumbnail":21},"Wed, 06 Mar 2019 08:00:00 GMT",[],"2019-03-06-introducing-activation-atlases.md","2019-03-11-openai-lp",{"id":2215,"data":2217,"filePath":2222,"digest":2223,"rendered":2224,"legacyId":2232},{"title":2218,"description":2219,"summary":2219,"pubDate":2220,"source":19,"url":2221,"thumbnail":21},"OpenAI LP","We’ve created OpenAI LP, a new “capped-profit” company that allows us to rapidly increase our investments in compute and talent while including checks and balances to actualize our mission.",["Date","2019-03-11T07:00:00.000Z"],"https://openai.com/blog/openai-lp","src/content/posts/2019-03-11-openai-lp.md","c236b47c0a27e7fe",{"html":25,"metadata":2225},{"headings":2226,"localImagePaths":2227,"remoteImagePaths":2228,"frontmatter":2229,"imagePaths":2231},[],[],[],{"title":2218,"description":2219,"summary":2219,"pubDate":2230,"source":19,"url":2221,"thumbnail":21},"Mon, 11 Mar 2019 07:00:00 GMT",[],"2019-03-11-openai-lp.md","2019-03-13-openai-scholars-2019-meet-our-scholars",{"id":2233,"data":2235,"filePath":2240,"digest":2241,"rendered":2242,"legacyId":2250},{"title":2236,"description":2237,"summary":2237,"pubDate":2238,"source":19,"url":2239,"thumbnail":21},"OpenAI Scholars 2019: Meet our Scholars","Our class of eight scholars (out of 550 applicants) brings together collective expertise in literature, philosophy, cell biology, statistics, economics, quantum physics, and business innovation.",["Date","2019-03-13T07:00:00.000Z"],"https://openai.com/blog/openai-scholars-2019-meet-our-scholars","src/content/posts/2019-03-13-openai-scholars-2019-meet-our-scholars.md","9f9ff591bc311f77",{"html":25,"metadata":2243},{"headings":2244,"localImagePaths":2245,"remoteImagePaths":2246,"frontmatter":2247,"imagePaths":2249},[],[],[],{"title":2236,"description":2237,"summary":2237,"pubDate":2248,"source":19,"url":2239,"thumbnail":21},"Wed, 13 Mar 2019 07:00:00 GMT",[],"2019-03-13-openai-scholars-2019-meet-our-scholars.md","2019-03-21-implicit-generation-and-generalization-methods-for-energy-based-models",{"id":2251,"data":2253,"filePath":2258,"digest":2259,"rendered":2260,"legacyId":2268},{"title":2254,"description":2255,"summary":2255,"pubDate":2256,"source":19,"url":2257,"thumbnail":21},"Implicit generation and generalization methods for energy-based models","We’ve made progress towards stable and scalable training of energy-based models (EBMs) resulting in better sample quality and generalization ability than existing models. Generation in EBMs spends more compute to continually refine its answers and doing so can generate samples competitive with GANs at low temperatures, while also having mode coverage guarantees of likelihood-based models. We hope these findings stimulate further research into this promising class of models.",["Date","2019-03-21T07:00:00.000Z"],"https://openai.com/blog/energy-based-models","src/content/posts/2019-03-21-implicit-generation-and-generalization-methods-for-energy-based-models.md","2f82cfaa90b0f3e3",{"html":25,"metadata":2261},{"headings":2262,"localImagePaths":2263,"remoteImagePaths":2264,"frontmatter":2265,"imagePaths":2267},[],[],[],{"title":2254,"description":2255,"summary":2255,"pubDate":2266,"source":19,"url":2257,"thumbnail":21},"Thu, 21 Mar 2019 07:00:00 GMT",[],"2019-03-21-implicit-generation-and-generalization-methods-for-energy-based-models.md","2019-04-15-openai-five-defeats-dota-2-world-champions",{"id":2269,"data":2271,"filePath":2276,"digest":2277,"rendered":2278,"legacyId":2286},{"title":2272,"description":2273,"summary":2273,"pubDate":2274,"source":19,"url":2275,"thumbnail":21},"OpenAI Five defeats Dota 2 world champions","OpenAI Five is the first AI to beat the world champions in an esports game, having won two back-to-back games versus the world champion Dota 2 team, OG, at Finals this weekend. Both OpenAI Five and DeepMind’s AlphaStar had previously beaten good pros privately but lost their live pro matches, making this also the first time an AI has beaten esports pros on livestream.",["Date","2019-04-15T07:00:00.000Z"],"https://openai.com/blog/openai-five-defeats-dota-2-world-champions","src/content/posts/2019-04-15-openai-five-defeats-dota-2-world-champions.md","22388a01d1ad3942",{"html":25,"metadata":2279},{"headings":2280,"localImagePaths":2281,"remoteImagePaths":2282,"frontmatter":2283,"imagePaths":2285},[],[],[],{"title":2272,"description":2273,"summary":2273,"pubDate":2284,"source":19,"url":2275,"thumbnail":21},"Mon, 15 Apr 2019 07:00:00 GMT",[],"2019-04-15-openai-five-defeats-dota-2-world-champions.md","2019-03-26-openai-five-finals",{"id":2287,"data":2289,"filePath":2294,"digest":2295,"rendered":2296,"legacyId":2304},{"title":2290,"description":2291,"summary":2291,"pubDate":2292,"source":19,"url":2293,"thumbnail":21},"OpenAI Five Finals","We’ll be holding our final live event for OpenAI Five at 11:30am PT on April 13.",["Date","2019-03-26T07:00:00.000Z"],"https://openai.com/blog/openai-five-finals","src/content/posts/2019-03-26-openai-five-finals.md","cea4ab13f7b1f6ee",{"html":25,"metadata":2297},{"headings":2298,"localImagePaths":2299,"remoteImagePaths":2300,"frontmatter":2301,"imagePaths":2303},[],[],[],{"title":2290,"description":2291,"summary":2291,"pubDate":2302,"source":19,"url":2293,"thumbnail":21},"Tue, 26 Mar 2019 07:00:00 GMT",[],"2019-03-26-openai-five-finals.md","2019-04-23-generative-modeling-with-sparse-transformers",{"id":2305,"data":2307,"filePath":2312,"digest":2313,"rendered":2314,"legacyId":2322},{"title":2308,"description":2309,"summary":2309,"pubDate":2310,"source":19,"url":2311,"thumbnail":21},"Generative modeling with sparse transformers","We’ve developed the Sparse Transformer, a deep neural network which sets new records at predicting what comes next in a sequence—whether text, images, or sound. It uses an algorithmic improvement of the attention mechanism to extract patterns from sequences 30x longer than possible previously.",["Date","2019-04-23T07:00:00.000Z"],"https://openai.com/blog/sparse-transformer","src/content/posts/2019-04-23-generative-modeling-with-sparse-transformers.md","9cc46963f3608e6e",{"html":25,"metadata":2315},{"headings":2316,"localImagePaths":2317,"remoteImagePaths":2318,"frontmatter":2319,"imagePaths":2321},[],[],[],{"title":2308,"description":2309,"summary":2309,"pubDate":2320,"source":19,"url":2311,"thumbnail":21},"Tue, 23 Apr 2019 07:00:00 GMT",[],"2019-04-23-generative-modeling-with-sparse-transformers.md","2019-04-25-musenet",{"id":2323,"data":2325,"filePath":2330,"digest":2331,"rendered":2332,"legacyId":2340},{"title":2326,"description":2327,"summary":2327,"pubDate":2328,"source":19,"url":2329,"thumbnail":21},"MuseNet","We’ve created MuseNet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and can combine styles from country to Mozart to the Beatles. MuseNet was not explicitly programmed with our understanding of music, but instead discovered patterns of harmony, rhythm, and style by learning to predict the next token in hundreds of thousands of MIDI files. MuseNet uses the same general-purpose unsupervised technology as GPT-2, a large-scale transformer model trained to predict the next token in a sequence, whether audio or text.",["Date","2019-04-25T07:00:00.000Z"],"https://openai.com/blog/musenet","src/content/posts/2019-04-25-musenet.md","625f2579455bb08a",{"html":25,"metadata":2333},{"headings":2334,"localImagePaths":2335,"remoteImagePaths":2336,"frontmatter":2337,"imagePaths":2339},[],[],[],{"title":2326,"description":2327,"summary":2327,"pubDate":2338,"source":19,"url":2329,"thumbnail":21},"Thu, 25 Apr 2019 07:00:00 GMT",[],"2019-04-25-musenet.md","2019-05-03-transfer-of-adversarial-robustness-between-perturbation-types",{"id":2341,"data":2343,"filePath":2347,"digest":2348,"rendered":2349,"legacyId":2357},{"title":2344,"description":25,"summary":25,"pubDate":2345,"source":19,"url":2346,"thumbnail":21},"Transfer of adversarial robustness between perturbation types",["Date","2019-05-03T07:00:00.000Z"],"https://openai.com/blog/transfer-of-adversarial-robustness-between-perturbation-types","src/content/posts/2019-05-03-transfer-of-adversarial-robustness-between-perturbation-types.md","8c204b87e4947826",{"html":25,"metadata":2350},{"headings":2351,"localImagePaths":2352,"remoteImagePaths":2353,"frontmatter":2354,"imagePaths":2356},[],[],[],{"title":2344,"description":25,"summary":25,"pubDate":2355,"source":19,"url":2346,"thumbnail":21},"Fri, 03 May 2019 07:00:00 GMT",[],"2019-05-03-transfer-of-adversarial-robustness-between-perturbation-types.md","2019-05-17-openai-fellows-fall-2018-final-projects",{"id":2358,"data":2360,"filePath":2365,"digest":2366,"rendered":2367,"legacyId":2375},{"title":2361,"description":2362,"summary":2362,"pubDate":2363,"source":19,"url":2364,"thumbnail":21},"OpenAI Fellows Fall 2018: Final projects","Our second class of OpenAI Fellows has wrapped up, with each Fellow going from a machine learning beginner to core OpenAI contributor in the course of a 6-month apprenticeship. We are currently reviewing applications on a rolling basis for our next round of OpenAI Fellows Summer 2019.",["Date","2019-05-17T07:00:00.000Z"],"https://openai.com/blog/openai-fellows-fall-2018","src/content/posts/2019-05-17-openai-fellows-fall-2018-final-projects.md","e95bb7c3e77e2f24",{"html":25,"metadata":2368},{"headings":2369,"localImagePaths":2370,"remoteImagePaths":2371,"frontmatter":2372,"imagePaths":2374},[],[],[],{"title":2361,"description":2362,"summary":2362,"pubDate":2373,"source":19,"url":2364,"thumbnail":21},"Fri, 17 May 2019 07:00:00 GMT",[],"2019-05-17-openai-fellows-fall-2018-final-projects.md","2019-06-05-openai-robotics-symposium-2019",{"id":2376,"data":2378,"filePath":2383,"digest":2384,"rendered":2385,"legacyId":2393},{"title":2379,"description":2380,"summary":2380,"pubDate":2381,"source":19,"url":2382,"thumbnail":21},"OpenAI Robotics Symposium 2019","We hosted the first OpenAI Robotics Symposium on April 27, 2019.",["Date","2019-06-05T07:00:00.000Z"],"https://openai.com/blog/symposium-2019","src/content/posts/2019-06-05-openai-robotics-symposium-2019.md","3d8ca937ad50df3a",{"html":25,"metadata":2386},{"headings":2387,"localImagePaths":2388,"remoteImagePaths":2389,"frontmatter":2390,"imagePaths":2392},[],[],[],{"title":2379,"description":2380,"summary":2380,"pubDate":2391,"source":19,"url":2382,"thumbnail":21},"Wed, 05 Jun 2019 07:00:00 GMT",[],"2019-06-05-openai-robotics-symposium-2019.md","2019-05-23-openai-scholars-2019-final-projects",{"id":2394,"data":2396,"filePath":2401,"digest":2402,"rendered":2403,"legacyId":2411},{"title":2397,"description":2398,"summary":2398,"pubDate":2399,"source":19,"url":2400,"thumbnail":21},"OpenAI Scholars 2019: Final projects","Our second class of OpenAI Scholars has concluded, with all eight scholars producing an exciting final project showcased at Scholars Demo Day at OpenAI.",["Date","2019-05-23T07:00:00.000Z"],"https://openai.com/blog/openai-scholars-2019-final-projects","src/content/posts/2019-05-23-openai-scholars-2019-final-projects.md","351d3aa7abf0de9b",{"html":25,"metadata":2404},{"headings":2405,"localImagePaths":2406,"remoteImagePaths":2407,"frontmatter":2408,"imagePaths":2410},[],[],[],{"title":2397,"description":2398,"summary":2398,"pubDate":2409,"source":19,"url":2400,"thumbnail":21},"Thu, 23 May 2019 07:00:00 GMT",[],"2019-05-23-openai-scholars-2019-final-projects.md","2019-07-10-why-responsible-ai-development-needs-cooperation-on-safety",{"id":2412,"data":2414,"filePath":2419,"digest":2420,"rendered":2421,"legacyId":2429},{"title":2415,"description":2416,"summary":2416,"pubDate":2417,"source":19,"url":2418,"thumbnail":21},"Why responsible AI development needs cooperation on safety","We’ve written a policy research paper identifying four strategies that can be used today to improve the likelihood of long-term industry cooperation on safety norms in AI: communicating risks and benefits, technical collaboration, increased transparency, and incentivizing standards. Our analysis shows that industry cooperation on safety will be instrumental in ensuring that AI systems are safe and beneficial, but competitive pressures could lead to a collective action problem, potentially causing AI companies to under-invest in safety. We hope these strategies will encourage greater cooperation on the safe development of AI and lead to better global outcomes of AI.",["Date","2019-07-10T07:00:00.000Z"],"https://openai.com/blog/cooperation-on-safety","src/content/posts/2019-07-10-why-responsible-ai-development-needs-cooperation-on-safety.md","98bc6014abefb526",{"html":25,"metadata":2422},{"headings":2423,"localImagePaths":2424,"remoteImagePaths":2425,"frontmatter":2426,"imagePaths":2428},[],[],[],{"title":2415,"description":2416,"summary":2416,"pubDate":2427,"source":19,"url":2418,"thumbnail":21},"Wed, 10 Jul 2019 07:00:00 GMT",[],"2019-07-10-why-responsible-ai-development-needs-cooperation-on-safety.md","2019-07-22-microsoft-invests-in-and-partners-with-openai-to-support-us-building-beneficial-agi",{"id":2430,"data":2432,"filePath":2437,"digest":2438,"rendered":2439,"legacyId":2447},{"title":2433,"description":2434,"summary":2434,"pubDate":2435,"source":19,"url":2436,"thumbnail":21},"Microsoft invests in and partners with OpenAI to support us building beneficial AGI","Microsoft is investing $1 billion in OpenAI to support us building artificial general intelligence (AGI) with widely distributed economic benefits. We’re partnering to develop a hardware and software platform within Microsoft Azure which will scale to AGI. We’ll jointly develop new Azure AI supercomputing technologies, and Microsoft will become our exclusive cloud provider—so we’ll be working hard together to further extend Microsoft Azure’s capabilities in large-scale AI systems.",["Date","2019-07-22T07:00:00.000Z"],"https://openai.com/blog/microsoft-invests-in-and-partners-with-openai","src/content/posts/2019-07-22-microsoft-invests-in-and-partners-with-openai-to-support-us-building-beneficial-agi.md","67bf43a7413cc8d7",{"html":25,"metadata":2440},{"headings":2441,"localImagePaths":2442,"remoteImagePaths":2443,"frontmatter":2444,"imagePaths":2446},[],[],[],{"title":2433,"description":2434,"summary":2434,"pubDate":2445,"source":19,"url":2436,"thumbnail":21},"Mon, 22 Jul 2019 07:00:00 GMT",[],"2019-07-22-microsoft-invests-in-and-partners-with-openai-to-support-us-building-beneficial-agi.md","2019-08-01-learning-day",{"id":2448,"data":2450,"filePath":2455,"digest":2456,"rendered":2457,"legacyId":2465},{"title":2451,"description":2452,"summary":2452,"pubDate":2453,"source":19,"url":2454,"thumbnail":21},"Learning Day","At OpenAI, each Thursday is Learning Day: a day where employees have the option to self-study technical skills that will make them better at their job but which aren’t being learned from daily work.",["Date","2019-08-01T07:00:00.000Z"],"https://openai.com/blog/learning-day","src/content/posts/2019-08-01-learning-day.md","5d65a8e24a83c9d0",{"html":25,"metadata":2458},{"headings":2459,"localImagePaths":2460,"remoteImagePaths":2461,"frontmatter":2462,"imagePaths":2464},[],[],[],{"title":2451,"description":2452,"summary":2452,"pubDate":2463,"source":19,"url":2454,"thumbnail":21},"Thu, 01 Aug 2019 07:00:00 GMT",[],"2019-08-01-learning-day.md","2019-08-20-gpt-2-6-month-follow-up",{"id":2466,"data":2468,"filePath":2473,"digest":2474,"rendered":2475,"legacyId":2483},{"title":2469,"description":2470,"summary":2470,"pubDate":2471,"source":19,"url":2472,"thumbnail":21},"GPT-2: 6-month follow-up","We’re releasing the 774 million parameter GPT-2 language model after the release of our small 124M model in February, staged release of our medium 355M model in May, and subsequent research with partners and the AI community into the model’s potential for misuse and societal benefit. We’re also releasing an open-source legal agreement to make it easier for organizations to initiate model-sharing partnerships with each other, and are publishing a technical report about our experience in coordinating with the wider AI research community on publication norms.",["Date","2019-08-20T07:00:00.000Z"],"https://openai.com/blog/gpt-2-6-month-follow-up","src/content/posts/2019-08-20-gpt-2-6-month-follow-up.md","380d96b2b0f0d67c",{"html":25,"metadata":2476},{"headings":2477,"localImagePaths":2478,"remoteImagePaths":2479,"frontmatter":2480,"imagePaths":2482},[],[],[],{"title":2469,"description":2470,"summary":2470,"pubDate":2481,"source":19,"url":2472,"thumbnail":21},"Tue, 20 Aug 2019 07:00:00 GMT",[],"2019-08-20-gpt-2-6-month-follow-up.md","2019-09-17-emergent-tool-use-from-multi-agent-interaction",{"id":2484,"data":2486,"filePath":2491,"digest":2492,"rendered":2493,"legacyId":2501},{"title":2487,"description":2488,"summary":2488,"pubDate":2489,"source":19,"url":2490,"thumbnail":21},"Emergent tool use from multi-agent interaction","We’ve observed agents discovering progressively more complex tool use while playing a simple game of hide-and-seek. Through training in our new simulated hide-and-seek environment, agents build a series of six distinct strategies and counterstrategies, some of which we did not know our environment supported. The self-supervised emergent complexity in this simple environment further suggests that multi-agent co-adaptation may one day produce extremely complex and intelligent behavior.",["Date","2019-09-17T07:00:00.000Z"],"https://openai.com/blog/emergent-tool-use","src/content/posts/2019-09-17-emergent-tool-use-from-multi-agent-interaction.md","2a870e935d961663",{"html":25,"metadata":2494},{"headings":2495,"localImagePaths":2496,"remoteImagePaths":2497,"frontmatter":2498,"imagePaths":2500},[],[],[],{"title":2487,"description":2488,"summary":2488,"pubDate":2499,"source":19,"url":2490,"thumbnail":21},"Tue, 17 Sep 2019 07:00:00 GMT",[],"2019-09-17-emergent-tool-use-from-multi-agent-interaction.md","2019-08-22-testing-robustness-against-unforeseen-adversaries",{"id":2502,"data":2504,"filePath":2509,"digest":2510,"rendered":2511,"legacyId":2519},{"title":2505,"description":2506,"summary":2506,"pubDate":2507,"source":19,"url":2508,"thumbnail":21},"Testing robustness against unforeseen adversaries","We’ve developed a method to assess whether a neural network classifier can reliably defend against adversarial attacks not seen during training. Our method yields a new metric, UAR (Unforeseen Attack Robustness), which evaluates the robustness of a single model against an unanticipated attack, and highlights the need to measure performance across a more diverse range of unforeseen attacks.",["Date","2019-08-22T07:00:00.000Z"],"https://openai.com/blog/testing-robustness","src/content/posts/2019-08-22-testing-robustness-against-unforeseen-adversaries.md","dcd152116271f829",{"html":25,"metadata":2512},{"headings":2513,"localImagePaths":2514,"remoteImagePaths":2515,"frontmatter":2516,"imagePaths":2518},[],[],[],{"title":2505,"description":2506,"summary":2506,"pubDate":2517,"source":19,"url":2508,"thumbnail":21},"Thu, 22 Aug 2019 07:00:00 GMT",[],"2019-08-22-testing-robustness-against-unforeseen-adversaries.md","2019-09-19-fine-tuning-gpt-2-from-human-preferences",{"id":2520,"data":2522,"filePath":2527,"digest":2528,"rendered":2529,"legacyId":2537},{"title":2523,"description":2524,"summary":2524,"pubDate":2525,"source":19,"url":2526,"thumbnail":21},"Fine-tuning GPT-2 from human preferences","We’ve fine-tuned the 774M parameter GPT-2 language model using human feedback for various tasks, successfully matching the preferences of the external human labelers, though those preferences did not always match our own. Specifically, for summarization tasks the labelers preferred sentences copied wholesale from the input (we’d only asked them to ensure accuracy), so our models learned to copy. Summarization required 60k human labels; simpler tasks which continue text in various styles required only 5k. Our motivation is to move safety techniques closer to the general task of “machines talking to humans,” which we believe is key to extracting information about human values.",["Date","2019-09-19T07:00:00.000Z"],"https://openai.com/blog/fine-tuning-gpt-2","src/content/posts/2019-09-19-fine-tuning-gpt-2-from-human-preferences.md","9423d828fab76d42",{"html":25,"metadata":2530},{"headings":2531,"localImagePaths":2532,"remoteImagePaths":2533,"frontmatter":2534,"imagePaths":2536},[],[],[],{"title":2523,"description":2524,"summary":2524,"pubDate":2535,"source":19,"url":2526,"thumbnail":21},"Thu, 19 Sep 2019 07:00:00 GMT",[],"2019-09-19-fine-tuning-gpt-2-from-human-preferences.md","2019-10-11-openai-scholars-2020-applications-open",{"id":2538,"data":2540,"filePath":2545,"digest":2546,"rendered":2547,"legacyId":2555},{"title":2541,"description":2542,"summary":2542,"pubDate":2543,"source":19,"url":2544,"thumbnail":21},"OpenAI Scholars 2020: Applications open","We are now accepting applications for our third class of OpenAI Scholars.",["Date","2019-10-11T07:00:00.000Z"],"https://openai.com/blog/openai-scholars-2020","src/content/posts/2019-10-11-openai-scholars-2020-applications-open.md","65ff4b3ff9cfbd68",{"html":25,"metadata":2548},{"headings":2549,"localImagePaths":2550,"remoteImagePaths":2551,"frontmatter":2552,"imagePaths":2554},[],[],[],{"title":2541,"description":2542,"summary":2542,"pubDate":2553,"source":19,"url":2544,"thumbnail":21},"Fri, 11 Oct 2019 07:00:00 GMT",[],"2019-10-11-openai-scholars-2020-applications-open.md","2019-10-15-solving-rubiks-cube-with-a-robot-hand",{"id":2556,"data":2558,"filePath":2563,"digest":2564,"rendered":2565,"legacyId":2573},{"title":2559,"description":2560,"summary":2560,"pubDate":2561,"source":19,"url":2562,"thumbnail":21},"Solving Rubik’s Cube with a robot hand","We’ve trained a pair of neural networks to solve the Rubik’s Cube with a human-like robot hand. The neural networks are trained entirely in simulation, using the same reinforcement learning code as OpenAI Five paired with a new technique called Automatic Domain Randomization (ADR). The system can handle situations it never saw during training, such as being prodded by a stuffed giraffe. This shows that reinforcement learning isn’t just a tool for virtual tasks, but can solve physical-world problems requiring unprecedented dexterity.",["Date","2019-10-15T07:00:00.000Z"],"https://openai.com/blog/solving-rubiks-cube","src/content/posts/2019-10-15-solving-rubiks-cube-with-a-robot-hand.md","bad98db84e329145",{"html":25,"metadata":2566},{"headings":2567,"localImagePaths":2568,"remoteImagePaths":2569,"frontmatter":2570,"imagePaths":2572},[],[],[],{"title":2559,"description":2560,"summary":2560,"pubDate":2571,"source":19,"url":2562,"thumbnail":21},"Tue, 15 Oct 2019 07:00:00 GMT",[],"2019-10-15-solving-rubiks-cube-with-a-robot-hand.md","2019-11-05-gpt-2-15b-release",{"id":2574,"data":2576,"filePath":2581,"digest":2582,"rendered":2583,"legacyId":2591},{"title":2577,"description":2578,"summary":2578,"pubDate":2579,"source":19,"url":2580,"thumbnail":21},"GPT-2: 1.5B release","As the final model release of GPT-2’s staged release, we’re releasing the largest version (1.5B parameters) of GPT-2 along with code and model weights to facilitate detection of outputs of GPT-2 models. While there have been larger language models released since August, we’ve continued with our original staged release plan in order to provide the community with a test case of a full staged release process. We hope that this test case will be useful to developers of future powerful models, and we’re actively continuing the conversation with the AI community on responsible publication.",["Date","2019-11-05T08:00:00.000Z"],"https://openai.com/blog/gpt-2-1-5b-release","src/content/posts/2019-11-05-gpt-2-15b-release.md","6c20cd08837a9581",{"html":25,"metadata":2584},{"headings":2585,"localImagePaths":2586,"remoteImagePaths":2587,"frontmatter":2588,"imagePaths":2590},[],[],[],{"title":2577,"description":2578,"summary":2578,"pubDate":2589,"source":19,"url":2580,"thumbnail":21},"Tue, 05 Nov 2019 08:00:00 GMT",[],"2019-11-05-gpt-2-15b-release.md","2019-11-21-benchmarking-safe-exploration-in-deep-reinforcement-learning",{"id":2592,"data":2594,"filePath":2598,"digest":2599,"rendered":2600,"legacyId":2608},{"title":2595,"description":25,"summary":25,"pubDate":2596,"source":19,"url":2597,"thumbnail":21},"Benchmarking safe exploration in deep reinforcement learning",["Date","2019-11-21T08:00:00.000Z"],"https://openai.com/blog/benchmarking-safe-exploration-in-deep-reinforcement-learning","src/content/posts/2019-11-21-benchmarking-safe-exploration-in-deep-reinforcement-learning.md","3176626cd6ae105a",{"html":25,"metadata":2601},{"headings":2602,"localImagePaths":2603,"remoteImagePaths":2604,"frontmatter":2605,"imagePaths":2607},[],[],[],{"title":2595,"description":25,"summary":25,"pubDate":2606,"source":19,"url":2597,"thumbnail":21},"Thu, 21 Nov 2019 08:00:00 GMT",[],"2019-11-21-benchmarking-safe-exploration-in-deep-reinforcement-learning.md","2019-12-03-procgen-benchmark",{"id":2609,"data":2611,"filePath":2616,"digest":2617,"rendered":2618,"legacyId":2626},{"title":2612,"description":2613,"summary":2613,"pubDate":2614,"source":19,"url":2615,"thumbnail":21},"Procgen Benchmark","We’re releasing Procgen Benchmark, 16 simple-to-use procedurally-generated environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills.",["Date","2019-12-03T08:00:00.000Z"],"https://openai.com/blog/procgen-benchmark","src/content/posts/2019-12-03-procgen-benchmark.md","f49f3b0bf61a1828",{"html":25,"metadata":2619},{"headings":2620,"localImagePaths":2621,"remoteImagePaths":2622,"frontmatter":2623,"imagePaths":2625},[],[],[],{"title":2612,"description":2613,"summary":2613,"pubDate":2624,"source":19,"url":2615,"thumbnail":21},"Tue, 03 Dec 2019 08:00:00 GMT",[],"2019-12-03-procgen-benchmark.md","2019-12-05-deep-double-descent",{"id":2627,"data":2629,"filePath":2634,"digest":2635,"rendered":2636,"legacyId":2644},{"title":2630,"description":2631,"summary":2631,"pubDate":2632,"source":19,"url":2633,"thumbnail":21},"Deep double descent","We show that the double descent phenomenon occurs in CNNs, ResNets, and transformers: performance first improves, then gets worse, and then improves again with increasing model size, data size, or training time. This effect is often avoided through careful regularization. While this behavior appears to be fairly universal, we don’t yet fully understand why it happens, and view further study of this phenomenon as an important research direction.",["Date","2019-12-05T08:00:00.000Z"],"https://openai.com/blog/deep-double-descent","src/content/posts/2019-12-05-deep-double-descent.md","eb1b63fd55f1b958",{"html":25,"metadata":2637},{"headings":2638,"localImagePaths":2639,"remoteImagePaths":2640,"frontmatter":2641,"imagePaths":2643},[],[],[],{"title":2630,"description":2631,"summary":2631,"pubDate":2642,"source":19,"url":2633,"thumbnail":21},"Thu, 05 Dec 2019 08:00:00 GMT",[],"2019-12-05-deep-double-descent.md","2019-12-13-dota-2-with-large-scale-deep-reinforcement-learning",{"id":2645,"data":2647,"filePath":2651,"digest":2652,"rendered":2653,"legacyId":2661},{"title":2648,"description":25,"summary":25,"pubDate":2649,"source":19,"url":2650,"thumbnail":21},"Dota 2 with large scale deep reinforcement learning",["Date","2019-12-13T08:00:00.000Z"],"https://openai.com/blog/dota-2-with-large-scale-deep-reinforcement-learning","src/content/posts/2019-12-13-dota-2-with-large-scale-deep-reinforcement-learning.md","a073cdfe4cb8c607",{"html":25,"metadata":2654},{"headings":2655,"localImagePaths":2656,"remoteImagePaths":2657,"frontmatter":2658,"imagePaths":2660},[],[],[],{"title":2648,"description":25,"summary":25,"pubDate":2659,"source":19,"url":2650,"thumbnail":21},"Fri, 13 Dec 2019 08:00:00 GMT",[],"2019-12-13-dota-2-with-large-scale-deep-reinforcement-learning.md","2019-11-21-safety-gym",{"id":2662,"data":2664,"filePath":2669,"digest":2670,"rendered":2671,"legacyId":2678},{"title":2665,"description":2666,"summary":2666,"pubDate":2667,"source":19,"url":2668,"thumbnail":21},"Safety Gym","We’re releasing Safety Gym, a suite of environments and tools for measuring progress towards reinforcement learning agents that respect safety constraints while training.",["Date","2019-11-21T08:00:00.000Z"],"https://openai.com/blog/safety-gym","src/content/posts/2019-11-21-safety-gym.md","dc16fa954671cc55",{"html":25,"metadata":2672},{"headings":2673,"localImagePaths":2674,"remoteImagePaths":2675,"frontmatter":2676,"imagePaths":2677},[],[],[],{"title":2665,"description":2666,"summary":2666,"pubDate":2606,"source":19,"url":2668,"thumbnail":21},[],"2019-11-21-safety-gym.md","2020-01-23-scaling-laws-for-neural-language-models",{"id":2679,"data":2681,"filePath":2685,"digest":2686,"rendered":2687,"legacyId":2695},{"title":2682,"description":25,"summary":25,"pubDate":2683,"source":19,"url":2684,"thumbnail":21},"Scaling laws for neural language models",["Date","2020-01-23T08:00:00.000Z"],"https://openai.com/blog/scaling-laws-for-neural-language-models","src/content/posts/2020-01-23-scaling-laws-for-neural-language-models.md","5136a303ba98bb6e",{"html":25,"metadata":2688},{"headings":2689,"localImagePaths":2690,"remoteImagePaths":2691,"frontmatter":2692,"imagePaths":2694},[],[],[],{"title":2682,"description":25,"summary":25,"pubDate":2693,"source":19,"url":2684,"thumbnail":21},"Thu, 23 Jan 2020 08:00:00 GMT",[],"2020-01-23-scaling-laws-for-neural-language-models.md","2020-01-30-openai-standardizes-on-pytorch",{"id":2696,"data":2698,"filePath":2703,"digest":2704,"rendered":2705,"legacyId":2713},{"title":2699,"description":2700,"summary":2700,"pubDate":2701,"source":19,"url":2702,"thumbnail":21},"OpenAI standardizes on PyTorch","We are standardizing OpenAI’s deep learning framework on PyTorch.",["Date","2020-01-30T08:00:00.000Z"],"https://openai.com/blog/openai-pytorch","src/content/posts/2020-01-30-openai-standardizes-on-pytorch.md","ea036a586091bb01",{"html":25,"metadata":2706},{"headings":2707,"localImagePaths":2708,"remoteImagePaths":2709,"frontmatter":2710,"imagePaths":2712},[],[],[],{"title":2699,"description":2700,"summary":2700,"pubDate":2711,"source":19,"url":2702,"thumbnail":21},"Thu, 30 Jan 2020 08:00:00 GMT",[],"2020-01-30-openai-standardizes-on-pytorch.md","2020-02-14-how-to-train-a-new-language-model-from-scratch-using-transformers-and-tokenizers",{"id":2714,"data":2716,"filePath":2723,"digest":2724,"rendered":2725,"legacyId":2733},{"title":2717,"description":25,"summary":2718,"pubDate":2719,"source":2720,"url":2721,"thumbnail":2722},"How to train a new language model from scratch using Transformers and Tokenizers","How to train a new language model from scratch using Transformers and Tokenizers Over the past few m...",["Date","2020-02-14T00:00:00.000Z"],"Hugging Face Blog","https://huggingface.co/blog/how-to-train","https://huggingface.co/blog/assets/01_how-to-train/how-to-train_blogpost.png","src/content/posts/2020-02-14-how-to-train-a-new-language-model-from-scratch-using-transformers-and-tokenizers.md","0a29ecbd6e088ac5",{"html":25,"metadata":2726},{"headings":2727,"localImagePaths":2728,"remoteImagePaths":2729,"frontmatter":2730,"imagePaths":2732},[],[],[],{"title":2717,"description":25,"summary":2718,"pubDate":2731,"source":2720,"url":2721,"thumbnail":2722},"Fri, 14 Feb 2020 00:00:00 GMT",[],"2020-02-14-how-to-train-a-new-language-model-from-scratch-using-transformers-and-tokenizers.md","2020-03-01-how-to-generate-text-using-different-decoding-methods-for-language-generation-with-transformers",{"id":2734,"data":2736,"filePath":2742,"digest":2743,"rendered":2744,"legacyId":2752},{"title":2737,"description":25,"summary":2738,"pubDate":2739,"source":2720,"url":2740,"thumbnail":2741},"How to generate text: using different decoding methods for language generation with Transformers","How to generate text: using different decoding methods for language generation with Transformers Not...",["Date","2020-03-01T00:00:00.000Z"],"https://huggingface.co/blog/how-to-generate","https://huggingface.co/blog/assets/02_how-to-generate/thumbnail.png","src/content/posts/2020-03-01-how-to-generate-text-using-different-decoding-methods-for-language-generation-with-transformers.md","63bc02c5b7c236eb",{"html":25,"metadata":2745},{"headings":2746,"localImagePaths":2747,"remoteImagePaths":2748,"frontmatter":2749,"imagePaths":2751},[],[],[],{"title":2737,"description":25,"summary":2738,"pubDate":2750,"source":2720,"url":2740,"thumbnail":2741},"Sun, 01 Mar 2020 00:00:00 GMT",[],"2020-03-01-how-to-generate-text-using-different-decoding-methods-for-language-generation-with-transformers.md","2020-04-14-openai-microscope",{"id":2753,"data":2755,"filePath":2760,"digest":2761,"rendered":2762,"legacyId":2770},{"title":2756,"description":2757,"summary":2757,"pubDate":2758,"source":19,"url":2759,"thumbnail":21},"OpenAI Microscope","We’re introducing OpenAI Microscope, a collection of visualizations of every significant layer and neuron of eight vision “model organisms” which are often studied in interpretability. Microscope makes it easier to analyze the features that form inside these neural networks, and we hope it will help the research community as we move towards understanding these complicated systems.",["Date","2020-04-14T07:00:00.000Z"],"https://openai.com/blog/microscope","src/content/posts/2020-04-14-openai-microscope.md","105c3493628441f3",{"html":25,"metadata":2763},{"headings":2764,"localImagePaths":2765,"remoteImagePaths":2766,"frontmatter":2767,"imagePaths":2769},[],[],[],{"title":2756,"description":2757,"summary":2757,"pubDate":2768,"source":19,"url":2759,"thumbnail":21},"Tue, 14 Apr 2020 07:00:00 GMT",[],"2020-04-14-openai-microscope.md","2020-04-16-improving-verifiability-in-ai-development",{"id":2771,"data":2773,"filePath":2778,"digest":2779,"rendered":2780,"legacyId":2788},{"title":2774,"description":2775,"summary":2775,"pubDate":2776,"source":19,"url":2777,"thumbnail":21},"Improving verifiability in AI development","We’ve contributed to a multi-stakeholder report by 58 co-authors at 30 organizations, including the Centre for the Future of Intelligence, Mila, Schwartz Reisman Institute for Technology and Society, Center for Advanced Study in the Behavioral Sciences, and Center for Security and Emerging Technologies. This report describes 10 mechanisms to improve the verifiability of claims made about AI systems. Developers can use these tools to provide evidence that AI systems are safe, secure, fair, or privacy-preserving. Users, policymakers, and civil society can use these tools to evaluate AI development processes.",["Date","2020-04-16T07:00:00.000Z"],"https://openai.com/blog/improving-verifiability","src/content/posts/2020-04-16-improving-verifiability-in-ai-development.md","0543c1e6a6d57be1",{"html":25,"metadata":2781},{"headings":2782,"localImagePaths":2783,"remoteImagePaths":2784,"frontmatter":2785,"imagePaths":2787},[],[],[],{"title":2774,"description":2775,"summary":2775,"pubDate":2786,"source":19,"url":2777,"thumbnail":21},"Thu, 16 Apr 2020 07:00:00 GMT",[],"2020-04-16-improving-verifiability-in-ai-development.md","2020-04-30-jukebox",{"id":2789,"data":2791,"filePath":2796,"digest":2797,"rendered":2798,"legacyId":2806},{"title":2792,"description":2793,"summary":2793,"pubDate":2794,"source":19,"url":2795,"thumbnail":21},"Jukebox","We’re introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We’re releasing the model weights and code, along with a tool to explore the generated samples.",["Date","2020-04-30T07:00:00.000Z"],"https://openai.com/blog/jukebox","src/content/posts/2020-04-30-jukebox.md","5279804e1c87fc81",{"html":25,"metadata":2799},{"headings":2800,"localImagePaths":2801,"remoteImagePaths":2802,"frontmatter":2803,"imagePaths":2805},[],[],[],{"title":2792,"description":2793,"summary":2793,"pubDate":2804,"source":19,"url":2795,"thumbnail":21},"Thu, 30 Apr 2020 07:00:00 GMT",[],"2020-04-30-jukebox.md","2020-05-05-ai-and-efficiency",{"id":2807,"data":2809,"filePath":2814,"digest":2815,"rendered":2816,"legacyId":2824},{"title":2810,"description":2811,"summary":2811,"pubDate":2812,"source":19,"url":2813,"thumbnail":21},"AI and efficiency","We’re releasing an analysis showing that since 2012 the amount of compute needed to train a neural net to the same performance on ImageNet classification has been decreasing by a factor of 2 every 16 months. Compared to 2012, it now takes 44 times less compute to train a neural network to the level of AlexNet (by contrast, Moore’s Law would yield an 11x cost improvement over this period). Our results suggest that for AI tasks with high levels of recent investment, algorithmic progress has yielded more gains than classical hardware efficiency.",["Date","2020-05-05T07:00:00.000Z"],"https://openai.com/blog/ai-and-efficiency","src/content/posts/2020-05-05-ai-and-efficiency.md","e83c6e8cfcfafe68",{"html":25,"metadata":2817},{"headings":2818,"localImagePaths":2819,"remoteImagePaths":2820,"frontmatter":2821,"imagePaths":2823},[],[],[],{"title":2810,"description":2811,"summary":2811,"pubDate":2822,"source":19,"url":2813,"thumbnail":21},"Tue, 05 May 2020 07:00:00 GMT",[],"2020-05-05-ai-and-efficiency.md","2020-05-28-language-models-are-few-shot-learners",{"id":2825,"data":2827,"filePath":2831,"digest":2832,"rendered":2833,"legacyId":2841},{"title":2828,"description":25,"summary":25,"pubDate":2829,"source":19,"url":2830,"thumbnail":21},"Language models are few-shot learners",["Date","2020-05-28T07:00:00.000Z"],"https://openai.com/blog/language-models-are-few-shot-learners","src/content/posts/2020-05-28-language-models-are-few-shot-learners.md","144db90c5486bf57",{"html":25,"metadata":2834},{"headings":2835,"localImagePaths":2836,"remoteImagePaths":2837,"frontmatter":2838,"imagePaths":2840},[],[],[],{"title":2828,"description":25,"summary":25,"pubDate":2839,"source":19,"url":2830,"thumbnail":21},"Thu, 28 May 2020 07:00:00 GMT",[],"2020-05-28-language-models-are-few-shot-learners.md","2020-06-11-openai-api",{"id":2842,"data":2844,"filePath":2849,"digest":2850,"rendered":2851,"legacyId":2859},{"title":2845,"description":2846,"summary":2846,"pubDate":2847,"source":19,"url":2848,"thumbnail":21},"OpenAI API","We’re releasing an API for accessing new AI models developed by OpenAI.",["Date","2020-06-11T07:00:00.000Z"],"https://openai.com/blog/openai-api","src/content/posts/2020-06-11-openai-api.md","b578ef3cbe860183",{"html":25,"metadata":2852},{"headings":2853,"localImagePaths":2854,"remoteImagePaths":2855,"frontmatter":2856,"imagePaths":2858},[],[],[],{"title":2845,"description":2846,"summary":2846,"pubDate":2857,"source":19,"url":2848,"thumbnail":21},"Thu, 11 Jun 2020 07:00:00 GMT",[],"2020-06-11-openai-api.md","2020-06-17-image-gpt",{"id":2860,"data":2862,"filePath":2867,"digest":2868,"rendered":2869,"legacyId":2877},{"title":2863,"description":2864,"summary":2864,"pubDate":2865,"source":19,"url":2866,"thumbnail":21},"Image GPT","We find that, just as a large transformer model trained on language can generate coherent text, the same exact model trained on pixel sequences can generate coherent image completions and samples. By establishing a correlation between sample quality and image classification accuracy, we show that our best generative model also contains features competitive with top convolutional nets in the unsupervised setting.",["Date","2020-06-17T07:00:00.000Z"],"https://openai.com/blog/image-gpt","src/content/posts/2020-06-17-image-gpt.md","e10d234ea923add0",{"html":25,"metadata":2870},{"headings":2871,"localImagePaths":2872,"remoteImagePaths":2873,"frontmatter":2874,"imagePaths":2876},[],[],[],{"title":2863,"description":2864,"summary":2864,"pubDate":2875,"source":19,"url":2866,"thumbnail":21},"Wed, 17 Jun 2020 07:00:00 GMT",[],"2020-06-17-image-gpt.md","2020-06-20-procgen-and-minerl-competitions",{"id":2878,"data":2880,"filePath":2885,"digest":2886,"rendered":2887,"legacyId":2895},{"title":2881,"description":2882,"summary":2882,"pubDate":2883,"source":19,"url":2884,"thumbnail":21},"Procgen and MineRL Competitions","We’re excited to announce that OpenAI is co-organizing two NeurIPS 2020 competitions with AIcrowd, Carnegie Mellon University, and DeepMind, using Procgen Benchmark and MineRL.",["Date","2020-06-20T07:00:00.000Z"],"https://openai.com/blog/procgen-minerl-competitions","src/content/posts/2020-06-20-procgen-and-minerl-competitions.md","bd361a143b9e1d11",{"html":25,"metadata":2888},{"headings":2889,"localImagePaths":2890,"remoteImagePaths":2891,"frontmatter":2892,"imagePaths":2894},[],[],[],{"title":2881,"description":2882,"summary":2882,"pubDate":2893,"source":19,"url":2884,"thumbnail":21},"Sat, 20 Jun 2020 07:00:00 GMT",[],"2020-06-20-procgen-and-minerl-competitions.md","2020-07-03-the-reformer---pushing-the-limits-of-language-modeling",{"id":2896,"data":2898,"filePath":2904,"digest":2905,"rendered":2906,"legacyId":2914},{"title":2899,"description":25,"summary":2900,"pubDate":2901,"source":2720,"url":2902,"thumbnail":2903},"The Reformer - Pushing the limits of language modeling","The Reformer - Pushing the limits of language modeling How the Reformer uses less than 8GB of RAM to...",["Date","2020-07-03T00:00:00.000Z"],"https://huggingface.co/blog/reformer","https://huggingface.co/blog/assets/03_reformer/thumbnail.png","src/content/posts/2020-07-03-the-reformer---pushing-the-limits-of-language-modeling.md","e8db1e3c0f9047b5",{"html":25,"metadata":2907},{"headings":2908,"localImagePaths":2909,"remoteImagePaths":2910,"frontmatter":2911,"imagePaths":2913},[],[],[],{"title":2899,"description":25,"summary":2900,"pubDate":2912,"source":2720,"url":2902,"thumbnail":2903},"Fri, 03 Jul 2020 00:00:00 GMT",[],"2020-07-03-the-reformer---pushing-the-limits-of-language-modeling.md","2020-07-09-openai-scholars-2020-final-projects",{"id":2915,"data":2917,"filePath":2922,"digest":2923,"rendered":2924,"legacyId":2932},{"title":2918,"description":2919,"summary":2919,"pubDate":2920,"source":19,"url":2921,"thumbnail":21},"OpenAI Scholars 2020: Final projects","Our third class of OpenAI Scholars presented their final projects at virtual Demo Day, showcasing their research results from over the past five months.",["Date","2020-07-09T07:00:00.000Z"],"https://openai.com/blog/openai-scholars-2020-final-projects","src/content/posts/2020-07-09-openai-scholars-2020-final-projects.md","7df796eadfb53898",{"html":25,"metadata":2925},{"headings":2926,"localImagePaths":2927,"remoteImagePaths":2928,"frontmatter":2929,"imagePaths":2931},[],[],[],{"title":2918,"description":2919,"summary":2919,"pubDate":2930,"source":19,"url":2921,"thumbnail":21},"Thu, 09 Jul 2020 07:00:00 GMT",[],"2020-07-09-openai-scholars-2020-final-projects.md","2020-09-04-learning-to-summarize-with-human-feedback",{"id":2933,"data":2935,"filePath":2940,"digest":2941,"rendered":2942,"legacyId":2950},{"title":2936,"description":2937,"summary":2937,"pubDate":2938,"source":19,"url":2939,"thumbnail":21},"Learning to summarize with human feedback","We’ve applied reinforcement learning from human feedback to train language models that are better at summarization.",["Date","2020-09-04T07:00:00.000Z"],"https://openai.com/blog/learning-to-summarize-with-human-feedback","src/content/posts/2020-09-04-learning-to-summarize-with-human-feedback.md","a367f0aa805b3bbd",{"html":25,"metadata":2943},{"headings":2944,"localImagePaths":2945,"remoteImagePaths":2946,"frontmatter":2947,"imagePaths":2949},[],[],[],{"title":2936,"description":2937,"summary":2937,"pubDate":2948,"source":19,"url":2939,"thumbnail":21},"Fri, 04 Sep 2020 07:00:00 GMT",[],"2020-09-04-learning-to-summarize-with-human-feedback.md","2020-09-07-generative-language-modeling-for-automated-theorem-proving",{"id":2951,"data":2953,"filePath":2957,"digest":2958,"rendered":2959,"legacyId":2967},{"title":2954,"description":25,"summary":25,"pubDate":2955,"source":19,"url":2956,"thumbnail":21},"Generative language modeling for automated theorem proving",["Date","2020-09-07T07:00:00.000Z"],"https://openai.com/blog/generative-language-modeling-for-automated-theorem-proving","src/content/posts/2020-09-07-generative-language-modeling-for-automated-theorem-proving.md","9764c7c924bf9305",{"html":25,"metadata":2960},{"headings":2961,"localImagePaths":2962,"remoteImagePaths":2963,"frontmatter":2964,"imagePaths":2966},[],[],[],{"title":2954,"description":25,"summary":25,"pubDate":2965,"source":19,"url":2956,"thumbnail":21},"Mon, 07 Sep 2020 07:00:00 GMT",[],"2020-09-07-generative-language-modeling-for-automated-theorem-proving.md","2020-09-10-block-sparse-matrices-for-smaller-and-faster-language-models",{"id":2968,"data":2970,"filePath":2976,"digest":2977,"rendered":2978,"legacyId":2986},{"title":2971,"description":25,"summary":2972,"pubDate":2973,"source":2720,"url":2974,"thumbnail":2975},"Block Sparse Matrices for Smaller and Faster Language Models","Block Sparse Matrices for Smaller and Faster Language Models Saving space and time, one zero at a ti...",["Date","2020-09-10T00:00:00.000Z"],"https://huggingface.co/blog/pytorch_block_sparse","https://huggingface.co/blog/assets/04_pytorch_block_sparse/thumbnail.png","src/content/posts/2020-09-10-block-sparse-matrices-for-smaller-and-faster-language-models.md","23136eba5db4cfe9",{"html":25,"metadata":2979},{"headings":2980,"localImagePaths":2981,"remoteImagePaths":2982,"frontmatter":2983,"imagePaths":2985},[],[],[],{"title":2971,"description":25,"summary":2972,"pubDate":2984,"source":2720,"url":2974,"thumbnail":2975},"Thu, 10 Sep 2020 00:00:00 GMT",[],"2020-09-10-block-sparse-matrices-for-smaller-and-faster-language-models.md","2020-09-22-openai-licenses-gpt-3-technology-to-microsoft",{"id":2987,"data":2989,"filePath":2994,"digest":2995,"rendered":2996,"legacyId":3004},{"title":2990,"description":2991,"summary":2991,"pubDate":2992,"source":19,"url":2993,"thumbnail":21},"OpenAI licenses GPT-3 technology to Microsoft","OpenAI has agreed to license GPT-3 to Microsoft for their own products and services.",["Date","2020-09-22T07:00:00.000Z"],"https://openai.com/blog/openai-licenses-gpt-3-technology-to-microsoft","src/content/posts/2020-09-22-openai-licenses-gpt-3-technology-to-microsoft.md","2481ca8b4dae16c3",{"html":25,"metadata":2997},{"headings":2998,"localImagePaths":2999,"remoteImagePaths":3000,"frontmatter":3001,"imagePaths":3003},[],[],[],{"title":2990,"description":2991,"summary":2991,"pubDate":3002,"source":19,"url":2993,"thumbnail":21},"Tue, 22 Sep 2020 07:00:00 GMT",[],"2020-09-22-openai-licenses-gpt-3-technology-to-microsoft.md","2020-10-10-transformer-based-encoder-decoder-models",{"id":3005,"data":3007,"filePath":3013,"digest":3014,"rendered":3015,"legacyId":3023},{"title":3008,"description":25,"summary":3009,"pubDate":3010,"source":2720,"url":3011,"thumbnail":3012},"Transformer-based Encoder-Decoder Models","Transformers-based Encoder-Decoder Models !pip install transformers==4.2.1 !pip install sentencepiec...",["Date","2020-10-10T00:00:00.000Z"],"https://huggingface.co/blog/encoder-decoder","https://huggingface.co/blog/assets/05_encoder_decoder/thumbnail.png","src/content/posts/2020-10-10-transformer-based-encoder-decoder-models.md","8e020e742d186d1f",{"html":25,"metadata":3016},{"headings":3017,"localImagePaths":3018,"remoteImagePaths":3019,"frontmatter":3020,"imagePaths":3022},[],[],[],{"title":3008,"description":25,"summary":3009,"pubDate":3021,"source":2720,"url":3011,"thumbnail":3012},"Sat, 10 Oct 2020 00:00:00 GMT",[],"2020-10-10-transformer-based-encoder-decoder-models.md","2020-11-02-hyperparameter-search-with-transformers-and-ray-tune",{"id":3024,"data":3026,"filePath":3032,"digest":3033,"rendered":3034,"legacyId":3042},{"title":3027,"description":25,"summary":3028,"pubDate":3029,"source":2720,"url":3030,"thumbnail":3031},"Hyperparameter Search with Transformers and Ray Tune","Hyperparameter Search with Transformers and Ray Tune A guest blog post by Richard Liaw from the Anys...",["Date","2020-11-02T00:00:00.000Z"],"https://huggingface.co/blog/ray-tune","https://huggingface.co/blog/assets/06_ray_tune/ray-hf.jpg","src/content/posts/2020-11-02-hyperparameter-search-with-transformers-and-ray-tune.md","d6bd91a11fdb4317",{"html":25,"metadata":3035},{"headings":3036,"localImagePaths":3037,"remoteImagePaths":3038,"frontmatter":3039,"imagePaths":3041},[],[],[],{"title":3027,"description":25,"summary":3028,"pubDate":3040,"source":2720,"url":3030,"thumbnail":3031},"Mon, 02 Nov 2020 00:00:00 GMT",[],"2020-11-02-hyperparameter-search-with-transformers-and-ray-tune.md","2020-11-03-porting-fairseq-wmt19-translation-system-to-transformers",{"id":3043,"data":3045,"filePath":3051,"digest":3052,"rendered":3053,"legacyId":3061},{"title":3046,"description":25,"summary":3047,"pubDate":3048,"source":2720,"url":3049,"thumbnail":3050},"Porting fairseq wmt19 translation system to transformers","Porting fairseq wmt19 translation system to transformers A guest blog post by Stas Bekman This artic...",["Date","2020-11-03T00:00:00.000Z"],"https://huggingface.co/blog/porting-fsmt","https://huggingface.co/blog/assets/07_porting_fsmt/thumbnail.png","src/content/posts/2020-11-03-porting-fairseq-wmt19-translation-system-to-transformers.md","ff44a9b88b88b2c2",{"html":25,"metadata":3054},{"headings":3055,"localImagePaths":3056,"remoteImagePaths":3057,"frontmatter":3058,"imagePaths":3060},[],[],[],{"title":3046,"description":25,"summary":3047,"pubDate":3059,"source":2720,"url":3049,"thumbnail":3050},"Tue, 03 Nov 2020 00:00:00 GMT",[],"2020-11-03-porting-fairseq-wmt19-translation-system-to-transformers.md","2020-11-09-leveraging-pre-trained-language-model-checkpoints-for-encoder-decoder-models",{"id":3062,"data":3064,"filePath":3070,"digest":3071,"rendered":3072,"legacyId":3080},{"title":3065,"description":25,"summary":3066,"pubDate":3067,"source":2720,"url":3068,"thumbnail":3069},"Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models","Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models Transformer-based encod...",["Date","2020-11-09T00:00:00.000Z"],"https://huggingface.co/blog/warm-starting-encoder-decoder","https://huggingface.co/blog/assets/08_warm_starting_encoder_decoder/thumbnail.png","src/content/posts/2020-11-09-leveraging-pre-trained-language-model-checkpoints-for-encoder-decoder-models.md","59db25f201f28a3c",{"html":25,"metadata":3073},{"headings":3074,"localImagePaths":3075,"remoteImagePaths":3076,"frontmatter":3077,"imagePaths":3079},[],[],[],{"title":3065,"description":25,"summary":3066,"pubDate":3078,"source":2720,"url":3068,"thumbnail":3069},"Mon, 09 Nov 2020 00:00:00 GMT",[],"2020-11-09-leveraging-pre-trained-language-model-checkpoints-for-encoder-decoder-models.md","2020-12-29-organizational-update-from-openai",{"id":3081,"data":3083,"filePath":3088,"digest":3089,"rendered":3090,"legacyId":3098},{"title":3084,"description":3085,"summary":3085,"pubDate":3086,"source":19,"url":3087,"thumbnail":21},"Organizational update from OpenAI","It’s been a year of dramatic change and growth at OpenAI.",["Date","2020-12-29T08:00:00.000Z"],"https://openai.com/blog/organizational-update","src/content/posts/2020-12-29-organizational-update-from-openai.md","ac91e50efd3e42df",{"html":25,"metadata":3091},{"headings":3092,"localImagePaths":3093,"remoteImagePaths":3094,"frontmatter":3095,"imagePaths":3097},[],[],[],{"title":3084,"description":3085,"summary":3085,"pubDate":3096,"source":19,"url":3087,"thumbnail":21},"Tue, 29 Dec 2020 08:00:00 GMT",[],"2020-12-29-organizational-update-from-openai.md","2021-01-05-clip-connecting-text-and-images",{"id":3099,"data":3101,"filePath":3106,"digest":3107,"rendered":3108,"legacyId":3116},{"title":3102,"description":3103,"summary":3103,"pubDate":3104,"source":19,"url":3105,"thumbnail":21},"CLIP: Connecting text and images","We’re introducing a neural network called CLIP which efficiently learns visual concepts from natural language supervision. CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized, similar to the “zero-shot” capabilities of GPT-2 and GPT-3.",["Date","2021-01-05T08:00:00.000Z"],"https://openai.com/blog/clip","src/content/posts/2021-01-05-clip-connecting-text-and-images.md","86c7cb1adc2b9d3d",{"html":25,"metadata":3109},{"headings":3110,"localImagePaths":3111,"remoteImagePaths":3112,"frontmatter":3113,"imagePaths":3115},[],[],[],{"title":3102,"description":3103,"summary":3103,"pubDate":3114,"source":19,"url":3105,"thumbnail":21},"Tue, 05 Jan 2021 08:00:00 GMT",[],"2021-01-05-clip-connecting-text-and-images.md","2021-01-18-how-we-sped-up-transformer-inference-100x-for-api-customers",{"id":3117,"data":3119,"filePath":3125,"digest":3126,"rendered":3127,"legacyId":3135},{"title":3120,"description":25,"summary":3121,"pubDate":3122,"source":2720,"url":3123,"thumbnail":3124},"How we sped up transformer inference 100x for 🤗 API customers","How we sped up transformer inference 100x for 🤗 API customers 🤗 Transformers has become the default ...",["Date","2021-01-18T00:00:00.000Z"],"https://huggingface.co/blog/accelerated-inference","https://huggingface.co/blog/assets/09_accelerated_inference/thumbnail.png","src/content/posts/2021-01-18-how-we-sped-up-transformer-inference-100x-for-api-customers.md","1e801028d0dd13a9",{"html":25,"metadata":3128},{"headings":3129,"localImagePaths":3130,"remoteImagePaths":3131,"frontmatter":3132,"imagePaths":3134},[],[],[],{"title":3120,"description":25,"summary":3121,"pubDate":3133,"source":2720,"url":3123,"thumbnail":3124},"Mon, 18 Jan 2021 00:00:00 GMT",[],"2021-01-18-how-we-sped-up-transformer-inference-100x-for-api-customers.md","2021-01-19-fit-more-and-train-faster-with-zero-via-deepspeed-and-fairscale",{"id":3136,"data":3138,"filePath":3144,"digest":3145,"rendered":3146,"legacyId":3154},{"title":3139,"description":25,"summary":3140,"pubDate":3141,"source":2720,"url":3142,"thumbnail":3143},"Fit More and Train Faster With ZeRO via DeepSpeed and FairScale","Fit More and Train Faster With ZeRO via DeepSpeed and FairScale A guest blog post by Hugging Face fe...",["Date","2021-01-19T00:00:00.000Z"],"https://huggingface.co/blog/zero-deepspeed-fairscale","https://huggingface.co/blog/assets/11_zero_deepspeed_fairscale/zero-partitioning.png","src/content/posts/2021-01-19-fit-more-and-train-faster-with-zero-via-deepspeed-and-fairscale.md","9c3b57261fa9aa52",{"html":25,"metadata":3147},{"headings":3148,"localImagePaths":3149,"remoteImagePaths":3150,"frontmatter":3151,"imagePaths":3153},[],[],[],{"title":3139,"description":25,"summary":3140,"pubDate":3152,"source":2720,"url":3142,"thumbnail":3143},"Tue, 19 Jan 2021 00:00:00 GMT",[],"2021-01-19-fit-more-and-train-faster-with-zero-via-deepspeed-and-fairscale.md","2021-01-25-scaling-kubernetes-to-7500-nodes",{"id":3155,"data":3157,"filePath":3162,"digest":3163,"rendered":3164,"legacyId":3172},{"title":3158,"description":3159,"summary":3159,"pubDate":3160,"source":19,"url":3161,"thumbnail":21},"Scaling Kubernetes to 7,500 nodes","We’ve scaled Kubernetes clusters to 7,500 nodes, producing a scalable infrastructure for large models like GPT-3, CLIP, and DALL·E, but also for rapid small-scale iterative research such as Scaling Laws for Neural Language Models.",["Date","2021-01-25T08:00:00.000Z"],"https://openai.com/blog/scaling-kubernetes-to-7500-nodes","src/content/posts/2021-01-25-scaling-kubernetes-to-7500-nodes.md","c7b4cd929682d9af",{"html":25,"metadata":3165},{"headings":3166,"localImagePaths":3167,"remoteImagePaths":3168,"frontmatter":3169,"imagePaths":3171},[],[],[],{"title":3158,"description":3159,"summary":3159,"pubDate":3170,"source":19,"url":3161,"thumbnail":21},"Mon, 25 Jan 2021 08:00:00 GMT",[],"2021-01-25-scaling-kubernetes-to-7500-nodes.md","2021-01-05-dalle-creating-images-from-text",{"id":3173,"data":3175,"filePath":3180,"digest":3181,"rendered":3182,"legacyId":3189},{"title":3176,"description":3177,"summary":3177,"pubDate":3178,"source":19,"url":3179,"thumbnail":21},"DALL·E: Creating images from text","We’ve trained a neural network called DALL·E that creates images from text captions for a wide range of concepts expressible in natural language.",["Date","2021-01-05T08:00:00.000Z"],"https://openai.com/blog/dall-e","src/content/posts/2021-01-05-dalle-creating-images-from-text.md","ea4c0694abbfb18d",{"html":25,"metadata":3183},{"headings":3184,"localImagePaths":3185,"remoteImagePaths":3186,"frontmatter":3187,"imagePaths":3188},[],[],[],{"title":3176,"description":3177,"summary":3177,"pubDate":3114,"source":19,"url":3179,"thumbnail":21},[],"2021-01-05-dalle-creating-images-from-text.md","2021-01-26-faster-tensorflow-models-in-hugging-face-transformers",{"id":3190,"data":3192,"filePath":3198,"digest":3199,"rendered":3200,"legacyId":3208},{"title":3193,"description":25,"summary":3194,"pubDate":3195,"source":2720,"url":3196,"thumbnail":3197},"Faster TensorFlow models in Hugging Face Transformers","Faster TensorFlow models in Hugging Face Transformers In the last few months, the Hugging Face team ...",["Date","2021-01-26T00:00:00.000Z"],"https://huggingface.co/blog/tf-serving","https://huggingface.co/blog/assets/10_tf-serving/thumbnail.png","src/content/posts/2021-01-26-faster-tensorflow-models-in-hugging-face-transformers.md","6748b8d71be322a2",{"html":25,"metadata":3201},{"headings":3202,"localImagePaths":3203,"remoteImagePaths":3204,"frontmatter":3205,"imagePaths":3207},[],[],[],{"title":3193,"description":25,"summary":3194,"pubDate":3206,"source":2720,"url":3196,"thumbnail":3197},"Tue, 26 Jan 2021 00:00:00 GMT",[],"2021-01-26-faster-tensorflow-models-in-hugging-face-transformers.md","2021-02-04-understanding-the-capabilities-limitations-and-societal-impact-of-large-language-models",{"id":3209,"data":3211,"filePath":3215,"digest":3216,"rendered":3217,"legacyId":3225},{"title":3212,"description":25,"summary":25,"pubDate":3213,"source":19,"url":3214,"thumbnail":21},"Understanding the capabilities, limitations, and societal impact of large language models",["Date","2021-02-04T08:00:00.000Z"],"https://openai.com/blog/understanding-the-capabilities-limitations-and-societal-impact-of-large-language-models","src/content/posts/2021-02-04-understanding-the-capabilities-limitations-and-societal-impact-of-large-language-models.md","1fe498880583a116",{"html":25,"metadata":3218},{"headings":3219,"localImagePaths":3220,"remoteImagePaths":3221,"frontmatter":3222,"imagePaths":3224},[],[],[],{"title":3212,"description":25,"summary":25,"pubDate":3223,"source":19,"url":3214,"thumbnail":21},"Thu, 04 Feb 2021 08:00:00 GMT",[],"2021-02-04-understanding-the-capabilities-limitations-and-societal-impact-of-large-language-models.md","2021-02-09-hugging-face-on-pytorch-xla-tpus",{"id":3226,"data":3228,"filePath":3234,"digest":3235,"rendered":3236,"legacyId":3244},{"title":3229,"description":25,"summary":3230,"pubDate":3231,"source":2720,"url":3232,"thumbnail":3233},"Hugging Face on PyTorch / XLA TPUs","Hugging Face on PyTorch / XLA TPUs: Faster and cheaper training Training Your Favorite Transformers ...",["Date","2021-02-09T00:00:00.000Z"],"https://huggingface.co/blog/pytorch-xla","https://huggingface.co/blog/assets/13_pytorch_xla/pytorch_xla_thumbnail.png","src/content/posts/2021-02-09-hugging-face-on-pytorch-xla-tpus.md","3f3bebbc50607960",{"html":25,"metadata":3237},{"headings":3238,"localImagePaths":3239,"remoteImagePaths":3240,"frontmatter":3241,"imagePaths":3243},[],[],[],{"title":3229,"description":25,"summary":3230,"pubDate":3242,"source":2720,"url":3232,"thumbnail":3233},"Tue, 09 Feb 2021 00:00:00 GMT",[],"2021-02-09-hugging-face-on-pytorch-xla-tpus.md","2021-02-10-retrieval-augmented-generation-with-huggingface-transformers-and-ray",{"id":3245,"data":3247,"filePath":3253,"digest":3254,"rendered":3255,"legacyId":3263},{"title":3248,"description":25,"summary":3249,"pubDate":3250,"source":2720,"url":3251,"thumbnail":3252},"Retrieval Augmented Generation with Huggingface Transformers and Ray","Retrieval Augmented Generation with Huggingface Transformers and Ray A guest blog post by Amog Kamse...",["Date","2021-02-10T00:00:00.000Z"],"https://huggingface.co/blog/ray-rag","https://huggingface.co/blog/assets/12_ray_rag/ray_arch_updated.png","src/content/posts/2021-02-10-retrieval-augmented-generation-with-huggingface-transformers-and-ray.md","f4f32482e058619c",{"html":25,"metadata":3256},{"headings":3257,"localImagePaths":3258,"remoteImagePaths":3259,"frontmatter":3260,"imagePaths":3262},[],[],[],{"title":3248,"description":25,"summary":3249,"pubDate":3261,"source":2720,"url":3251,"thumbnail":3252},"Wed, 10 Feb 2021 00:00:00 GMT",[],"2021-02-10-retrieval-augmented-generation-with-huggingface-transformers-and-ray.md","2021-02-25-simple-considerations-for-simple-people-building-fancy-neural-networks",{"id":3264,"data":3266,"filePath":3272,"digest":3273,"rendered":3274,"legacyId":3282},{"title":3267,"description":25,"summary":3268,"pubDate":3269,"source":2720,"url":3270,"thumbnail":3271},"Simple considerations for simple people building fancy neural networks","🚧 Simple considerations for simple people building fancy neural networks Photo by Henry & Co. on Uns...",["Date","2021-02-25T00:00:00.000Z"],"https://huggingface.co/blog/simple-considerations","https://huggingface.co/blog/assets/13_simple-considerations/henry-co-3coKbdfnAFg-unsplash.jpg","src/content/posts/2021-02-25-simple-considerations-for-simple-people-building-fancy-neural-networks.md","1c2b7d34b4370ac3",{"html":25,"metadata":3275},{"headings":3276,"localImagePaths":3277,"remoteImagePaths":3278,"frontmatter":3279,"imagePaths":3281},[],[],[],{"title":3267,"description":25,"summary":3268,"pubDate":3280,"source":2720,"url":3270,"thumbnail":3271},"Thu, 25 Feb 2021 00:00:00 GMT",[],"2021-02-25-simple-considerations-for-simple-people-building-fancy-neural-networks.md","2021-03-04-multimodal-neurons-in-artificial-neural-networks",{"id":3283,"data":3285,"filePath":3290,"digest":3291,"rendered":3292,"legacyId":3300},{"title":3286,"description":3287,"summary":3287,"pubDate":3288,"source":19,"url":3289,"thumbnail":21},"Multimodal neurons in artificial neural networks","We’ve discovered neurons in CLIP that respond to the same concept whether presented literally, symbolically, or conceptually. This may explain CLIP’s accuracy in classifying surprising visual renditions of concepts, and is also an important step toward understanding the associations and biases that CLIP and similar models learn.",["Date","2021-03-04T08:00:00.000Z"],"https://openai.com/blog/multimodal-neurons","src/content/posts/2021-03-04-multimodal-neurons-in-artificial-neural-networks.md","824d71428d2180b0",{"html":25,"metadata":3293},{"headings":3294,"localImagePaths":3295,"remoteImagePaths":3296,"frontmatter":3297,"imagePaths":3299},[],[],[],{"title":3286,"description":3287,"summary":3287,"pubDate":3298,"source":19,"url":3289,"thumbnail":21},"Thu, 04 Mar 2021 08:00:00 GMT",[],"2021-03-04-multimodal-neurons-in-artificial-neural-networks.md","2021-03-12-fine-tune-wav2vec2-for-english-asr-with-transformers",{"id":3301,"data":3303,"filePath":3309,"digest":3310,"rendered":3311,"legacyId":3319},{"title":3304,"description":25,"summary":3305,"pubDate":3306,"source":2720,"url":3307,"thumbnail":3308},"Fine-Tune Wav2Vec2 for English ASR with 🤗 Transformers","Fine-Tune Wav2Vec2 for English ASR with 🤗 Transformers Wav2Vec2 is a pretrained model for Automatic ...",["Date","2021-03-12T00:00:00.000Z"],"https://huggingface.co/blog/fine-tune-wav2vec2-english","https://huggingface.co/blog/assets/15_fine_tune_wav2vec2/wav2vec2.png","src/content/posts/2021-03-12-fine-tune-wav2vec2-for-english-asr-with-transformers.md","0bd06a52fdb681d4",{"html":25,"metadata":3312},{"headings":3313,"localImagePaths":3314,"remoteImagePaths":3315,"frontmatter":3316,"imagePaths":3318},[],[],[],{"title":3304,"description":25,"summary":3305,"pubDate":3317,"source":2720,"url":3307,"thumbnail":3308},"Fri, 12 Mar 2021 00:00:00 GMT",[],"2021-03-12-fine-tune-wav2vec2-for-english-asr-with-transformers.md","2021-03-18-my-journey-to-a-serverless-transformers-pipeline-on-google-cloud",{"id":3320,"data":3322,"filePath":3328,"digest":3329,"rendered":3330,"legacyId":3338},{"title":3323,"description":25,"summary":3324,"pubDate":3325,"source":2720,"url":3326,"thumbnail":3327},"My Journey to a serverless transformers pipeline on Google Cloud","My Journey to a serverless transformers pipeline on Google Cloud A guest blog post by community memb...",["Date","2021-03-18T00:00:00.000Z"],"https://huggingface.co/blog/how-to-deploy-a-pipeline-to-google-clouds","https://huggingface.co/blog/assets/14_how_to_deploy_a_pipeline_to_google_clouds/thumbnail.png","src/content/posts/2021-03-18-my-journey-to-a-serverless-transformers-pipeline-on-google-cloud.md","d38be7cf93e50dd1",{"html":25,"metadata":3331},{"headings":3332,"localImagePaths":3333,"remoteImagePaths":3334,"frontmatter":3335,"imagePaths":3337},[],[],[],{"title":3323,"description":25,"summary":3324,"pubDate":3336,"source":2720,"url":3326,"thumbnail":3327},"Thu, 18 Mar 2021 00:00:00 GMT",[],"2021-03-18-my-journey-to-a-serverless-transformers-pipeline-on-google-cloud.md","2021-03-23-the-partnership-amazon-sagemaker-and-hugging-face",{"id":3339,"data":3341,"filePath":3347,"digest":3348,"rendered":3349,"legacyId":3357},{"title":3342,"description":25,"summary":3343,"pubDate":3344,"source":2720,"url":3345,"thumbnail":3346},"The Partnership: Amazon SageMaker and Hugging Face","The Partnership: Amazon SageMaker and Hugging Face Look at these smiles! Today, we announce a strate...",["Date","2021-03-23T00:00:00.000Z"],"https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face","https://huggingface.co/blog/assets/17_the_partnership_amazon_sagemaker_and_hugging_face/thumbnail.png","src/content/posts/2021-03-23-the-partnership-amazon-sagemaker-and-hugging-face.md","d419a16132f7a1e2",{"html":25,"metadata":3350},{"headings":3351,"localImagePaths":3352,"remoteImagePaths":3353,"frontmatter":3354,"imagePaths":3356},[],[],[],{"title":3342,"description":25,"summary":3343,"pubDate":3355,"source":2720,"url":3345,"thumbnail":3346},"Tue, 23 Mar 2021 00:00:00 GMT",[],"2021-03-23-the-partnership-amazon-sagemaker-and-hugging-face.md","2021-03-25-gpt-3-powers-the-next-generation-of-apps",{"id":3358,"data":3360,"filePath":3365,"digest":3366,"rendered":3367,"legacyId":3375},{"title":3361,"description":3362,"summary":3362,"pubDate":3363,"source":19,"url":3364,"thumbnail":21},"GPT-3 powers the next generation of apps","Over 300 applications are delivering GPT-3–powered search, conversation, text completion, and other advanced AI features through our API.",["Date","2021-03-25T07:00:00.000Z"],"https://openai.com/blog/gpt-3-apps","src/content/posts/2021-03-25-gpt-3-powers-the-next-generation-of-apps.md","5065aca786009254",{"html":25,"metadata":3368},{"headings":3369,"localImagePaths":3370,"remoteImagePaths":3371,"frontmatter":3372,"imagePaths":3374},[],[],[],{"title":3361,"description":3362,"summary":3362,"pubDate":3373,"source":19,"url":3364,"thumbnail":21},"Thu, 25 Mar 2021 07:00:00 GMT",[],"2021-03-25-gpt-3-powers-the-next-generation-of-apps.md","2021-03-31-understanding-bigbirds-block-sparse-attention",{"id":3376,"data":3378,"filePath":3384,"digest":3385,"rendered":3386,"legacyId":3394},{"title":3379,"description":25,"summary":3380,"pubDate":3381,"source":2720,"url":3382,"thumbnail":3383},"Understanding BigBird's Block Sparse Attention","Understanding BigBird's Block Sparse Attention Introduction Transformer-based models have shown to b...",["Date","2021-03-31T00:00:00.000Z"],"https://huggingface.co/blog/big-bird","https://huggingface.co/blog/assets/18_big_bird/attn.png","src/content/posts/2021-03-31-understanding-bigbirds-block-sparse-attention.md","376c3dee3592cd9a",{"html":25,"metadata":3387},{"headings":3388,"localImagePaths":3389,"remoteImagePaths":3390,"frontmatter":3391,"imagePaths":3393},[],[],[],{"title":3379,"description":25,"summary":3380,"pubDate":3392,"source":2720,"url":3382,"thumbnail":3383},"Wed, 31 Mar 2021 00:00:00 GMT",[],"2021-03-31-understanding-bigbirds-block-sparse-attention.md","2021-04-08-distributed-training-train-bartt5-for-summarization-using-transformers-and-amazon-sagemaker",{"id":3395,"data":3397,"filePath":3403,"digest":3404,"rendered":3405,"legacyId":3413},{"title":3398,"description":25,"summary":3399,"pubDate":3400,"source":2720,"url":3401,"thumbnail":3402},"Distributed Training: Train BART/T5 for Summarization using 🤗 Transformers and Amazon SageMaker","Distributed Training: Train BART/T5 for Summarization using 🤗 Transformers and Amazon SageMaker In c...",["Date","2021-04-08T00:00:00.000Z"],"https://huggingface.co/blog/sagemaker-distributed-training-seq2seq","https://huggingface.co/blog/assets/19_sagemaker_distributed_training_seq2seq/thumbnail.png","src/content/posts/2021-04-08-distributed-training-train-bartt5-for-summarization-using-transformers-and-amazon-sagemaker.md","e011600d2de42614",{"html":25,"metadata":3406},{"headings":3407,"localImagePaths":3408,"remoteImagePaths":3409,"frontmatter":3410,"imagePaths":3412},[],[],[],{"title":3398,"description":25,"summary":3399,"pubDate":3411,"source":2720,"url":3401,"thumbnail":3402},"Thu, 08 Apr 2021 00:00:00 GMT",[],"2021-04-08-distributed-training-train-bartt5-for-summarization-using-transformers-and-amazon-sagemaker.md","2021-04-16-introducing-accelerate",{"id":3414,"data":3416,"filePath":3422,"digest":3423,"rendered":3424,"legacyId":3432},{"title":3417,"description":25,"summary":3418,"pubDate":3419,"source":2720,"url":3420,"thumbnail":3421},"Introducing 🤗 Accelerate","Introducing 🤗 Accelerate 🤗 Accelerate Run your raw PyTorch training scripts on any kind of device. M...",["Date","2021-04-16T00:00:00.000Z"],"https://huggingface.co/blog/accelerate-library","https://huggingface.co/blog/assets/20_accelerate_library/accelerate_diff.png","src/content/posts/2021-04-16-introducing-accelerate.md","d75e8f05b5dc74c2",{"html":25,"metadata":3425},{"headings":3426,"localImagePaths":3427,"remoteImagePaths":3428,"frontmatter":3429,"imagePaths":3431},[],[],[],{"title":3417,"description":25,"summary":3418,"pubDate":3430,"source":2720,"url":3420,"thumbnail":3421},"Fri, 16 Apr 2021 00:00:00 GMT",[],"2021-04-16-introducing-accelerate.md","2021-04-20-scaling-up-bert-inference-on-cpu-part-1",{"id":3433,"data":3435,"filePath":3441,"digest":3442,"rendered":3443,"legacyId":3451},{"title":3436,"description":25,"summary":3437,"pubDate":3438,"source":2720,"url":3439,"thumbnail":3440},"Scaling-up BERT Inference on CPU (Part 1)","Scaling up BERT-like model Inference on modern CPU - Part 1 1. Context and Motivations Back in Octob...",["Date","2021-04-20T00:00:00.000Z"],"https://huggingface.co/blog/bert-cpu-scaling-part-1","https://huggingface.co/blog/assets/21_bert_cpu_scaling_part_1/imgs/numa_set.png","src/content/posts/2021-04-20-scaling-up-bert-inference-on-cpu-part-1.md","e6c4a364d6d0e20e",{"html":25,"metadata":3444},{"headings":3445,"localImagePaths":3446,"remoteImagePaths":3447,"frontmatter":3448,"imagePaths":3450},[],[],[],{"title":3436,"description":25,"summary":3437,"pubDate":3449,"source":2720,"url":3439,"thumbnail":3440},"Tue, 20 Apr 2021 00:00:00 GMT",[],"2021-04-20-scaling-up-bert-inference-on-cpu-part-1.md","2021-05-03-will-hurd-joins-openais-board-of-directors",{"id":3452,"data":3454,"filePath":3459,"digest":3460,"rendered":3461,"legacyId":3469},{"title":3455,"description":3456,"summary":3456,"pubDate":3457,"source":19,"url":3458,"thumbnail":21},"Will Hurd joins OpenAI’s board of directors","OpenAI is committed to developing general-purpose artificial intelligence that benefits all humanity, and we believe that achieving our goal requires expertise in public policy as well as technology. So, we’re delighted to announce that Congressman Will Hurd has joined our board of directors.",["Date","2021-05-03T07:00:00.000Z"],"https://openai.com/blog/will-hurd-joins","src/content/posts/2021-05-03-will-hurd-joins-openais-board-of-directors.md","4110c73eb226b336",{"html":25,"metadata":3462},{"headings":3463,"localImagePaths":3464,"remoteImagePaths":3465,"frontmatter":3466,"imagePaths":3468},[],[],[],{"title":3455,"description":3456,"summary":3456,"pubDate":3467,"source":19,"url":3458,"thumbnail":21},"Mon, 03 May 2021 07:00:00 GMT",[],"2021-05-03-will-hurd-joins-openais-board-of-directors.md","2021-05-10-openai-scholars-2021-final-projects",{"id":3470,"data":3472,"filePath":3477,"digest":3478,"rendered":3479,"legacyId":3487},{"title":3473,"description":3474,"summary":3474,"pubDate":3475,"source":19,"url":3476,"thumbnail":21},"OpenAI Scholars 2021: Final projects","We’re proud to announce that the 2021 class of OpenAI Scholars has completed our six-month mentorship program and have produced an open-source research project with stipends and support from OpenAI.",["Date","2021-05-10T07:00:00.000Z"],"https://openai.com/blog/openai-scholars-2021-final-projects","src/content/posts/2021-05-10-openai-scholars-2021-final-projects.md","1d0a3e97c6fcd813",{"html":25,"metadata":3480},{"headings":3481,"localImagePaths":3482,"remoteImagePaths":3483,"frontmatter":3484,"imagePaths":3486},[],[],[],{"title":3473,"description":3474,"summary":3474,"pubDate":3485,"source":19,"url":3476,"thumbnail":21},"Mon, 10 May 2021 07:00:00 GMT",[],"2021-05-10-openai-scholars-2021-final-projects.md","2021-05-25-using-mixing-hugging-face-models-with-gradio-20",{"id":3488,"data":3490,"filePath":3496,"digest":3497,"rendered":3498,"legacyId":3506},{"title":3491,"description":25,"summary":3492,"pubDate":3493,"source":2720,"url":3494,"thumbnail":3495},"Using & Mixing Hugging Face Models with Gradio 2.0","Using & Mixing Hugging Face Models with Gradio 2.0 Cross-posted from the Gradio blog. The Hugging Fa...",["Date","2021-05-25T00:00:00.000Z"],"https://huggingface.co/blog/gradio","https://huggingface.co/blog/assets/22_gradio/gradio.png","src/content/posts/2021-05-25-using-mixing-hugging-face-models-with-gradio-20.md","bafefbab94615fcf",{"html":25,"metadata":3499},{"headings":3500,"localImagePaths":3501,"remoteImagePaths":3502,"frontmatter":3503,"imagePaths":3505},[],[],[],{"title":3491,"description":25,"summary":3492,"pubDate":3504,"source":2720,"url":3494,"thumbnail":3495},"Tue, 25 May 2021 00:00:00 GMT",[],"2021-05-25-using-mixing-hugging-face-models-with-gradio-20.md","2021-06-03-few-shot-learning-in-practice-gpt-neo-and-the-accelerated-inference-api",{"id":3507,"data":3509,"filePath":3515,"digest":3516,"rendered":3517,"legacyId":3525},{"title":3510,"description":25,"summary":3511,"pubDate":3512,"source":2720,"url":3513,"thumbnail":3514},"Few-shot learning in practice: GPT-NEO and the 🤗 Accelerated Inference API","Few-shot learning in practice: GPT-Neo and the 🤗 Accelerated Inference API In many Machine Learning ...",["Date","2021-06-03T00:00:00.000Z"],"https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api","https://huggingface.co/front/thumbnails/v2-2.png","src/content/posts/2021-06-03-few-shot-learning-in-practice-gpt-neo-and-the-accelerated-inference-api.md","956ffadf838ac546",{"html":25,"metadata":3518},{"headings":3519,"localImagePaths":3520,"remoteImagePaths":3521,"frontmatter":3522,"imagePaths":3524},[],[],[],{"title":3510,"description":25,"summary":3511,"pubDate":3523,"source":2720,"url":3513,"thumbnail":3514},"Thu, 03 Jun 2021 00:00:00 GMT",[],"2021-06-03-few-shot-learning-in-practice-gpt-neo-and-the-accelerated-inference-api.md","2021-03-09-hugging-face-reads-feb-2021---long-range-transformers",{"id":3526,"data":3528,"filePath":3534,"digest":3535,"rendered":3536,"legacyId":3544},{"title":3529,"description":25,"summary":3530,"pubDate":3531,"source":2720,"url":3532,"thumbnail":3533},"Hugging Face Reads, Feb. 2021 - Long-range Transformers","Hugging Face Reads, Feb. 2021 - Long-range Transformers Co-written by Teven Le Scao, Patrick Von Pla...",["Date","2021-03-09T00:00:00.000Z"],"https://huggingface.co/blog/long-range-transformers","https://huggingface.co/blog/assets/14_long_range_transformers/EfficientTransformerTaxonomy.png","src/content/posts/2021-03-09-hugging-face-reads-feb-2021---long-range-transformers.md","adf73a4e35044e43",{"html":25,"metadata":3537},{"headings":3538,"localImagePaths":3539,"remoteImagePaths":3540,"frontmatter":3541,"imagePaths":3543},[],[],[],{"title":3529,"description":25,"summary":3530,"pubDate":3542,"source":2720,"url":3532,"thumbnail":3533},"Tue, 09 Mar 2021 00:00:00 GMT",[],"2021-03-09-hugging-face-reads-feb-2021---long-range-transformers.md","2021-06-10-improving-language-model-behavior-by-training-on-a-curated-dataset",{"id":3545,"data":3547,"filePath":3552,"digest":3553,"rendered":3554,"legacyId":3562},{"title":3548,"description":3549,"summary":3549,"pubDate":3550,"source":19,"url":3551,"thumbnail":21},"Improving language model behavior by training on a curated dataset","Our latest research finds we can improve language model behavior with respect to specific behavioral values by fine-tuning on a small, curated dataset.",["Date","2021-06-10T07:00:00.000Z"],"https://openai.com/blog/improving-language-model-behavior","src/content/posts/2021-06-10-improving-language-model-behavior-by-training-on-a-curated-dataset.md","7b64303d556d9fbf",{"html":25,"metadata":3555},{"headings":3556,"localImagePaths":3557,"remoteImagePaths":3558,"frontmatter":3559,"imagePaths":3561},[],[],[],{"title":3548,"description":3549,"summary":3549,"pubDate":3560,"source":19,"url":3551,"thumbnail":21},"Thu, 10 Jun 2021 07:00:00 GMT",[],"2021-06-10-improving-language-model-behavior-by-training-on-a-curated-dataset.md","2021-06-28-sentence-transformers-in-the-hub",{"id":3563,"data":3565,"filePath":3570,"digest":3571,"rendered":3572,"legacyId":3580},{"title":3566,"description":25,"summary":3567,"pubDate":3568,"source":2720,"url":3569,"thumbnail":3514},"Sentence Transformers in the 🤗 Hub","Sentence Transformers in the Hugging Face Hub Over the past few weeks, we've built collaborations wi...",["Date","2021-06-28T00:00:00.000Z"],"https://huggingface.co/blog/sentence-transformers-in-the-hub","src/content/posts/2021-06-28-sentence-transformers-in-the-hub.md","20b267c4453e4eb7",{"html":25,"metadata":3573},{"headings":3574,"localImagePaths":3575,"remoteImagePaths":3576,"frontmatter":3577,"imagePaths":3579},[],[],[],{"title":3566,"description":25,"summary":3567,"pubDate":3578,"source":2720,"url":3569,"thumbnail":3514},"Mon, 28 Jun 2021 00:00:00 GMT",[],"2021-06-28-sentence-transformers-in-the-hub.md","2021-07-07-evaluating-large-language-models-trained-on-code",{"id":3581,"data":3583,"filePath":3587,"digest":3588,"rendered":3589,"legacyId":3597},{"title":3584,"description":25,"summary":25,"pubDate":3585,"source":19,"url":3586,"thumbnail":21},"Evaluating large language models trained on code",["Date","2021-07-07T07:00:00.000Z"],"https://openai.com/blog/evaluating-large-language-models-trained-on-code","src/content/posts/2021-07-07-evaluating-large-language-models-trained-on-code.md","b738e07bb5de02f9",{"html":25,"metadata":3590},{"headings":3591,"localImagePaths":3592,"remoteImagePaths":3593,"frontmatter":3594,"imagePaths":3596},[],[],[],{"title":3584,"description":25,"summary":25,"pubDate":3595,"source":19,"url":3586,"thumbnail":21},"Wed, 07 Jul 2021 07:00:00 GMT",[],"2021-07-07-evaluating-large-language-models-trained-on-code.md","2021-07-08-deploy-hugging-face-models-easily-with-amazon-sagemaker",{"id":3598,"data":3600,"filePath":3605,"digest":3606,"rendered":3607,"legacyId":3615},{"title":3601,"description":25,"summary":3602,"pubDate":3603,"source":2720,"url":3604,"thumbnail":3346},"Deploy Hugging Face models easily with Amazon SageMaker","Deploy Hugging Face models easily with Amazon SageMaker 🏎 Earlier this year we announced a strategic...",["Date","2021-07-08T00:00:00.000Z"],"https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker","src/content/posts/2021-07-08-deploy-hugging-face-models-easily-with-amazon-sagemaker.md","0ed921bf3c46d744",{"html":25,"metadata":3608},{"headings":3609,"localImagePaths":3610,"remoteImagePaths":3611,"frontmatter":3612,"imagePaths":3614},[],[],[],{"title":3601,"description":25,"summary":3602,"pubDate":3613,"source":2720,"url":3604,"thumbnail":3346},"Thu, 08 Jul 2021 00:00:00 GMT",[],"2021-07-08-deploy-hugging-face-models-easily-with-amazon-sagemaker.md","2021-07-13-welcome-spacy-to-the-hub",{"id":3616,"data":3618,"filePath":3624,"digest":3625,"rendered":3626,"legacyId":3634},{"title":3619,"description":25,"summary":3620,"pubDate":3621,"source":2720,"url":3622,"thumbnail":3623},"Welcome spaCy to the 🤗 Hub","Welcome spaCy to the Hugging Face Hub spaCy is a popular library for advanced Natural Language Proce...",["Date","2021-07-13T00:00:00.000Z"],"https://huggingface.co/blog/spacy","https://huggingface.co/blog/assets/23_spacy/thumbnail.png","src/content/posts/2021-07-13-welcome-spacy-to-the-hub.md","57b51c8d3d832110",{"html":25,"metadata":3627},{"headings":3628,"localImagePaths":3629,"remoteImagePaths":3630,"frontmatter":3631,"imagePaths":3633},[],[],[],{"title":3619,"description":25,"summary":3620,"pubDate":3632,"source":2720,"url":3622,"thumbnail":3623},"Tue, 13 Jul 2021 00:00:00 GMT",[],"2021-07-13-welcome-spacy-to-the-hub.md","2021-07-15-deep-learning-over-the-internet-training-language-models-collaboratively",{"id":3635,"data":3637,"filePath":3643,"digest":3644,"rendered":3645,"legacyId":3653},{"title":3638,"description":25,"summary":3639,"pubDate":3640,"source":2720,"url":3641,"thumbnail":3642},"Deep Learning over the Internet: Training Language Models Collaboratively","Deep Learning over the Internet: Training Language Models Collaboratively Modern language models oft...",["Date","2021-07-15T00:00:00.000Z"],"https://huggingface.co/blog/collaborative-training","https://huggingface.co/blog/assets/24_sahajBERT/thumbnail.png","src/content/posts/2021-07-15-deep-learning-over-the-internet-training-language-models-collaboratively.md","cf2bf18da747f139",{"html":25,"metadata":3646},{"headings":3647,"localImagePaths":3648,"remoteImagePaths":3649,"frontmatter":3650,"imagePaths":3652},[],[],[],{"title":3638,"description":25,"summary":3639,"pubDate":3651,"source":2720,"url":3641,"thumbnail":3642},"Thu, 15 Jul 2021 00:00:00 GMT",[],"2021-07-15-deep-learning-over-the-internet-training-language-models-collaboratively.md","2021-07-28-introducing-triton-open-source-gpu-programming-for-neural-networks",{"id":3654,"data":3656,"filePath":3661,"digest":3662,"rendered":3663,"legacyId":3671},{"title":3657,"description":3658,"summary":3658,"pubDate":3659,"source":19,"url":3660,"thumbnail":21},"Introducing Triton: Open-source GPU programming for neural networks","We’re releasing Triton 1.0, an open-source Python-like programming language which enables researchers with no CUDA experience to write highly efficient GPU code—most of the time on par with what an expert would be able to produce.",["Date","2021-07-28T07:00:00.000Z"],"https://openai.com/blog/triton","src/content/posts/2021-07-28-introducing-triton-open-source-gpu-programming-for-neural-networks.md","c774fe3b89e75728",{"html":25,"metadata":3664},{"headings":3665,"localImagePaths":3666,"remoteImagePaths":3667,"frontmatter":3668,"imagePaths":3670},[],[],[],{"title":3657,"description":3658,"summary":3658,"pubDate":3669,"source":19,"url":3660,"thumbnail":21},"Wed, 28 Jul 2021 07:00:00 GMT",[],"2021-07-28-introducing-triton-open-source-gpu-programming-for-neural-networks.md","2021-08-10-openai-codex",{"id":3672,"data":3674,"filePath":3679,"digest":3680,"rendered":3681,"legacyId":3689},{"title":3675,"description":3676,"summary":3676,"pubDate":3677,"source":19,"url":3678,"thumbnail":21},"OpenAI Codex","We’ve created an improved version of OpenAI Codex, our AI system that translates natural language to code, and we are releasing it through our API in private beta starting today.",["Date","2021-08-10T07:00:00.000Z"],"https://openai.com/blog/openai-codex","src/content/posts/2021-08-10-openai-codex.md","2f35d23848b61aa9",{"html":25,"metadata":3682},{"headings":3683,"localImagePaths":3684,"remoteImagePaths":3685,"frontmatter":3686,"imagePaths":3688},[],[],[],{"title":3675,"description":3676,"summary":3676,"pubDate":3687,"source":19,"url":3678,"thumbnail":21},"Tue, 10 Aug 2021 07:00:00 GMT",[],"2021-08-10-openai-codex.md","2021-09-08-helen-toner-joins-openais-board-of-directors",{"id":3690,"data":3692,"filePath":3697,"digest":3698,"rendered":3699,"legacyId":3707},{"title":3693,"description":3694,"summary":3694,"pubDate":3695,"source":19,"url":3696,"thumbnail":21},"Helen Toner joins OpenAI’s board of directors","Today, we’re excited to announce the appointment of Helen Toner to our board of directors.",["Date","2021-09-08T07:00:00.000Z"],"https://openai.com/blog/helen-toner-joins","src/content/posts/2021-09-08-helen-toner-joins-openais-board-of-directors.md","9a04081d702d787e",{"html":25,"metadata":3700},{"headings":3701,"localImagePaths":3702,"remoteImagePaths":3703,"frontmatter":3704,"imagePaths":3706},[],[],[],{"title":3693,"description":3694,"summary":3694,"pubDate":3705,"source":19,"url":3696,"thumbnail":21},"Wed, 08 Sep 2021 07:00:00 GMT",[],"2021-09-08-helen-toner-joins-openais-board-of-directors.md","2021-09-14-hugging-face-and-graphcore-partner-for-ipu-optimized-transformers",{"id":3708,"data":3710,"filePath":3716,"digest":3717,"rendered":3718,"legacyId":3726},{"title":3711,"description":25,"summary":3712,"pubDate":3713,"source":2720,"url":3714,"thumbnail":3715},"Hugging Face and Graphcore partner for IPU-optimized Transformers","Hugging Face and Graphcore partner for IPU-optimized Transformers Speaking at the 2021 AI Hardware S...",["Date","2021-09-14T00:00:00.000Z"],"https://huggingface.co/blog/graphcore","https://huggingface.co/blog/assets/26_graphcore-ipu/thumbnail.png","src/content/posts/2021-09-14-hugging-face-and-graphcore-partner-for-ipu-optimized-transformers.md","50c4e6b2655781fc",{"html":25,"metadata":3719},{"headings":3720,"localImagePaths":3721,"remoteImagePaths":3722,"frontmatter":3723,"imagePaths":3725},[],[],[],{"title":3711,"description":25,"summary":3712,"pubDate":3724,"source":2720,"url":3714,"thumbnail":3715},"Tue, 14 Sep 2021 00:00:00 GMT",[],"2021-09-14-hugging-face-and-graphcore-partner-for-ipu-optimized-transformers.md","2021-09-14-introducing-optimum-the-optimization-toolkit-for-transformers-at-scale",{"id":3727,"data":3729,"filePath":3734,"digest":3735,"rendered":3736,"legacyId":3743},{"title":3730,"description":25,"summary":3731,"pubDate":3732,"source":2720,"url":3733,"thumbnail":3514},"Introducing Optimum: The Optimization Toolkit for Transformers at Scale","Introducing 🤗 Optimum: The Optimization Toolkit for Transformers at Scale This post is the first ste...",["Date","2021-09-14T00:00:00.000Z"],"https://huggingface.co/blog/hardware-partners-program","src/content/posts/2021-09-14-introducing-optimum-the-optimization-toolkit-for-transformers-at-scale.md","75cff8bac2f0a9c3",{"html":25,"metadata":3737},{"headings":3738,"localImagePaths":3739,"remoteImagePaths":3740,"frontmatter":3741,"imagePaths":3742},[],[],[],{"title":3730,"description":25,"summary":3731,"pubDate":3724,"source":2720,"url":3733,"thumbnail":3514},[],"2021-09-14-introducing-optimum-the-optimization-toolkit-for-transformers-at-scale.md","2021-09-08-truthfulqa-measuring-how-models-mimic-human-falsehoods",{"id":3744,"data":3746,"filePath":3750,"digest":3751,"rendered":3752,"legacyId":3759},{"title":3747,"description":25,"summary":25,"pubDate":3748,"source":19,"url":3749,"thumbnail":21},"TruthfulQA: Measuring how models mimic human falsehoods",["Date","2021-09-08T07:00:00.000Z"],"https://openai.com/blog/truthfulqa","src/content/posts/2021-09-08-truthfulqa-measuring-how-models-mimic-human-falsehoods.md","160f58757d093090",{"html":25,"metadata":3753},{"headings":3754,"localImagePaths":3755,"remoteImagePaths":3756,"frontmatter":3757,"imagePaths":3758},[],[],[],{"title":3747,"description":25,"summary":25,"pubDate":3705,"source":19,"url":3749,"thumbnail":21},[],"2021-09-08-truthfulqa-measuring-how-models-mimic-human-falsehoods.md","2021-09-23-summarizing-books-with-human-feedback",{"id":3760,"data":3762,"filePath":3767,"digest":3768,"rendered":3769,"legacyId":3777},{"title":3763,"description":3764,"summary":3764,"pubDate":3765,"source":19,"url":3766,"thumbnail":21},"Summarizing books with human feedback","Scaling human oversight of AI systems for tasks that are difficult to evaluate.",["Date","2021-09-23T07:00:00.000Z"],"https://openai.com/blog/summarizing-books","src/content/posts/2021-09-23-summarizing-books-with-human-feedback.md","218ef709d30bf38a",{"html":25,"metadata":3770},{"headings":3771,"localImagePaths":3772,"remoteImagePaths":3773,"frontmatter":3774,"imagePaths":3776},[],[],[],{"title":3763,"description":3764,"summary":3764,"pubDate":3775,"source":19,"url":3766,"thumbnail":21},"Thu, 23 Sep 2021 07:00:00 GMT",[],"2021-09-23-summarizing-books-with-human-feedback.md","2021-09-24-summer-at-hugging-face",{"id":3778,"data":3780,"filePath":3786,"digest":3787,"rendered":3788,"legacyId":3796},{"title":3781,"description":25,"summary":3782,"pubDate":3783,"source":2720,"url":3784,"thumbnail":3785},"Summer at Hugging Face ☀️","Summer At Hugging Face 😎 Summer is now officially over and these last few months have been quite bus...",["Date","2021-09-24T00:00:00.000Z"],"https://huggingface.co/blog/summer-at-huggingface","https://huggingface.co/blog/assets/27_summer_at_huggingface/summer_intro.gif","src/content/posts/2021-09-24-summer-at-hugging-face.md","a804884e60127894",{"html":25,"metadata":3789},{"headings":3790,"localImagePaths":3791,"remoteImagePaths":3792,"frontmatter":3793,"imagePaths":3795},[],[],[],{"title":3781,"description":25,"summary":3782,"pubDate":3794,"source":2720,"url":3784,"thumbnail":3785},"Fri, 24 Sep 2021 00:00:00 GMT",[],"2021-09-24-summer-at-hugging-face.md","2021-10-05-hosting-your-models-and-datasets-on-hugging-face-spaces-using-streamlit",{"id":3797,"data":3799,"filePath":3805,"digest":3806,"rendered":3807,"legacyId":3815},{"title":3800,"description":25,"summary":3801,"pubDate":3802,"source":2720,"url":3803,"thumbnail":3804},"Hosting your Models and Datasets on Hugging Face Spaces using Streamlit","Hosting your Models and Datasets on Hugging Face Spaces using Streamlit Showcase your Datasets and M...",["Date","2021-10-05T00:00:00.000Z"],"https://huggingface.co/blog/streamlit-spaces","https://huggingface.co/blog/assets/29_streamlit-spaces/thumbnail.png","src/content/posts/2021-10-05-hosting-your-models-and-datasets-on-hugging-face-spaces-using-streamlit.md","d9fa54e89ffa5315",{"html":25,"metadata":3808},{"headings":3809,"localImagePaths":3810,"remoteImagePaths":3811,"frontmatter":3812,"imagePaths":3814},[],[],[],{"title":3800,"description":25,"summary":3801,"pubDate":3813,"source":2720,"url":3803,"thumbnail":3804},"Tue, 05 Oct 2021 00:00:00 GMT",[],"2021-10-05-hosting-your-models-and-datasets-on-hugging-face-spaces-using-streamlit.md","2021-10-05-showcase-your-projects-in-spaces-using-gradio",{"id":3816,"data":3818,"filePath":3824,"digest":3825,"rendered":3826,"legacyId":3833},{"title":3819,"description":25,"summary":3820,"pubDate":3821,"source":2720,"url":3822,"thumbnail":3823},"Showcase Your Projects in Spaces using Gradio","Showcase Your Projects in Spaces using Gradio It's so easy to demonstrate a Machine Learning project...",["Date","2021-10-05T00:00:00.000Z"],"https://huggingface.co/blog/gradio-spaces","https://huggingface.co/blog/assets/28_gradio-spaces/thumbnail.png","src/content/posts/2021-10-05-showcase-your-projects-in-spaces-using-gradio.md","28922f3d6055318c",{"html":25,"metadata":3827},{"headings":3828,"localImagePaths":3829,"remoteImagePaths":3830,"frontmatter":3831,"imagePaths":3832},[],[],[],{"title":3819,"description":25,"summary":3820,"pubDate":3813,"source":2720,"url":3822,"thumbnail":3823},[],"2021-10-05-showcase-your-projects-in-spaces-using-gradio.md","2021-10-13-fine-tuning-clip-with-remote-sensing-satellite-images-and-captions",{"id":3834,"data":3836,"filePath":3842,"digest":3843,"rendered":3844,"legacyId":3852},{"title":3837,"description":25,"summary":3838,"pubDate":3839,"source":2720,"url":3840,"thumbnail":3841},"Fine tuning CLIP with Remote Sensing (Satellite) images and captions","Fine tuning CLIP with Remote Sensing (Satellite) images and captions Fine tuning CLIP with Remote Se...",["Date","2021-10-13T00:00:00.000Z"],"https://huggingface.co/blog/fine-tune-clip-rsicd","https://huggingface.co/blog/assets/30_clip_rsicd/clip_schematic.png","src/content/posts/2021-10-13-fine-tuning-clip-with-remote-sensing-satellite-images-and-captions.md","b530fd526120b135",{"html":25,"metadata":3845},{"headings":3846,"localImagePaths":3847,"remoteImagePaths":3848,"frontmatter":3849,"imagePaths":3851},[],[],[],{"title":3837,"description":25,"summary":3838,"pubDate":3850,"source":2720,"url":3840,"thumbnail":3841},"Wed, 13 Oct 2021 00:00:00 GMT",[],"2021-10-13-fine-tuning-clip-with-remote-sensing-satellite-images-and-captions.md","2021-10-25-train-a-sentence-embedding-model-with-1b-training-pairs",{"id":3853,"data":3855,"filePath":3860,"digest":3861,"rendered":3862,"legacyId":3870},{"title":3856,"description":25,"summary":3857,"pubDate":3858,"source":2720,"url":3859,"thumbnail":3514},"Train a Sentence Embedding Model with 1B Training Pairs","Train a Sentence Embedding Model with 1 Billion Training Pairs Sentence embedding is a method that m...",["Date","2021-10-25T00:00:00.000Z"],"https://huggingface.co/blog/1b-sentence-embeddings","src/content/posts/2021-10-25-train-a-sentence-embedding-model-with-1b-training-pairs.md","2dbb0a17502ac869",{"html":25,"metadata":3863},{"headings":3864,"localImagePaths":3865,"remoteImagePaths":3866,"frontmatter":3867,"imagePaths":3869},[],[],[],{"title":3856,"description":25,"summary":3857,"pubDate":3868,"source":2720,"url":3859,"thumbnail":3514},"Mon, 25 Oct 2021 00:00:00 GMT",[],"2021-10-25-train-a-sentence-embedding-model-with-1b-training-pairs.md","2021-10-20-the-age-of-machine-learning-as-code-has-arrived",{"id":3871,"data":3873,"filePath":3879,"digest":3880,"rendered":3881,"legacyId":3889},{"title":3874,"description":25,"summary":3875,"pubDate":3876,"source":2720,"url":3877,"thumbnail":3878},"The Age of Machine Learning As Code Has Arrived","The Age of Machine Learning As Code Has Arrived The 2021 edition of the State of AI Report came out ...",["Date","2021-10-20T00:00:00.000Z"],"https://huggingface.co/blog/the-age-of-ml-as-code","https://huggingface.co/blog/assets/31_age_of_ml_as_code/05_vision_transformer.png","src/content/posts/2021-10-20-the-age-of-machine-learning-as-code-has-arrived.md","729327a472163469",{"html":25,"metadata":3882},{"headings":3883,"localImagePaths":3884,"remoteImagePaths":3885,"frontmatter":3886,"imagePaths":3888},[],[],[],{"title":3874,"description":25,"summary":3875,"pubDate":3887,"source":2720,"url":3877,"thumbnail":3878},"Wed, 20 Oct 2021 00:00:00 GMT",[],"2021-10-20-the-age-of-machine-learning-as-code-has-arrived.md","2021-10-26-large-language-models-a-new-moores-law",{"id":3890,"data":3892,"filePath":3898,"digest":3899,"rendered":3900,"legacyId":3908},{"title":3893,"description":25,"summary":3894,"pubDate":3895,"source":2720,"url":3896,"thumbnail":3897},"Large Language Models: A New Moore's Law?","Large Language Models: A New Moore's Law? A few days ago, Microsoft and NVIDIA introduced Megatron-T...",["Date","2021-10-26T00:00:00.000Z"],"https://huggingface.co/blog/large-language-models","https://huggingface.co/blog/assets/33_large_language_models/01_model_size.jpg","src/content/posts/2021-10-26-large-language-models-a-new-moores-law.md","38e8b837a2896edf",{"html":25,"metadata":3901},{"headings":3902,"localImagePaths":3903,"remoteImagePaths":3904,"frontmatter":3905,"imagePaths":3907},[],[],[],{"title":3893,"description":25,"summary":3894,"pubDate":3906,"source":2720,"url":3896,"thumbnail":3897},"Tue, 26 Oct 2021 00:00:00 GMT",[],"2021-10-26-large-language-models-a-new-moores-law.md","2021-10-26-course-launch-community-event",{"id":3909,"data":3911,"filePath":3917,"digest":3918,"rendered":3919,"legacyId":3926},{"title":3912,"description":25,"summary":3913,"pubDate":3914,"source":2720,"url":3915,"thumbnail":3916},"Course Launch Community Event","Course Launch Community Event We are excited to share that after a lot of work from the Hugging Face...",["Date","2021-10-26T00:00:00.000Z"],"https://huggingface.co/blog/course-launch-event","https://huggingface.co/blog/assets/34_course_launch/speakers_day1_thumb.png","src/content/posts/2021-10-26-course-launch-community-event.md","eceb77cf5535a620",{"html":25,"metadata":3920},{"headings":3921,"localImagePaths":3922,"remoteImagePaths":3923,"frontmatter":3924,"imagePaths":3925},[],[],[],{"title":3912,"description":25,"summary":3913,"pubDate":3906,"source":2720,"url":3915,"thumbnail":3916},[],"2021-10-26-course-launch-community-event.md","2021-10-29-solving-math-word-problems",{"id":3927,"data":3929,"filePath":3934,"digest":3935,"rendered":3936,"legacyId":3944},{"title":3930,"description":3931,"summary":3931,"pubDate":3932,"source":19,"url":3933,"thumbnail":21},"Solving math word problems","We’ve trained a system that solves grade school math problems with nearly twice the accuracy of a fine-tuned GPT-3 model. It solves about 90% as many problems as real kids: a small sample of 9-12 year olds scored 60% on a test from our dataset, while our system scored 55% on those same problems.",["Date","2021-10-29T07:00:00.000Z"],"https://openai.com/blog/solving-math-word-problems","src/content/posts/2021-10-29-solving-math-word-problems.md","ec408368f8bfc072",{"html":25,"metadata":3937},{"headings":3938,"localImagePaths":3939,"remoteImagePaths":3940,"frontmatter":3941,"imagePaths":3943},[],[],[],{"title":3930,"description":3931,"summary":3931,"pubDate":3942,"source":19,"url":3933,"thumbnail":21},"Fri, 29 Oct 2021 07:00:00 GMT",[],"2021-10-29-solving-math-word-problems.md","2021-11-04-scaling-up-bert-like-model-inference-on-modern-cpu---part-2",{"id":3945,"data":3947,"filePath":3952,"digest":3953,"rendered":3954,"legacyId":3962},{"title":3948,"description":25,"summary":3949,"pubDate":3950,"source":2720,"url":3951,"thumbnail":3514},"Scaling up BERT-like model Inference on modern CPU - Part 2","Scaling up BERT-like model Inference on modern CPU - Part 2 Introduction: Using Intel Software to Op...",["Date","2021-11-04T00:00:00.000Z"],"https://huggingface.co/blog/bert-cpu-scaling-part-2","src/content/posts/2021-11-04-scaling-up-bert-like-model-inference-on-modern-cpu---part-2.md","44c8c158aab6fe93",{"html":25,"metadata":3955},{"headings":3956,"localImagePaths":3957,"remoteImagePaths":3958,"frontmatter":3959,"imagePaths":3961},[],[],[],{"title":3948,"description":25,"summary":3949,"pubDate":3960,"source":2720,"url":3951,"thumbnail":3514},"Thu, 04 Nov 2021 00:00:00 GMT",[],"2021-11-04-scaling-up-bert-like-model-inference-on-modern-cpu---part-2.md","2021-11-15-fine-tuning-xls-r-for-multi-lingual-asr-with-transformers",{"id":3963,"data":3965,"filePath":3971,"digest":3972,"rendered":3973,"legacyId":3981},{"title":3966,"description":25,"summary":3967,"pubDate":3968,"source":2720,"url":3969,"thumbnail":3970},"Fine-tuning XLS-R for Multi-Lingual ASR with 🤗 Transformers","Fine-tuning XLS-R for Multi-Lingual ASR with 🤗 Transformers New (11/2021): This blog post has been u...",["Date","2021-11-15T00:00:00.000Z"],"https://huggingface.co/blog/fine-tune-xlsr-wav2vec2","https://huggingface.co/blog/assets/xlsr_wav2vec2.png","src/content/posts/2021-11-15-fine-tuning-xls-r-for-multi-lingual-asr-with-transformers.md","685824afb7cd9bb2",{"html":25,"metadata":3974},{"headings":3975,"localImagePaths":3976,"remoteImagePaths":3977,"frontmatter":3978,"imagePaths":3980},[],[],[],{"title":3966,"description":25,"summary":3967,"pubDate":3979,"source":2720,"url":3969,"thumbnail":3970},"Mon, 15 Nov 2021 00:00:00 GMT",[],"2021-11-15-fine-tuning-xls-r-for-multi-lingual-asr-with-transformers.md","2021-11-19-accelerating-pytorch-distributed-fine-tuning-with-intel-technologies",{"id":3982,"data":3984,"filePath":3990,"digest":3991,"rendered":3992,"legacyId":4000},{"title":3985,"description":25,"summary":3986,"pubDate":3987,"source":2720,"url":3988,"thumbnail":3989},"Accelerating PyTorch distributed fine-tuning with Intel technologies","Accelerating PyTorch distributed fine-tuning with Intel technologies For all their amazing performan...",["Date","2021-11-19T00:00:00.000Z"],"https://huggingface.co/blog/accelerating-pytorch","https://huggingface.co/blog/assets/36_accelerating_pytorch/04_four_nodes.png","src/content/posts/2021-11-19-accelerating-pytorch-distributed-fine-tuning-with-intel-technologies.md","56b577a3f25c2a65",{"html":25,"metadata":3993},{"headings":3994,"localImagePaths":3995,"remoteImagePaths":3996,"frontmatter":3997,"imagePaths":3999},[],[],[],{"title":3985,"description":25,"summary":3986,"pubDate":3998,"source":2720,"url":3988,"thumbnail":3989},"Fri, 19 Nov 2021 00:00:00 GMT",[],"2021-11-19-accelerating-pytorch-distributed-fine-tuning-with-intel-technologies.md","2021-11-18-openais-api-now-available-with-no-waitlist",{"id":4001,"data":4003,"filePath":4008,"digest":4009,"rendered":4010,"legacyId":4018},{"title":4004,"description":4005,"summary":4005,"pubDate":4006,"source":19,"url":4007,"thumbnail":21},"OpenAI’s API now available with no waitlist","Wider availability made possible by safety progress.",["Date","2021-11-18T08:00:00.000Z"],"https://openai.com/blog/api-no-waitlist","src/content/posts/2021-11-18-openais-api-now-available-with-no-waitlist.md","6ba5c052a3141fb2",{"html":25,"metadata":4011},{"headings":4012,"localImagePaths":4013,"remoteImagePaths":4014,"frontmatter":4015,"imagePaths":4017},[],[],[],{"title":4004,"description":4005,"summary":4005,"pubDate":4016,"source":19,"url":4007,"thumbnail":21},"Thu, 18 Nov 2021 08:00:00 GMT",[],"2021-11-18-openais-api-now-available-with-no-waitlist.md","2021-11-29-introducing-the-data-measurements-tool-an-interactive-tool-for-looking-at-datasets",{"id":4019,"data":4021,"filePath":4027,"digest":4028,"rendered":4029,"legacyId":4037},{"title":4022,"description":25,"summary":4023,"pubDate":4024,"source":2720,"url":4025,"thumbnail":4026},"Introducing the Data Measurements Tool: an Interactive Tool for Looking at Datasets","Introducing the 🤗 Data Measurements Tool: an Interactive Tool for Looking at Datasets tl;dr: We made...",["Date","2021-11-29T00:00:00.000Z"],"https://huggingface.co/blog/data-measurements-tool","https://huggingface.co/blog/assets/37_data-measurements-tool/datametrics.png","src/content/posts/2021-11-29-introducing-the-data-measurements-tool-an-interactive-tool-for-looking-at-datasets.md","818250cbb5c59309",{"html":25,"metadata":4030},{"headings":4031,"localImagePaths":4032,"remoteImagePaths":4033,"frontmatter":4034,"imagePaths":4036},[],[],[],{"title":4022,"description":25,"summary":4023,"pubDate":4035,"source":2720,"url":4025,"thumbnail":4026},"Mon, 29 Nov 2021 00:00:00 GMT",[],"2021-11-29-introducing-the-data-measurements-tool-an-interactive-tool-for-looking-at-datasets.md","2021-11-30-getting-started-with-hugging-face-transformers-for-ipus-with-optimum",{"id":4038,"data":4040,"filePath":4046,"digest":4047,"rendered":4048,"legacyId":4056},{"title":4041,"description":25,"summary":4042,"pubDate":4043,"source":2720,"url":4044,"thumbnail":4045},"Getting Started with Hugging Face Transformers for IPUs with Optimum","Getting Started with Hugging Face Transformers for IPUs with Optimum Transformer models have proven ...",["Date","2021-11-30T00:00:00.000Z"],"https://huggingface.co/blog/graphcore-getting-started","https://huggingface.co/blog/assets/38_getting_started_graphcore/graphcore_1.png","src/content/posts/2021-11-30-getting-started-with-hugging-face-transformers-for-ipus-with-optimum.md","42abfb02f5dc0988",{"html":25,"metadata":4049},{"headings":4050,"localImagePaths":4051,"remoteImagePaths":4052,"frontmatter":4053,"imagePaths":4055},[],[],[],{"title":4041,"description":25,"summary":4042,"pubDate":4054,"source":2720,"url":4044,"thumbnail":4045},"Tue, 30 Nov 2021 00:00:00 GMT",[],"2021-11-30-getting-started-with-hugging-face-transformers-for-ipus-with-optimum.md","2021-11-30-openai-residency",{"id":4057,"data":4059,"filePath":4064,"digest":4065,"rendered":4066,"legacyId":4074},{"title":4060,"description":4061,"summary":4061,"pubDate":4062,"source":19,"url":4063,"thumbnail":21},"OpenAI Residency","As part of our effort to support and develop AI talent, we’re excited to announce the OpenAI Residency.",["Date","2021-11-30T08:00:00.000Z"],"https://openai.com/blog/openai-residency","src/content/posts/2021-11-30-openai-residency.md","104ce8f533efa626",{"html":25,"metadata":4067},{"headings":4068,"localImagePaths":4069,"remoteImagePaths":4070,"frontmatter":4071,"imagePaths":4073},[],[],[],{"title":4060,"description":4061,"summary":4061,"pubDate":4072,"source":19,"url":4063,"thumbnail":21},"Tue, 30 Nov 2021 08:00:00 GMT",[],"2021-11-30-openai-residency.md","2021-12-02-introducing-snowball-fight-our-first-ml-agents-environment",{"id":4075,"data":4077,"filePath":4083,"digest":4084,"rendered":4085,"legacyId":4093},{"title":4078,"description":25,"summary":4079,"pubDate":4080,"source":2720,"url":4081,"thumbnail":4082},"Introducing Snowball Fight ☃️, our First ML-Agents Environment","Introducing Snowball Fight ☃️, our First ML-Agents Environment We're excited to share our first cust...",["Date","2021-12-02T00:00:00.000Z"],"https://huggingface.co/blog/snowball-fight","https://huggingface.co/blog/assets/39_introducing_snowball_fight/thumbnail.png","src/content/posts/2021-12-02-introducing-snowball-fight-our-first-ml-agents-environment.md","14e744083dcff274",{"html":25,"metadata":4086},{"headings":4087,"localImagePaths":4088,"remoteImagePaths":4089,"frontmatter":4090,"imagePaths":4092},[],[],[],{"title":4078,"description":25,"summary":4079,"pubDate":4091,"source":2720,"url":4081,"thumbnail":4082},"Thu, 02 Dec 2021 00:00:00 GMT",[],"2021-12-02-introducing-snowball-fight-our-first-ml-agents-environment.md","2021-12-14-customizing-gpt-3-for-your-application",{"id":4094,"data":4096,"filePath":4101,"digest":4102,"rendered":4103,"legacyId":4111},{"title":4097,"description":4098,"summary":4098,"pubDate":4099,"source":19,"url":4100,"thumbnail":21},"Customizing GPT-3 for your application","Fine-tune with a single command.",["Date","2021-12-14T08:00:00.000Z"],"https://openai.com/blog/customizing-gpt-3","src/content/posts/2021-12-14-customizing-gpt-3-for-your-application.md","1fe545fb3676f9ca",{"html":25,"metadata":4104},{"headings":4105,"localImagePaths":4106,"remoteImagePaths":4107,"frontmatter":4108,"imagePaths":4110},[],[],[],{"title":4097,"description":4098,"summary":4098,"pubDate":4109,"source":19,"url":4100,"thumbnail":21},"Tue, 14 Dec 2021 08:00:00 GMT",[],"2021-12-14-customizing-gpt-3-for-your-application.md","2021-12-08-training-codeparrot-from-scratch",{"id":4112,"data":4114,"filePath":4120,"digest":4121,"rendered":4122,"legacyId":4130},{"title":4115,"description":25,"summary":4116,"pubDate":4117,"source":2720,"url":4118,"thumbnail":4119},"Training CodeParrot 🦜 from Scratch","Training CodeParrot 🦜 from Scratch In this blog post we'll take a look at what it takes to build the...",["Date","2021-12-08T00:00:00.000Z"],"https://huggingface.co/blog/codeparrot","https://huggingface.co/blog/assets/40_codeparrot/thumbnail.png","src/content/posts/2021-12-08-training-codeparrot-from-scratch.md","10ff2ce5a03173dd",{"html":25,"metadata":4123},{"headings":4124,"localImagePaths":4125,"remoteImagePaths":4126,"frontmatter":4127,"imagePaths":4129},[],[],[],{"title":4115,"description":25,"summary":4116,"pubDate":4128,"source":2720,"url":4118,"thumbnail":4119},"Wed, 08 Dec 2021 00:00:00 GMT",[],"2021-12-08-training-codeparrot-from-scratch.md","2021-12-15-perceiver-io-a-scalable-fully-attentional-model-that-works-on-any-modality",{"id":4131,"data":4133,"filePath":4139,"digest":4140,"rendered":4141,"legacyId":4149},{"title":4134,"description":25,"summary":4135,"pubDate":4136,"source":2720,"url":4137,"thumbnail":4138},"Perceiver IO: a scalable, fully-attentional model that works on any modality","Perceiver IO: a scalable, fully-attentional model that works on any modality TLDR We've added Percei...",["Date","2021-12-15T00:00:00.000Z"],"https://huggingface.co/blog/perceiver","https://huggingface.co/blog/assets/41_perceiver/thumbnail.png","src/content/posts/2021-12-15-perceiver-io-a-scalable-fully-attentional-model-that-works-on-any-modality.md","838b8dcdfa61fd2f",{"html":25,"metadata":4142},{"headings":4143,"localImagePaths":4144,"remoteImagePaths":4145,"frontmatter":4146,"imagePaths":4148},[],[],[],{"title":4134,"description":25,"summary":4135,"pubDate":4147,"source":2720,"url":4137,"thumbnail":4138},"Wed, 15 Dec 2021 00:00:00 GMT",[],"2021-12-15-perceiver-io-a-scalable-fully-attentional-model-that-works-on-any-modality.md","2021-12-16-webgpt-improving-the-factual-accuracy-of-language-models-through-web-browsing",{"id":4150,"data":4152,"filePath":4157,"digest":4158,"rendered":4159,"legacyId":4167},{"title":4153,"description":4154,"summary":4154,"pubDate":4155,"source":19,"url":4156,"thumbnail":21},"WebGPT: Improving the factual accuracy of language models through web browsing","We’ve fine-tuned GPT-3 to more accurately answer open-ended questions using a text-based web browser.",["Date","2021-12-16T08:00:00.000Z"],"https://openai.com/blog/webgpt","src/content/posts/2021-12-16-webgpt-improving-the-factual-accuracy-of-language-models-through-web-browsing.md","8087caae8b331bf9",{"html":25,"metadata":4160},{"headings":4161,"localImagePaths":4162,"remoteImagePaths":4163,"frontmatter":4164,"imagePaths":4166},[],[],[],{"title":4153,"description":4154,"summary":4154,"pubDate":4165,"source":19,"url":4156,"thumbnail":21},"Thu, 16 Dec 2021 08:00:00 GMT",[],"2021-12-16-webgpt-improving-the-factual-accuracy-of-language-models-through-web-browsing.md","2021-12-21-gradio-joins-hugging-face",{"id":4168,"data":4170,"filePath":4176,"digest":4177,"rendered":4178,"legacyId":4186},{"title":4171,"description":25,"summary":4172,"pubDate":4173,"source":2720,"url":4174,"thumbnail":4175},"Gradio joins Hugging Face!","Gradio is joining Hugging Face! Gradio is joining Hugging Face! By acquiring Gradio, a machine learn...",["Date","2021-12-21T00:00:00.000Z"],"https://huggingface.co/blog/gradio-joins-hf","https://huggingface.co/blog/assets/42_gradio_joins_hf/thumbnail.png","src/content/posts/2021-12-21-gradio-joins-hugging-face.md","af8593b9018cd5cf",{"html":25,"metadata":4179},{"headings":4180,"localImagePaths":4181,"remoteImagePaths":4182,"frontmatter":4183,"imagePaths":4185},[],[],[],{"title":4171,"description":25,"summary":4172,"pubDate":4184,"source":2720,"url":4174,"thumbnail":4175},"Tue, 21 Dec 2021 00:00:00 GMT",[],"2021-12-21-gradio-joins-hugging-face.md","2021-12-23-active-learning-with-autonlp-and-prodigy",{"id":4187,"data":4189,"filePath":4195,"digest":4196,"rendered":4197,"legacyId":4205},{"title":4190,"description":25,"summary":4191,"pubDate":4192,"source":2720,"url":4193,"thumbnail":4194},"Active Learning with AutoNLP and Prodigy","Active Learning with AutoNLP and Prodigy Active learning in the context of Machine Learning is a pro...",["Date","2021-12-23T00:00:00.000Z"],"https://huggingface.co/blog/autonlp-prodigy","https://huggingface.co/blog/assets/43_autonlp_prodigy/thumbnail.png","src/content/posts/2021-12-23-active-learning-with-autonlp-and-prodigy.md","7021a1fadd34b10f",{"html":25,"metadata":4198},{"headings":4199,"localImagePaths":4200,"remoteImagePaths":4201,"frontmatter":4202,"imagePaths":4204},[],[],[],{"title":4190,"description":25,"summary":4191,"pubDate":4203,"source":2720,"url":4193,"thumbnail":4194},"Thu, 23 Dec 2021 00:00:00 GMT",[],"2021-12-23-active-learning-with-autonlp-and-prodigy.md","2022-01-11-deploy-gpt-j-6b-for-inference-using-hugging-face-transformers-and-amazon-sagemaker",{"id":4206,"data":4208,"filePath":4214,"digest":4215,"rendered":4216,"legacyId":4224},{"title":4209,"description":25,"summary":4210,"pubDate":4211,"source":2720,"url":4212,"thumbnail":4213},"Deploy GPT-J 6B for inference using Hugging Face Transformers and Amazon SageMaker","Deploy GPT-J 6B for inference using Hugging Face Transformers and Amazon SageMaker Almost 6 months a...",["Date","2022-01-11T00:00:00.000Z"],"https://huggingface.co/blog/gptj-sagemaker","https://huggingface.co/blog/assets/45_gptj_sagemaker/thumbnail.png","src/content/posts/2022-01-11-deploy-gpt-j-6b-for-inference-using-hugging-face-transformers-and-amazon-sagemaker.md","d362fa6704d21516",{"html":25,"metadata":4217},{"headings":4218,"localImagePaths":4219,"remoteImagePaths":4220,"frontmatter":4221,"imagePaths":4223},[],[],[],{"title":4209,"description":25,"summary":4210,"pubDate":4222,"source":2720,"url":4212,"thumbnail":4213},"Tue, 11 Jan 2022 00:00:00 GMT",[],"2022-01-11-deploy-gpt-j-6b-for-inference-using-hugging-face-transformers-and-amazon-sagemaker.md","2022-01-12-boost-wav2vec2-with-n-gram-lm-in-transformers",{"id":4225,"data":4227,"filePath":4233,"digest":4234,"rendered":4235,"legacyId":4243},{"title":4228,"description":25,"summary":4229,"pubDate":4230,"source":2720,"url":4231,"thumbnail":4232},"Boost Wav2Vec2 with n-gram LM in 🤗 Transformers","Boosting Wav2Vec2 with n-grams in 🤗 Transformers Wav2Vec2 is a popular pre-trained model for speech ...",["Date","2022-01-12T00:00:00.000Z"],"https://huggingface.co/blog/wav2vec2-with-ngram","https://huggingface.co/blog/assets/44_boost_wav2vec2_ngram/wav2vec2_ngram.png","src/content/posts/2022-01-12-boost-wav2vec2-with-n-gram-lm-in-transformers.md","97317252963d0855",{"html":25,"metadata":4236},{"headings":4237,"localImagePaths":4238,"remoteImagePaths":4239,"frontmatter":4240,"imagePaths":4242},[],[],[],{"title":4228,"description":25,"summary":4229,"pubDate":4241,"source":2720,"url":4231,"thumbnail":4232},"Wed, 12 Jan 2022 00:00:00 GMT",[],"2022-01-12-boost-wav2vec2-with-n-gram-lm-in-transformers.md","2022-01-13-case-study-millisecond-latency-using-hugging-face-infinity-and-modern-cpus",{"id":4244,"data":4246,"filePath":4252,"digest":4253,"rendered":4254,"legacyId":4262},{"title":4247,"description":25,"summary":4248,"pubDate":4249,"source":2720,"url":4250,"thumbnail":4251},"Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs","Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs Inference Endpoints to e...",["Date","2022-01-13T00:00:00.000Z"],"https://huggingface.co/blog/infinity-cpu-performance","https://huggingface.co/blog/assets/46_infinity_cpu_performance/thumbnail.png","src/content/posts/2022-01-13-case-study-millisecond-latency-using-hugging-face-infinity-and-modern-cpus.md","8352d09c043338e4",{"html":25,"metadata":4255},{"headings":4256,"localImagePaths":4257,"remoteImagePaths":4258,"frontmatter":4259,"imagePaths":4261},[],[],[],{"title":4247,"description":25,"summary":4248,"pubDate":4260,"source":2720,"url":4250,"thumbnail":4251},"Thu, 13 Jan 2022 00:00:00 GMT",[],"2022-01-13-case-study-millisecond-latency-using-hugging-face-infinity-and-modern-cpus.md","2022-01-21-welcome-stable-baselines3-to-the-hugging-face-hub",{"id":4263,"data":4265,"filePath":4271,"digest":4272,"rendered":4273,"legacyId":4281},{"title":4266,"description":25,"summary":4267,"pubDate":4268,"source":2720,"url":4269,"thumbnail":4270},"Welcome Stable-baselines3 to the Hugging Face Hub 🤗","Welcome Stable-baselines3 to the Hugging Face Hub 🤗 At Hugging Face, we are contributing to the ecos...",["Date","2022-01-21T00:00:00.000Z"],"https://huggingface.co/blog/sb3","https://huggingface.co/blog/assets/47_sb3/thumbnail.png","src/content/posts/2022-01-21-welcome-stable-baselines3-to-the-hugging-face-hub.md","15aa122ca6daf41a",{"html":25,"metadata":4274},{"headings":4275,"localImagePaths":4276,"remoteImagePaths":4277,"frontmatter":4278,"imagePaths":4280},[],[],[],{"title":4266,"description":25,"summary":4267,"pubDate":4279,"source":2720,"url":4269,"thumbnail":4270},"Fri, 21 Jan 2022 00:00:00 GMT",[],"2022-01-21-welcome-stable-baselines3-to-the-hugging-face-hub.md","2022-01-24-text-and-code-embeddings-by-contrastive-pre-training",{"id":4282,"data":4284,"filePath":4288,"digest":4289,"rendered":4290,"legacyId":4298},{"title":4285,"description":25,"summary":25,"pubDate":4286,"source":19,"url":4287,"thumbnail":21},"Text and code embeddings by contrastive pre-training",["Date","2022-01-24T08:00:00.000Z"],"https://openai.com/blog/text-and-code-embeddings-by-contrastive-pre-training","src/content/posts/2022-01-24-text-and-code-embeddings-by-contrastive-pre-training.md","f4537b98bd05bcd7",{"html":25,"metadata":4291},{"headings":4292,"localImagePaths":4293,"remoteImagePaths":4294,"frontmatter":4295,"imagePaths":4297},[],[],[],{"title":4285,"description":25,"summary":25,"pubDate":4296,"source":19,"url":4287,"thumbnail":21},"Mon, 24 Jan 2022 08:00:00 GMT",[],"2022-01-24-text-and-code-embeddings-by-contrastive-pre-training.md","2022-01-25-introducing-text-and-code-embeddings",{"id":4299,"data":4301,"filePath":4306,"digest":4307,"rendered":4308,"legacyId":4316},{"title":4302,"description":4303,"summary":4303,"pubDate":4304,"source":19,"url":4305,"thumbnail":21},"Introducing text and code embeddings","We are introducing embeddings, a new endpoint in the OpenAI API that makes it easy to perform natural language and code tasks like semantic search, clustering, topic modeling, and classification.",["Date","2022-01-25T08:00:00.000Z"],"https://openai.com/blog/introducing-text-and-code-embeddings","src/content/posts/2022-01-25-introducing-text-and-code-embeddings.md","1a4ec364a30198d4",{"html":25,"metadata":4309},{"headings":4310,"localImagePaths":4311,"remoteImagePaths":4312,"frontmatter":4313,"imagePaths":4315},[],[],[],{"title":4302,"description":4303,"summary":4303,"pubDate":4314,"source":19,"url":4305,"thumbnail":21},"Tue, 25 Jan 2022 08:00:00 GMT",[],"2022-01-25-introducing-text-and-code-embeddings.md","2022-01-27-aligning-language-models-to-follow-instructions",{"id":4317,"data":4319,"filePath":4324,"digest":4325,"rendered":4326,"legacyId":4334},{"title":4320,"description":4321,"summary":4321,"pubDate":4322,"source":19,"url":4323,"thumbnail":21},"Aligning language models to follow instructions","We’ve trained language models that are much better at following user intentions than GPT-3 while also making them more truthful and less toxic, using techniques developed through our alignment research. These InstructGPT models, which are trained with humans in the loop, are now deployed as the default language models on our API.",["Date","2022-01-27T08:00:00.000Z"],"https://openai.com/blog/instruction-following","src/content/posts/2022-01-27-aligning-language-models-to-follow-instructions.md","6ef1b0e7f723b6b8",{"html":25,"metadata":4327},{"headings":4328,"localImagePaths":4329,"remoteImagePaths":4330,"frontmatter":4331,"imagePaths":4333},[],[],[],{"title":4320,"description":4321,"summary":4321,"pubDate":4332,"source":19,"url":4323,"thumbnail":21},"Thu, 27 Jan 2022 08:00:00 GMT",[],"2022-01-27-aligning-language-models-to-follow-instructions.md","2022-01-25-supercharged-searching-on-the-hugging-face-hub",{"id":4335,"data":4337,"filePath":4343,"digest":4344,"rendered":4345,"legacyId":4353},{"title":4338,"description":25,"summary":4339,"pubDate":4340,"source":2720,"url":4341,"thumbnail":4342},"Supercharged Searching on the Hugging Face Hub","Supercharged Searching on the Hugging Face Hub The huggingface_hub library is a lightweight interfac...",["Date","2022-01-25T00:00:00.000Z"],"https://huggingface.co/blog/searching-the-hub","https://huggingface.co/blog/assets/48_hubsearch/thumbnail.png","src/content/posts/2022-01-25-supercharged-searching-on-the-hugging-face-hub.md","b727256a8b6f1b75",{"html":25,"metadata":4346},{"headings":4347,"localImagePaths":4348,"remoteImagePaths":4349,"frontmatter":4350,"imagePaths":4352},[],[],[],{"title":4338,"description":25,"summary":4339,"pubDate":4351,"source":2720,"url":4341,"thumbnail":4342},"Tue, 25 Jan 2022 00:00:00 GMT",[],"2022-01-25-supercharged-searching-on-the-hugging-face-hub.md","2022-02-01-making-automatic-speech-recognition-work-on-large-files-with-wav2vec2-in-transformers",{"id":4354,"data":4356,"filePath":4362,"digest":4363,"rendered":4364,"legacyId":4372},{"title":4357,"description":25,"summary":4358,"pubDate":4359,"source":2720,"url":4360,"thumbnail":4361},"Making automatic speech recognition work on large files with Wav2Vec2 in 🤗 Transformers","Making automatic speech recognition work on large files with Wav2Vec2 in 🤗 Transformers Tl;dr: This ...",["Date","2022-02-01T00:00:00.000Z"],"https://huggingface.co/blog/asr-chunking","https://huggingface.co/blog/assets/49_asr_chunking/thumbnail.png","src/content/posts/2022-02-01-making-automatic-speech-recognition-work-on-large-files-with-wav2vec2-in-transformers.md","c549552288d10af3",{"html":25,"metadata":4365},{"headings":4366,"localImagePaths":4367,"remoteImagePaths":4368,"frontmatter":4369,"imagePaths":4371},[],[],[],{"title":4357,"description":25,"summary":4358,"pubDate":4370,"source":2720,"url":4360,"thumbnail":4361},"Tue, 01 Feb 2022 00:00:00 GMT",[],"2022-02-01-making-automatic-speech-recognition-work-on-large-files-with-wav2vec2-in-transformers.md","2022-02-02-getting-started-with-sentiment-analysis-using-python",{"id":4373,"data":4375,"filePath":4381,"digest":4382,"rendered":4383,"legacyId":4391},{"title":4376,"description":25,"summary":4377,"pubDate":4378,"source":2720,"url":4379,"thumbnail":4380},"Getting Started with Sentiment Analysis using Python","Getting Started with Sentiment Analysis using Python Sentiment analysis is the automated process of ...",["Date","2022-02-02T00:00:00.000Z"],"https://huggingface.co/blog/sentiment-analysis-python","https://huggingface.co/blog/assets/50_sentiment_python/thumbnail.png","src/content/posts/2022-02-02-getting-started-with-sentiment-analysis-using-python.md","035e2c253004d75c",{"html":25,"metadata":4384},{"headings":4385,"localImagePaths":4386,"remoteImagePaths":4387,"frontmatter":4388,"imagePaths":4390},[],[],[],{"title":4376,"description":25,"summary":4377,"pubDate":4389,"source":2720,"url":4379,"thumbnail":4380},"Wed, 02 Feb 2022 00:00:00 GMT",[],"2022-02-02-getting-started-with-sentiment-analysis-using-python.md","2022-02-02-solving-some-formal-math-olympiad-problems",{"id":4392,"data":4394,"filePath":4399,"digest":4400,"rendered":4401,"legacyId":4409},{"title":4395,"description":4396,"summary":4396,"pubDate":4397,"source":19,"url":4398,"thumbnail":21},"Solving (some) formal math olympiad problems","We built a neural theorem prover for Lean that learned to solve a variety of challenging high-school olympiad problems, including problems from the AMC12 and AIME competitions, as well as two problems adapted from the IMO.",["Date","2022-02-02T08:00:00.000Z"],"https://openai.com/blog/formal-math","src/content/posts/2022-02-02-solving-some-formal-math-olympiad-problems.md","29fe32af9ad40768",{"html":25,"metadata":4402},{"headings":4403,"localImagePaths":4404,"remoteImagePaths":4405,"frontmatter":4406,"imagePaths":4408},[],[],[],{"title":4395,"description":4396,"summary":4396,"pubDate":4407,"source":19,"url":4398,"thumbnail":21},"Wed, 02 Feb 2022 08:00:00 GMT",[],"2022-02-02-solving-some-formal-math-olympiad-problems.md","2022-02-11-fine-tune-vit-for-image-classification-with-transformers",{"id":4410,"data":4412,"filePath":4418,"digest":4419,"rendered":4420,"legacyId":4428},{"title":4413,"description":25,"summary":4414,"pubDate":4415,"source":2720,"url":4416,"thumbnail":4417},"Fine-Tune ViT for Image Classification with 🤗 Transformers","Fine-Tune ViT for Image Classification with 🤗 Transformers Just as transformers-based models have re...",["Date","2022-02-11T00:00:00.000Z"],"https://huggingface.co/blog/fine-tune-vit","https://huggingface.co/blog/assets/51_fine_tune_vit/vit-thumbnail.jpg","src/content/posts/2022-02-11-fine-tune-vit-for-image-classification-with-transformers.md","c4ad181195f7b344",{"html":25,"metadata":4421},{"headings":4422,"localImagePaths":4423,"remoteImagePaths":4424,"frontmatter":4425,"imagePaths":4427},[],[],[],{"title":4413,"description":25,"summary":4414,"pubDate":4426,"source":2720,"url":4416,"thumbnail":4417},"Fri, 11 Feb 2022 00:00:00 GMT",[],"2022-02-11-fine-tune-vit-for-image-classification-with-transformers.md","2022-03-02-bert-101-state-of-the-art-nlp-model-explained",{"id":4429,"data":4431,"filePath":4437,"digest":4438,"rendered":4439,"legacyId":4447},{"title":4432,"description":25,"summary":4433,"pubDate":4434,"source":2720,"url":4435,"thumbnail":4436},"BERT 101 🤗 State Of The Art NLP Model Explained","BERT 101 🤗 State Of The Art NLP Model Explained What is BERT? BERT, short for Bidirectional Encoder ...",["Date","2022-03-02T00:00:00.000Z"],"https://huggingface.co/blog/bert-101","https://huggingface.co/blog/assets/52_bert_101/thumbnail.jpg","src/content/posts/2022-03-02-bert-101-state-of-the-art-nlp-model-explained.md","502d43e2f0e8bf41",{"html":25,"metadata":4440},{"headings":4441,"localImagePaths":4442,"remoteImagePaths":4443,"frontmatter":4444,"imagePaths":4446},[],[],[],{"title":4432,"description":25,"summary":4433,"pubDate":4445,"source":2720,"url":4435,"thumbnail":4436},"Wed, 02 Mar 2022 00:00:00 GMT",[],"2022-03-02-bert-101-state-of-the-art-nlp-model-explained.md","2022-03-03-a-research-agenda-for-assessing-the-economic-impacts-of-code-generation-models",{"id":4448,"data":4450,"filePath":4454,"digest":4455,"rendered":4456,"legacyId":4464},{"title":4451,"description":25,"summary":25,"pubDate":4452,"source":19,"url":4453,"thumbnail":21},"A research agenda for assessing the economic impacts of code generation models",["Date","2022-03-03T08:00:00.000Z"],"https://openai.com/blog/economic-impacts-research","src/content/posts/2022-03-03-a-research-agenda-for-assessing-the-economic-impacts-of-code-generation-models.md","30f76cb4662fcd27",{"html":25,"metadata":4457},{"headings":4458,"localImagePaths":4459,"remoteImagePaths":4460,"frontmatter":4461,"imagePaths":4463},[],[],[],{"title":4451,"description":25,"summary":25,"pubDate":4462,"source":19,"url":4453,"thumbnail":21},"Thu, 03 Mar 2022 08:00:00 GMT",[],"2022-03-03-a-research-agenda-for-assessing-the-economic-impacts-of-code-generation-models.md","2022-03-03-economic-impacts-research-at-openai",{"id":4465,"data":4467,"filePath":4472,"digest":4473,"rendered":4474,"legacyId":4481},{"title":4468,"description":4469,"summary":4469,"pubDate":4470,"source":19,"url":4471,"thumbnail":21},"Economic impacts research at OpenAI","Call for expressions of interest to study the economic impacts of large language models.",["Date","2022-03-03T08:00:00.000Z"],"https://openai.com/blog/economic-impacts","src/content/posts/2022-03-03-economic-impacts-research-at-openai.md","289b7564c0384c37",{"html":25,"metadata":4475},{"headings":4476,"localImagePaths":4477,"remoteImagePaths":4478,"frontmatter":4479,"imagePaths":4480},[],[],[],{"title":4468,"description":4469,"summary":4469,"pubDate":4462,"source":19,"url":4471,"thumbnail":21},[],"2022-03-03-economic-impacts-research-at-openai.md","2022-03-03-lessons-learned-on-language-model-safety-and-misuse",{"id":4482,"data":4484,"filePath":4489,"digest":4490,"rendered":4491,"legacyId":4498},{"title":4485,"description":4486,"summary":4486,"pubDate":4487,"source":19,"url":4488,"thumbnail":21},"Lessons learned on language model safety and misuse","We describe our latest thinking in the hope of helping other AI developers address safety and misuse of deployed models.",["Date","2022-03-03T08:00:00.000Z"],"https://openai.com/blog/language-model-safety-and-misuse","src/content/posts/2022-03-03-lessons-learned-on-language-model-safety-and-misuse.md","9750737166b62805",{"html":25,"metadata":4492},{"headings":4493,"localImagePaths":4494,"remoteImagePaths":4495,"frontmatter":4496,"imagePaths":4497},[],[],[],{"title":4485,"description":4486,"summary":4486,"pubDate":4462,"source":19,"url":4488,"thumbnail":21},[],"2022-03-03-lessons-learned-on-language-model-safety-and-misuse.md","2022-03-11-guiding-text-generation-with-constrained-beam-search-in-transformers",{"id":4499,"data":4501,"filePath":4507,"digest":4508,"rendered":4509,"legacyId":4517},{"title":4502,"description":25,"summary":4503,"pubDate":4504,"source":2720,"url":4505,"thumbnail":4506},"Guiding Text Generation with Constrained Beam Search in 🤗 Transformers","Guiding Text Generation with Constrained Beam Search in 🤗 Transformers Introduction This blog post a...",["Date","2022-03-11T00:00:00.000Z"],"https://huggingface.co/blog/constrained-beam-search","https://huggingface.co/blog/assets/53_constrained_beam_search/thumbnail.png","src/content/posts/2022-03-11-guiding-text-generation-with-constrained-beam-search-in-transformers.md","f301a56677358e2b",{"html":25,"metadata":4510},{"headings":4511,"localImagePaths":4512,"remoteImagePaths":4513,"frontmatter":4514,"imagePaths":4516},[],[],[],{"title":4502,"description":25,"summary":4503,"pubDate":4515,"source":2720,"url":4505,"thumbnail":4506},"Fri, 11 Mar 2022 00:00:00 GMT",[],"2022-03-11-guiding-text-generation-with-constrained-beam-search-in-transformers.md","2022-03-15-new-gpt-3-capabilities-edit-insert",{"id":4518,"data":4520,"filePath":4525,"digest":4526,"rendered":4527,"legacyId":4535},{"title":4521,"description":4522,"summary":4522,"pubDate":4523,"source":19,"url":4524,"thumbnail":21},"New GPT-3 capabilities: Edit & insert","We’ve released new versions of GPT-3 and Codex which can edit or insert content into existing text, rather than just completing existing text.",["Date","2022-03-15T07:00:00.000Z"],"https://openai.com/blog/gpt-3-edit-insert","src/content/posts/2022-03-15-new-gpt-3-capabilities-edit-insert.md","2868013ab470bd46",{"html":25,"metadata":4528},{"headings":4529,"localImagePaths":4530,"remoteImagePaths":4531,"frontmatter":4532,"imagePaths":4534},[],[],[],{"title":4521,"description":4522,"summary":4522,"pubDate":4533,"source":19,"url":4524,"thumbnail":21},"Tue, 15 Mar 2022 07:00:00 GMT",[],"2022-03-15-new-gpt-3-capabilities-edit-insert.md","2022-03-16-accelerate-bert-inference-with-hugging-face-transformers-and-aws-inferentia",{"id":4536,"data":4538,"filePath":4544,"digest":4545,"rendered":4546,"legacyId":4554},{"title":4539,"description":25,"summary":4540,"pubDate":4541,"source":2720,"url":4542,"thumbnail":4543},"Accelerate BERT inference with Hugging Face Transformers and AWS inferentia","Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia notebook: sagemaker/18_i...",["Date","2022-03-16T00:00:00.000Z"],"https://huggingface.co/blog/bert-inferentia-sagemaker","https://huggingface.co/blog//assets/55_bert_inferentia_sagemaker/thumbnail.png","src/content/posts/2022-03-16-accelerate-bert-inference-with-hugging-face-transformers-and-aws-inferentia.md","a2434da07c7874d3",{"html":25,"metadata":4547},{"headings":4548,"localImagePaths":4549,"remoteImagePaths":4550,"frontmatter":4551,"imagePaths":4553},[],[],[],{"title":4539,"description":25,"summary":4540,"pubDate":4552,"source":2720,"url":4542,"thumbnail":4543},"Wed, 16 Mar 2022 00:00:00 GMT",[],"2022-03-16-accelerate-bert-inference-with-hugging-face-transformers-and-aws-inferentia.md","2022-03-16-image-search-with-datasets",{"id":4555,"data":4557,"filePath":4563,"digest":4564,"rendered":4565,"legacyId":4572},{"title":4558,"description":25,"summary":4559,"pubDate":4560,"source":2720,"url":4561,"thumbnail":4562},"Image search with 🤗 datasets","Image search with 🤗 datasets 🤗 datasets is a library that makes it easy to access and share datasets...",["Date","2022-03-16T00:00:00.000Z"],"https://huggingface.co/blog/image-search-datasets","https://huggingface.co/blog/assets/54_image_search_datasets/spaces_image_search.jpg","src/content/posts/2022-03-16-image-search-with-datasets.md","d59f24db742f0f19",{"html":25,"metadata":4566},{"headings":4567,"localImagePaths":4568,"remoteImagePaths":4569,"frontmatter":4570,"imagePaths":4571},[],[],[],{"title":4558,"description":25,"summary":4559,"pubDate":4552,"source":2720,"url":4561,"thumbnail":4562},[],"2022-03-16-image-search-with-datasets.md","2022-03-17-fine-tune-a-semantic-segmentation-model-with-a-custom-dataset",{"id":4573,"data":4575,"filePath":4581,"digest":4582,"rendered":4583,"legacyId":4591},{"title":4576,"description":25,"summary":4577,"pubDate":4578,"source":2720,"url":4579,"thumbnail":4580},"Fine-Tune a Semantic Segmentation Model with a Custom Dataset","Fine-Tune a Semantic Segmentation Model with a Custom Dataset This guide shows how you can fine-tune...",["Date","2022-03-17T00:00:00.000Z"],"https://huggingface.co/blog/fine-tune-segformer","https://huggingface.co/blog/assets/56_fine_tune_segformer/thumb.png","src/content/posts/2022-03-17-fine-tune-a-semantic-segmentation-model-with-a-custom-dataset.md","96260ba743349ec5",{"html":25,"metadata":4584},{"headings":4585,"localImagePaths":4586,"remoteImagePaths":4587,"frontmatter":4588,"imagePaths":4590},[],[],[],{"title":4576,"description":25,"summary":4577,"pubDate":4589,"source":2720,"url":4579,"thumbnail":4580},"Thu, 17 Mar 2022 00:00:00 GMT",[],"2022-03-17-fine-tune-a-semantic-segmentation-model-with-a-custom-dataset.md","2022-03-22-announcing-the-ai-research-residency-program",{"id":4592,"data":4594,"filePath":4600,"digest":4601,"rendered":4602,"legacyId":4610},{"title":4595,"description":25,"summary":4596,"pubDate":4597,"source":2720,"url":4598,"thumbnail":4599},"Announcing the 🤗 AI Research Residency Program","Announcing the 🤗 AI Research Residency Program 🎉 🎉 🎉 The 🤗 Research Residency Program is a 9-month o...",["Date","2022-03-22T00:00:00.000Z"],"https://huggingface.co/blog/ai-residency","https://huggingface.co/blog/assets/57_ai_residency/residency-thumbnail.jpg","src/content/posts/2022-03-22-announcing-the-ai-research-residency-program.md","1a1fd382ce493568",{"html":25,"metadata":4603},{"headings":4604,"localImagePaths":4605,"remoteImagePaths":4606,"frontmatter":4607,"imagePaths":4609},[],[],[],{"title":4595,"description":25,"summary":4596,"pubDate":4608,"source":2720,"url":4598,"thumbnail":4599},"Tue, 22 Mar 2022 00:00:00 GMT",[],"2022-03-22-announcing-the-ai-research-residency-program.md","2022-03-23-machine-learning-experts---meg-mitchell-interview",{"id":4611,"data":4613,"filePath":4619,"digest":4620,"rendered":4621,"legacyId":4629},{"title":4614,"description":25,"summary":4615,"pubDate":4616,"source":2720,"url":4617,"thumbnail":4618},"Machine Learning Experts - Meg Mitchell Interview","Machine Learning Experts - Margaret Mitchell Hey friends! Welcome to Machine Learning Experts. I'm y...",["Date","2022-03-23T00:00:00.000Z"],"https://huggingface.co/blog/meg-mitchell-interview","https://huggingface.co/blog/assets/57_meg_mitchell_interview/thumbnail.png","src/content/posts/2022-03-23-machine-learning-experts---meg-mitchell-interview.md","8c73b7e5e9246229",{"html":25,"metadata":4622},{"headings":4623,"localImagePaths":4624,"remoteImagePaths":4625,"frontmatter":4626,"imagePaths":4628},[],[],[],{"title":4614,"description":25,"summary":4615,"pubDate":4627,"source":2720,"url":4617,"thumbnail":4618},"Wed, 23 Mar 2022 00:00:00 GMT",[],"2022-03-23-machine-learning-experts---meg-mitchell-interview.md","2022-03-28-introducing-decision-transformers-on-hugging-face",{"id":4630,"data":4632,"filePath":4638,"digest":4639,"rendered":4640,"legacyId":4648},{"title":4633,"description":25,"summary":4634,"pubDate":4635,"source":2720,"url":4636,"thumbnail":4637},"Introducing Decision Transformers on Hugging Face 🤗","Introducing Decision Transformers on Hugging Face 🤗 At Hugging Face, we are contributing to the ecos...",["Date","2022-03-28T00:00:00.000Z"],"https://huggingface.co/blog/decision-transformers","https://huggingface.co/blog/assets/58_decision-transformers/thumbnail.jpg","src/content/posts/2022-03-28-introducing-decision-transformers-on-hugging-face.md","c45ce0bef172443c",{"html":25,"metadata":4641},{"headings":4642,"localImagePaths":4643,"remoteImagePaths":4644,"frontmatter":4645,"imagePaths":4647},[],[],[],{"title":4633,"description":25,"summary":4634,"pubDate":4646,"source":2720,"url":4636,"thumbnail":4637},"Mon, 28 Mar 2022 00:00:00 GMT",[],"2022-03-28-introducing-decision-transformers-on-hugging-face.md","2022-04-05-dont-repeat-yourself---transformers-design-philosophy",{"id":4649,"data":4651,"filePath":4657,"digest":4658,"rendered":4659,"legacyId":4667},{"title":4652,"description":25,"summary":4653,"pubDate":4654,"source":2720,"url":4655,"thumbnail":4656},"Don't repeat yourself - 🤗 Transformers Design Philosophy","Don't Repeat Yourself* Designing open-source libraries for modern machine learning 🤗 Transformers De...",["Date","2022-04-05T00:00:00.000Z"],"https://huggingface.co/blog/transformers-design-philosophy","https://huggingface.co/blog/assets/59_transformers_philosophy/transformers.png","src/content/posts/2022-04-05-dont-repeat-yourself---transformers-design-philosophy.md","8ba1dd9655500ecb",{"html":25,"metadata":4660},{"headings":4661,"localImagePaths":4662,"remoteImagePaths":4663,"frontmatter":4664,"imagePaths":4666},[],[],[],{"title":4652,"description":25,"summary":4653,"pubDate":4665,"source":2720,"url":4655,"thumbnail":4656},"Tue, 05 Apr 2022 00:00:00 GMT",[],"2022-04-05-dont-repeat-yourself---transformers-design-philosophy.md","2022-04-12-habana-labs-and-hugging-face-partner-to-accelerate-transformer-model-training",{"id":4668,"data":4670,"filePath":4676,"digest":4677,"rendered":4678,"legacyId":4686},{"title":4671,"description":25,"summary":4672,"pubDate":4673,"source":2720,"url":4674,"thumbnail":4675},"Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training","Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training Santa Clara and San Fr...",["Date","2022-04-12T00:00:00.000Z"],"https://huggingface.co/blog/habana","https://huggingface.co/blog/assets/60_habana/habana.png","src/content/posts/2022-04-12-habana-labs-and-hugging-face-partner-to-accelerate-transformer-model-training.md","6681ebd55be5e8f1",{"html":25,"metadata":4679},{"headings":4680,"localImagePaths":4681,"remoteImagePaths":4682,"frontmatter":4683,"imagePaths":4685},[],[],[],{"title":4671,"description":25,"summary":4672,"pubDate":4684,"source":2720,"url":4674,"thumbnail":4675},"Tue, 12 Apr 2022 00:00:00 GMT",[],"2022-04-12-habana-labs-and-hugging-face-partner-to-accelerate-transformer-model-training.md","2022-04-13-hierarchical-text-conditional-image-generation-with-clip-latents",{"id":4687,"data":4689,"filePath":4693,"digest":4694,"rendered":4695,"legacyId":4703},{"title":4690,"description":25,"summary":25,"pubDate":4691,"source":19,"url":4692,"thumbnail":21},"Hierarchical text-conditional image generation with CLIP latents",["Date","2022-04-13T07:00:00.000Z"],"https://openai.com/blog/hierarchical-text-conditional-image-generation-with-clip-latents","src/content/posts/2022-04-13-hierarchical-text-conditional-image-generation-with-clip-latents.md","7d0aedd5131e0208",{"html":25,"metadata":4696},{"headings":4697,"localImagePaths":4698,"remoteImagePaths":4699,"frontmatter":4700,"imagePaths":4702},[],[],[],{"title":4690,"description":25,"summary":25,"pubDate":4701,"source":19,"url":4692,"thumbnail":21},"Wed, 13 Apr 2022 07:00:00 GMT",[],"2022-04-13-hierarchical-text-conditional-image-generation-with-clip-latents.md","2022-04-13-machine-learning-experts---lewis-tunstall-interview",{"id":4704,"data":4706,"filePath":4712,"digest":4713,"rendered":4714,"legacyId":4722},{"title":4707,"description":25,"summary":4708,"pubDate":4709,"source":2720,"url":4710,"thumbnail":4711},"Machine Learning Experts - Lewis Tunstall Interview","Machine Learning Experts - Lewis Tunstall 🤗 Welcome to Machine Learning Experts - Lewis Tunstall Hey...",["Date","2022-04-13T00:00:00.000Z"],"https://huggingface.co/blog/lewis-tunstall-interview","https://huggingface.co/blog/assets/60_lewis_tunstall_interview/thumbnail.png","src/content/posts/2022-04-13-machine-learning-experts---lewis-tunstall-interview.md","0286faddf39c78f0",{"html":25,"metadata":4715},{"headings":4716,"localImagePaths":4717,"remoteImagePaths":4718,"frontmatter":4719,"imagePaths":4721},[],[],[],{"title":4707,"description":25,"summary":4708,"pubDate":4720,"source":2720,"url":4710,"thumbnail":4711},"Wed, 13 Apr 2022 00:00:00 GMT",[],"2022-04-13-machine-learning-experts---lewis-tunstall-interview.md","2022-04-13-measuring-goodharts-law",{"id":4723,"data":4725,"filePath":4730,"digest":4731,"rendered":4732,"legacyId":4739},{"title":4726,"description":4727,"summary":4727,"pubDate":4728,"source":19,"url":4729,"thumbnail":21},"Measuring Goodhart’s law","Goodhart’s law famously says: “When a measure becomes a target, it ceases to be a good measure.” Although originally from economics, it’s something we have to grapple with at OpenAI when figuring out how to optimize objectives that are difficult or costly to measure.",["Date","2022-04-13T07:00:00.000Z"],"https://openai.com/blog/measuring-goodharts-law","src/content/posts/2022-04-13-measuring-goodharts-law.md","100cf4fdaa942949",{"html":25,"metadata":4733},{"headings":4734,"localImagePaths":4735,"remoteImagePaths":4736,"frontmatter":4737,"imagePaths":4738},[],[],[],{"title":4726,"description":4727,"summary":4727,"pubDate":4701,"source":19,"url":4729,"thumbnail":21},[],"2022-04-13-measuring-goodharts-law.md","2022-04-22-co2-emissions-and-the-hub-leading-the-charge",{"id":4740,"data":4742,"filePath":4748,"digest":4749,"rendered":4750,"legacyId":4758},{"title":4743,"description":25,"summary":4744,"pubDate":4745,"source":2720,"url":4746,"thumbnail":4747},"CO2 Emissions and the 🤗 Hub: Leading the Charge","CO2 Emissions and the 🤗 Hub: Leading the Charge What are CO2 Emissions and why are they important? C...",["Date","2022-04-22T00:00:00.000Z"],"https://huggingface.co/blog/carbon-emissions-on-the-hub","https://huggingface.co/blog/assets/60_carbon_emissions_on_the_hub/thumbnail.jpg","src/content/posts/2022-04-22-co2-emissions-and-the-hub-leading-the-charge.md","453e7b2fdd27a88b",{"html":25,"metadata":4751},{"headings":4752,"localImagePaths":4753,"remoteImagePaths":4754,"frontmatter":4755,"imagePaths":4757},[],[],[],{"title":4743,"description":25,"summary":4744,"pubDate":4756,"source":2720,"url":4746,"thumbnail":4747},"Fri, 22 Apr 2022 00:00:00 GMT",[],"2022-04-22-co2-emissions-and-the-hub-leading-the-charge.md","2022-04-25-introducing-hugging-face-for-education",{"id":4759,"data":4761,"filePath":4767,"digest":4768,"rendered":4769,"legacyId":4777},{"title":4762,"description":25,"summary":4763,"pubDate":4764,"source":2720,"url":4765,"thumbnail":4766},"Introducing Hugging Face for Education","Introducing Hugging Face for Education 🤗 Given that machine learning will make up the overwhelming m...",["Date","2022-04-25T00:00:00.000Z"],"https://huggingface.co/blog/education","https://huggingface.co/blog/assets/61_education/thumbnail.png","src/content/posts/2022-04-25-introducing-hugging-face-for-education.md","e111adfc2d7ed211",{"html":25,"metadata":4770},{"headings":4771,"localImagePaths":4772,"remoteImagePaths":4773,"frontmatter":4774,"imagePaths":4776},[],[],[],{"title":4762,"description":25,"summary":4763,"pubDate":4775,"source":2720,"url":4765,"thumbnail":4766},"Mon, 25 Apr 2022 00:00:00 GMT",[],"2022-04-25-introducing-hugging-face-for-education.md","2022-04-25-supercharged-customer-service-with-machine-learning",{"id":4778,"data":4780,"filePath":4786,"digest":4787,"rendered":4788,"legacyId":4795},{"title":4781,"description":25,"summary":4782,"pubDate":4783,"source":2720,"url":4784,"thumbnail":4785},"Supercharged Customer Service with Machine Learning","Supercharged Customer Service with Machine Learning In this blog post, we will simulate a real-world...",["Date","2022-04-25T00:00:00.000Z"],"https://huggingface.co/blog/supercharge-customer-service-with-machine-learning","https://huggingface.co/blog/assets/61_supercharged_customer_service_with_nlp/thumbnail.png","src/content/posts/2022-04-25-supercharged-customer-service-with-machine-learning.md","41f3b160e776338b",{"html":25,"metadata":4789},{"headings":4790,"localImagePaths":4791,"remoteImagePaths":4792,"frontmatter":4793,"imagePaths":4794},[],[],[],{"title":4781,"description":25,"summary":4782,"pubDate":4775,"source":2720,"url":4784,"thumbnail":4785},[],"2022-04-25-supercharged-customer-service-with-machine-learning.md","2022-04-26-getting-started-with-transformers-on-habana-gaudi",{"id":4796,"data":4798,"filePath":4804,"digest":4805,"rendered":4806,"legacyId":4814},{"title":4799,"description":25,"summary":4800,"pubDate":4801,"source":2720,"url":4802,"thumbnail":4803},"Getting Started with Transformers on Habana Gaudi","Getting Started with Transformers on Habana Gaudi A couple of weeks ago, we've had the pleasure to a...",["Date","2022-04-26T00:00:00.000Z"],"https://huggingface.co/blog/getting-started-habana","https://huggingface.co/blog/assets/61_getting_started_habana/habana01.png","src/content/posts/2022-04-26-getting-started-with-transformers-on-habana-gaudi.md","28d0e35d48133277",{"html":25,"metadata":4807},{"headings":4808,"localImagePaths":4809,"remoteImagePaths":4810,"frontmatter":4811,"imagePaths":4813},[],[],[],{"title":4799,"description":25,"summary":4800,"pubDate":4812,"source":2720,"url":4802,"thumbnail":4803},"Tue, 26 Apr 2022 00:00:00 GMT",[],"2022-04-26-getting-started-with-transformers-on-habana-gaudi.md","2022-04-27-director-of-machine-learning-insights-series",{"id":4815,"data":4817,"filePath":4823,"digest":4824,"rendered":4825,"legacyId":4833},{"title":4818,"description":25,"summary":4819,"pubDate":4820,"source":2720,"url":4821,"thumbnail":4822},"Director of Machine Learning Insights [Series]","Director of Machine Learning Insights [Part 1] Few seats at the Machine Learning table span both tec...",["Date","2022-04-27T00:00:00.000Z"],"https://huggingface.co/blog/ml-director-insights","https://huggingface.co/blog/assets/61_ml_director_insights/thumbnail.png","src/content/posts/2022-04-27-director-of-machine-learning-insights-series.md","57282979dc6ebd3a",{"html":25,"metadata":4826},{"headings":4827,"localImagePaths":4828,"remoteImagePaths":4829,"frontmatter":4830,"imagePaths":4832},[],[],[],{"title":4818,"description":25,"summary":4819,"pubDate":4831,"source":2720,"url":4821,"thumbnail":4822},"Wed, 27 Apr 2022 00:00:00 GMT",[],"2022-04-27-director-of-machine-learning-insights-series.md","2022-04-28-opinion-classification-with-kili-and-huggingface-autotrain",{"id":4834,"data":4836,"filePath":4842,"digest":4843,"rendered":4844,"legacyId":4852},{"title":4837,"description":25,"summary":4838,"pubDate":4839,"source":2720,"url":4840,"thumbnail":4841},"Opinion Classification with Kili and HuggingFace AutoTrain","Opinion Classification with Kili and HuggingFace AutoTrain Introduction Understanding your users’ ne...",["Date","2022-04-28T00:00:00.000Z"],"https://huggingface.co/blog/opinion-classification-with-kili","https://huggingface.co/blog/assets/59_opinion-classification-with-kili/thumbnail.png","src/content/posts/2022-04-28-opinion-classification-with-kili-and-huggingface-autotrain.md","06445c2af3146196",{"html":25,"metadata":4845},{"headings":4846,"localImagePaths":4847,"remoteImagePaths":4848,"frontmatter":4849,"imagePaths":4851},[],[],[],{"title":4837,"description":25,"summary":4838,"pubDate":4850,"source":2720,"url":4840,"thumbnail":4841},"Thu, 28 Apr 2022 00:00:00 GMT",[],"2022-04-28-opinion-classification-with-kili-and-huggingface-autotrain.md","2022-05-02-accelerate-large-model-training-using-pytorch-fully-sharded-data-parallel",{"id":4853,"data":4855,"filePath":4861,"digest":4862,"rendered":4863,"legacyId":4871},{"title":4856,"description":25,"summary":4857,"pubDate":4858,"source":2720,"url":4859,"thumbnail":4860},"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel","Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel In this post we will look ...",["Date","2022-05-02T00:00:00.000Z"],"https://huggingface.co/blog/pytorch-fsdp","https://huggingface.co/blog/assets/62_pytorch_fsdp/fsdp-thumbnail.png","src/content/posts/2022-05-02-accelerate-large-model-training-using-pytorch-fully-sharded-data-parallel.md","c9af1f6cf835c97b",{"html":25,"metadata":4864},{"headings":4865,"localImagePaths":4866,"remoteImagePaths":4867,"frontmatter":4868,"imagePaths":4870},[],[],[],{"title":4856,"description":25,"summary":4857,"pubDate":4869,"source":2720,"url":4859,"thumbnail":4860},"Mon, 02 May 2022 00:00:00 GMT",[],"2022-05-02-accelerate-large-model-training-using-pytorch-fully-sharded-data-parallel.md","2022-05-05-openai-leadership-team-update",{"id":4872,"data":4874,"filePath":4879,"digest":4880,"rendered":4881,"legacyId":4889},{"title":4875,"description":4876,"summary":4876,"pubDate":4877,"source":19,"url":4878,"thumbnail":21},"OpenAI leadership team update","We’re happy to announce several executive role changes that reflect our recent progress and will ensure continued momentum toward our next major milestones.",["Date","2022-05-05T07:00:00.000Z"],"https://openai.com/blog/leadership-team-update","src/content/posts/2022-05-05-openai-leadership-team-update.md","86481a53e5bd66b7",{"html":25,"metadata":4882},{"headings":4883,"localImagePaths":4884,"remoteImagePaths":4885,"frontmatter":4886,"imagePaths":4888},[],[],[],{"title":4875,"description":4876,"summary":4876,"pubDate":4887,"source":19,"url":4878,"thumbnail":21},"Thu, 05 May 2022 07:00:00 GMT",[],"2022-05-05-openai-leadership-team-update.md","2022-05-09-we-raised-100-million-for-open-collaborative-machine-learning",{"id":4890,"data":4892,"filePath":4898,"digest":4899,"rendered":4900,"legacyId":4908},{"title":4893,"description":25,"summary":4894,"pubDate":4895,"source":2720,"url":4896,"thumbnail":4897},"We Raised $100 Million for Open & Collaborative Machine Learning 🚀","We Raised $100 Million for Open & Collaborative Machine Learning 🚀 Today we have some exciting news ...",["Date","2022-05-09T00:00:00.000Z"],"https://huggingface.co/blog/series-c","https://huggingface.co/blog/assets/65_series_c/thumbnail.jpg","src/content/posts/2022-05-09-we-raised-100-million-for-open-collaborative-machine-learning.md","a1bb39ff89bb42f2",{"html":25,"metadata":4901},{"headings":4902,"localImagePaths":4903,"remoteImagePaths":4904,"frontmatter":4905,"imagePaths":4907},[],[],[],{"title":4893,"description":25,"summary":4894,"pubDate":4906,"source":2720,"url":4896,"thumbnail":4897},"Mon, 09 May 2022 00:00:00 GMT",[],"2022-05-09-we-raised-100-million-for-open-collaborative-machine-learning.md","2022-05-04-an-introduction-to-deep-reinforcement-learning",{"id":4909,"data":4911,"filePath":4917,"digest":4918,"rendered":4919,"legacyId":4927},{"title":4912,"description":25,"summary":4913,"pubDate":4914,"source":2720,"url":4915,"thumbnail":4916},"An Introduction to Deep Reinforcement Learning","An Introduction to Deep Reinforcement Learning Deep Reinforcement Learning Class with Hugging Face 🤗...",["Date","2022-05-04T00:00:00.000Z"],"https://huggingface.co/blog/deep-rl-intro","https://huggingface.co/blog/assets/63_deep_rl_intro/thumbnail.png","src/content/posts/2022-05-04-an-introduction-to-deep-reinforcement-learning.md","0fa2264232fe2d03",{"html":25,"metadata":4920},{"headings":4921,"localImagePaths":4922,"remoteImagePaths":4923,"frontmatter":4924,"imagePaths":4926},[],[],[],{"title":4912,"description":25,"summary":4913,"pubDate":4925,"source":2720,"url":4915,"thumbnail":4916},"Wed, 04 May 2022 00:00:00 GMT",[],"2022-05-04-an-introduction-to-deep-reinforcement-learning.md","2022-05-10-accelerated-inference-with-optimum-and-transformers-pipelines",{"id":4928,"data":4930,"filePath":4936,"digest":4937,"rendered":4938,"legacyId":4946},{"title":4931,"description":25,"summary":4932,"pubDate":4933,"source":2720,"url":4934,"thumbnail":4935},"Accelerated Inference with Optimum and Transformers Pipelines","Accelerated Inference with Optimum and Transformers Pipelines Inference has landed in Optimum with s...",["Date","2022-05-10T00:00:00.000Z"],"https://huggingface.co/blog/optimum-inference","https://huggingface.co/blog/assets/66_optimum_inference/thumbnail.png","src/content/posts/2022-05-10-accelerated-inference-with-optimum-and-transformers-pipelines.md","6bb5b5194767e6ac",{"html":25,"metadata":4939},{"headings":4940,"localImagePaths":4941,"remoteImagePaths":4942,"frontmatter":4943,"imagePaths":4945},[],[],[],{"title":4931,"description":25,"summary":4932,"pubDate":4944,"source":2720,"url":4934,"thumbnail":4935},"Tue, 10 May 2022 00:00:00 GMT",[],"2022-05-10-accelerated-inference-with-optimum-and-transformers-pipelines.md","2022-05-13-director-of-machine-learning-insights-part-2-saas-edition",{"id":4947,"data":4949,"filePath":4955,"digest":4956,"rendered":4957,"legacyId":4965},{"title":4950,"description":25,"summary":4951,"pubDate":4952,"source":2720,"url":4953,"thumbnail":4954},"Director of Machine Learning Insights [Part 2: SaaS Edition]","Director of Machine Learning Insights [Part 2: SaaS Edition] If you or your team are interested in b...",["Date","2022-05-13T00:00:00.000Z"],"https://huggingface.co/blog/ml-director-insights-2","https://huggingface.co/blog/assets/67_ml_director_insights/thumbnail.png","src/content/posts/2022-05-13-director-of-machine-learning-insights-part-2-saas-edition.md","66a12c90243ee2ce",{"html":25,"metadata":4958},{"headings":4959,"localImagePaths":4960,"remoteImagePaths":4961,"frontmatter":4962,"imagePaths":4964},[],[],[],{"title":4950,"description":25,"summary":4951,"pubDate":4963,"source":2720,"url":4953,"thumbnail":4954},"Fri, 13 May 2022 00:00:00 GMT",[],"2022-05-13-director-of-machine-learning-insights-part-2-saas-edition.md","2022-05-06-welcome-fastai-to-the-hugging-face-hub",{"id":4966,"data":4968,"filePath":4974,"digest":4975,"rendered":4976,"legacyId":4984},{"title":4969,"description":25,"summary":4970,"pubDate":4971,"source":2720,"url":4972,"thumbnail":4973},"Welcome fastai to the Hugging Face Hub","Welcome fastai to the Hugging Face Hub Making neural nets uncool again... and sharing them Few have ...",["Date","2022-05-06T00:00:00.000Z"],"https://huggingface.co/blog/fastai","https://huggingface.co/blog/assets/64_fastai/fastai_hf_blog.png","src/content/posts/2022-05-06-welcome-fastai-to-the-hugging-face-hub.md","7a43327d69600d77",{"html":25,"metadata":4977},{"headings":4978,"localImagePaths":4979,"remoteImagePaths":4980,"frontmatter":4981,"imagePaths":4983},[],[],[],{"title":4969,"description":25,"summary":4970,"pubDate":4982,"source":2720,"url":4972,"thumbnail":4973},"Fri, 06 May 2022 00:00:00 GMT",[],"2022-05-06-welcome-fastai-to-the-hugging-face-hub.md","2022-05-13-student-ambassador-programs-call-for-applications-is-open",{"id":4985,"data":4987,"filePath":4993,"digest":4994,"rendered":4995,"legacyId":5002},{"title":4988,"description":25,"summary":4989,"pubDate":4990,"source":2720,"url":4991,"thumbnail":4992},"Student Ambassador Program's call for applications is open!","Student Ambassador Program’s call for applications is open! As an open-source company democratizing ...",["Date","2022-05-13T00:00:00.000Z"],"https://huggingface.co/blog/ambassadors","https://huggingface.co/blog/assets/67_ambassadors/thumbnail.png","src/content/posts/2022-05-13-student-ambassador-programs-call-for-applications-is-open.md","32a07aa9b4f59a48",{"html":25,"metadata":4996},{"headings":4997,"localImagePaths":4998,"remoteImagePaths":4999,"frontmatter":5000,"imagePaths":5001},[],[],[],{"title":4988,"description":25,"summary":4989,"pubDate":4963,"source":2720,"url":4991,"thumbnail":4992},[],"2022-05-13-student-ambassador-programs-call-for-applications-is-open.md","2022-05-16-gradio-30-is-out",{"id":5003,"data":5005,"filePath":5011,"digest":5012,"rendered":5013,"legacyId":5021},{"title":5006,"description":25,"summary":5007,"pubDate":5008,"source":2720,"url":5009,"thumbnail":5010},"Gradio 3.0 is Out!","Gradio 3.0 is Out! Machine Learning Demos Machine learning demos are an increasingly vital part of r...",["Date","2022-05-16T00:00:00.000Z"],"https://huggingface.co/blog/gradio-blocks","https://huggingface.co/blog/assets/68_gradio_blocks/block-party.png","src/content/posts/2022-05-16-gradio-30-is-out.md","c0abb2599d57afec",{"html":25,"metadata":5014},{"headings":5015,"localImagePaths":5016,"remoteImagePaths":5017,"frontmatter":5018,"imagePaths":5020},[],[],[],{"title":5006,"description":25,"summary":5007,"pubDate":5019,"source":2720,"url":5009,"thumbnail":5010},"Mon, 16 May 2022 00:00:00 GMT",[],"2022-05-16-gradio-30-is-out.md","2022-05-17-announcing-the-hugging-face-fellowship-program",{"id":5022,"data":5024,"filePath":5030,"digest":5031,"rendered":5032,"legacyId":5040},{"title":5025,"description":25,"summary":5026,"pubDate":5027,"source":2720,"url":5028,"thumbnail":5029},"Announcing the Hugging Face Fellowship Program","Announcing the Hugging Face Fellowship Program The Fellowship is a network of exceptional people fro...",["Date","2022-05-17T00:00:00.000Z"],"https://huggingface.co/blog/fellowship","https://huggingface.co/blog/assets/62_fellowship/fellowship-thumbnail.png","src/content/posts/2022-05-17-announcing-the-hugging-face-fellowship-program.md","12d401ca9df2ed9f",{"html":25,"metadata":5033},{"headings":5034,"localImagePaths":5035,"remoteImagePaths":5036,"frontmatter":5037,"imagePaths":5039},[],[],[],{"title":5025,"description":25,"summary":5026,"pubDate":5038,"source":2720,"url":5028,"thumbnail":5029},"Tue, 17 May 2022 00:00:00 GMT",[],"2022-05-17-announcing-the-hugging-face-fellowship-program.md","2022-05-17-machine-learning-experts---sasha-luccioni-interview",{"id":5041,"data":5043,"filePath":5049,"digest":5050,"rendered":5051,"legacyId":5058},{"title":5044,"description":25,"summary":5045,"pubDate":5046,"source":2720,"url":5047,"thumbnail":5048},"Machine Learning Experts - Sasha Luccioni Interview","Machine Learning Experts - Sasha Luccioni 🤗 Welcome to Machine Learning Experts - Sasha Luccioni 🚀 I...",["Date","2022-05-17T00:00:00.000Z"],"https://huggingface.co/blog/sasha-luccioni-interview","https://huggingface.co/blog/assets/69_sasha_luccioni_interview/thumbnail.png","src/content/posts/2022-05-17-machine-learning-experts---sasha-luccioni-interview.md","1d5bc5b57404124b",{"html":25,"metadata":5052},{"headings":5053,"localImagePaths":5054,"remoteImagePaths":5055,"frontmatter":5056,"imagePaths":5057},[],[],[],{"title":5044,"description":25,"summary":5045,"pubDate":5038,"source":2720,"url":5047,"thumbnail":5048},[],"2022-05-17-machine-learning-experts---sasha-luccioni-interview.md","2022-05-18-an-introduction-to-q-learning-part-1",{"id":5059,"data":5061,"filePath":5067,"digest":5068,"rendered":5069,"legacyId":5077},{"title":5062,"description":25,"summary":5063,"pubDate":5064,"source":2720,"url":5065,"thumbnail":5066},"An Introduction to Q-Learning Part 1","An Introduction to Q-Learning Part 1 Deep Reinforcement Learning Class with Hugging Face 🤗 Unit 2, p...",["Date","2022-05-18T00:00:00.000Z"],"https://huggingface.co/blog/deep-rl-q-part1","https://huggingface.co/blog/assets/70_deep_rl_q_part1/thumbnail.gif","src/content/posts/2022-05-18-an-introduction-to-q-learning-part-1.md","4259680a99ee515e",{"html":25,"metadata":5070},{"headings":5071,"localImagePaths":5072,"remoteImagePaths":5073,"frontmatter":5074,"imagePaths":5076},[],[],[],{"title":5062,"description":25,"summary":5063,"pubDate":5075,"source":2720,"url":5065,"thumbnail":5066},"Wed, 18 May 2022 00:00:00 GMT",[],"2022-05-18-an-introduction-to-q-learning-part-1.md","2022-05-18-dalle-2-research-preview-update",{"id":5078,"data":5080,"filePath":5085,"digest":5086,"rendered":5087,"legacyId":5095},{"title":5081,"description":5082,"summary":5082,"pubDate":5083,"source":19,"url":5084,"thumbnail":21},"DALL·E 2 research preview update","Early users have created over 3 million images to date and helped us improve our safety processes. We’re excited to begin adding up to 1,000 new users from our waitlist each week.",["Date","2022-05-18T07:00:00.000Z"],"https://openai.com/blog/dall-e-2-update","src/content/posts/2022-05-18-dalle-2-research-preview-update.md","a7d51fc8ab7ff167",{"html":25,"metadata":5088},{"headings":5089,"localImagePaths":5090,"remoteImagePaths":5091,"frontmatter":5092,"imagePaths":5094},[],[],[],{"title":5081,"description":5082,"summary":5082,"pubDate":5093,"source":19,"url":5084,"thumbnail":21},"Wed, 18 May 2022 07:00:00 GMT",[],"2022-05-18-dalle-2-research-preview-update.md","2022-05-19-how-sempre-health-is-leveraging-the-expert-acceleration-program-to-accelerate-their-ml-roadmap",{"id":5096,"data":5098,"filePath":5104,"digest":5105,"rendered":5106,"legacyId":5114},{"title":5099,"description":25,"summary":5100,"pubDate":5101,"source":2720,"url":5102,"thumbnail":5103},"How Sempre Health is leveraging the Expert Acceleration Program to accelerate their ML roadmap","How Sempre Health is leveraging the Expert Acceleration Program to accelerate their ML roadmap 👋 Hel...",["Date","2022-05-19T00:00:00.000Z"],"https://huggingface.co/blog/sempre-health-eap-case-study","https://huggingface.co/blog/assets/70_sempre_health/thumbnail.jpg","src/content/posts/2022-05-19-how-sempre-health-is-leveraging-the-expert-acceleration-program-to-accelerate-their-ml-roadmap.md","1df758b2ed7261c7",{"html":25,"metadata":5107},{"headings":5108,"localImagePaths":5109,"remoteImagePaths":5110,"frontmatter":5111,"imagePaths":5113},[],[],[],{"title":5099,"description":25,"summary":5100,"pubDate":5112,"source":2720,"url":5102,"thumbnail":5103},"Thu, 19 May 2022 00:00:00 GMT",[],"2022-05-19-how-sempre-health-is-leveraging-the-expert-acceleration-program-to-accelerate-their-ml-roadmap.md","2022-05-19-putting-ethical-principles-at-the-core-of-research-lifecycle",{"id":5115,"data":5117,"filePath":5123,"digest":5124,"rendered":5125,"legacyId":5132},{"title":5118,"description":25,"summary":5119,"pubDate":5120,"source":2720,"url":5121,"thumbnail":5122},"Putting ethical principles at the core of research lifecycle","Putting ethical principles at the core of the research lifecycle Ethical charter - Multimodal projec...",["Date","2022-05-19T00:00:00.000Z"],"https://huggingface.co/blog/ethical-charter-multimodal","https://huggingface.co/blog/assets/71_ethical-charter/thumbnail.jpg","src/content/posts/2022-05-19-putting-ethical-principles-at-the-core-of-research-lifecycle.md","e41f675acbe664ba",{"html":25,"metadata":5126},{"headings":5127,"localImagePaths":5128,"remoteImagePaths":5129,"frontmatter":5130,"imagePaths":5131},[],[],[],{"title":5118,"description":25,"summary":5119,"pubDate":5112,"source":2720,"url":5121,"thumbnail":5122},[],"2022-05-19-putting-ethical-principles-at-the-core-of-research-lifecycle.md","2022-05-20-an-introduction-to-q-learning-part-2",{"id":5133,"data":5135,"filePath":5141,"digest":5142,"rendered":5143,"legacyId":5151},{"title":5136,"description":25,"summary":5137,"pubDate":5138,"source":2720,"url":5139,"thumbnail":5140},"An Introduction to Q-Learning Part 2","An Introduction to Q-Learning Part 2/2 Deep Reinforcement Learning Class with Hugging Face 🤗 Unit 2,...",["Date","2022-05-20T00:00:00.000Z"],"https://huggingface.co/blog/deep-rl-q-part2","https://huggingface.co/blog/assets/73_deep_rl_q_part2/thumbnail.gif","src/content/posts/2022-05-20-an-introduction-to-q-learning-part-2.md","5c5a3f10564c3e97",{"html":25,"metadata":5144},{"headings":5145,"localImagePaths":5146,"remoteImagePaths":5147,"frontmatter":5148,"imagePaths":5150},[],[],[],{"title":5136,"description":25,"summary":5137,"pubDate":5149,"source":2720,"url":5139,"thumbnail":5140},"Fri, 20 May 2022 00:00:00 GMT",[],"2022-05-20-an-introduction-to-q-learning-part-2.md","2022-05-24-powering-next-generation-applications-with-openai-codex",{"id":5152,"data":5154,"filePath":5159,"digest":5160,"rendered":5161,"legacyId":5169},{"title":5155,"description":5156,"summary":5156,"pubDate":5157,"source":19,"url":5158,"thumbnail":21},"Powering next generation applications with OpenAI Codex","Codex is now powering 70 different applications across a variety of use cases through the OpenAI API.",["Date","2022-05-24T07:00:00.000Z"],"https://openai.com/blog/codex-apps","src/content/posts/2022-05-24-powering-next-generation-applications-with-openai-codex.md","2949631574ad9790",{"html":25,"metadata":5162},{"headings":5163,"localImagePaths":5164,"remoteImagePaths":5165,"frontmatter":5166,"imagePaths":5168},[],[],[],{"title":5155,"description":5156,"summary":5156,"pubDate":5167,"source":19,"url":5158,"thumbnail":21},"Tue, 24 May 2022 07:00:00 GMT",[],"2022-05-24-powering-next-generation-applications-with-openai-codex.md","2022-05-23-efficient-table-pre-training-without-real-data-an-introduction-to-tapex",{"id":5170,"data":5172,"filePath":5178,"digest":5179,"rendered":5180,"legacyId":5188},{"title":5173,"description":25,"summary":5174,"pubDate":5175,"source":2720,"url":5176,"thumbnail":5177},"Efficient Table Pre-training without Real Data: An Introduction to TAPEX","Efficient Table Pre-training without Real Data: An Introduction to TAPEX In recent years, language m...",["Date","2022-05-23T00:00:00.000Z"],"https://huggingface.co/blog/tapex","https://huggingface.co/blog/assets/74_tapex/thumbnail.png","src/content/posts/2022-05-23-efficient-table-pre-training-without-real-data-an-introduction-to-tapex.md","f68550dbb8f6f0ac",{"html":25,"metadata":5181},{"headings":5182,"localImagePaths":5183,"remoteImagePaths":5184,"frontmatter":5185,"imagePaths":5187},[],[],[],{"title":5173,"description":25,"summary":5174,"pubDate":5186,"source":2720,"url":5176,"thumbnail":5177},"Mon, 23 May 2022 00:00:00 GMT",[],"2022-05-23-efficient-table-pre-training-without-real-data-an-introduction-to-tapex.md","2022-05-25-introducing-pull-requests-and-discussions",{"id":5189,"data":5191,"filePath":5197,"digest":5198,"rendered":5199,"legacyId":5207},{"title":5192,"description":25,"summary":5193,"pubDate":5194,"source":2720,"url":5195,"thumbnail":5196},"Introducing Pull Requests and Discussions 🥳","Introducing Pull Requests and Discussions 🥳 We are thrilled to announce the release of our latest co...",["Date","2022-05-25T00:00:00.000Z"],"https://huggingface.co/blog/community-update","https://huggingface.co/blog/assets/76_community_update/thumbnail.png","src/content/posts/2022-05-25-introducing-pull-requests-and-discussions.md","34794475c47844ae",{"html":25,"metadata":5200},{"headings":5201,"localImagePaths":5202,"remoteImagePaths":5203,"frontmatter":5204,"imagePaths":5206},[],[],[],{"title":5192,"description":25,"summary":5193,"pubDate":5205,"source":2720,"url":5195,"thumbnail":5196},"Wed, 25 May 2022 00:00:00 GMT",[],"2022-05-25-introducing-pull-requests-and-discussions.md","2022-05-26-graphcore-and-hugging-face-launch-new-lineup-of-ipu-ready-transformers",{"id":5208,"data":5210,"filePath":5216,"digest":5217,"rendered":5218,"legacyId":5226},{"title":5211,"description":25,"summary":5212,"pubDate":5213,"source":2720,"url":5214,"thumbnail":5215},"Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers","Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers Graphcore and Hugging Face ha...",["Date","2022-05-26T00:00:00.000Z"],"https://huggingface.co/blog/graphcore-update","https://huggingface.co/blog/assets/77_graphcore-update/graphcore_update.png","src/content/posts/2022-05-26-graphcore-and-hugging-face-launch-new-lineup-of-ipu-ready-transformers.md","36456788cda55eff",{"html":25,"metadata":5219},{"headings":5220,"localImagePaths":5221,"remoteImagePaths":5222,"frontmatter":5223,"imagePaths":5225},[],[],[],{"title":5211,"description":25,"summary":5212,"pubDate":5224,"source":2720,"url":5214,"thumbnail":5215},"Thu, 26 May 2022 00:00:00 GMT",[],"2022-05-26-graphcore-and-hugging-face-launch-new-lineup-of-ipu-ready-transformers.md","2022-05-28-teaching-models-to-express-their-uncertainty-in-words",{"id":5227,"data":5229,"filePath":5233,"digest":5234,"rendered":5235,"legacyId":5243},{"title":5230,"description":25,"summary":25,"pubDate":5231,"source":19,"url":5232,"thumbnail":21},"Teaching models to express their uncertainty in words",["Date","2022-05-28T07:00:00.000Z"],"https://openai.com/blog/teaching-models-to-express-their-uncertainty-in-words","src/content/posts/2022-05-28-teaching-models-to-express-their-uncertainty-in-words.md","5d8aa42229a9ea97",{"html":25,"metadata":5236},{"headings":5237,"localImagePaths":5238,"remoteImagePaths":5239,"frontmatter":5240,"imagePaths":5242},[],[],[],{"title":5230,"description":25,"summary":25,"pubDate":5241,"source":19,"url":5232,"thumbnail":21},"Sat, 28 May 2022 07:00:00 GMT",[],"2022-05-28-teaching-models-to-express-their-uncertainty-in-words.md","2022-06-02-best-practices-for-deploying-language-models",{"id":5244,"data":5246,"filePath":5251,"digest":5252,"rendered":5253,"legacyId":5261},{"title":5247,"description":5248,"summary":5248,"pubDate":5249,"source":19,"url":5250,"thumbnail":21},"Best practices for deploying language models","Cohere, OpenAI, and AI21 Labs have developed a preliminary set of best practices applicable to any organization developing or deploying large language models.",["Date","2022-06-02T07:00:00.000Z"],"https://openai.com/blog/best-practices-for-deploying-language-models","src/content/posts/2022-06-02-best-practices-for-deploying-language-models.md","79987dcb3ed255ff",{"html":25,"metadata":5254},{"headings":5255,"localImagePaths":5256,"remoteImagePaths":5257,"frontmatter":5258,"imagePaths":5260},[],[],[],{"title":5247,"description":5248,"summary":5248,"pubDate":5259,"source":19,"url":5250,"thumbnail":21},"Thu, 02 Jun 2022 07:00:00 GMT",[],"2022-06-02-best-practices-for-deploying-language-models.md","2022-06-07-deep-q-learning-with-atari",{"id":5262,"data":5264,"filePath":5270,"digest":5271,"rendered":5272,"legacyId":5280},{"title":5265,"description":25,"summary":5266,"pubDate":5267,"source":2720,"url":5268,"thumbnail":5269},"Deep Q-Learning with Atari","Deep Q-Learning with Space Invaders Deep Reinforcement Learning Class with Hugging Face 🤗 Unit 3, of...",["Date","2022-06-07T00:00:00.000Z"],"https://huggingface.co/blog/deep-rl-dqn","https://huggingface.co/blog/assets/78_deep_rl_dqn/thumbnail.gif","src/content/posts/2022-06-07-deep-q-learning-with-atari.md","a1b0dff6b0c69bcb",{"html":25,"metadata":5273},{"headings":5274,"localImagePaths":5275,"remoteImagePaths":5276,"frontmatter":5277,"imagePaths":5279},[],[],[],{"title":5265,"description":25,"summary":5266,"pubDate":5278,"source":2720,"url":5268,"thumbnail":5269},"Tue, 07 Jun 2022 00:00:00 GMT",[],"2022-06-07-deep-q-learning-with-atari.md","2022-06-07-the-annotated-diffusion-model",{"id":5281,"data":5283,"filePath":5289,"digest":5290,"rendered":5291,"legacyId":5298},{"title":5284,"description":25,"summary":5285,"pubDate":5286,"source":2720,"url":5287,"thumbnail":5288},"The Annotated Diffusion Model","The Annotated Diffusion Model In this blog post, we'll take a deeper look into Denoising Diffusion P...",["Date","2022-06-07T00:00:00.000Z"],"https://huggingface.co/blog/annotated-diffusion","https://huggingface.co/blog/assets/78_annotated-diffusion/thumbnail.png","src/content/posts/2022-06-07-the-annotated-diffusion-model.md","a6fea163b882abd8",{"html":25,"metadata":5292},{"headings":5293,"localImagePaths":5294,"remoteImagePaths":5295,"frontmatter":5296,"imagePaths":5297},[],[],[],{"title":5284,"description":25,"summary":5285,"pubDate":5278,"source":2720,"url":5287,"thumbnail":5288},[],"2022-06-07-the-annotated-diffusion-model.md","2022-06-09-techniques-for-training-large-neural-networks",{"id":5299,"data":5301,"filePath":5306,"digest":5307,"rendered":5308,"legacyId":5316},{"title":5302,"description":5303,"summary":5303,"pubDate":5304,"source":19,"url":5305,"thumbnail":21},"Techniques for training large neural networks","Large neural networks are at the core of many recent advances in AI, but training them is a difficult engineering and research challenge which requires orchestrating a cluster of GPUs to perform a single synchronized calculation.",["Date","2022-06-09T07:00:00.000Z"],"https://openai.com/blog/techniques-for-training-large-neural-networks","src/content/posts/2022-06-09-techniques-for-training-large-neural-networks.md","acd065e33566662b",{"html":25,"metadata":5309},{"headings":5310,"localImagePaths":5311,"remoteImagePaths":5312,"frontmatter":5313,"imagePaths":5315},[],[],[],{"title":5302,"description":5303,"summary":5303,"pubDate":5314,"source":19,"url":5305,"thumbnail":21},"Thu, 09 Jun 2022 07:00:00 GMT",[],"2022-06-09-techniques-for-training-large-neural-networks.md","2022-06-13-ai-written-critiques-help-humans-notice-flaws",{"id":5317,"data":5319,"filePath":5324,"digest":5325,"rendered":5326,"legacyId":5334},{"title":5320,"description":5321,"summary":5321,"pubDate":5322,"source":19,"url":5323,"thumbnail":21},"AI-written critiques help humans notice flaws","We trained “critique-writing” models to describe flaws in summaries. Human evaluators find flaws in summaries much more often when shown our model’s critiques. Larger models are better at self-critiquing, with scale improving critique-writing more than summary-writing. This shows promise for using AI systems to assist human supervision of AI systems on difficult tasks.",["Date","2022-06-13T07:00:00.000Z"],"https://openai.com/blog/critiques","src/content/posts/2022-06-13-ai-written-critiques-help-humans-notice-flaws.md","7f9ea49a28713d65",{"html":25,"metadata":5327},{"headings":5328,"localImagePaths":5329,"remoteImagePaths":5330,"frontmatter":5331,"imagePaths":5333},[],[],[],{"title":5320,"description":5321,"summary":5321,"pubDate":5332,"source":19,"url":5323,"thumbnail":21},"Mon, 13 Jun 2022 07:00:00 GMT",[],"2022-06-13-ai-written-critiques-help-humans-notice-flaws.md","2022-06-14-director-of-machine-learning-insights-part-3-finance-edition",{"id":5335,"data":5337,"filePath":5343,"digest":5344,"rendered":5345,"legacyId":5353},{"title":5338,"description":25,"summary":5339,"pubDate":5340,"source":2720,"url":5341,"thumbnail":5342},"Director of Machine Learning Insights [Part 3: Finance Edition]","Director of Machine Learning Insights [Part 3: Finance Edition] If you're interested in building ML ...",["Date","2022-06-14T00:00:00.000Z"],"https://huggingface.co/blog/ml-director-insights-3","https://huggingface.co/blog/assets/78_ml_director_insights/thumbnail.png","src/content/posts/2022-06-14-director-of-machine-learning-insights-part-3-finance-edition.md","0d088804a3d69355",{"html":25,"metadata":5346},{"headings":5347,"localImagePaths":5348,"remoteImagePaths":5349,"frontmatter":5350,"imagePaths":5352},[],[],[],{"title":5338,"description":25,"summary":5339,"pubDate":5351,"source":2720,"url":5341,"thumbnail":5342},"Tue, 14 Jun 2022 00:00:00 GMT",[],"2022-06-14-director-of-machine-learning-insights-part-3-finance-edition.md","2022-06-15-intel-and-hugging-face-partner-to-democratize-machine-learning-hardware-acceleration",{"id":5354,"data":5356,"filePath":5362,"digest":5363,"rendered":5364,"legacyId":5372},{"title":5357,"description":25,"summary":5358,"pubDate":5359,"source":2720,"url":5360,"thumbnail":5361},"Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration","Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration The mission of ...",["Date","2022-06-15T00:00:00.000Z"],"https://huggingface.co/blog/intel","https://huggingface.co/blog/assets/80_intel/01.png","src/content/posts/2022-06-15-intel-and-hugging-face-partner-to-democratize-machine-learning-hardware-acceleration.md","d985689cb47bf309",{"html":25,"metadata":5365},{"headings":5366,"localImagePaths":5367,"remoteImagePaths":5368,"frontmatter":5369,"imagePaths":5371},[],[],[],{"title":5357,"description":25,"summary":5358,"pubDate":5370,"source":2720,"url":5360,"thumbnail":5361},"Wed, 15 Jun 2022 00:00:00 GMT",[],"2022-06-15-intel-and-hugging-face-partner-to-democratize-machine-learning-hardware-acceleration.md","2022-06-17-evolution-through-large-models",{"id":5373,"data":5375,"filePath":5379,"digest":5380,"rendered":5381,"legacyId":5389},{"title":5376,"description":25,"summary":25,"pubDate":5377,"source":19,"url":5378,"thumbnail":21},"Evolution through large models",["Date","2022-06-17T07:00:00.000Z"],"https://openai.com/blog/evolution-through-large-models","src/content/posts/2022-06-17-evolution-through-large-models.md","edde626f8d7ea736",{"html":25,"metadata":5382},{"headings":5383,"localImagePaths":5384,"remoteImagePaths":5385,"frontmatter":5386,"imagePaths":5388},[],[],[],{"title":5376,"description":25,"summary":25,"pubDate":5387,"source":19,"url":5378,"thumbnail":21},"Fri, 17 Jun 2022 07:00:00 GMT",[],"2022-06-17-evolution-through-large-models.md","2022-06-22-convert-transformers-to-onnx-with-hugging-face-optimum",{"id":5390,"data":5392,"filePath":5398,"digest":5399,"rendered":5400,"legacyId":5408},{"title":5393,"description":25,"summary":5394,"pubDate":5395,"source":2720,"url":5396,"thumbnail":5397},"Convert Transformers to ONNX with Hugging Face Optimum","Convert Transformers to ONNX with Hugging Face Optimum Hundreds of Transformers experiments and mode...",["Date","2022-06-22T00:00:00.000Z"],"https://huggingface.co/blog/convert-transformers-to-onnx","https://huggingface.co/blog/assets/81_convert_transformers_to_onnx/thumbnail.png","src/content/posts/2022-06-22-convert-transformers-to-onnx-with-hugging-face-optimum.md","195d72cb1d6a7693",{"html":25,"metadata":5401},{"headings":5402,"localImagePaths":5403,"remoteImagePaths":5404,"frontmatter":5405,"imagePaths":5407},[],[],[],{"title":5393,"description":25,"summary":5394,"pubDate":5406,"source":2720,"url":5396,"thumbnail":5397},"Wed, 22 Jun 2022 00:00:00 GMT",[],"2022-06-22-convert-transformers-to-onnx-with-hugging-face-optimum.md","2022-06-23-getting-started-with-embeddings",{"id":5409,"data":5411,"filePath":5417,"digest":5418,"rendered":5419,"legacyId":5427},{"title":5412,"description":25,"summary":5413,"pubDate":5414,"source":2720,"url":5415,"thumbnail":5416},"Getting Started With Embeddings","Getting Started With Embeddings Check out this tutorial with the Notebook Companion: Understanding e...",["Date","2022-06-23T00:00:00.000Z"],"https://huggingface.co/blog/getting-started-with-embeddings","https://huggingface.co/blog/assets/80_getting_started_with_embeddings/thumbnail.png","src/content/posts/2022-06-23-getting-started-with-embeddings.md","ca7271f221930e9f",{"html":25,"metadata":5420},{"headings":5421,"localImagePaths":5422,"remoteImagePaths":5423,"frontmatter":5424,"imagePaths":5426},[],[],[],{"title":5412,"description":25,"summary":5413,"pubDate":5425,"source":2720,"url":5415,"thumbnail":5416},"Thu, 23 Jun 2022 00:00:00 GMT",[],"2022-06-23-getting-started-with-embeddings.md","2022-06-23-learning-to-play-minecraft-with-video-pretraining",{"id":5428,"data":5430,"filePath":5435,"digest":5436,"rendered":5437,"legacyId":5445},{"title":5431,"description":5432,"summary":5432,"pubDate":5433,"source":19,"url":5434,"thumbnail":21},"Learning to play Minecraft with Video PreTraining","We trained a neural network to play Minecraft by Video PreTraining (VPT) on a massive unlabeled video dataset of human Minecraft play, while using only a small amount of labeled contractor data. With fine-tuning, our model can learn to craft diamond tools, a task that usually takes proficient humans over 20 minutes (24,000 actions). Our model uses the native human interface of keypresses and mouse movements, making it quite general, and represents a step towards general computer-using agents.",["Date","2022-06-23T07:00:00.000Z"],"https://openai.com/blog/vpt","src/content/posts/2022-06-23-learning-to-play-minecraft-with-video-pretraining.md","70471a7a0cb736b8",{"html":25,"metadata":5438},{"headings":5439,"localImagePaths":5440,"remoteImagePaths":5441,"frontmatter":5442,"imagePaths":5444},[],[],[],{"title":5431,"description":5432,"summary":5432,"pubDate":5443,"source":19,"url":5434,"thumbnail":21},"Thu, 23 Jun 2022 07:00:00 GMT",[],"2022-06-23-learning-to-play-minecraft-with-video-pretraining.md","2022-06-28-accelerate-large-model-training-using-deepspeed",{"id":5446,"data":5448,"filePath":5454,"digest":5455,"rendered":5456,"legacyId":5464},{"title":5449,"description":25,"summary":5450,"pubDate":5451,"source":2720,"url":5452,"thumbnail":5453},"Accelerate Large Model Training using DeepSpeed","Accelerate Large Model Training using DeepSpeed In this post we will look at how we can leverage the...",["Date","2022-06-28T00:00:00.000Z"],"https://huggingface.co/blog/accelerate-deepspeed","https://huggingface.co/blog/assets/83_accelerate_deepspeed/deepspeed-thumbnail.png","src/content/posts/2022-06-28-accelerate-large-model-training-using-deepspeed.md","57c8abe976082806",{"html":25,"metadata":5457},{"headings":5458,"localImagePaths":5459,"remoteImagePaths":5460,"frontmatter":5461,"imagePaths":5463},[],[],[],{"title":5449,"description":25,"summary":5450,"pubDate":5462,"source":2720,"url":5452,"thumbnail":5453},"Tue, 28 Jun 2022 00:00:00 GMT",[],"2022-06-28-accelerate-large-model-training-using-deepspeed.md","2022-06-28-announcing-evaluation-on-the-hub",{"id":5465,"data":5467,"filePath":5473,"digest":5474,"rendered":5475,"legacyId":5482},{"title":5468,"description":25,"summary":5469,"pubDate":5470,"source":2720,"url":5471,"thumbnail":5472},"Announcing Evaluation on the Hub","Announcing Evaluation on the Hub This project has been archived. If you want to evaluate LLMs on the...",["Date","2022-06-28T00:00:00.000Z"],"https://huggingface.co/blog/eval-on-the-hub","https://huggingface.co/blog/assets/82_eval_on_the_hub/thumbnail.png","src/content/posts/2022-06-28-announcing-evaluation-on-the-hub.md","c996be580a22f354",{"html":25,"metadata":5476},{"headings":5477,"localImagePaths":5478,"remoteImagePaths":5479,"frontmatter":5480,"imagePaths":5481},[],[],[],{"title":5468,"description":25,"summary":5469,"pubDate":5462,"source":2720,"url":5471,"thumbnail":5472},[],"2022-06-28-announcing-evaluation-on-the-hub.md","2022-06-28-dalle-2-pre-training-mitigations",{"id":5483,"data":5485,"filePath":5490,"digest":5491,"rendered":5492,"legacyId":5500},{"title":5486,"description":5487,"summary":5487,"pubDate":5488,"source":19,"url":5489,"thumbnail":21},"DALL·E 2 pre-training mitigations","In order to share the magic of DALL·E 2 with a broad audience, we needed to reduce the risks associated with powerful image generation models. To this end, we put various guardrails in place to prevent generated images from violating our content policy.",["Date","2022-06-28T07:00:00.000Z"],"https://openai.com/blog/dall-e-2-pre-training-mitigations","src/content/posts/2022-06-28-dalle-2-pre-training-mitigations.md","3a81ba2f0901e26f",{"html":25,"metadata":5493},{"headings":5494,"localImagePaths":5495,"remoteImagePaths":5496,"frontmatter":5497,"imagePaths":5499},[],[],[],{"title":5486,"description":5487,"summary":5487,"pubDate":5498,"source":19,"url":5489,"thumbnail":21},"Tue, 28 Jun 2022 07:00:00 GMT",[],"2022-06-28-dalle-2-pre-training-mitigations.md","2022-06-29-liftoff-how-to-get-started-with-your-first-ml-project",{"id":5501,"data":5503,"filePath":5509,"digest":5510,"rendered":5511,"legacyId":5519},{"title":5504,"description":25,"summary":5505,"pubDate":5506,"source":2720,"url":5507,"thumbnail":5508},"Liftoff! How to get started with your first ML project 🚀","Liftoff! How to get started with your first ML project 🚀 People who are new to the Machine Learning ...",["Date","2022-06-29T00:00:00.000Z"],"https://huggingface.co/blog/your-first-ml-project","https://huggingface.co/blog/assets/84_first_ml_project/thumbnail.png","src/content/posts/2022-06-29-liftoff-how-to-get-started-with-your-first-ml-project.md","5b285a2ef34a53bf",{"html":25,"metadata":5512},{"headings":5513,"localImagePaths":5514,"remoteImagePaths":5515,"frontmatter":5516,"imagePaths":5518},[],[],[],{"title":5504,"description":25,"summary":5505,"pubDate":5517,"source":2720,"url":5507,"thumbnail":5508},"Wed, 29 Jun 2022 00:00:00 GMT",[],"2022-06-29-liftoff-how-to-get-started-with-your-first-ml-project.md","2022-06-30-policy-gradient-with-pytorch",{"id":5520,"data":5522,"filePath":5528,"digest":5529,"rendered":5530,"legacyId":5538},{"title":5523,"description":25,"summary":5524,"pubDate":5525,"source":2720,"url":5526,"thumbnail":5527},"Policy Gradient with PyTorch","Policy Gradient with PyTorch Deep Reinforcement Learning Class with Hugging Face 🤗 Unit 5, of the⚠️ ...",["Date","2022-06-30T00:00:00.000Z"],"https://huggingface.co/blog/deep-rl-pg","https://huggingface.co/blog/assets/85_policy_gradient/thumbnail.gif","src/content/posts/2022-06-30-policy-gradient-with-pytorch.md","4482b5c072910cd8",{"html":25,"metadata":5531},{"headings":5532,"localImagePaths":5533,"remoteImagePaths":5534,"frontmatter":5535,"imagePaths":5537},[],[],[],{"title":5523,"description":25,"summary":5524,"pubDate":5536,"source":2720,"url":5526,"thumbnail":5527},"Thu, 30 Jun 2022 00:00:00 GMT",[],"2022-06-30-policy-gradient-with-pytorch.md","2022-07-07-getting-started-with-sentiment-analysis-on-twitter",{"id":5539,"data":5541,"filePath":5547,"digest":5548,"rendered":5549,"legacyId":5557},{"title":5542,"description":25,"summary":5543,"pubDate":5544,"source":2720,"url":5545,"thumbnail":5546},"Getting Started with Sentiment Analysis on Twitter","Getting Started with Sentiment Analysis on Twitter Sentiment analysis is the automatic process of cl...",["Date","2022-07-07T00:00:00.000Z"],"https://huggingface.co/blog/sentiment-analysis-twitter","https://huggingface.co/blog/assets/85_sentiment_analysis_twitter/thumbnail.png","src/content/posts/2022-07-07-getting-started-with-sentiment-analysis-on-twitter.md","7a9bb084cafca093",{"html":25,"metadata":5550},{"headings":5551,"localImagePaths":5552,"remoteImagePaths":5553,"frontmatter":5554,"imagePaths":5556},[],[],[],{"title":5542,"description":25,"summary":5543,"pubDate":5555,"source":2720,"url":5545,"thumbnail":5546},"Thu, 07 Jul 2022 00:00:00 GMT",[],"2022-07-07-getting-started-with-sentiment-analysis-on-twitter.md","2022-07-12-introducing-the-worlds-largest-open-multilingual-language-model-bloom",{"id":5558,"data":5560,"filePath":5566,"digest":5567,"rendered":5568,"legacyId":5576},{"title":5561,"description":25,"summary":5562,"pubDate":5563,"source":2720,"url":5564,"thumbnail":5565},"Introducing The World's Largest Open Multilingual Language Model: BLOOM","🌸 Introducing The World's Largest Open Multilingual Language Model: BLOOM 🌸 Large language models (L...",["Date","2022-07-12T00:00:00.000Z"],"https://huggingface.co/blog/bloom","https://huggingface.co/blog/assets/86_bloom/thumbnail.png","src/content/posts/2022-07-12-introducing-the-worlds-largest-open-multilingual-language-model-bloom.md","df53240ce42b5781",{"html":25,"metadata":5569},{"headings":5570,"localImagePaths":5571,"remoteImagePaths":5572,"frontmatter":5573,"imagePaths":5575},[],[],[],{"title":5561,"description":25,"summary":5562,"pubDate":5574,"source":2720,"url":5564,"thumbnail":5565},"Tue, 12 Jul 2022 00:00:00 GMT",[],"2022-07-12-introducing-the-worlds-largest-open-multilingual-language-model-bloom.md","2022-07-14-dalle-2-extending-creativity",{"id":5577,"data":5579,"filePath":5584,"digest":5585,"rendered":5586,"legacyId":5594},{"title":5580,"description":5581,"summary":5581,"pubDate":5582,"source":19,"url":5583,"thumbnail":21},"DALL·E 2: Extending creativity","As part of our DALL·E 2 research preview, more than 3,000 artists from more than 118 countries have incorporated DALL·E into their creative workflows. The artists in our early access group have helped us discover new uses for DALL·E and have served as key voices as we’ve made decisions about DALL·E’s features.",["Date","2022-07-14T07:00:00.000Z"],"https://openai.com/blog/dall-e-2-extending-creativity","src/content/posts/2022-07-14-dalle-2-extending-creativity.md","633546d86a94780d",{"html":25,"metadata":5587},{"headings":5588,"localImagePaths":5589,"remoteImagePaths":5590,"frontmatter":5591,"imagePaths":5593},[],[],[],{"title":5580,"description":5581,"summary":5581,"pubDate":5592,"source":19,"url":5583,"thumbnail":21},"Thu, 14 Jul 2022 07:00:00 GMT",[],"2022-07-14-dalle-2-extending-creativity.md","2022-07-13-building-a-playlist-generator-with-sentence-transformers",{"id":5595,"data":5597,"filePath":5603,"digest":5604,"rendered":5605,"legacyId":5613},{"title":5598,"description":25,"summary":5599,"pubDate":5600,"source":2720,"url":5601,"thumbnail":5602},"Building a Playlist Generator with Sentence Transformers","Building a Playlist Generator with Sentence Transformers A short while ago I published a playlist ge...",["Date","2022-07-13T00:00:00.000Z"],"https://huggingface.co/blog/playlist-generator","https://huggingface.co/blog/assets/87_playlist_generator/thumbnail.png","src/content/posts/2022-07-13-building-a-playlist-generator-with-sentence-transformers.md","d4d56f302dc0c7c9",{"html":25,"metadata":5606},{"headings":5607,"localImagePaths":5608,"remoteImagePaths":5609,"frontmatter":5610,"imagePaths":5612},[],[],[],{"title":5598,"description":25,"summary":5599,"pubDate":5611,"source":2720,"url":5601,"thumbnail":5602},"Wed, 13 Jul 2022 00:00:00 GMT",[],"2022-07-13-building-a-playlist-generator-with-sentence-transformers.md","2022-07-14-the-technology-behind-bloom-training",{"id":5614,"data":5616,"filePath":5622,"digest":5623,"rendered":5624,"legacyId":5632},{"title":5617,"description":25,"summary":5618,"pubDate":5619,"source":2720,"url":5620,"thumbnail":5621},"The Technology Behind BLOOM Training","The Technology Behind BLOOM Training In recent years, training ever larger language models has becom...",["Date","2022-07-14T00:00:00.000Z"],"https://huggingface.co/blog/bloom-megatron-deepspeed","https://huggingface.co/blog/assets/86_bloom_megatron_deepspeed/thumbnail.png","src/content/posts/2022-07-14-the-technology-behind-bloom-training.md","898f8d6d6a3cbc32",{"html":25,"metadata":5625},{"headings":5626,"localImagePaths":5627,"remoteImagePaths":5628,"frontmatter":5629,"imagePaths":5631},[],[],[],{"title":5617,"description":25,"summary":5618,"pubDate":5630,"source":2720,"url":5620,"thumbnail":5621},"Thu, 14 Jul 2022 00:00:00 GMT",[],"2022-07-14-the-technology-behind-bloom-training.md","2022-07-18-reducing-bias-and-improving-safety-in-dalle-2",{"id":5633,"data":5635,"filePath":5640,"digest":5641,"rendered":5642,"legacyId":5650},{"title":5636,"description":5637,"summary":5637,"pubDate":5638,"source":19,"url":5639,"thumbnail":21},"Reducing bias and improving safety in DALL·E 2","Today, we are implementing a new technique so that DALL·E generates images of people that more accurately reflect the diversity of the world’s population.",["Date","2022-07-18T07:00:00.000Z"],"https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2","src/content/posts/2022-07-18-reducing-bias-and-improving-safety-in-dalle-2.md","f3defdeaa766ac01",{"html":25,"metadata":5643},{"headings":5644,"localImagePaths":5645,"remoteImagePaths":5646,"frontmatter":5647,"imagePaths":5649},[],[],[],{"title":5636,"description":5637,"summary":5637,"pubDate":5648,"source":19,"url":5639,"thumbnail":21},"Mon, 18 Jul 2022 07:00:00 GMT",[],"2022-07-18-reducing-bias-and-improving-safety-in-dalle-2.md","2022-07-16-how-to-train-your-model-dynamically-using-adversarial-data",{"id":5651,"data":5653,"filePath":5659,"digest":5660,"rendered":5661,"legacyId":5669},{"title":5654,"description":25,"summary":5655,"pubDate":5656,"source":2720,"url":5657,"thumbnail":5658},"How to train your model dynamically using adversarial data","How to train your model dynamically using adversarial data What you will learn here - 💡the basic ide...",["Date","2022-07-16T00:00:00.000Z"],"https://huggingface.co/blog/mnist-adversarial","https://huggingface.co/blog/assets/88_mnist_adversarial/mnist-adversarial.png","src/content/posts/2022-07-16-how-to-train-your-model-dynamically-using-adversarial-data.md","5c16873158291e22",{"html":25,"metadata":5662},{"headings":5663,"localImagePaths":5664,"remoteImagePaths":5665,"frontmatter":5666,"imagePaths":5668},[],[],[],{"title":5654,"description":25,"summary":5655,"pubDate":5667,"source":2720,"url":5657,"thumbnail":5658},"Sat, 16 Jul 2022 00:00:00 GMT",[],"2022-07-16-how-to-train-your-model-dynamically-using-adversarial-data.md","2022-07-20-dalle-now-available-in-beta",{"id":5670,"data":5672,"filePath":5677,"digest":5678,"rendered":5679,"legacyId":5687},{"title":5673,"description":5674,"summary":5674,"pubDate":5675,"source":19,"url":5676,"thumbnail":21},"DALL·E now available in beta","We’ll invite 1 million people from our waitlist over the coming weeks. Users can create with DALL·E using free credits that refill every month, and buy additional credits in 115-generation increments for $15.",["Date","2022-07-20T07:00:00.000Z"],"https://openai.com/blog/dall-e-now-available-in-beta","src/content/posts/2022-07-20-dalle-now-available-in-beta.md","8e4669b8e3ce5684",{"html":25,"metadata":5680},{"headings":5681,"localImagePaths":5682,"remoteImagePaths":5683,"frontmatter":5684,"imagePaths":5686},[],[],[],{"title":5673,"description":5674,"summary":5674,"pubDate":5685,"source":19,"url":5676,"thumbnail":21},"Wed, 20 Jul 2022 07:00:00 GMT",[],"2022-07-20-dalle-now-available-in-beta.md","2022-07-22-advantage-actor-critic-a2c",{"id":5688,"data":5690,"filePath":5696,"digest":5697,"rendered":5698,"legacyId":5706},{"title":5691,"description":25,"summary":5692,"pubDate":5693,"source":2720,"url":5694,"thumbnail":5695},"Advantage Actor Critic (A2C)","Advantage Actor Critic (A2C) Deep Reinforcement Learning Class with Hugging Face 🤗 Unit 7, of the⚠️ ...",["Date","2022-07-22T00:00:00.000Z"],"https://huggingface.co/blog/deep-rl-a2c","https://huggingface.co/blog/assets/89_deep_rl_a2c/thumbnail.gif","src/content/posts/2022-07-22-advantage-actor-critic-a2c.md","45bfa9cea0e028a8",{"html":25,"metadata":5699},{"headings":5700,"localImagePaths":5701,"remoteImagePaths":5702,"frontmatter":5703,"imagePaths":5705},[],[],[],{"title":5691,"description":25,"summary":5692,"pubDate":5704,"source":2720,"url":5694,"thumbnail":5695},"Fri, 22 Jul 2022 00:00:00 GMT",[],"2022-07-22-advantage-actor-critic-a2c.md","2022-07-25-a-hazard-analysis-framework-for-code-synthesis-large-language-models",{"id":5707,"data":5709,"filePath":5713,"digest":5714,"rendered":5715,"legacyId":5723},{"title":5710,"description":25,"summary":25,"pubDate":5711,"source":19,"url":5712,"thumbnail":21},"A hazard analysis framework for code synthesis large language models",["Date","2022-07-25T07:00:00.000Z"],"https://openai.com/blog/a-hazard-analysis-framework-for-code-synthesis-large-language-models","src/content/posts/2022-07-25-a-hazard-analysis-framework-for-code-synthesis-large-language-models.md","1a16162c1050c5f5",{"html":25,"metadata":5716},{"headings":5717,"localImagePaths":5718,"remoteImagePaths":5719,"frontmatter":5720,"imagePaths":5722},[],[],[],{"title":5710,"description":25,"summary":25,"pubDate":5721,"source":19,"url":5712,"thumbnail":21},"Mon, 25 Jul 2022 07:00:00 GMT",[],"2022-07-25-a-hazard-analysis-framework-for-code-synthesis-large-language-models.md","2022-07-25-deploying-tensorflow-vision-models-in-hugging-face-with-tf-serving",{"id":5724,"data":5726,"filePath":5732,"digest":5733,"rendered":5734,"legacyId":5742},{"title":5727,"description":25,"summary":5728,"pubDate":5729,"source":2720,"url":5730,"thumbnail":5731},"Deploying TensorFlow Vision Models in Hugging Face with TF Serving","Deploying TensorFlow Vision Models in Hugging Face with TF Serving In the past few months, the Huggi...",["Date","2022-07-25T00:00:00.000Z"],"https://huggingface.co/blog/tf-serving-vision","https://huggingface.co/blog/assets/90_tf_serving_vision/thumbnail.png","src/content/posts/2022-07-25-deploying-tensorflow-vision-models-in-hugging-face-with-tf-serving.md","c7073e6916d35ad4",{"html":25,"metadata":5735},{"headings":5736,"localImagePaths":5737,"remoteImagePaths":5738,"frontmatter":5739,"imagePaths":5741},[],[],[],{"title":5727,"description":25,"summary":5728,"pubDate":5740,"source":2720,"url":5730,"thumbnail":5731},"Mon, 25 Jul 2022 00:00:00 GMT",[],"2022-07-25-deploying-tensorflow-vision-models-in-hugging-face-with-tf-serving.md","2022-07-28-efficient-training-of-language-models-to-fill-in-the-middle",{"id":5743,"data":5745,"filePath":5749,"digest":5750,"rendered":5751,"legacyId":5759},{"title":5746,"description":25,"summary":25,"pubDate":5747,"source":19,"url":5748,"thumbnail":21},"Efficient training of language models to fill in the middle",["Date","2022-07-28T07:00:00.000Z"],"https://openai.com/blog/efficient-training-of-language-models-to-fill-in-the-middle","src/content/posts/2022-07-28-efficient-training-of-language-models-to-fill-in-the-middle.md","59da287984b4e622",{"html":25,"metadata":5752},{"headings":5753,"localImagePaths":5754,"remoteImagePaths":5755,"frontmatter":5756,"imagePaths":5758},[],[],[],{"title":5746,"description":25,"summary":25,"pubDate":5757,"source":19,"url":5748,"thumbnail":21},"Thu, 28 Jul 2022 07:00:00 GMT",[],"2022-07-28-efficient-training-of-language-models-to-fill-in-the-middle.md","2022-07-27-faster-text-generation-with-tensorflow-and-xla",{"id":5760,"data":5762,"filePath":5768,"digest":5769,"rendered":5770,"legacyId":5778},{"title":5763,"description":25,"summary":5764,"pubDate":5765,"source":2720,"url":5766,"thumbnail":5767},"Faster Text Generation with TensorFlow and XLA","Faster Text Generation with TensorFlow and XLA TL;DR: Text Generation on 🤗 transformers using Tensor...",["Date","2022-07-27T00:00:00.000Z"],"https://huggingface.co/blog/tf-xla-generate","https://huggingface.co/blog/assets/91_tf_xla_generate/thumbnail.png","src/content/posts/2022-07-27-faster-text-generation-with-tensorflow-and-xla.md","4b9be39586d4f928",{"html":25,"metadata":5771},{"headings":5772,"localImagePaths":5773,"remoteImagePaths":5774,"frontmatter":5775,"imagePaths":5777},[],[],[],{"title":5763,"description":25,"summary":5764,"pubDate":5776,"source":2720,"url":5766,"thumbnail":5767},"Wed, 27 Jul 2022 00:00:00 GMT",[],"2022-07-27-faster-text-generation-with-tensorflow-and-xla.md","2022-07-28-introducing-new-audio-and-vision-documentation-in-datasets",{"id":5779,"data":5781,"filePath":5787,"digest":5788,"rendered":5789,"legacyId":5797},{"title":5782,"description":25,"summary":5783,"pubDate":5784,"source":2720,"url":5785,"thumbnail":5786},"Introducing new audio and vision documentation in 🤗 Datasets","Introducing new audio and vision documentation in 🤗 Datasets Open and reproducible datasets are esse...",["Date","2022-07-28T00:00:00.000Z"],"https://huggingface.co/blog/datasets-docs-update","https://huggingface.co/blog/assets/87_datasets-docs-update/thumbnail.gif","src/content/posts/2022-07-28-introducing-new-audio-and-vision-documentation-in-datasets.md","3288ec142c6c9aad",{"html":25,"metadata":5790},{"headings":5791,"localImagePaths":5792,"remoteImagePaths":5793,"frontmatter":5794,"imagePaths":5796},[],[],[],{"title":5782,"description":25,"summary":5783,"pubDate":5795,"source":2720,"url":5785,"thumbnail":5786},"Thu, 28 Jul 2022 00:00:00 GMT",[],"2022-07-28-introducing-new-audio-and-vision-documentation-in-datasets.md","2022-08-01-ai-policy-comments-on-us-national-ai-research-resource-interim-report",{"id":5798,"data":5800,"filePath":5806,"digest":5807,"rendered":5808,"legacyId":5816},{"title":5801,"description":25,"summary":5802,"pubDate":5803,"source":2720,"url":5804,"thumbnail":5805},"AI Policy @🤗: Comments on U.S. National AI Research Resource Interim Report","AI Policy @🤗: Comments on U.S. National AI Research Resource Interim Report In late June 2022, Huggi...",["Date","2022-08-01T00:00:00.000Z"],"https://huggingface.co/blog/us-national-ai-research-resource","https://huggingface.co/blog/assets/92_us_national_ai_research_resource/nairr_thumbnail.png","src/content/posts/2022-08-01-ai-policy-comments-on-us-national-ai-research-resource-interim-report.md","49eb8b09f315e68f",{"html":25,"metadata":5809},{"headings":5810,"localImagePaths":5811,"remoteImagePaths":5812,"frontmatter":5813,"imagePaths":5815},[],[],[],{"title":5801,"description":25,"summary":5802,"pubDate":5814,"source":2720,"url":5804,"thumbnail":5805},"Mon, 01 Aug 2022 00:00:00 GMT",[],"2022-08-01-ai-policy-comments-on-us-national-ai-research-resource-interim-report.md","2022-08-02-nyströmformer-approximating-self-attention-in-linear-time-and-memory-via-the-nyström-method",{"id":5817,"data":5819,"filePath":5825,"digest":5826,"rendered":5827,"legacyId":5835},{"title":5820,"description":25,"summary":5821,"pubDate":5822,"source":2720,"url":5823,"thumbnail":5824},"Nyströmformer, Approximating self-attention in linear time and memory via the Nyström method","Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method Introdu...",["Date","2022-08-02T00:00:00.000Z"],"https://huggingface.co/blog/nystromformer","https://huggingface.co/blog/assets/86_nystromformer/thumbnail.png","src/content/posts/2022-08-02-nyströmformer-approximating-self-attention-in-linear-time-and-memory-via-the-nyström-method.md","61418786e1efb34f",{"html":25,"metadata":5828},{"headings":5829,"localImagePaths":5830,"remoteImagePaths":5831,"frontmatter":5832,"imagePaths":5834},[],[],[],{"title":5820,"description":25,"summary":5821,"pubDate":5833,"source":2720,"url":5823,"thumbnail":5824},"Tue, 02 Aug 2022 00:00:00 GMT",[],"2022-08-02-nyströmformer-approximating-self-attention-in-linear-time-and-memory-via-the-nyström-method.md","2022-08-03-introducing-the-private-hub-a-new-way-to-build-with-machine-learning",{"id":5836,"data":5838,"filePath":5844,"digest":5845,"rendered":5846,"legacyId":5854},{"title":5839,"description":25,"summary":5840,"pubDate":5841,"source":2720,"url":5842,"thumbnail":5843},"Introducing the Private Hub: A New Way to Build With Machine Learning","Introducing the Private Hub: A New Way to Build With Machine Learning June 2023 Update: The Private ...",["Date","2022-08-03T00:00:00.000Z"],"https://huggingface.co/blog/introducing-private-hub","https://huggingface.co/blog/assets/92_introducing_private_hub/thumbnail.png","src/content/posts/2022-08-03-introducing-the-private-hub-a-new-way-to-build-with-machine-learning.md","5b7d54c22e010914",{"html":25,"metadata":5847},{"headings":5848,"localImagePaths":5849,"remoteImagePaths":5850,"frontmatter":5851,"imagePaths":5853},[],[],[],{"title":5839,"description":25,"summary":5840,"pubDate":5852,"source":2720,"url":5842,"thumbnail":5843},"Wed, 03 Aug 2022 00:00:00 GMT",[],"2022-08-03-introducing-the-private-hub-a-new-way-to-build-with-machine-learning.md","2022-08-05-proximal-policy-optimization-ppo",{"id":5855,"data":5857,"filePath":5863,"digest":5864,"rendered":5865,"legacyId":5873},{"title":5858,"description":25,"summary":5859,"pubDate":5860,"source":2720,"url":5861,"thumbnail":5862},"Proximal Policy Optimization (PPO)","Proximal Policy Optimization (PPO) Deep Reinforcement Learning Class with Hugging Face 🤗 Unit 8, of ...",["Date","2022-08-05T00:00:00.000Z"],"https://huggingface.co/blog/deep-rl-ppo","https://huggingface.co/blog/assets/93_deep_rl_ppo/thumbnail.png","src/content/posts/2022-08-05-proximal-policy-optimization-ppo.md","51d6a6d33d51a3b1",{"html":25,"metadata":5866},{"headings":5867,"localImagePaths":5868,"remoteImagePaths":5869,"frontmatter":5870,"imagePaths":5872},[],[],[],{"title":5858,"description":25,"summary":5859,"pubDate":5871,"source":2720,"url":5861,"thumbnail":5862},"Fri, 05 Aug 2022 00:00:00 GMT",[],"2022-08-05-proximal-policy-optimization-ppo.md","2022-08-10-new-and-improved-content-moderation-tooling",{"id":5874,"data":5876,"filePath":5881,"digest":5882,"rendered":5883,"legacyId":5891},{"title":5877,"description":5878,"summary":5878,"pubDate":5879,"source":19,"url":5880,"thumbnail":21},"New and improved content moderation tooling","We are introducing a new and improved content moderation tool. The Moderation endpoint improves upon our previous content filter, and is available for free today to OpenAI API developers.",["Date","2022-08-10T07:00:00.000Z"],"https://openai.com/blog/new-and-improved-content-moderation-tooling","src/content/posts/2022-08-10-new-and-improved-content-moderation-tooling.md","5d5070b5434ff0bc",{"html":25,"metadata":5884},{"headings":5885,"localImagePaths":5886,"remoteImagePaths":5887,"frontmatter":5888,"imagePaths":5890},[],[],[],{"title":5877,"description":5878,"summary":5878,"pubDate":5889,"source":19,"url":5880,"thumbnail":21},"Wed, 10 Aug 2022 07:00:00 GMT",[],"2022-08-10-new-and-improved-content-moderation-tooling.md","2022-08-10-train-and-fine-tune-sentence-transformers-models",{"id":5892,"data":5894,"filePath":5900,"digest":5901,"rendered":5902,"legacyId":5910},{"title":5895,"description":25,"summary":5896,"pubDate":5897,"source":2720,"url":5898,"thumbnail":5899},"Train and Fine-Tune Sentence Transformers Models","Train and Fine-Tune Sentence Transformers Models This guide is only suited for Sentence Transformers...",["Date","2022-08-10T00:00:00.000Z"],"https://huggingface.co/blog/how-to-train-sentence-transformers","https://huggingface.co/blog/assets/95_training_st_models/thumbnail.png","src/content/posts/2022-08-10-train-and-fine-tune-sentence-transformers-models.md","2da21f0f5c7a332a",{"html":25,"metadata":5903},{"headings":5904,"localImagePaths":5905,"remoteImagePaths":5906,"frontmatter":5907,"imagePaths":5909},[],[],[],{"title":5895,"description":25,"summary":5896,"pubDate":5908,"source":2720,"url":5898,"thumbnail":5899},"Wed, 10 Aug 2022 00:00:00 GMT",[],"2022-08-10-train-and-fine-tune-sentence-transformers-models.md","2022-08-11-deploying-vit-on-kubernetes-with-tf-serving",{"id":5911,"data":5913,"filePath":5919,"digest":5920,"rendered":5921,"legacyId":5929},{"title":5914,"description":25,"summary":5915,"pubDate":5916,"source":2720,"url":5917,"thumbnail":5918},"Deploying 🤗 ViT on Kubernetes with TF Serving","Deploying 🤗 ViT on Kubernetes with TF Serving In the previous post, we showed how to deploy a Vision...",["Date","2022-08-11T00:00:00.000Z"],"https://huggingface.co/blog/deploy-tfserving-kubernetes","https://huggingface.co/blog/assets/94_tf_serving_kubernetes/thumb.png","src/content/posts/2022-08-11-deploying-vit-on-kubernetes-with-tf-serving.md","d976993bc4d41b5d",{"html":25,"metadata":5922},{"headings":5923,"localImagePaths":5924,"remoteImagePaths":5925,"frontmatter":5926,"imagePaths":5928},[],[],[],{"title":5914,"description":25,"summary":5915,"pubDate":5927,"source":2720,"url":5917,"thumbnail":5918},"Thu, 11 Aug 2022 00:00:00 GMT",[],"2022-08-11-deploying-vit-on-kubernetes-with-tf-serving.md","2022-08-12-hugging-faces-tensorflow-philosophy",{"id":5930,"data":5932,"filePath":5938,"digest":5939,"rendered":5940,"legacyId":5948},{"title":5933,"description":25,"summary":5934,"pubDate":5935,"source":2720,"url":5936,"thumbnail":5937},"Hugging Face's TensorFlow Philosophy","Hugging Face's TensorFlow Philosophy Introduction Despite increasing competition from PyTorch and JA...",["Date","2022-08-12T00:00:00.000Z"],"https://huggingface.co/blog/tensorflow-philosophy","https://huggingface.co/blog/assets/96_tensorflow_philosophy/thumbnail.png","src/content/posts/2022-08-12-hugging-faces-tensorflow-philosophy.md","74e2833ca61dc510",{"html":25,"metadata":5941},{"headings":5942,"localImagePaths":5943,"remoteImagePaths":5944,"frontmatter":5945,"imagePaths":5947},[],[],[],{"title":5933,"description":25,"summary":5934,"pubDate":5946,"source":2720,"url":5936,"thumbnail":5937},"Fri, 12 Aug 2022 00:00:00 GMT",[],"2022-08-12-hugging-faces-tensorflow-philosophy.md","2022-08-12-introducing-skops",{"id":5949,"data":5951,"filePath":5957,"digest":5958,"rendered":5959,"legacyId":5966},{"title":5952,"description":25,"summary":5953,"pubDate":5954,"source":2720,"url":5955,"thumbnail":5956},"Introducing Skops","Introducing Skops Introducing Skops At Hugging Face, we are working on tackling various problems in ...",["Date","2022-08-12T00:00:00.000Z"],"https://huggingface.co/blog/skops","https://huggingface.co/blog/assets/94_skops/introducing_skops.png","src/content/posts/2022-08-12-introducing-skops.md","6feb4809da1431e4",{"html":25,"metadata":5960},{"headings":5961,"localImagePaths":5962,"remoteImagePaths":5963,"frontmatter":5964,"imagePaths":5965},[],[],[],{"title":5952,"description":25,"summary":5953,"pubDate":5946,"source":2720,"url":5955,"thumbnail":5956},[],"2022-08-12-introducing-skops.md","2022-08-17-a-gentle-introduction-to-8-bit-matrix-multiplication-for-transformers-at-scale-using-transformers-accelerate-and-bitsandbytes",{"id":5967,"data":5969,"filePath":5975,"digest":5976,"rendered":5977,"legacyId":5985},{"title":5970,"description":25,"summary":5971,"pubDate":5972,"source":2720,"url":5973,"thumbnail":5974},"A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes","A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Tr...",["Date","2022-08-17T00:00:00.000Z"],"https://huggingface.co/blog/hf-bitsandbytes-integration","https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/Thumbnail_blue.png","src/content/posts/2022-08-17-a-gentle-introduction-to-8-bit-matrix-multiplication-for-transformers-at-scale-using-transformers-accelerate-and-bitsandbytes.md","0a91993a18469d56",{"html":25,"metadata":5978},{"headings":5979,"localImagePaths":5980,"remoteImagePaths":5981,"frontmatter":5982,"imagePaths":5984},[],[],[],{"title":5970,"description":25,"summary":5971,"pubDate":5983,"source":2720,"url":5973,"thumbnail":5974},"Wed, 17 Aug 2022 00:00:00 GMT",[],"2022-08-17-a-gentle-introduction-to-8-bit-matrix-multiplication-for-transformers-at-scale-using-transformers-accelerate-and-bitsandbytes.md","2022-08-18-deep-dive-vision-transformers-on-hugging-face-optimum-graphcore",{"id":5986,"data":5988,"filePath":5994,"digest":5995,"rendered":5996,"legacyId":6004},{"title":5989,"description":25,"summary":5990,"pubDate":5991,"source":2720,"url":5992,"thumbnail":5993},"Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore","Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore This blog post will show how easy i...",["Date","2022-08-18T00:00:00.000Z"],"https://huggingface.co/blog/vision-transformers","https://huggingface.co/blog/assets/97_vision_transformers/thumbnail.png","src/content/posts/2022-08-18-deep-dive-vision-transformers-on-hugging-face-optimum-graphcore.md","3e0836b4030bf54e",{"html":25,"metadata":5997},{"headings":5998,"localImagePaths":5999,"remoteImagePaths":6000,"frontmatter":6001,"imagePaths":6003},[],[],[],{"title":5989,"description":25,"summary":5990,"pubDate":6002,"source":2720,"url":5992,"thumbnail":5993},"Thu, 18 Aug 2022 00:00:00 GMT",[],"2022-08-18-deep-dive-vision-transformers-on-hugging-face-optimum-graphcore.md","2022-08-19-deploying-vit-on-vertex-ai",{"id":6005,"data":6007,"filePath":6013,"digest":6014,"rendered":6015,"legacyId":6023},{"title":6008,"description":25,"summary":6009,"pubDate":6010,"source":2720,"url":6011,"thumbnail":6012},"Deploying 🤗 ViT on Vertex AI","Deploying 🤗 ViT on Vertex AI In the previous posts, we showed how to deploy a Vision Transformers (V...",["Date","2022-08-19T00:00:00.000Z"],"https://huggingface.co/blog/deploy-vertex-ai","https://huggingface.co/blog/assets/97_vertex_ai/image1.png","src/content/posts/2022-08-19-deploying-vit-on-vertex-ai.md","3703c060b9ca34b3",{"html":25,"metadata":6016},{"headings":6017,"localImagePaths":6018,"remoteImagePaths":6019,"frontmatter":6020,"imagePaths":6022},[],[],[],{"title":6008,"description":25,"summary":6009,"pubDate":6021,"source":2720,"url":6011,"thumbnail":6012},"Fri, 19 Aug 2022 00:00:00 GMT",[],"2022-08-19-deploying-vit-on-vertex-ai.md","2022-08-22-pre-train-bert-with-hugging-face-transformers-and-habana-gaudi",{"id":6024,"data":6026,"filePath":6032,"digest":6033,"rendered":6034,"legacyId":6042},{"title":6027,"description":25,"summary":6028,"pubDate":6029,"source":2720,"url":6030,"thumbnail":6031},"Pre-Train BERT with Hugging Face Transformers and Habana Gaudi","Pre-Training BERT with Hugging Face Transformers and Habana Gaudi In this Tutorial, you will learn h...",["Date","2022-08-22T00:00:00.000Z"],"https://huggingface.co/blog/pretraining-bert","https://huggingface.co/blog/assets/99_pretraining_bert/thumbnail.png","src/content/posts/2022-08-22-pre-train-bert-with-hugging-face-transformers-and-habana-gaudi.md","b2130c3072f0ffe9",{"html":25,"metadata":6035},{"headings":6036,"localImagePaths":6037,"remoteImagePaths":6038,"frontmatter":6039,"imagePaths":6041},[],[],[],{"title":6027,"description":25,"summary":6028,"pubDate":6040,"source":2720,"url":6030,"thumbnail":6031},"Mon, 22 Aug 2022 00:00:00 GMT",[],"2022-08-22-pre-train-bert-with-hugging-face-transformers-and-habana-gaudi.md","2022-08-22-stable-diffusion-with-diffusers",{"id":6043,"data":6045,"filePath":6051,"digest":6052,"rendered":6053,"legacyId":6060},{"title":6046,"description":25,"summary":6047,"pubDate":6048,"source":2720,"url":6049,"thumbnail":6050},"Stable Diffusion with 🧨 Diffusers","Stable Diffusion with 🧨 Diffusers Stable Diffusion 🎨 ...using 🧨 Diffusers Stable Diffusion is a text...",["Date","2022-08-22T00:00:00.000Z"],"https://huggingface.co/blog/stable_diffusion","https://huggingface.co/blog/assets/98_stable_diffusion/thumbnail.png","src/content/posts/2022-08-22-stable-diffusion-with-diffusers.md","e14048362277c28a",{"html":25,"metadata":6054},{"headings":6055,"localImagePaths":6056,"remoteImagePaths":6057,"frontmatter":6058,"imagePaths":6059},[],[],[],{"title":6046,"description":25,"summary":6047,"pubDate":6040,"source":2720,"url":6049,"thumbnail":6050},[],"2022-08-22-stable-diffusion-with-diffusers.md","2022-08-24-our-approach-to-alignment-research",{"id":6061,"data":6063,"filePath":6068,"digest":6069,"rendered":6070,"legacyId":6078},{"title":6064,"description":6065,"summary":6065,"pubDate":6066,"source":19,"url":6067,"thumbnail":21},"Our approach to alignment research","We are improving our AI systems’ ability to learn from human feedback and to assist humans at evaluating AI. Our goal is to build a sufficiently aligned AI system that can help us solve all other alignment problems.",["Date","2022-08-24T07:00:00.000Z"],"https://openai.com/blog/our-approach-to-alignment-research","src/content/posts/2022-08-24-our-approach-to-alignment-research.md","9cb81832e5e30892",{"html":25,"metadata":6071},{"headings":6072,"localImagePaths":6073,"remoteImagePaths":6074,"frontmatter":6075,"imagePaths":6077},[],[],[],{"title":6064,"description":6065,"summary":6065,"pubDate":6076,"source":19,"url":6067,"thumbnail":21},"Wed, 24 Aug 2022 07:00:00 GMT",[],"2022-08-24-our-approach-to-alignment-research.md","2022-08-24-visualize-proteins-on-hugging-face-spaces",{"id":6079,"data":6081,"filePath":6087,"digest":6088,"rendered":6089,"legacyId":6097},{"title":6082,"description":25,"summary":6083,"pubDate":6084,"source":2720,"url":6085,"thumbnail":6086},"Visualize proteins on Hugging Face Spaces","Visualize proteins on Hugging Face Spaces In this post we will look at how we can visualize proteins...",["Date","2022-08-24T00:00:00.000Z"],"https://huggingface.co/blog/spaces_3dmoljs","https://huggingface.co/blog/assets/98_spaces_3dmoljs/thumbnail.png","src/content/posts/2022-08-24-visualize-proteins-on-hugging-face-spaces.md","6303f68e2a829bb8",{"html":25,"metadata":6090},{"headings":6091,"localImagePaths":6092,"remoteImagePaths":6093,"frontmatter":6094,"imagePaths":6096},[],[],[],{"title":6082,"description":25,"summary":6083,"pubDate":6095,"source":2720,"url":6085,"thumbnail":6086},"Wed, 24 Aug 2022 00:00:00 GMT",[],"2022-08-24-visualize-proteins-on-hugging-face-spaces.md","2022-08-31-dalle-introducing-outpainting",{"id":6098,"data":6100,"filePath":6105,"digest":6106,"rendered":6107,"legacyId":6115},{"title":6101,"description":6102,"summary":6102,"pubDate":6103,"source":19,"url":6104,"thumbnail":21},"DALL·E: Introducing outpainting","Extend creativity and tell a bigger story with DALL·E images of any size.",["Date","2022-08-31T07:00:00.000Z"],"https://openai.com/blog/dall-e-introducing-outpainting","src/content/posts/2022-08-31-dalle-introducing-outpainting.md","5fd12febd53b6f71",{"html":25,"metadata":6108},{"headings":6109,"localImagePaths":6110,"remoteImagePaths":6111,"frontmatter":6112,"imagePaths":6114},[],[],[],{"title":6101,"description":6102,"summary":6102,"pubDate":6113,"source":19,"url":6104,"thumbnail":21},"Wed, 31 Aug 2022 07:00:00 GMT",[],"2022-08-31-dalle-introducing-outpainting.md","2022-08-31-openrail-towards-open-and-responsible-ai-licensing-frameworks",{"id":6116,"data":6118,"filePath":6124,"digest":6125,"rendered":6126,"legacyId":6134},{"title":6119,"description":25,"summary":6120,"pubDate":6121,"source":2720,"url":6122,"thumbnail":6123},"OpenRAIL: Towards open and responsible AI licensing frameworks","OpenRAIL: Towards open and responsible AI licensing frameworks Open & Responsible AI licenses ('Open...",["Date","2022-08-31T00:00:00.000Z"],"https://huggingface.co/blog/open_rail","https://huggingface.co/blog/assets/100_open_rail/100_open-rail.png","src/content/posts/2022-08-31-openrail-towards-open-and-responsible-ai-licensing-frameworks.md","02e9e626f2a1a16b",{"html":25,"metadata":6127},{"headings":6128,"localImagePaths":6129,"remoteImagePaths":6130,"frontmatter":6131,"imagePaths":6133},[],[],[],{"title":6119,"description":25,"summary":6120,"pubDate":6132,"source":2720,"url":6122,"thumbnail":6123},"Wed, 31 Aug 2022 00:00:00 GMT",[],"2022-08-31-openrail-towards-open-and-responsible-ai-licensing-frameworks.md","2022-09-07-how-to-train-a-language-model-with-megatron-lm",{"id":6135,"data":6137,"filePath":6143,"digest":6144,"rendered":6145,"legacyId":6153},{"title":6138,"description":25,"summary":6139,"pubDate":6140,"source":2720,"url":6141,"thumbnail":6142},"How to train a Language Model with Megatron-LM","How to train a Language Model with Megatron-LM Training large language models in Pytorch requires mo...",["Date","2022-09-07T00:00:00.000Z"],"https://huggingface.co/blog/megatron-training","https://huggingface.co/blog/assets/100_megatron_training/thumbnail.png","src/content/posts/2022-09-07-how-to-train-a-language-model-with-megatron-lm.md","dfc5b4a570f127f0",{"html":25,"metadata":6146},{"headings":6147,"localImagePaths":6148,"remoteImagePaths":6149,"frontmatter":6150,"imagePaths":6152},[],[],[],{"title":6138,"description":25,"summary":6139,"pubDate":6151,"source":2720,"url":6141,"thumbnail":6142},"Wed, 07 Sep 2022 00:00:00 GMT",[],"2022-09-07-how-to-train-a-language-model-with-megatron-lm.md","2022-09-08-train-your-first-decision-transformer",{"id":6154,"data":6156,"filePath":6162,"digest":6163,"rendered":6164,"legacyId":6172},{"title":6157,"description":25,"summary":6158,"pubDate":6159,"source":2720,"url":6160,"thumbnail":6161},"Train your first Decision Transformer","Train your first Decision Transformer In a previous post, we announced the launch of Decision Transf...",["Date","2022-09-08T00:00:00.000Z"],"https://huggingface.co/blog/train-decision-transformers","https://huggingface.co/blog/assets/101_train-decision-transformers/thumbnail.gif","src/content/posts/2022-09-08-train-your-first-decision-transformer.md","be8da246df004815",{"html":25,"metadata":6165},{"headings":6166,"localImagePaths":6167,"remoteImagePaths":6168,"frontmatter":6169,"imagePaths":6171},[],[],[],{"title":6157,"description":25,"summary":6158,"pubDate":6170,"source":2720,"url":6160,"thumbnail":6161},"Thu, 08 Sep 2022 00:00:00 GMT",[],"2022-09-08-train-your-first-decision-transformer.md","2022-09-12-whats-new-in-diffusers",{"id":6173,"data":6175,"filePath":6181,"digest":6182,"rendered":6183,"legacyId":6191},{"title":6176,"description":25,"summary":6177,"pubDate":6178,"source":2720,"url":6179,"thumbnail":6180},"What's new in Diffusers? 🎨","What's new in Diffusers? 🎨 A month and a half ago we released diffusers , a library that provides a ...",["Date","2022-09-12T00:00:00.000Z"],"https://huggingface.co/blog/diffusers-2nd-month","https://huggingface.co/blog/assets/102_diffusers_2nd_month/inpainting.png","src/content/posts/2022-09-12-whats-new-in-diffusers.md","b013e943916bfca8",{"html":25,"metadata":6184},{"headings":6185,"localImagePaths":6186,"remoteImagePaths":6187,"frontmatter":6188,"imagePaths":6190},[],[],[],{"title":6176,"description":25,"summary":6177,"pubDate":6189,"source":2720,"url":6179,"thumbnail":6180},"Mon, 12 Sep 2022 00:00:00 GMT",[],"2022-09-12-whats-new-in-diffusers.md","2022-09-16-incredibly-fast-bloom-inference-with-deepspeed-and-accelerate",{"id":6192,"data":6194,"filePath":6200,"digest":6201,"rendered":6202,"legacyId":6210},{"title":6195,"description":25,"summary":6196,"pubDate":6197,"source":2720,"url":6198,"thumbnail":6199},"Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate","Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate This article shows how to get an incre...",["Date","2022-09-16T00:00:00.000Z"],"https://huggingface.co/blog/bloom-inference-pytorch-scripts","https://huggingface.co/blog/assets/bloom-inference-pytorch-scripts/thumbnail.png","src/content/posts/2022-09-16-incredibly-fast-bloom-inference-with-deepspeed-and-accelerate.md","7ee3c0f9820e913a",{"html":25,"metadata":6203},{"headings":6204,"localImagePaths":6205,"remoteImagePaths":6206,"frontmatter":6207,"imagePaths":6209},[],[],[],{"title":6195,"description":25,"summary":6196,"pubDate":6208,"source":2720,"url":6198,"thumbnail":6199},"Fri, 16 Sep 2022 00:00:00 GMT",[],"2022-09-16-incredibly-fast-bloom-inference-with-deepspeed-and-accelerate.md","2022-09-21-introducing-whisper",{"id":6211,"data":6213,"filePath":6218,"digest":6219,"rendered":6220,"legacyId":6228},{"title":6214,"description":6215,"summary":6215,"pubDate":6216,"source":19,"url":6217,"thumbnail":21},"Introducing Whisper","We’ve trained and are open-sourcing a neural net called Whisper that approaches human level robustness and accuracy on English speech recognition.",["Date","2022-09-21T07:00:00.000Z"],"https://openai.com/blog/whisper","src/content/posts/2022-09-21-introducing-whisper.md","9c50bfc1ebafb00f",{"html":25,"metadata":6221},{"headings":6222,"localImagePaths":6223,"remoteImagePaths":6224,"frontmatter":6225,"imagePaths":6227},[],[],[],{"title":6214,"description":6215,"summary":6215,"pubDate":6226,"source":19,"url":6217,"thumbnail":21},"Wed, 21 Sep 2022 07:00:00 GMT",[],"2022-09-21-introducing-whisper.md","2022-09-22-ethics-and-society-newsletter-1",{"id":6229,"data":6231,"filePath":6237,"digest":6238,"rendered":6239,"legacyId":6247},{"title":6232,"description":25,"summary":6233,"pubDate":6234,"source":2720,"url":6235,"thumbnail":6236},"Ethics and Society Newsletter #1","Ethics and Society Newsletter #1 Hello, world! Originating as an open-source company, Hugging Face w...",["Date","2022-09-22T00:00:00.000Z"],"https://huggingface.co/blog/ethics-soc-1","https://huggingface.co/blog/assets/103_ethics-soc-1/thumbnail.png","src/content/posts/2022-09-22-ethics-and-society-newsletter-1.md","cac3d7fde51a9c69",{"html":25,"metadata":6240},{"headings":6241,"localImagePaths":6242,"remoteImagePaths":6243,"frontmatter":6244,"imagePaths":6246},[],[],[],{"title":6232,"description":25,"summary":6233,"pubDate":6245,"source":2720,"url":6235,"thumbnail":6236},"Thu, 22 Sep 2022 00:00:00 GMT",[],"2022-09-22-ethics-and-society-newsletter-1.md","2022-09-26-setfit-efficient-few-shot-learning-without-prompts",{"id":6248,"data":6250,"filePath":6256,"digest":6257,"rendered":6258,"legacyId":6266},{"title":6251,"description":25,"summary":6252,"pubDate":6253,"source":2720,"url":6254,"thumbnail":6255},"SetFit: Efficient Few-Shot Learning Without Prompts","SetFit: Efficient Few-Shot Learning Without Prompts SetFit is significantly more sample efficient an...",["Date","2022-09-26T00:00:00.000Z"],"https://huggingface.co/blog/setfit","https://huggingface.co/blog/assets/103_setfit/intel_hf_logo.png","src/content/posts/2022-09-26-setfit-efficient-few-shot-learning-without-prompts.md","093abe80dd720231",{"html":25,"metadata":6259},{"headings":6260,"localImagePaths":6261,"remoteImagePaths":6262,"frontmatter":6263,"imagePaths":6265},[],[],[],{"title":6251,"description":25,"summary":6252,"pubDate":6264,"source":2720,"url":6254,"thumbnail":6255},"Mon, 26 Sep 2022 00:00:00 GMT",[],"2022-09-26-setfit-efficient-few-shot-learning-without-prompts.md","2022-09-27-how-accelerate-runs-very-large-models-thanks-to-pytorch",{"id":6267,"data":6269,"filePath":6275,"digest":6276,"rendered":6277,"legacyId":6285},{"title":6270,"description":25,"summary":6271,"pubDate":6272,"source":2720,"url":6273,"thumbnail":6274},"How 🤗 Accelerate runs very large models thanks to PyTorch","How 🤗 Accelerate runs very large models thanks to PyTorch Load and run large models Meta AI and BigS...",["Date","2022-09-27T00:00:00.000Z"],"https://huggingface.co/blog/accelerate-large-models","https://huggingface.co/blog/assets/104_accelerate-large-models/thumbnail.png","src/content/posts/2022-09-27-how-accelerate-runs-very-large-models-thanks-to-pytorch.md","ccd08274ae10d9a7",{"html":25,"metadata":6278},{"headings":6279,"localImagePaths":6280,"remoteImagePaths":6281,"frontmatter":6282,"imagePaths":6284},[],[],[],{"title":6270,"description":25,"summary":6271,"pubDate":6283,"source":2720,"url":6273,"thumbnail":6274},"Tue, 27 Sep 2022 00:00:00 GMT",[],"2022-09-27-how-accelerate-runs-very-large-models-thanks-to-pytorch.md","2022-09-28-dalle-now-available-without-waitlist",{"id":6286,"data":6288,"filePath":6293,"digest":6294,"rendered":6295,"legacyId":6303},{"title":6289,"description":6290,"summary":6290,"pubDate":6291,"source":19,"url":6292,"thumbnail":21},"DALL·E now available without waitlist","New users can start creating straight away. Lessons learned from deployment and improvements to our safety systems make wider availability possible.",["Date","2022-09-28T07:00:00.000Z"],"https://openai.com/blog/dall-e-now-available-without-waitlist","src/content/posts/2022-09-28-dalle-now-available-without-waitlist.md","4b7151cb85ed7d71",{"html":25,"metadata":6296},{"headings":6297,"localImagePaths":6298,"remoteImagePaths":6299,"frontmatter":6300,"imagePaths":6302},[],[],[],{"title":6289,"description":6290,"summary":6290,"pubDate":6301,"source":19,"url":6292,"thumbnail":21},"Wed, 28 Sep 2022 07:00:00 GMT",[],"2022-09-28-dalle-now-available-without-waitlist.md","2022-09-28-image-classification-with-autotrain",{"id":6304,"data":6306,"filePath":6312,"digest":6313,"rendered":6314,"legacyId":6322},{"title":6307,"description":25,"summary":6308,"pubDate":6309,"source":2720,"url":6310,"thumbnail":6311},"Image Classification with AutoTrain","Image Classification with AutoTrain So you’ve heard all about the cool things that are happening in ...",["Date","2022-09-28T00:00:00.000Z"],"https://huggingface.co/blog/autotrain-image-classification","https://huggingface.co/blog/assets/105_autotrain-image-classification/thumbnail.png","src/content/posts/2022-09-28-image-classification-with-autotrain.md","9938528758310cfb",{"html":25,"metadata":6315},{"headings":6316,"localImagePaths":6317,"remoteImagePaths":6318,"frontmatter":6319,"imagePaths":6321},[],[],[],{"title":6307,"description":25,"summary":6308,"pubDate":6320,"source":2720,"url":6310,"thumbnail":6311},"Wed, 28 Sep 2022 00:00:00 GMT",[],"2022-09-28-image-classification-with-autotrain.md","2022-10-03-very-large-language-models-and-how-to-evaluate-them",{"id":6323,"data":6325,"filePath":6331,"digest":6332,"rendered":6333,"legacyId":6341},{"title":6326,"description":25,"summary":6327,"pubDate":6328,"source":2720,"url":6329,"thumbnail":6330},"Very Large Language Models and How to Evaluate Them","Very Large Language Models and How to Evaluate Them Large language models can now be evaluated on ze...",["Date","2022-10-03T00:00:00.000Z"],"https://huggingface.co/blog/zero-shot-eval-on-the-hub","https://huggingface.co/blog/assets/106_zero_shot_eval_on_the_hub/thumbnail.png","src/content/posts/2022-10-03-very-large-language-models-and-how-to-evaluate-them.md","47dd9ce26f5f5f00",{"html":25,"metadata":6334},{"headings":6335,"localImagePaths":6336,"remoteImagePaths":6337,"frontmatter":6338,"imagePaths":6340},[],[],[],{"title":6326,"description":25,"summary":6327,"pubDate":6339,"source":2720,"url":6329,"thumbnail":6330},"Mon, 03 Oct 2022 00:00:00 GMT",[],"2022-10-03-very-large-language-models-and-how-to-evaluate-them.md","2022-10-05-japanese-stable-diffusion",{"id":6342,"data":6344,"filePath":6350,"digest":6351,"rendered":6352,"legacyId":6360},{"title":6345,"description":25,"summary":6346,"pubDate":6347,"source":2720,"url":6348,"thumbnail":6349},"Japanese Stable Diffusion","Japanese Stable Diffusion Stable Diffusion, developed by CompVis, Stability AI, and LAION, has gener...",["Date","2022-10-05T00:00:00.000Z"],"https://huggingface.co/blog/japanese-stable-diffusion","https://huggingface.co/blog/assets/106_japanese_stable_diffusion/jsd_thumbnail.png","src/content/posts/2022-10-05-japanese-stable-diffusion.md","27499dfd4684a799",{"html":25,"metadata":6353},{"headings":6354,"localImagePaths":6355,"remoteImagePaths":6356,"frontmatter":6357,"imagePaths":6359},[],[],[],{"title":6345,"description":25,"summary":6346,"pubDate":6358,"source":2720,"url":6348,"thumbnail":6349},"Wed, 05 Oct 2022 00:00:00 GMT",[],"2022-10-05-japanese-stable-diffusion.md","2022-10-12-optimization-story-bloom-inference",{"id":6361,"data":6363,"filePath":6368,"digest":6369,"rendered":6370,"legacyId":6378},{"title":6364,"description":25,"summary":6365,"pubDate":6366,"source":2720,"url":6367,"thumbnail":6199},"Optimization story: Bloom inference","Optimization story: Bloom inference This article gives you the behind-the-scenes of how we made an e...",["Date","2022-10-12T00:00:00.000Z"],"https://huggingface.co/blog/bloom-inference-optimization","src/content/posts/2022-10-12-optimization-story-bloom-inference.md","0adcb73fc88dbea3",{"html":25,"metadata":6371},{"headings":6372,"localImagePaths":6373,"remoteImagePaths":6374,"frontmatter":6375,"imagePaths":6377},[],[],[],{"title":6364,"description":25,"summary":6365,"pubDate":6376,"source":2720,"url":6367,"thumbnail":6199},"Wed, 12 Oct 2022 00:00:00 GMT",[],"2022-10-12-optimization-story-bloom-inference.md","2022-10-07-introducing-doi-the-digital-object-identifier-to-datasets-and-models",{"id":6379,"data":6381,"filePath":6387,"digest":6388,"rendered":6389,"legacyId":6397},{"title":6382,"description":25,"summary":6383,"pubDate":6384,"source":2720,"url":6385,"thumbnail":6386},"Introducing DOI: the Digital Object Identifier to Datasets and Models","Introducing DOI: the Digital Object Identifier to Datasets and Models Our mission at Hugging Face is...",["Date","2022-10-07T00:00:00.000Z"],"https://huggingface.co/blog/introducing-doi","https://huggingface.co/blog/assets/107_launching_doi/thumbnail.jpeg","src/content/posts/2022-10-07-introducing-doi-the-digital-object-identifier-to-datasets-and-models.md","f25d0ce08dad3057",{"html":25,"metadata":6390},{"headings":6391,"localImagePaths":6392,"remoteImagePaths":6393,"frontmatter":6394,"imagePaths":6396},[],[],[],{"title":6382,"description":25,"summary":6383,"pubDate":6395,"source":2720,"url":6385,"thumbnail":6386},"Fri, 07 Oct 2022 00:00:00 GMT",[],"2022-10-07-introducing-doi-the-digital-object-identifier-to-datasets-and-models.md","2022-10-13-stable-diffusion-in-jaxflax",{"id":6398,"data":6400,"filePath":6406,"digest":6407,"rendered":6408,"legacyId":6416},{"title":6401,"description":25,"summary":6402,"pubDate":6403,"source":2720,"url":6404,"thumbnail":6405},"Stable Diffusion in JAX/Flax 🚀","🧨 Stable Diffusion in JAX / Flax ! 🤗 Hugging Face Diffusers supports Flax since version 0.5.1 ! This...",["Date","2022-10-13T00:00:00.000Z"],"https://huggingface.co/blog/stable_diffusion_jax","https://huggingface.co/blog/assets/108_stable_diffusion_jax/thumbnail.png","src/content/posts/2022-10-13-stable-diffusion-in-jaxflax.md","9565be259d83b530",{"html":25,"metadata":6409},{"headings":6410,"localImagePaths":6411,"remoteImagePaths":6412,"frontmatter":6413,"imagePaths":6415},[],[],[],{"title":6401,"description":25,"summary":6402,"pubDate":6414,"source":2720,"url":6404,"thumbnail":6405},"Thu, 13 Oct 2022 00:00:00 GMT",[],"2022-10-13-stable-diffusion-in-jaxflax.md","2022-10-13-stopping-malaria-in-its-tracks",{"id":6417,"data":6419,"filePath":6426,"digest":6427,"rendered":6428,"legacyId":6436},{"title":6420,"description":6421,"summary":6421,"pubDate":6422,"source":6423,"url":6424,"thumbnail":6425},"Stopping malaria in its tracks","Developing a vaccine that could save hundreds of thousands of lives",["Date","2022-10-13T15:00:00.000Z"],"DeepMind Blog","https://deepmind.google/discover/blog/stopping-malaria-in-its-tracks/","https://lh3.googleusercontent.com/8EXA5jqukU4EEWDHB9rJG25ir12WetmJlMuErPLe7hJUaGdIjXIA51D-PcxCMjNf9IVu3QxaZRbs4isgJsBsVpaHZjbgK4XM3MCc-8XOgcQ9-sqYWQ=w1200-h630-n-nu","src/content/posts/2022-10-13-stopping-malaria-in-its-tracks.md","0af970dc63dc07ef",{"html":25,"metadata":6429},{"headings":6430,"localImagePaths":6431,"remoteImagePaths":6432,"frontmatter":6433,"imagePaths":6435},[],[],[],{"title":6420,"description":6421,"summary":6421,"pubDate":6434,"source":6423,"url":6424,"thumbnail":6425},"Thu, 13 Oct 2022 15:00:00 +0000",[],"2022-10-13-stopping-malaria-in-its-tracks.md","2022-10-14-getting-started-with-hugging-face-inference-endpoints",{"id":6437,"data":6439,"filePath":6445,"digest":6446,"rendered":6447,"legacyId":6455},{"title":6440,"description":25,"summary":6441,"pubDate":6442,"source":2720,"url":6443,"thumbnail":6444},"Getting started with Hugging Face Inference Endpoints","Getting Started with Hugging Face Inference Endpoints Training machine learning models has become qu...",["Date","2022-10-14T00:00:00.000Z"],"https://huggingface.co/blog/inference-endpoints","https://huggingface.co/blog/assets/109_inference_endpoints/endpoints05.png","src/content/posts/2022-10-14-getting-started-with-hugging-face-inference-endpoints.md","6b976deaa5eba467",{"html":25,"metadata":6448},{"headings":6449,"localImagePaths":6450,"remoteImagePaths":6451,"frontmatter":6452,"imagePaths":6454},[],[],[],{"title":6440,"description":25,"summary":6441,"pubDate":6453,"source":2720,"url":6443,"thumbnail":6444},"Fri, 14 Oct 2022 00:00:00 GMT",[],"2022-10-14-getting-started-with-hugging-face-inference-endpoints.md","2022-10-19-mteb-massive-text-embedding-benchmark",{"id":6456,"data":6458,"filePath":6464,"digest":6465,"rendered":6466,"legacyId":6474},{"title":6459,"description":25,"summary":6460,"pubDate":6461,"source":2720,"url":6462,"thumbnail":6463},"MTEB: Massive Text Embedding Benchmark","MTEB: Massive Text Embedding Benchmark MTEB is a massive benchmark for measuring the performance of ...",["Date","2022-10-19T00:00:00.000Z"],"https://huggingface.co/blog/mteb","https://huggingface.co/blog/assets/110_mteb/thumbnail.png","src/content/posts/2022-10-19-mteb-massive-text-embedding-benchmark.md","66b9c8d0558c0858",{"html":25,"metadata":6467},{"headings":6468,"localImagePaths":6469,"remoteImagePaths":6470,"frontmatter":6471,"imagePaths":6473},[],[],[],{"title":6459,"description":25,"summary":6460,"pubDate":6472,"source":2720,"url":6462,"thumbnail":6463},"Wed, 19 Oct 2022 00:00:00 GMT",[],"2022-10-19-mteb-massive-text-embedding-benchmark.md","2022-10-19-scaling-laws-for-reward-model-overoptimization",{"id":6475,"data":6477,"filePath":6481,"digest":6482,"rendered":6483,"legacyId":6491},{"title":6478,"description":25,"summary":25,"pubDate":6479,"source":19,"url":6480,"thumbnail":21},"Scaling laws for reward model overoptimization",["Date","2022-10-19T07:00:00.000Z"],"https://openai.com/blog/scaling-laws-for-reward-model-overoptimization","src/content/posts/2022-10-19-scaling-laws-for-reward-model-overoptimization.md","d6e7bb76a48e0e50",{"html":25,"metadata":6484},{"headings":6485,"localImagePaths":6486,"remoteImagePaths":6487,"frontmatter":6488,"imagePaths":6490},[],[],[],{"title":6478,"description":25,"summary":25,"pubDate":6489,"source":19,"url":6480,"thumbnail":21},"Wed, 19 Oct 2022 07:00:00 GMT",[],"2022-10-19-scaling-laws-for-reward-model-overoptimization.md","2022-10-21-from-pytorch-ddp-to-accelerate-to-trainer-mastery-of-distributed-training-with-ease",{"id":6492,"data":6494,"filePath":6500,"digest":6501,"rendered":6502,"legacyId":6510},{"title":6495,"description":25,"summary":6496,"pubDate":6497,"source":2720,"url":6498,"thumbnail":6499},"From PyTorch DDP to 🤗 Accelerate to 🤗 Trainer, mastery of distributed training with ease","From PyTorch DDP to Accelerate to Trainer, mastery of distributed training with ease General Overvie...",["Date","2022-10-21T00:00:00.000Z"],"https://huggingface.co/blog/pytorch-ddp-accelerate-transformers","https://huggingface.co/blog/assets/111_pytorch_ddp_accelerate_transformers/thumbnail.png","src/content/posts/2022-10-21-from-pytorch-ddp-to-accelerate-to-trainer-mastery-of-distributed-training-with-ease.md","863e28c7cfccaa87",{"html":25,"metadata":6503},{"headings":6504,"localImagePaths":6505,"remoteImagePaths":6506,"frontmatter":6507,"imagePaths":6509},[],[],[],{"title":6495,"description":25,"summary":6496,"pubDate":6508,"source":2720,"url":6498,"thumbnail":6499},"Fri, 21 Oct 2022 00:00:00 GMT",[],"2022-10-21-from-pytorch-ddp-to-accelerate-to-trainer-mastery-of-distributed-training-with-ease.md","2022-10-24-evaluating-language-model-bias-with-evaluate",{"id":6511,"data":6513,"filePath":6519,"digest":6520,"rendered":6521,"legacyId":6529},{"title":6514,"description":25,"summary":6515,"pubDate":6516,"source":2720,"url":6517,"thumbnail":6518},"Evaluating Language Model Bias with 🤗 Evaluate","Evaluating Language Model Bias with 🤗 Evaluate While the size and capabilities of large language mod...",["Date","2022-10-24T00:00:00.000Z"],"https://huggingface.co/blog/evaluating-llm-bias","https://huggingface.co/blog/assets/112_evaluating-llm-bias/thumbnail.png","src/content/posts/2022-10-24-evaluating-language-model-bias-with-evaluate.md","911aec5b601860f4",{"html":25,"metadata":6522},{"headings":6523,"localImagePaths":6524,"remoteImagePaths":6525,"frontmatter":6526,"imagePaths":6528},[],[],[],{"title":6514,"description":25,"summary":6515,"pubDate":6527,"source":2720,"url":6517,"thumbnail":6518},"Mon, 24 Oct 2022 00:00:00 GMT",[],"2022-10-24-evaluating-language-model-bias-with-evaluate.md","2022-11-02-accelerate-your-models-with-optimum-intel-and-openvino",{"id":6530,"data":6532,"filePath":6538,"digest":6539,"rendered":6540,"legacyId":6548},{"title":6533,"description":25,"summary":6534,"pubDate":6535,"source":2720,"url":6536,"thumbnail":6537},"Accelerate your models with 🤗 Optimum Intel and OpenVINO","Accelerate your models with 🤗 Optimum Intel and OpenVINO Last July, we announced that Intel and Hugg...",["Date","2022-11-02T00:00:00.000Z"],"https://huggingface.co/blog/openvino","https://huggingface.co/blog/assets/113_openvino/thumbnail.png","src/content/posts/2022-11-02-accelerate-your-models-with-optimum-intel-and-openvino.md","b29da7aed4e5ef98",{"html":25,"metadata":6541},{"headings":6542,"localImagePaths":6543,"remoteImagePaths":6544,"frontmatter":6545,"imagePaths":6547},[],[],[],{"title":6533,"description":25,"summary":6534,"pubDate":6546,"source":2720,"url":6536,"thumbnail":6537},"Wed, 02 Nov 2022 00:00:00 GMT",[],"2022-11-02-accelerate-your-models-with-optimum-intel-and-openvino.md","2022-11-03-dalle-api-now-available-in-public-beta",{"id":6549,"data":6551,"filePath":6556,"digest":6557,"rendered":6558,"legacyId":6566},{"title":6552,"description":6553,"summary":6553,"pubDate":6554,"source":19,"url":6555,"thumbnail":21},"DALL·E API now available in public beta","Starting today, developers can begin building apps with the DALL·E API.",["Date","2022-11-03T07:00:00.000Z"],"https://openai.com/blog/dall-e-api-now-available-in-public-beta","src/content/posts/2022-11-03-dalle-api-now-available-in-public-beta.md","f016d584728b70dc",{"html":25,"metadata":6559},{"headings":6560,"localImagePaths":6561,"remoteImagePaths":6562,"frontmatter":6563,"imagePaths":6565},[],[],[],{"title":6552,"description":6553,"summary":6553,"pubDate":6564,"source":19,"url":6555,"thumbnail":21},"Thu, 03 Nov 2022 07:00:00 GMT",[],"2022-11-03-dalle-api-now-available-in-public-beta.md","2022-11-03-fine-tune-whisper-with-transformers",{"id":6567,"data":6569,"filePath":6575,"digest":6576,"rendered":6577,"legacyId":6585},{"title":6570,"description":25,"summary":6571,"pubDate":6572,"source":2720,"url":6573,"thumbnail":6574},"Fine-Tune Whisper with 🤗 Transformers","Fine-Tune Whisper For Multilingual ASR with 🤗 Transformers In this blog, we present a step-by-step g...",["Date","2022-11-03T00:00:00.000Z"],"https://huggingface.co/blog/fine-tune-whisper","https://huggingface.co/blog/assets/111_fine_tune_whisper/thumbnail.jpg","src/content/posts/2022-11-03-fine-tune-whisper-with-transformers.md","36c9169dca9041d0",{"html":25,"metadata":6578},{"headings":6579,"localImagePaths":6580,"remoteImagePaths":6581,"frontmatter":6582,"imagePaths":6584},[],[],[],{"title":6570,"description":25,"summary":6571,"pubDate":6583,"source":2720,"url":6573,"thumbnail":6574},"Thu, 03 Nov 2022 00:00:00 GMT",[],"2022-11-03-fine-tune-whisper-with-transformers.md","2022-11-07-training-stable-diffusion-with-dreambooth-using-diffusers",{"id":6586,"data":6588,"filePath":6594,"digest":6595,"rendered":6596,"legacyId":6604},{"title":6589,"description":25,"summary":6590,"pubDate":6591,"source":2720,"url":6592,"thumbnail":6593},"Training Stable Diffusion with Dreambooth using 🧨 Diffusers","Training Stable Diffusion with Dreambooth using 🧨 Diffusers Dreambooth is a technique to teach new c...",["Date","2022-11-07T00:00:00.000Z"],"https://huggingface.co/blog/dreambooth","https://huggingface.co/blog/assets/sd_dreambooth_training/thumbnail.jpg","src/content/posts/2022-11-07-training-stable-diffusion-with-dreambooth-using-diffusers.md","f3529e9d7cafc700",{"html":25,"metadata":6597},{"headings":6598,"localImagePaths":6599,"remoteImagePaths":6600,"frontmatter":6601,"imagePaths":6603},[],[],[],{"title":6589,"description":25,"summary":6590,"pubDate":6602,"source":2720,"url":6592,"thumbnail":6593},"Mon, 07 Nov 2022 00:00:00 GMT",[],"2022-11-07-training-stable-diffusion-with-dreambooth-using-diffusers.md","2022-11-08-generating-human-level-text-with-contrastive-search-in-transformers",{"id":6605,"data":6607,"filePath":6613,"digest":6614,"rendered":6615,"legacyId":6623},{"title":6608,"description":25,"summary":6609,"pubDate":6610,"source":2720,"url":6611,"thumbnail":6612},"Generating Human-level Text with Contrastive Search in Transformers 🤗","Generating Human-level Text with Contrastive Search in Transformers 🤗 1. Introduction: Natural langu...",["Date","2022-11-08T00:00:00.000Z"],"https://huggingface.co/blog/introducing-csearch","https://huggingface.co/blog/assets/115_introducing_contrastive_search/thumbnail.png","src/content/posts/2022-11-08-generating-human-level-text-with-contrastive-search-in-transformers.md","5350a6959492aec4",{"html":25,"metadata":6616},{"headings":6617,"localImagePaths":6618,"remoteImagePaths":6619,"frontmatter":6620,"imagePaths":6622},[],[],[],{"title":6608,"description":25,"summary":6609,"pubDate":6621,"source":2720,"url":6611,"thumbnail":6612},"Tue, 08 Nov 2022 00:00:00 GMT",[],"2022-11-08-generating-human-level-text-with-contrastive-search-in-transformers.md","2022-11-08-introducing-our-new-pricing",{"id":6624,"data":6626,"filePath":6632,"digest":6633,"rendered":6634,"legacyId":6641},{"title":6627,"description":25,"summary":6628,"pubDate":6629,"source":2720,"url":6630,"thumbnail":6631},"Introducing our new pricing","Introducing our new pricing As you might have noticed, our pricing page has changed a lot recently. ...",["Date","2022-11-08T00:00:00.000Z"],"https://huggingface.co/blog/pricing-update","https://huggingface.co/blog/assets/114_pricing-update/thumbnail.png","src/content/posts/2022-11-08-introducing-our-new-pricing.md","a5c4cec58d646517",{"html":25,"metadata":6635},{"headings":6636,"localImagePaths":6637,"remoteImagePaths":6638,"frontmatter":6639,"imagePaths":6640},[],[],[],{"title":6627,"description":25,"summary":6628,"pubDate":6621,"source":2720,"url":6630,"thumbnail":6631},[],"2022-11-08-introducing-our-new-pricing.md","2022-11-16-best-practices-for-data-enrichment",{"id":6642,"data":6644,"filePath":6650,"digest":6651,"rendered":6652,"legacyId":6660},{"title":6645,"description":6646,"summary":6646,"pubDate":6647,"source":6423,"url":6648,"thumbnail":6649},"Best practices for data enrichment","Building a responsible approach to data collection with the Partnership on AI...",["Date","2022-11-16T00:00:00.000Z"],"https://deepmind.google/discover/blog/best-practices-for-data-enrichment/","https://lh3.googleusercontent.com/nYvgmCtFKpHCzKOzqvlm-YK_l5qbPvz570PQbWv2ZxKVIoZxraa7euQLCY65a7ecdDRzBQtbQY2jxAoYKO8PC90snL6QvwNAzhp5-8x31cL5cJV-_OY=w1200-h630-n-nu","src/content/posts/2022-11-16-best-practices-for-data-enrichment.md","e5036b295e2fb115",{"html":25,"metadata":6653},{"headings":6654,"localImagePaths":6655,"remoteImagePaths":6656,"frontmatter":6657,"imagePaths":6659},[],[],[],{"title":6645,"description":6646,"summary":6646,"pubDate":6658,"source":6423,"url":6648,"thumbnail":6649},"Wed, 16 Nov 2022 00:00:00 +0000",[],"2022-11-16-best-practices-for-data-enrichment.md","2022-11-17-hugging-face-machine-learning-demos-on-arxiv",{"id":6661,"data":6663,"filePath":6669,"digest":6670,"rendered":6671,"legacyId":6679},{"title":6664,"description":25,"summary":6665,"pubDate":6666,"source":2720,"url":6667,"thumbnail":6668},"Hugging Face Machine Learning Demos on arXiv","Hugging Face Machine Learning Demos on arXiv We’re very excited to announce that Hugging Face has co...",["Date","2022-11-17T00:00:00.000Z"],"https://huggingface.co/blog/arxiv","https://huggingface.co/blog/assets/arxiv/thumbnail.png","src/content/posts/2022-11-17-hugging-face-machine-learning-demos-on-arxiv.md","69bd10aef02ac446",{"html":25,"metadata":6672},{"headings":6673,"localImagePaths":6674,"remoteImagePaths":6675,"frontmatter":6676,"imagePaths":6678},[],[],[],{"title":6664,"description":25,"summary":6665,"pubDate":6677,"source":2720,"url":6667,"thumbnail":6668},"Thu, 17 Nov 2022 00:00:00 GMT",[],"2022-11-17-hugging-face-machine-learning-demos-on-arxiv.md","2022-11-17-sentiment-classification-with-fully-homomorphic-encryption-using-concrete-ml",{"id":6680,"data":6682,"filePath":6688,"digest":6689,"rendered":6690,"legacyId":6697},{"title":6683,"description":25,"summary":6684,"pubDate":6685,"source":2720,"url":6686,"thumbnail":6687},"Sentiment Classification with Fully Homomorphic Encryption using Concrete ML","Sentiment Analysis on Encrypted Data with Homomorphic Encryption It is well-known that a sentiment a...",["Date","2022-11-17T00:00:00.000Z"],"https://huggingface.co/blog/sentiment-analysis-fhe","https://huggingface.co/blog/assets/sentiment-analysis-fhe/thumbnail.png","src/content/posts/2022-11-17-sentiment-classification-with-fully-homomorphic-encryption-using-concrete-ml.md","12950eeaf0e1c99d",{"html":25,"metadata":6691},{"headings":6692,"localImagePaths":6693,"remoteImagePaths":6694,"frontmatter":6695,"imagePaths":6696},[],[],[],{"title":6683,"description":25,"summary":6684,"pubDate":6677,"source":2720,"url":6686,"thumbnail":6687},[],"2022-11-17-sentiment-classification-with-fully-homomorphic-encryption-using-concrete-ml.md","2022-11-21-accelerating-document-ai",{"id":6698,"data":6700,"filePath":6706,"digest":6707,"rendered":6708,"legacyId":6716},{"title":6701,"description":25,"summary":6702,"pubDate":6703,"source":2720,"url":6704,"thumbnail":6705},"Accelerating Document AI","Accelerating Document AI Enterprises are full of documents containing knowledge that isn't accessibl...",["Date","2022-11-21T00:00:00.000Z"],"https://huggingface.co/blog/document-ai","https://huggingface.co/blog/assets/112_document-ai/thumbnail.png","src/content/posts/2022-11-21-accelerating-document-ai.md","da970138a89b9495",{"html":25,"metadata":6709},{"headings":6710,"localImagePaths":6711,"remoteImagePaths":6712,"frontmatter":6713,"imagePaths":6715},[],[],[],{"title":6701,"description":25,"summary":6702,"pubDate":6714,"source":2720,"url":6704,"thumbnail":6705},"Mon, 21 Nov 2022 00:00:00 GMT",[],"2022-11-21-accelerating-document-ai.md","2022-11-21-an-overview-of-inference-solutions-on-hugging-face",{"id":6717,"data":6719,"filePath":6725,"digest":6726,"rendered":6727,"legacyId":6734},{"title":6720,"description":25,"summary":6721,"pubDate":6722,"source":2720,"url":6723,"thumbnail":6724},"An Overview of Inference Solutions on Hugging Face","An Overview of Inference Solutions on Hugging Face Every day, developers and organizations are adopt...",["Date","2022-11-21T00:00:00.000Z"],"https://huggingface.co/blog/inference-update","https://huggingface.co/blog/assets/116_inference_update/widget.png","src/content/posts/2022-11-21-an-overview-of-inference-solutions-on-hugging-face.md","8ddb8cd10fe65da1",{"html":25,"metadata":6728},{"headings":6729,"localImagePaths":6730,"remoteImagePaths":6731,"frontmatter":6732,"imagePaths":6733},[],[],[],{"title":6720,"description":25,"summary":6721,"pubDate":6714,"source":2720,"url":6723,"thumbnail":6724},[],"2022-11-21-an-overview-of-inference-solutions-on-hugging-face.md","2022-11-22-benchmarking-the-next-generation-of-never-ending-learners",{"id":6735,"data":6737,"filePath":6743,"digest":6744,"rendered":6745,"legacyId":6753},{"title":6738,"description":6739,"summary":6739,"pubDate":6740,"source":6423,"url":6741,"thumbnail":6742},"Benchmarking the next generation of never-ending learners","Learning how to build upon knowledge by tapping 30 years of computer vision research",["Date","2022-11-22T00:00:00.000Z"],"https://deepmind.google/discover/blog/benchmarking-the-next-generation-of-never-ending-learners/","https://lh3.googleusercontent.com/VEIJiplOab4catyNZs6QjZxwjbqVmrh2fIZF8Gj7Xd7TQRq1q4bqDmbeSuVzHPzDhC8vKYI5nZLft79VWP5Oi7j_ARAzyFVxMdJIMKxDD5VfRpGm=w1200-h630-n-nu","src/content/posts/2022-11-22-benchmarking-the-next-generation-of-never-ending-learners.md","eb084563cd7a2475",{"html":25,"metadata":6746},{"headings":6747,"localImagePaths":6748,"remoteImagePaths":6749,"frontmatter":6750,"imagePaths":6752},[],[],[],{"title":6738,"description":6739,"summary":6739,"pubDate":6751,"source":6423,"url":6741,"thumbnail":6742},"Tue, 22 Nov 2022 00:00:00 +0000",[],"2022-11-22-benchmarking-the-next-generation-of-never-ending-learners.md","2022-11-23-building-interactive-agents-in-video-game-worlds",{"id":6754,"data":6756,"filePath":6762,"digest":6763,"rendered":6764,"legacyId":6772},{"title":6757,"description":6758,"summary":6758,"pubDate":6759,"source":6423,"url":6760,"thumbnail":6761},"Building interactive agents in video game worlds","Most artificial intelligence (AI) researchers now believe that writing computer code which can capture the nuances of situated interactions is impossible. Alternatively, modern machine learning (ML) researchers have focused on learning about these types of interactions from data. To explore these learning-based approaches and quickly build agents that can make sense of human instructions and safely perform actions in open-ended conditions, we created a research framework within a video game environment.Today, we’re publishing a paper [INSERT LINK] and collection of videos, showing our early steps in building video game AIs that can understand fuzzy human concepts – and therefore, can begin to interact with people on their own terms.",["Date","2022-11-23T00:00:00.000Z"],"https://deepmind.google/discover/blog/building-interactive-agents-in-video-game-worlds/","https://lh3.googleusercontent.com/6DSrkFaInWqKD1eN4IJJN31ZRa3LW447A1ZYoK19FDzJGSLD5dlVw1rJRf52O_dmQUDq11XqYsiqMR8uFDnWLWGkl8xFY5KXYxD7LvQNPvTEuR_h=w1200-h630-n-nu","src/content/posts/2022-11-23-building-interactive-agents-in-video-game-worlds.md","f0279005b7be1d4a",{"html":25,"metadata":6765},{"headings":6766,"localImagePaths":6767,"remoteImagePaths":6768,"frontmatter":6769,"imagePaths":6771},[],[],[],{"title":6757,"description":6758,"summary":6758,"pubDate":6770,"source":6423,"url":6760,"thumbnail":6761},"Wed, 23 Nov 2022 00:00:00 +0000",[],"2022-11-23-building-interactive-agents-in-video-game-worlds.md","2022-11-23-director-of-machine-learning-insights-part-4",{"id":6773,"data":6775,"filePath":6781,"digest":6782,"rendered":6783,"legacyId":6791},{"title":6776,"description":25,"summary":6777,"pubDate":6778,"source":2720,"url":6779,"thumbnail":6780},"Director of Machine Learning Insights [Part 4]","Director of Machine Learning Insights [Part 4] If you're interested in building ML solutions faster ...",["Date","2022-11-23T00:00:00.000Z"],"https://huggingface.co/blog/ml-director-insights-4","https://huggingface.co/blog/assets/78_ml_director_insights/part4.png","src/content/posts/2022-11-23-director-of-machine-learning-insights-part-4.md","52b1359f4f207e1e",{"html":25,"metadata":6784},{"headings":6785,"localImagePaths":6786,"remoteImagePaths":6787,"frontmatter":6788,"imagePaths":6790},[],[],[],{"title":6776,"description":25,"summary":6777,"pubDate":6789,"source":2720,"url":6779,"thumbnail":6780},"Wed, 23 Nov 2022 00:00:00 GMT",[],"2022-11-23-director-of-machine-learning-insights-part-4.md","2022-11-25-deepminds-latest-research-at-neurips-2022",{"id":6792,"data":6794,"filePath":6800,"digest":6801,"rendered":6802,"legacyId":6810},{"title":6795,"description":6796,"summary":6796,"pubDate":6797,"source":6423,"url":6798,"thumbnail":6799},"DeepMind’s latest research at NeurIPS 2022","NeurIPS is the world’s largest conference in artificial intelligence (AI) and machine learning (ML), and we’re proud to support the event as Diamond sponsors, helping foster the exchange of research advances in the AI and ML community. Teams from across DeepMind are presenting 47 papers, including 35 external collaborations in virtual panels and poster sessions.",["Date","2022-11-25T00:00:00.000Z"],"https://deepmind.google/discover/blog/deepminds-latest-research-at-neurips-2022/","https://lh3.googleusercontent.com/MFZKdGWHOzJ6nM8NufhIfpts0R-v9D4jQqnC416FT8ArwmNC2Ztke2S50WVtUhO0g1u8AGmYEyWMDC7LO0a16ydHBMei9GmJO4NjykhpLKw1TVtd4Mg=w1200-h630-n-nu","src/content/posts/2022-11-25-deepminds-latest-research-at-neurips-2022.md","591e1c4b1a28c9e8",{"html":25,"metadata":6803},{"headings":6804,"localImagePaths":6805,"remoteImagePaths":6806,"frontmatter":6807,"imagePaths":6809},[],[],[],{"title":6795,"description":6796,"summary":6796,"pubDate":6808,"source":6423,"url":6798,"thumbnail":6799},"Fri, 25 Nov 2022 00:00:00 +0000",[],"2022-11-25-deepminds-latest-research-at-neurips-2022.md","2022-11-25-diffusion-models-live-event",{"id":6811,"data":6813,"filePath":6819,"digest":6820,"rendered":6821,"legacyId":6829},{"title":6814,"description":25,"summary":6815,"pubDate":6816,"source":2720,"url":6817,"thumbnail":6818},"Diffusion Models Live Event","Diffusion Models Live Event We are excited to share that the Diffusion Models Class with Hugging Fac...",["Date","2022-11-25T00:00:00.000Z"],"https://huggingface.co/blog/diffusion-models-event","https://huggingface.co/blog/assets/diffusion-models-event/thumbnail.png","src/content/posts/2022-11-25-diffusion-models-live-event.md","c9f5ad9b2461d601",{"html":25,"metadata":6822},{"headings":6823,"localImagePaths":6824,"remoteImagePaths":6825,"frontmatter":6826,"imagePaths":6828},[],[],[],{"title":6814,"description":25,"summary":6815,"pubDate":6827,"source":2720,"url":6817,"thumbnail":6818},"Fri, 25 Nov 2022 00:00:00 GMT",[],"2022-11-25-diffusion-models-live-event.md","2022-11-30-introducing-chatgpt",{"id":6830,"data":6832,"filePath":6837,"digest":6838,"rendered":6839,"legacyId":6847},{"title":6833,"description":6834,"summary":6834,"pubDate":6835,"source":19,"url":6836,"thumbnail":21},"Introducing ChatGPT","We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.",["Date","2022-11-30T08:00:00.000Z"],"https://openai.com/blog/chatgpt","src/content/posts/2022-11-30-introducing-chatgpt.md","530842b32b88e14a",{"html":25,"metadata":6840},{"headings":6841,"localImagePaths":6842,"remoteImagePaths":6843,"frontmatter":6844,"imagePaths":6846},[],[],[],{"title":6833,"description":6834,"summary":6834,"pubDate":6845,"source":19,"url":6836,"thumbnail":21},"Wed, 30 Nov 2022 08:00:00 GMT",[],"2022-11-30-introducing-chatgpt.md","2022-11-29-we-are-hiring-interns",{"id":6848,"data":6850,"filePath":6856,"digest":6857,"rendered":6858,"legacyId":6866},{"title":6851,"description":25,"summary":6852,"pubDate":6853,"source":2720,"url":6854,"thumbnail":6855},"We are hiring interns!","We are hiring interns! Want to help build the future at -- if we may say so ourselves -- one of the ...",["Date","2022-11-29T00:00:00.000Z"],"https://huggingface.co/blog/interns-2023","https://huggingface.co/blog/assets/interns-2023/thumbnail.png","src/content/posts/2022-11-29-we-are-hiring-interns.md","7987da1890768aca",{"html":25,"metadata":6859},{"headings":6860,"localImagePaths":6861,"remoteImagePaths":6862,"frontmatter":6863,"imagePaths":6865},[],[],[],{"title":6851,"description":25,"summary":6852,"pubDate":6864,"source":2720,"url":6854,"thumbnail":6855},"Tue, 29 Nov 2022 00:00:00 GMT",[],"2022-11-29-we-are-hiring-interns.md","2022-11-30-vq-diffusion-with-diffusers",{"id":6867,"data":6869,"filePath":6875,"digest":6876,"rendered":6877,"legacyId":6885},{"title":6870,"description":25,"summary":6871,"pubDate":6872,"source":2720,"url":6873,"thumbnail":6874},"VQ Diffusion with 🧨 Diffusers","VQ-Diffusion Vector Quantized Diffusion (VQ-Diffusion) is a conditional latent diffusion model devel...",["Date","2022-11-30T00:00:00.000Z"],"https://huggingface.co/blog/vq-diffusion","https://huggingface.co/blog/assets/117_vq_diffusion/thumbnail.png","src/content/posts/2022-11-30-vq-diffusion-with-diffusers.md","aadd5250d05a3957",{"html":25,"metadata":6878},{"headings":6879,"localImagePaths":6880,"remoteImagePaths":6881,"frontmatter":6882,"imagePaths":6884},[],[],[],{"title":6870,"description":25,"summary":6871,"pubDate":6883,"source":2720,"url":6873,"thumbnail":6874},"Wed, 30 Nov 2022 00:00:00 GMT",[],"2022-11-30-vq-diffusion-with-diffusers.md","2022-12-01-mastering-stratego-the-classic-game-of-imperfect-information",{"id":6886,"data":6888,"filePath":6894,"digest":6895,"rendered":6896,"legacyId":6904},{"title":6889,"description":6890,"summary":6890,"pubDate":6891,"source":6423,"url":6892,"thumbnail":6893},"Mastering Stratego, the classic game of imperfect information","Game-playing artificial intelligence (AI) systems have advanced to a new frontier.",["Date","2022-12-01T00:00:00.000Z"],"https://deepmind.google/discover/blog/mastering-stratego-the-classic-game-of-imperfect-information/","https://lh3.googleusercontent.com/nvWTaah_1s2OEAt4CsxX5gKok_0V6-Q5eH3aW3GF6YyZdEVM0OBdgFxNa4DAbmUCXpvTqTfslfUB7_3ZBYr6kIQuk2u46khXH41IU16EZghstwt72Mk=w1200-h630-n-nu","src/content/posts/2022-12-01-mastering-stratego-the-classic-game-of-imperfect-information.md","ce8f28744c8ccc3d",{"html":25,"metadata":6897},{"headings":6898,"localImagePaths":6899,"remoteImagePaths":6900,"frontmatter":6901,"imagePaths":6903},[],[],[],{"title":6889,"description":6890,"summary":6890,"pubDate":6902,"source":6423,"url":6892,"thumbnail":6893},"Thu, 01 Dec 2022 00:00:00 +0000",[],"2022-12-01-mastering-stratego-the-classic-game-of-imperfect-information.md","2022-12-01-probabilistic-time-series-forecasting-with-transformers",{"id":6905,"data":6907,"filePath":6913,"digest":6914,"rendered":6915,"legacyId":6923},{"title":6908,"description":25,"summary":6909,"pubDate":6910,"source":2720,"url":6911,"thumbnail":6912},"Probabilistic Time Series Forecasting with 🤗 Transformers","Probabilistic Time Series Forecasting with 🤗 Transformers Introduction Time series forecasting is an...",["Date","2022-12-01T00:00:00.000Z"],"https://huggingface.co/blog/time-series-transformers","https://huggingface.co/blog/assets/118_time-series-transformers/thumbnail.png","src/content/posts/2022-12-01-probabilistic-time-series-forecasting-with-transformers.md","a35a8179c22762b0",{"html":25,"metadata":6916},{"headings":6917,"localImagePaths":6918,"remoteImagePaths":6919,"frontmatter":6920,"imagePaths":6922},[],[],[],{"title":6908,"description":25,"summary":6909,"pubDate":6921,"source":2720,"url":6911,"thumbnail":6912},"Thu, 01 Dec 2022 00:00:00 GMT",[],"2022-12-01-probabilistic-time-series-forecasting-with-transformers.md","2022-12-01-using-stable-diffusion-with-core-ml-on-apple-silicon",{"id":6924,"data":6926,"filePath":6932,"digest":6933,"rendered":6934,"legacyId":6941},{"title":6927,"description":25,"summary":6928,"pubDate":6929,"source":2720,"url":6930,"thumbnail":6931},"Using Stable Diffusion with Core ML on Apple Silicon","Using Stable Diffusion with Core ML on Apple Silicon Thanks to Apple engineers, you can now run Stab...",["Date","2022-12-01T00:00:00.000Z"],"https://huggingface.co/blog/diffusers-coreml","https://huggingface.co/blog/assets/diffusers_coreml/thumbnail.png","src/content/posts/2022-12-01-using-stable-diffusion-with-core-ml-on-apple-silicon.md","ad54f730c2de955b",{"html":25,"metadata":6935},{"headings":6936,"localImagePaths":6937,"remoteImagePaths":6938,"frontmatter":6939,"imagePaths":6940},[],[],[],{"title":6927,"description":25,"summary":6928,"pubDate":6921,"source":2720,"url":6930,"thumbnail":6931},[],"2022-12-01-using-stable-diffusion-with-core-ml-on-apple-silicon.md","2022-12-02-deep-learning-with-proteins",{"id":6942,"data":6944,"filePath":6950,"digest":6951,"rendered":6952,"legacyId":6960},{"title":6945,"description":25,"summary":6946,"pubDate":6947,"source":2720,"url":6948,"thumbnail":6949},"Deep Learning with Proteins","Deep Learning With Proteins I have two audiences in mind while writing this. One is biologists who a...",["Date","2022-12-02T00:00:00.000Z"],"https://huggingface.co/blog/deep-learning-with-proteins","https://huggingface.co/blog/assets/119_deep_learning_with_proteins/folding_example.png","src/content/posts/2022-12-02-deep-learning-with-proteins.md","ed69fc112ffe957a",{"html":25,"metadata":6953},{"headings":6954,"localImagePaths":6955,"remoteImagePaths":6956,"frontmatter":6957,"imagePaths":6959},[],[],[],{"title":6945,"description":25,"summary":6946,"pubDate":6958,"source":2720,"url":6948,"thumbnail":6949},"Fri, 02 Dec 2022 00:00:00 GMT",[],"2022-12-02-deep-learning-with-proteins.md","2022-12-06-ai-for-the-board-game-diplomacy",{"id":6961,"data":6963,"filePath":6968,"digest":6969,"rendered":6970,"legacyId":6978},{"title":6964,"description":6965,"summary":6965,"pubDate":6966,"source":6423,"url":6967,"thumbnail":6742},"AI for the board game Diplomacy","Successful communication and cooperation have been crucial for helping societies advance throughout history. The closed environments of board games can serve as a sandbox for modelling and investigating interaction and communication – and we can learn a lot from playing them. In our recent paper, published today in Nature Communications, we show how artificial agents can use communication to better cooperate in the board game Diplomacy, a vibrant domain in artificial intelligence (AI) research, known for its focus on alliance building.",["Date","2022-12-06T00:00:00.000Z"],"https://deepmind.google/discover/blog/ai-for-the-board-game-diplomacy/","src/content/posts/2022-12-06-ai-for-the-board-game-diplomacy.md","89d8230be24ee949",{"html":25,"metadata":6971},{"headings":6972,"localImagePaths":6973,"remoteImagePaths":6974,"frontmatter":6975,"imagePaths":6977},[],[],[],{"title":6964,"description":6965,"summary":6965,"pubDate":6976,"source":6423,"url":6967,"thumbnail":6742},"Tue, 06 Dec 2022 00:00:00 +0000",[],"2022-12-06-ai-for-the-board-game-diplomacy.md","2022-12-08-competitive-programming-with-alphacode",{"id":6979,"data":6981,"filePath":6987,"digest":6988,"rendered":6989,"legacyId":6997},{"title":6982,"description":6983,"summary":6983,"pubDate":6984,"source":6423,"url":6985,"thumbnail":6986},"Competitive programming with AlphaCode","Solving novel problems and setting a new milestone in competitive programming.",["Date","2022-12-08T00:00:00.000Z"],"https://deepmind.google/discover/blog/competitive-programming-with-alphacode/","https://lh3.googleusercontent.com/vQ0Ow6LwCpigfPyTGUhXEfdMBWPyHmaCo7eoQW7bv3QoZXW6EIj18FPiCLI1vlMYlUAOvEXta1KSkl8P2KScquYJb-Dm_QygP9kdlLYkpF4nVyEH=w1200-h630-n-nu","src/content/posts/2022-12-08-competitive-programming-with-alphacode.md","5f764feb671e7a6b",{"html":25,"metadata":6990},{"headings":6991,"localImagePaths":6992,"remoteImagePaths":6993,"frontmatter":6994,"imagePaths":6996},[],[],[],{"title":6982,"description":6983,"summary":6983,"pubDate":6995,"source":6423,"url":6985,"thumbnail":6986},"Thu, 08 Dec 2022 00:00:00 +0000",[],"2022-12-08-competitive-programming-with-alphacode.md","2022-12-08-discovering-the-minutiae-of-backend-systems",{"id":6998,"data":7000,"filePath":7005,"digest":7006,"rendered":7007,"legacyId":7015},{"title":7001,"description":7002,"summary":7002,"pubDate":7003,"source":19,"url":7004,"thumbnail":21},"Discovering the minutiae of backend systems","Christian Gibson is an engineer on the Supercomputing team at OpenAI.",["Date","2022-12-08T08:00:00.000Z"],"https://openai.com/blog/discovering-the-minutiae-of-backend-systems","src/content/posts/2022-12-08-discovering-the-minutiae-of-backend-systems.md","2c742d09cfaca7c2",{"html":25,"metadata":7008},{"headings":7009,"localImagePaths":7010,"remoteImagePaths":7011,"frontmatter":7012,"imagePaths":7014},[],[],[],{"title":7001,"description":7002,"summary":7002,"pubDate":7013,"source":19,"url":7004,"thumbnail":21},"Thu, 08 Dec 2022 08:00:00 GMT",[],"2022-12-08-discovering-the-minutiae-of-backend-systems.md","2022-12-09-from-gpt2-to-stable-diffusion-hugging-face-arrives-to-the-elixir-community",{"id":7016,"data":7018,"filePath":7024,"digest":7025,"rendered":7026,"legacyId":7034},{"title":7019,"description":25,"summary":7020,"pubDate":7021,"source":2720,"url":7022,"thumbnail":7023},"From GPT2 to Stable Diffusion: Hugging Face arrives to the Elixir community","From GPT2 to Stable Diffusion: Hugging Face arrives to the Elixir community The Elixir community is ...",["Date","2022-12-09T00:00:00.000Z"],"https://huggingface.co/blog/elixir-bumblebee","https://huggingface.co/blog/assets/120_elixir-bumblebee/thumbnail.png","src/content/posts/2022-12-09-from-gpt2-to-stable-diffusion-hugging-face-arrives-to-the-elixir-community.md","66df20da0190b4ca",{"html":25,"metadata":7027},{"headings":7028,"localImagePaths":7029,"remoteImagePaths":7030,"frontmatter":7031,"imagePaths":7033},[],[],[],{"title":7019,"description":25,"summary":7020,"pubDate":7032,"source":2720,"url":7022,"thumbnail":7023},"Fri, 09 Dec 2022 00:00:00 GMT",[],"2022-12-09-from-gpt2-to-stable-diffusion-hugging-face-arrives-to-the-elixir-community.md","2022-12-09-illustrating-reinforcement-learning-from-human-feedback-rlhf",{"id":7035,"data":7037,"filePath":7043,"digest":7044,"rendered":7045,"legacyId":7052},{"title":7038,"description":25,"summary":7039,"pubDate":7040,"source":2720,"url":7041,"thumbnail":7042},"Illustrating Reinforcement Learning from Human Feedback (RLHF)","Illustrating Reinforcement Learning from Human Feedback (RLHF) This article has been translated to C...",["Date","2022-12-09T00:00:00.000Z"],"https://huggingface.co/blog/rlhf","https://huggingface.co/blog/assets/120_rlhf/thumbnail.png","src/content/posts/2022-12-09-illustrating-reinforcement-learning-from-human-feedback-rlhf.md","731960f7283aadbe",{"html":25,"metadata":7046},{"headings":7047,"localImagePaths":7048,"remoteImagePaths":7049,"frontmatter":7050,"imagePaths":7051},[],[],[],{"title":7038,"description":25,"summary":7039,"pubDate":7032,"source":2720,"url":7041,"thumbnail":7042},[],"2022-12-09-illustrating-reinforcement-learning-from-human-feedback-rlhf.md","2022-12-14-faster-training-and-inference-habana-gaudi2-vs-nvidia-a100-80gb",{"id":7053,"data":7055,"filePath":7061,"digest":7062,"rendered":7063,"legacyId":7071},{"title":7056,"description":25,"summary":7057,"pubDate":7058,"source":2720,"url":7059,"thumbnail":7060},"Faster Training and Inference: Habana Gaudi®2 vs Nvidia A100 80GB","Faster Training and Inference: Habana Gaudi®-2 vs Nvidia A100 80GB In this article, you will learn h...",["Date","2022-12-14T00:00:00.000Z"],"https://huggingface.co/blog/habana-gaudi-2-benchmark","https://huggingface.co/blog/assets/habana-gaudi-2-benchmark/thumbnail.png","src/content/posts/2022-12-14-faster-training-and-inference-habana-gaudi2-vs-nvidia-a100-80gb.md","ee2ac4db4c642e5f",{"html":25,"metadata":7064},{"headings":7065,"localImagePaths":7066,"remoteImagePaths":7067,"frontmatter":7068,"imagePaths":7070},[],[],[],{"title":7056,"description":25,"summary":7057,"pubDate":7069,"source":2720,"url":7059,"thumbnail":7060},"Wed, 14 Dec 2022 00:00:00 GMT",[],"2022-12-14-faster-training-and-inference-habana-gaudi2-vs-nvidia-a100-80gb.md","2022-12-15-a-complete-guide-to-audio-datasets",{"id":7072,"data":7074,"filePath":7080,"digest":7081,"rendered":7082,"legacyId":7090},{"title":7075,"description":25,"summary":7076,"pubDate":7077,"source":2720,"url":7078,"thumbnail":7079},"A Complete Guide to Audio Datasets","A Complete Guide to Audio Datasets Introduction 🤗 Datasets is an open-source library for downloading...",["Date","2022-12-15T00:00:00.000Z"],"https://huggingface.co/blog/audio-datasets","https://huggingface.co/blog/assets/116_audio_datasets/thumbnail.jpg","src/content/posts/2022-12-15-a-complete-guide-to-audio-datasets.md","a993f673c25f2d59",{"html":25,"metadata":7083},{"headings":7084,"localImagePaths":7085,"remoteImagePaths":7086,"frontmatter":7087,"imagePaths":7089},[],[],[],{"title":7075,"description":25,"summary":7076,"pubDate":7088,"source":2720,"url":7078,"thumbnail":7079},"Thu, 15 Dec 2022 00:00:00 GMT",[],"2022-12-15-a-complete-guide-to-audio-datasets.md","2022-12-15-ethics-and-society-newsletter-2-lets-talk-about-bias",{"id":7091,"data":7093,"filePath":7099,"digest":7100,"rendered":7101,"legacyId":7108},{"title":7094,"description":25,"summary":7095,"pubDate":7096,"source":2720,"url":7097,"thumbnail":7098},"Ethics and Society Newsletter #2: Let's talk about bias!","Machine Learning in development: Let's talk about bias! Bias in ML is ubiquitous, and Bias in ML is ...",["Date","2022-12-15T00:00:00.000Z"],"https://huggingface.co/blog/ethics-soc-2","https://huggingface.co/blog/assets/122_ethics_soc_2/thumbnail-solstice.png","src/content/posts/2022-12-15-ethics-and-society-newsletter-2-lets-talk-about-bias.md","ed6a80d3293dd278",{"html":25,"metadata":7102},{"headings":7103,"localImagePaths":7104,"remoteImagePaths":7105,"frontmatter":7106,"imagePaths":7107},[],[],[],{"title":7094,"description":25,"summary":7095,"pubDate":7088,"source":2720,"url":7097,"thumbnail":7098},[],"2022-12-15-ethics-and-society-newsletter-2-lets-talk-about-bias.md","2022-12-15-new-and-improved-embedding-model",{"id":7109,"data":7111,"filePath":7116,"digest":7117,"rendered":7118,"legacyId":7126},{"title":7112,"description":7113,"summary":7113,"pubDate":7114,"source":19,"url":7115,"thumbnail":21},"New and improved embedding model","We are excited to announce a new embedding model which is significantly more capable, cost effective, and simpler to use.",["Date","2022-12-15T08:00:00.000Z"],"https://openai.com/blog/new-and-improved-embedding-model","src/content/posts/2022-12-15-new-and-improved-embedding-model.md","3679da5a45722a53",{"html":25,"metadata":7119},{"headings":7120,"localImagePaths":7121,"remoteImagePaths":7122,"frontmatter":7123,"imagePaths":7125},[],[],[],{"title":7112,"description":7113,"summary":7113,"pubDate":7124,"source":19,"url":7115,"thumbnail":21},"Thu, 15 Dec 2022 08:00:00 GMT",[],"2022-12-15-new-and-improved-embedding-model.md","2022-12-16-point-e-a-system-for-generating-3d-point-clouds-from-complex-prompts",{"id":7127,"data":7129,"filePath":7133,"digest":7134,"rendered":7135,"legacyId":7143},{"title":7130,"description":25,"summary":25,"pubDate":7131,"source":19,"url":7132,"thumbnail":21},"Point-E: A system for generating 3D point clouds from complex prompts",["Date","2022-12-16T08:00:00.000Z"],"https://openai.com/blog/point-e","src/content/posts/2022-12-16-point-e-a-system-for-generating-3d-point-clouds-from-complex-prompts.md","c34073f3034dcc15",{"html":25,"metadata":7136},{"headings":7137,"localImagePaths":7138,"remoteImagePaths":7139,"frontmatter":7140,"imagePaths":7142},[],[],[],{"title":7130,"description":25,"summary":25,"pubDate":7141,"source":19,"url":7132,"thumbnail":21},"Fri, 16 Dec 2022 08:00:00 GMT",[],"2022-12-16-point-e-a-system-for-generating-3d-point-clouds-from-complex-prompts.md","2022-12-23-the-power-of-continuous-learning",{"id":7144,"data":7146,"filePath":7151,"digest":7152,"rendered":7153,"legacyId":7161},{"title":7147,"description":7148,"summary":7148,"pubDate":7149,"source":19,"url":7150,"thumbnail":21},"The power of continuous learning","Lilian Weng works on Applied AI Research at OpenAI.",["Date","2022-12-23T08:00:00.000Z"],"https://openai.com/blog/the-power-of-continuous-learning","src/content/posts/2022-12-23-the-power-of-continuous-learning.md","d43a982a087626fa",{"html":25,"metadata":7154},{"headings":7155,"localImagePaths":7156,"remoteImagePaths":7157,"frontmatter":7158,"imagePaths":7160},[],[],[],{"title":7147,"description":7148,"summary":7148,"pubDate":7159,"source":19,"url":7150,"thumbnail":21},"Fri, 23 Dec 2022 08:00:00 GMT",[],"2022-12-23-the-power-of-continuous-learning.md","2022-12-21-zero-shot-image-segmentation-with-clipseg",{"id":7162,"data":7164,"filePath":7170,"digest":7171,"rendered":7172,"legacyId":7180},{"title":7165,"description":25,"summary":7166,"pubDate":7167,"source":2720,"url":7168,"thumbnail":7169},"Zero-shot image segmentation with CLIPSeg","Zero-shot image segmentation with CLIPSeg This guide shows how you can use CLIPSeg, a zero-shot imag...",["Date","2022-12-21T00:00:00.000Z"],"https://huggingface.co/blog/clipseg-zero-shot","https://huggingface.co/blog/assets/123_clipseg-zero-shot/thumb.png","src/content/posts/2022-12-21-zero-shot-image-segmentation-with-clipseg.md","1cfadf0e7221b0fb",{"html":25,"metadata":7173},{"headings":7174,"localImagePaths":7175,"remoteImagePaths":7176,"frontmatter":7177,"imagePaths":7179},[],[],[],{"title":7165,"description":25,"summary":7166,"pubDate":7178,"source":2720,"url":7168,"thumbnail":7169},"Wed, 21 Dec 2022 00:00:00 GMT",[],"2022-12-21-zero-shot-image-segmentation-with-clipseg.md","2023-01-02-accelerating-pytorch-transformers-with-intel-sapphire-rapids-part-1",{"id":7181,"data":7183,"filePath":7189,"digest":7190,"rendered":7191,"legacyId":7199},{"title":7184,"description":25,"summary":7185,"pubDate":7186,"source":2720,"url":7187,"thumbnail":7188},"Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 1","Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 1 About a year ago, we showed you...",["Date","2023-01-02T00:00:00.000Z"],"https://huggingface.co/blog/intel-sapphire-rapids","https://huggingface.co/blog/assets/124_intel_sapphire_rapids/02.png","src/content/posts/2023-01-02-accelerating-pytorch-transformers-with-intel-sapphire-rapids-part-1.md","31e80c4c41ab2a8e",{"html":25,"metadata":7192},{"headings":7193,"localImagePaths":7194,"remoteImagePaths":7195,"frontmatter":7196,"imagePaths":7198},[],[],[],{"title":7184,"description":25,"summary":7185,"pubDate":7197,"source":2720,"url":7187,"thumbnail":7188},"Mon, 02 Jan 2023 00:00:00 GMT",[],"2023-01-02-accelerating-pytorch-transformers-with-intel-sapphire-rapids-part-1.md","2023-01-01-creating-next-gen-characters",{"id":7200,"data":7202,"filePath":7207,"digest":7208,"rendered":7209,"legacyId":7217},{"title":7203,"description":7204,"summary":7204,"pubDate":7205,"source":19,"url":7206,"thumbnail":21},"Creating next-gen characters","Using GPT-3 to create the next generation of AI-powered characters.",["Date","2023-01-01T08:00:00.000Z"],"https://openai.com/blog/inworld-ai","src/content/posts/2023-01-01-creating-next-gen-characters.md","f4390109ed260fc7",{"html":25,"metadata":7210},{"headings":7211,"localImagePaths":7212,"remoteImagePaths":7213,"frontmatter":7214,"imagePaths":7216},[],[],[],{"title":7203,"description":7204,"summary":7204,"pubDate":7215,"source":19,"url":7206,"thumbnail":21},"Sun, 01 Jan 2023 08:00:00 GMT",[],"2023-01-01-creating-next-gen-characters.md","2023-01-02-ai-for-game-development-creating-a-farming-game-in-5-days-part-1",{"id":7218,"data":7220,"filePath":7226,"digest":7227,"rendered":7228,"legacyId":7235},{"title":7221,"description":25,"summary":7222,"pubDate":7223,"source":2720,"url":7224,"thumbnail":7225},"AI for Game Development: Creating a Farming Game in 5 Days. Part 1","AI for Game Development: Creating a Farming Game in 5 Days. Part 1 Welcome to AI for Game Developmen...",["Date","2023-01-02T00:00:00.000Z"],"https://huggingface.co/blog/ml-for-games-1","https://huggingface.co/blog/assets/124_ml-for-games/thumbnail.png","src/content/posts/2023-01-02-ai-for-game-development-creating-a-farming-game-in-5-days-part-1.md","4d9416809dabd3bb",{"html":25,"metadata":7229},{"headings":7230,"localImagePaths":7231,"remoteImagePaths":7232,"frontmatter":7233,"imagePaths":7234},[],[],[],{"title":7221,"description":25,"summary":7222,"pubDate":7197,"source":2720,"url":7224,"thumbnail":7225},[],"2023-01-02-ai-for-game-development-creating-a-farming-game-in-5-days-part-1.md","2023-01-03-introduction-to-graph-machine-learning",{"id":7236,"data":7238,"filePath":7244,"digest":7245,"rendered":7246,"legacyId":7254},{"title":7239,"description":25,"summary":7240,"pubDate":7241,"source":2720,"url":7242,"thumbnail":7243},"Introduction to Graph Machine Learning","Introduction to Graph Machine Learning In this blog post, we cover the basics of graph machine learn...",["Date","2023-01-03T00:00:00.000Z"],"https://huggingface.co/blog/intro-graphml","https://huggingface.co/blog/assets/125_intro-to-graphml/thumbnail.png","src/content/posts/2023-01-03-introduction-to-graph-machine-learning.md","26abcc21d1e532dd",{"html":25,"metadata":7247},{"headings":7248,"localImagePaths":7249,"remoteImagePaths":7250,"frontmatter":7251,"imagePaths":7253},[],[],[],{"title":7239,"description":25,"summary":7240,"pubDate":7252,"source":2720,"url":7242,"thumbnail":7243},"Tue, 03 Jan 2023 00:00:00 GMT",[],"2023-01-03-introduction-to-graph-machine-learning.md","2023-01-03-fine-tuning-gpt-3-to-scale-video-creation",{"id":7255,"data":7257,"filePath":7262,"digest":7263,"rendered":7264,"legacyId":7272},{"title":7258,"description":7259,"summary":7259,"pubDate":7260,"source":19,"url":7261,"thumbnail":21},"Fine-tuning GPT-3 to scale video creation","Fine-tuning GPT-3 to power and scale done-for-you video creation.",["Date","2023-01-03T08:00:00.000Z"],"https://openai.com/blog/waymark","src/content/posts/2023-01-03-fine-tuning-gpt-3-to-scale-video-creation.md","7ac127f682e5b719",{"html":25,"metadata":7265},{"headings":7266,"localImagePaths":7267,"remoteImagePaths":7268,"frontmatter":7269,"imagePaths":7271},[],[],[],{"title":7258,"description":7259,"summary":7259,"pubDate":7270,"source":19,"url":7261,"thumbnail":21},"Tue, 03 Jan 2023 08:00:00 GMT",[],"2023-01-03-fine-tuning-gpt-3-to-scale-video-creation.md","2023-01-04-delivering-nuanced-insights-from-customer-feedback",{"id":7273,"data":7275,"filePath":7280,"digest":7281,"rendered":7282,"legacyId":7290},{"title":7276,"description":7277,"summary":7277,"pubDate":7278,"source":19,"url":7279,"thumbnail":21},"Delivering nuanced insights from customer feedback","Using GPT-3 to deliver fast, nuanced insights from customer feedback.",["Date","2023-01-04T00:00:00.000Z"],"https://openai.com/blog/yabble","src/content/posts/2023-01-04-delivering-nuanced-insights-from-customer-feedback.md","a56fc9a6a6530596",{"html":25,"metadata":7283},{"headings":7284,"localImagePaths":7285,"remoteImagePaths":7286,"frontmatter":7287,"imagePaths":7289},[],[],[],{"title":7276,"description":7277,"summary":7277,"pubDate":7288,"source":19,"url":7279,"thumbnail":21},"Wed, 04 Jan 2023 00:00:00 GMT",[],"2023-01-04-delivering-nuanced-insights-from-customer-feedback.md","2023-01-09-ai-for-game-development-creating-a-farming-game-in-5-days-part-2",{"id":7291,"data":7293,"filePath":7299,"digest":7300,"rendered":7301,"legacyId":7309},{"title":7294,"description":25,"summary":7295,"pubDate":7296,"source":2720,"url":7297,"thumbnail":7298},"AI for Game Development: Creating a Farming Game in 5 Days. Part 2","AI for Game Development: Creating a Farming Game in 5 Days. Part 2 Welcome to AI for Game Developmen...",["Date","2023-01-09T00:00:00.000Z"],"https://huggingface.co/blog/ml-for-games-2","https://huggingface.co/blog/assets/124_ml-for-games/thumbnail2.png","src/content/posts/2023-01-09-ai-for-game-development-creating-a-farming-game-in-5-days-part-2.md","6188b87b103d1377",{"html":25,"metadata":7302},{"headings":7303,"localImagePaths":7304,"remoteImagePaths":7305,"frontmatter":7306,"imagePaths":7308},[],[],[],{"title":7294,"description":25,"summary":7295,"pubDate":7307,"source":2720,"url":7297,"thumbnail":7298},"Mon, 09 Jan 2023 00:00:00 GMT",[],"2023-01-09-ai-for-game-development-creating-a-farming-game-in-5-days-part-2.md","2023-01-11-forecasting-potential-misuses-of-language-models-for-disinformation-campaigns-and-how-to-reduce-risk",{"id":7310,"data":7312,"filePath":7317,"digest":7318,"rendered":7319,"legacyId":7327},{"title":7313,"description":7314,"summary":7314,"pubDate":7315,"source":19,"url":7316,"thumbnail":21},"Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk","OpenAI researchers collaborated with Georgetown University’s Center for Security and Emerging Technology and the Stanford Internet Observatory to investigate how large language models might be misused for disinformation purposes. The collaboration included an October 2021 workshop bringing together 30 disinformation researchers, machine learning experts, and policy analysts, and culminated in a co-authored report building on more than a year of research. This report outlines the threats that language models pose to the information environment if used to augment disinformation campaigns and introduces a framework for analyzing potential mitigations. Read the full report here.",["Date","2023-01-11T08:00:00.000Z"],"https://openai.com/blog/forecasting-misuse","src/content/posts/2023-01-11-forecasting-potential-misuses-of-language-models-for-disinformation-campaigns-and-how-to-reduce-risk.md","6b5cc58cf7a928fd",{"html":25,"metadata":7320},{"headings":7321,"localImagePaths":7322,"remoteImagePaths":7323,"frontmatter":7324,"imagePaths":7326},[],[],[],{"title":7313,"description":7314,"summary":7314,"pubDate":7325,"source":19,"url":7316,"thumbnail":21},"Wed, 11 Jan 2023 08:00:00 GMT",[],"2023-01-11-forecasting-potential-misuses-of-language-models-for-disinformation-campaigns-and-how-to-reduce-risk.md","2023-01-16-image-similarity-with-hugging-face-datasets-and-transformers",{"id":7328,"data":7330,"filePath":7336,"digest":7337,"rendered":7338,"legacyId":7346},{"title":7331,"description":25,"summary":7332,"pubDate":7333,"source":2720,"url":7334,"thumbnail":7335},"Image Similarity with Hugging Face Datasets and Transformers","Image Similarity with Hugging Face Datasets and Transformers In this post, you'll learn to build an ...",["Date","2023-01-16T00:00:00.000Z"],"https://huggingface.co/blog/image-similarity","https://huggingface.co/blog/assets/image_similarity/thumbnail.png","src/content/posts/2023-01-16-image-similarity-with-hugging-face-datasets-and-transformers.md","705c8af65c45ff26",{"html":25,"metadata":7339},{"headings":7340,"localImagePaths":7341,"remoteImagePaths":7342,"frontmatter":7343,"imagePaths":7345},[],[],[],{"title":7331,"description":25,"summary":7332,"pubDate":7344,"source":2720,"url":7334,"thumbnail":7335},"Mon, 16 Jan 2023 00:00:00 GMT",[],"2023-01-16-image-similarity-with-hugging-face-datasets-and-transformers.md","2023-01-17-welcome-paddlepaddle-to-the-hugging-face-hub",{"id":7347,"data":7349,"filePath":7355,"digest":7356,"rendered":7357,"legacyId":7365},{"title":7350,"description":25,"summary":7351,"pubDate":7352,"source":2720,"url":7353,"thumbnail":7354},"Welcome PaddlePaddle to the Hugging Face Hub","Welcome PaddlePaddle to the Hugging Face Hub We are happy to share an open source collaboration betw...",["Date","2023-01-17T00:00:00.000Z"],"https://huggingface.co/blog/paddlepaddle","https://huggingface.co/blog/assets/126_paddlepaddle/thumbnail.jpg","src/content/posts/2023-01-17-welcome-paddlepaddle-to-the-hugging-face-hub.md","b5d9eab6e1cfa2e0",{"html":25,"metadata":7358},{"headings":7359,"localImagePaths":7360,"remoteImagePaths":7361,"frontmatter":7362,"imagePaths":7364},[],[],[],{"title":7350,"description":25,"summary":7351,"pubDate":7363,"source":2720,"url":7353,"thumbnail":7354},"Tue, 17 Jan 2023 00:00:00 GMT",[],"2023-01-17-welcome-paddlepaddle-to-the-hugging-face-hub.md","2023-01-19-universal-image-segmentation-with-mask2former-and-oneformer",{"id":7366,"data":7368,"filePath":7374,"digest":7375,"rendered":7376,"legacyId":7384},{"title":7369,"description":25,"summary":7370,"pubDate":7371,"source":2720,"url":7372,"thumbnail":7373},"Universal Image Segmentation with Mask2Former and OneFormer","Universal Image Segmentation with Mask2Former and OneFormer This guide introduces Mask2Former and On...",["Date","2023-01-19T00:00:00.000Z"],"https://huggingface.co/blog/mask2former","https://huggingface.co/blog/assets/127_mask2former/thumbnail.png","src/content/posts/2023-01-19-universal-image-segmentation-with-mask2former-and-oneformer.md","09dae5da14bd76a8",{"html":25,"metadata":7377},{"headings":7378,"localImagePaths":7379,"remoteImagePaths":7380,"frontmatter":7381,"imagePaths":7383},[],[],[],{"title":7369,"description":25,"summary":7370,"pubDate":7382,"source":2720,"url":7372,"thumbnail":7373},"Thu, 19 Jan 2023 00:00:00 GMT",[],"2023-01-19-universal-image-segmentation-with-mask2former-and-oneformer.md","2023-01-20-3d-asset-generation-ai-for-game-development-3",{"id":7385,"data":7387,"filePath":7393,"digest":7394,"rendered":7395,"legacyId":7403},{"title":7388,"description":25,"summary":7389,"pubDate":7390,"source":2720,"url":7391,"thumbnail":7392},"3D Asset Generation: AI for Game Development #3","3D Asset Generation: AI for Game Development #3 Welcome to AI for Game Development! In this series, ...",["Date","2023-01-20T00:00:00.000Z"],"https://huggingface.co/blog/ml-for-games-3","https://huggingface.co/blog/assets/124_ml-for-games/thumbnail3.png","src/content/posts/2023-01-20-3d-asset-generation-ai-for-game-development-3.md","21876e13b7ffd16b",{"html":25,"metadata":7396},{"headings":7397,"localImagePaths":7398,"remoteImagePaths":7399,"frontmatter":7400,"imagePaths":7402},[],[],[],{"title":7388,"description":25,"summary":7389,"pubDate":7401,"source":2720,"url":7391,"thumbnail":7392},"Fri, 20 Jan 2023 00:00:00 GMT",[],"2023-01-20-3d-asset-generation-ai-for-game-development-3.md","2023-01-23-openai-and-microsoft-extend-partnership",{"id":7404,"data":7406,"filePath":7411,"digest":7412,"rendered":7413,"legacyId":7421},{"title":7407,"description":7408,"summary":7408,"pubDate":7409,"source":19,"url":7410,"thumbnail":21},"OpenAI and Microsoft extend partnership","We’re happy to announce that OpenAI and Microsoft are extending our partnership.",["Date","2023-01-23T08:00:00.000Z"],"https://openai.com/blog/openai-and-microsoft-extend-partnership","src/content/posts/2023-01-23-openai-and-microsoft-extend-partnership.md","e4ede3f2f47eb04e",{"html":25,"metadata":7414},{"headings":7415,"localImagePaths":7416,"remoteImagePaths":7417,"frontmatter":7418,"imagePaths":7420},[],[],[],{"title":7407,"description":7408,"summary":7408,"pubDate":7419,"source":19,"url":7410,"thumbnail":21},"Mon, 23 Jan 2023 08:00:00 GMT",[],"2023-01-23-openai-and-microsoft-extend-partnership.md","2023-01-24-optimumonnx-runtime---easier-faster-training-for-your-hugging-face-models",{"id":7422,"data":7424,"filePath":7430,"digest":7431,"rendered":7432,"legacyId":7440},{"title":7425,"description":25,"summary":7426,"pubDate":7427,"source":2720,"url":7428,"thumbnail":7429},"Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models","Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models Introduction Transforme...",["Date","2023-01-24T00:00:00.000Z"],"https://huggingface.co/blog/optimum-onnxruntime-training","https://huggingface.co/blog/assets/optimum_onnxruntime-training/thumbnail.png","src/content/posts/2023-01-24-optimumonnx-runtime---easier-faster-training-for-your-hugging-face-models.md","47b5c58f5a192d90",{"html":25,"metadata":7433},{"headings":7434,"localImagePaths":7435,"remoteImagePaths":7436,"frontmatter":7437,"imagePaths":7439},[],[],[],{"title":7425,"description":25,"summary":7426,"pubDate":7438,"source":2720,"url":7428,"thumbnail":7429},"Tue, 24 Jan 2023 00:00:00 GMT",[],"2023-01-24-optimumonnx-runtime---easier-faster-training-for-your-hugging-face-models.md","2022-12-20-model-cards-introducing-hf-model-documentation-tools",{"id":7441,"data":7443,"filePath":7449,"digest":7450,"rendered":7451,"legacyId":7459},{"title":7444,"description":25,"summary":7445,"pubDate":7446,"source":2720,"url":7447,"thumbnail":7448},"Model Cards: Introducing HF Model documentation tools","Model Cards Introduction Model cards are an important documentation framework for understanding, sha...",["Date","2022-12-20T00:00:00.000Z"],"https://huggingface.co/blog/model-cards","https://huggingface.co/blog/assets/121_model-cards/thumbnail.png","src/content/posts/2022-12-20-model-cards-introducing-hf-model-documentation-tools.md","632aface894ecef3",{"html":25,"metadata":7452},{"headings":7453,"localImagePaths":7454,"remoteImagePaths":7455,"frontmatter":7456,"imagePaths":7458},[],[],[],{"title":7444,"description":25,"summary":7445,"pubDate":7457,"source":2720,"url":7447,"thumbnail":7448},"Tue, 20 Dec 2022 00:00:00 GMT",[],"2022-12-20-model-cards-introducing-hf-model-documentation-tools.md","2023-01-24-what-makes-a-dialog-agent-useful",{"id":7460,"data":7462,"filePath":7468,"digest":7469,"rendered":7470,"legacyId":7477},{"title":7463,"description":25,"summary":7464,"pubDate":7465,"source":2720,"url":7466,"thumbnail":7467},"What Makes a Dialog Agent Useful?","What Makes a Dialog Agent Useful? The techniques behind ChatGPT: RLHF, IFT, CoT, Red teaming, and mo...",["Date","2023-01-24T00:00:00.000Z"],"https://huggingface.co/blog/dialog-agents","https://huggingface.co/blog/assets/dialog-agents/thumbnail.png","src/content/posts/2023-01-24-what-makes-a-dialog-agent-useful.md","8d3b5d0a750a1391",{"html":25,"metadata":7471},{"headings":7472,"localImagePaths":7473,"remoteImagePaths":7474,"frontmatter":7475,"imagePaths":7476},[],[],[],{"title":7463,"description":25,"summary":7464,"pubDate":7438,"source":2720,"url":7466,"thumbnail":7467},[],"2023-01-24-what-makes-a-dialog-agent-useful.md","2023-01-26-2d-asset-generation-ai-for-game-development-4",{"id":7478,"data":7480,"filePath":7486,"digest":7487,"rendered":7488,"legacyId":7496},{"title":7481,"description":25,"summary":7482,"pubDate":7483,"source":2720,"url":7484,"thumbnail":7485},"2D Asset Generation: AI for Game Development #4","2D Asset Generation: AI for Game Development #4 Welcome to AI for Game Development! In this series, ...",["Date","2023-01-26T00:00:00.000Z"],"https://huggingface.co/blog/ml-for-games-4","https://huggingface.co/blog/assets/124_ml-for-games/thumbnail4.png","src/content/posts/2023-01-26-2d-asset-generation-ai-for-game-development-4.md","c8d4beb127969ca1",{"html":25,"metadata":7489},{"headings":7490,"localImagePaths":7491,"remoteImagePaths":7492,"frontmatter":7493,"imagePaths":7495},[],[],[],{"title":7481,"description":25,"summary":7482,"pubDate":7494,"source":2720,"url":7484,"thumbnail":7485},"Thu, 26 Jan 2023 00:00:00 GMT",[],"2023-01-26-2d-asset-generation-ai-for-game-development-4.md","2023-01-26-using-lora-for-efficient-stable-diffusion-fine-tuning",{"id":7497,"data":7499,"filePath":7505,"digest":7506,"rendered":7507,"legacyId":7514},{"title":7500,"description":25,"summary":7501,"pubDate":7502,"source":2720,"url":7503,"thumbnail":7504},"Using LoRA for Efficient Stable Diffusion Fine-Tuning","Using LoRA for Efficient Stable Diffusion Fine-Tuning LoRA: Low-Rank Adaptation of Large Language Mo...",["Date","2023-01-26T00:00:00.000Z"],"https://huggingface.co/blog/lora","https://huggingface.co/blog/assets/lora/thumbnail.png","src/content/posts/2023-01-26-using-lora-for-efficient-stable-diffusion-fine-tuning.md","38c46f48c51974fe",{"html":25,"metadata":7508},{"headings":7509,"localImagePaths":7510,"remoteImagePaths":7511,"frontmatter":7512,"imagePaths":7513},[],[],[],{"title":7500,"description":25,"summary":7501,"pubDate":7494,"source":2720,"url":7503,"thumbnail":7504},[],"2023-01-26-using-lora-for-efficient-stable-diffusion-fine-tuning.md","2023-01-31-new-ai-classifier-for-indicating-ai-written-text",{"id":7515,"data":7517,"filePath":7522,"digest":7523,"rendered":7524,"legacyId":7532},{"title":7518,"description":7519,"summary":7519,"pubDate":7520,"source":19,"url":7521,"thumbnail":21},"New AI classifier for indicating AI-written text","We’re launching a classifier trained to distinguish between AI-written and human-written text.",["Date","2023-01-31T08:00:00.000Z"],"https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text","src/content/posts/2023-01-31-new-ai-classifier-for-indicating-ai-written-text.md","197a4236d90eb9be",{"html":25,"metadata":7525},{"headings":7526,"localImagePaths":7527,"remoteImagePaths":7528,"frontmatter":7529,"imagePaths":7531},[],[],[],{"title":7518,"description":7519,"summary":7519,"pubDate":7530,"source":19,"url":7521,"thumbnail":21},"Tue, 31 Jan 2023 08:00:00 GMT",[],"2023-01-31-new-ai-classifier-for-indicating-ai-written-text.md","2023-01-30-the-state-of-computer-vision-at-hugging-face",{"id":7533,"data":7535,"filePath":7541,"digest":7542,"rendered":7543,"legacyId":7551},{"title":7536,"description":25,"summary":7537,"pubDate":7538,"source":2720,"url":7539,"thumbnail":7540},"The State of Computer Vision at Hugging Face 🤗","The State of Computer Vision at Hugging Face 🤗 At Hugging Face, we pride ourselves on democratizing ...",["Date","2023-01-30T00:00:00.000Z"],"https://huggingface.co/blog/cv_state","https://huggingface.co/blog/assets/cv_state/thumbnail.png","src/content/posts/2023-01-30-the-state-of-computer-vision-at-hugging-face.md","9f89105a4d3335f4",{"html":25,"metadata":7544},{"headings":7545,"localImagePaths":7546,"remoteImagePaths":7547,"frontmatter":7548,"imagePaths":7550},[],[],[],{"title":7536,"description":25,"summary":7537,"pubDate":7549,"source":2720,"url":7539,"thumbnail":7540},"Mon, 30 Jan 2023 00:00:00 GMT",[],"2023-01-30-the-state-of-computer-vision-at-hugging-face.md","2023-02-01-introducing-chatgpt-plus",{"id":7552,"data":7554,"filePath":7559,"digest":7560,"rendered":7561,"legacyId":7569},{"title":7555,"description":7556,"summary":7556,"pubDate":7557,"source":19,"url":7558,"thumbnail":21},"Introducing ChatGPT Plus","We’re launching a pilot subscription plan for ChatGPT, a conversational AI that can chat with you, answer follow-up questions, and challenge incorrect assumptions.",["Date","2023-02-01T08:00:00.000Z"],"https://openai.com/blog/chatgpt-plus","src/content/posts/2023-02-01-introducing-chatgpt-plus.md","e3d2a896877e6af2",{"html":25,"metadata":7562},{"headings":7563,"localImagePaths":7564,"remoteImagePaths":7565,"frontmatter":7566,"imagePaths":7568},[],[],[],{"title":7555,"description":7556,"summary":7556,"pubDate":7567,"source":19,"url":7558,"thumbnail":21},"Wed, 01 Feb 2023 08:00:00 GMT",[],"2023-02-01-introducing-chatgpt-plus.md","2023-02-03-a-dive-into-pretraining-strategies-for-vision-language-models",{"id":7570,"data":7572,"filePath":7578,"digest":7579,"rendered":7580,"legacyId":7588},{"title":7573,"description":25,"summary":7574,"pubDate":7575,"source":2720,"url":7576,"thumbnail":7577},"A Dive into Pretraining Strategies for Vision-Language Models","A Dive into Vision-Language Models Human learning is inherently multi-modal as jointly leveraging mu...",["Date","2023-02-03T00:00:00.000Z"],"https://huggingface.co/blog/vision_language_pretraining","https://huggingface.co/blog//assets/128_vision_language_pretraining/thumbnail.png","src/content/posts/2023-02-03-a-dive-into-pretraining-strategies-for-vision-language-models.md","280ca1d50964e261",{"html":25,"metadata":7581},{"headings":7582,"localImagePaths":7583,"remoteImagePaths":7584,"frontmatter":7585,"imagePaths":7587},[],[],[],{"title":7573,"description":25,"summary":7574,"pubDate":7586,"source":2720,"url":7576,"thumbnail":7577},"Fri, 03 Feb 2023 00:00:00 GMT",[],"2023-02-03-a-dive-into-pretraining-strategies-for-vision-language-models.md","2023-02-07-generating-stories-ai-for-game-development-5",{"id":7589,"data":7591,"filePath":7597,"digest":7598,"rendered":7599,"legacyId":7607},{"title":7592,"description":25,"summary":7593,"pubDate":7594,"source":2720,"url":7595,"thumbnail":7596},"Generating Stories: AI for Game Development #5","Generating Stories: AI for Game Development #5 Welcome to AI for Game Development! In this series, w...",["Date","2023-02-07T00:00:00.000Z"],"https://huggingface.co/blog/ml-for-games-5","https://huggingface.co/blog/assets/124_ml-for-games/thumbnail5.png","src/content/posts/2023-02-07-generating-stories-ai-for-game-development-5.md","39f7b5aa905c0c10",{"html":25,"metadata":7600},{"headings":7601,"localImagePaths":7602,"remoteImagePaths":7603,"frontmatter":7604,"imagePaths":7606},[],[],[],{"title":7592,"description":25,"summary":7593,"pubDate":7605,"source":2720,"url":7595,"thumbnail":7596},"Tue, 07 Feb 2023 00:00:00 GMT",[],"2023-02-07-generating-stories-ai-for-game-development-5.md","2023-02-06-accelerating-pytorch-transformers-with-intel-sapphire-rapids-part-2",{"id":7608,"data":7610,"filePath":7616,"digest":7617,"rendered":7618,"legacyId":7626},{"title":7611,"description":25,"summary":7612,"pubDate":7613,"source":2720,"url":7614,"thumbnail":7615},"Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2","Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2 In a recent post, we introduced...",["Date","2023-02-06T00:00:00.000Z"],"https://huggingface.co/blog/intel-sapphire-rapids-inference","https://huggingface.co/blog/assets/129_intel_sapphire_rapids_inference/01.png","src/content/posts/2023-02-06-accelerating-pytorch-transformers-with-intel-sapphire-rapids-part-2.md","b847c7a384f6c2a4",{"html":25,"metadata":7619},{"headings":7620,"localImagePaths":7621,"remoteImagePaths":7622,"frontmatter":7623,"imagePaths":7625},[],[],[],{"title":7611,"description":25,"summary":7612,"pubDate":7624,"source":2720,"url":7614,"thumbnail":7615},"Mon, 06 Feb 2023 00:00:00 GMT",[],"2023-02-06-accelerating-pytorch-transformers-with-intel-sapphire-rapids-part-2.md","2023-02-07-introducing-ai-vs-ai-a-deep-reinforcement-learning-multi-agents-competition-system",{"id":7627,"data":7629,"filePath":7635,"digest":7636,"rendered":7637,"legacyId":7644},{"title":7630,"description":25,"summary":7631,"pubDate":7632,"source":2720,"url":7633,"thumbnail":7634},"Introducing ⚔️ AI vs. AI ⚔️ a deep reinforcement learning multi-agents competition system","Introducing ⚔️ AI vs. AI ⚔️ a deep reinforcement learning multi-agents competition system We’re exci...",["Date","2023-02-07T00:00:00.000Z"],"https://huggingface.co/blog/aivsai","https://huggingface.co/blog/assets/128_aivsai/thumbnail.png","src/content/posts/2023-02-07-introducing-ai-vs-ai-a-deep-reinforcement-learning-multi-agents-competition-system.md","dba1b8ee173fb5a4",{"html":25,"metadata":7638},{"headings":7639,"localImagePaths":7640,"remoteImagePaths":7641,"frontmatter":7642,"imagePaths":7643},[],[],[],{"title":7630,"description":25,"summary":7631,"pubDate":7605,"source":2720,"url":7633,"thumbnail":7634},[],"2023-02-07-introducing-ai-vs-ai-a-deep-reinforcement-learning-multi-agents-competition-system.md","2023-02-08-speech-synthesis-recognition-and-more-with-speecht5",{"id":7645,"data":7647,"filePath":7653,"digest":7654,"rendered":7655,"legacyId":7663},{"title":7648,"description":25,"summary":7649,"pubDate":7650,"source":2720,"url":7651,"thumbnail":7652},"Speech Synthesis, Recognition, and More With SpeechT5","Speech Synthesis, Recognition, and More With SpeechT5 We’re happy to announce that SpeechT5 is now a...",["Date","2023-02-08T00:00:00.000Z"],"https://huggingface.co/blog/speecht5","https://huggingface.co/blog/assets/speecht5/thumbnail.png","src/content/posts/2023-02-08-speech-synthesis-recognition-and-more-with-speecht5.md","1b4655a78b735e62",{"html":25,"metadata":7656},{"headings":7657,"localImagePaths":7658,"remoteImagePaths":7659,"frontmatter":7660,"imagePaths":7662},[],[],[],{"title":7648,"description":25,"summary":7649,"pubDate":7661,"source":2720,"url":7651,"thumbnail":7652},"Wed, 08 Feb 2023 00:00:00 GMT",[],"2023-02-08-speech-synthesis-recognition-and-more-with-speecht5.md","2023-02-10-peft-parameter-efficient-fine-tuning-of-billion-scale-models-on-low-resource-hardware",{"id":7664,"data":7666,"filePath":7672,"digest":7673,"rendered":7674,"legacyId":7682},{"title":7667,"description":25,"summary":7668,"pubDate":7669,"source":2720,"url":7670,"thumbnail":7671},"🤗 PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware","🤗 PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware Motivation ...",["Date","2023-02-10T00:00:00.000Z"],"https://huggingface.co/blog/peft","https://huggingface.co/blog/assets/130_peft/thumbnail.png","src/content/posts/2023-02-10-peft-parameter-efficient-fine-tuning-of-billion-scale-models-on-low-resource-hardware.md","780584313ffc8eaa",{"html":25,"metadata":7675},{"headings":7676,"localImagePaths":7677,"remoteImagePaths":7678,"frontmatter":7679,"imagePaths":7681},[],[],[],{"title":7667,"description":25,"summary":7668,"pubDate":7680,"source":2720,"url":7670,"thumbnail":7671},"Fri, 10 Feb 2023 00:00:00 GMT",[],"2023-02-10-peft-parameter-efficient-fine-tuning-of-billion-scale-models-on-low-resource-hardware.md","2023-02-15-why-were-switching-to-hugging-face-inference-endpoints-and-maybe-you-should-too",{"id":7683,"data":7685,"filePath":7691,"digest":7692,"rendered":7693,"legacyId":7701},{"title":7686,"description":25,"summary":7687,"pubDate":7688,"source":2720,"url":7689,"thumbnail":7690},"Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too","Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too Hugging Face recen...",["Date","2023-02-15T00:00:00.000Z"],"https://huggingface.co/blog/mantis-case-study","https://huggingface.co/blog/assets/78_ml_director_insights/mantis1.png","src/content/posts/2023-02-15-why-were-switching-to-hugging-face-inference-endpoints-and-maybe-you-should-too.md","0bfe57897738da7e",{"html":25,"metadata":7694},{"headings":7695,"localImagePaths":7696,"remoteImagePaths":7697,"frontmatter":7698,"imagePaths":7700},[],[],[],{"title":7686,"description":25,"summary":7687,"pubDate":7699,"source":2720,"url":7689,"thumbnail":7690},"Wed, 15 Feb 2023 00:00:00 GMT",[],"2023-02-15-why-were-switching-to-hugging-face-inference-endpoints-and-maybe-you-should-too.md","2023-02-15-zero-shot-image-to-text-generation-with-blip-2",{"id":7702,"data":7704,"filePath":7710,"digest":7711,"rendered":7712,"legacyId":7719},{"title":7705,"description":25,"summary":7706,"pubDate":7707,"source":2720,"url":7708,"thumbnail":7709},"Zero-shot image-to-text generation with BLIP-2","Zero-shot image-to-text generation with BLIP-2 This guide introduces BLIP-2 from Salesforce Research...",["Date","2023-02-15T00:00:00.000Z"],"https://huggingface.co/blog/blip-2","https://huggingface.co/blog/assets/blip-2/thumbnail.png","src/content/posts/2023-02-15-zero-shot-image-to-text-generation-with-blip-2.md","f661c715edb6159c",{"html":25,"metadata":7713},{"headings":7714,"localImagePaths":7715,"remoteImagePaths":7716,"frontmatter":7717,"imagePaths":7718},[],[],[],{"title":7705,"description":25,"summary":7706,"pubDate":7699,"source":2720,"url":7708,"thumbnail":7709},[],"2023-02-15-zero-shot-image-to-text-generation-with-blip-2.md","2023-02-16-how-should-ai-systems-behave-and-who-should-decide",{"id":7720,"data":7722,"filePath":7727,"digest":7728,"rendered":7729,"legacyId":7737},{"title":7723,"description":7724,"summary":7724,"pubDate":7725,"source":19,"url":7726,"thumbnail":21},"How should AI systems behave, and who should decide?","We’re clarifying how ChatGPT’s behavior is shaped and our plans for improving that behavior, allowing more user customization, and getting more public input into our decision-making in these areas.",["Date","2023-02-16T08:00:00.000Z"],"https://openai.com/blog/how-should-ai-systems-behave","src/content/posts/2023-02-16-how-should-ai-systems-behave-and-who-should-decide.md","e8e62f877d346cf6",{"html":25,"metadata":7730},{"headings":7731,"localImagePaths":7732,"remoteImagePaths":7733,"frontmatter":7734,"imagePaths":7736},[],[],[],{"title":7723,"description":7724,"summary":7724,"pubDate":7735,"source":19,"url":7726,"thumbnail":21},"Thu, 16 Feb 2023 08:00:00 GMT",[],"2023-02-16-how-should-ai-systems-behave-and-who-should-decide.md","2023-02-23-fetch-consolidates-ai-tools-and-saves-30-development-time-with-hugging-face-on-aws",{"id":7738,"data":7740,"filePath":7746,"digest":7747,"rendered":7748,"legacyId":7756},{"title":7741,"description":25,"summary":7742,"pubDate":7743,"source":2720,"url":7744,"thumbnail":7745},"Fetch Consolidates AI Tools and Saves 30% Development Time with Hugging Face on AWS","Fetch Consolidates AI Tools and Saves 30% Development Time with Hugging Face on AWS If you need supp...",["Date","2023-02-23T00:00:00.000Z"],"https://huggingface.co/blog/fetch-eap-case-study","https://huggingface.co/blog/assets/78_ml_director_insights/fetch2.png","src/content/posts/2023-02-23-fetch-consolidates-ai-tools-and-saves-30-development-time-with-hugging-face-on-aws.md","719d6b0e2507787b",{"html":25,"metadata":7749},{"headings":7750,"localImagePaths":7751,"remoteImagePaths":7752,"frontmatter":7753,"imagePaths":7755},[],[],[],{"title":7741,"description":25,"summary":7742,"pubDate":7754,"source":2720,"url":7744,"thumbnail":7745},"Thu, 23 Feb 2023 00:00:00 GMT",[],"2023-02-23-fetch-consolidates-ai-tools-and-saves-30-development-time-with-hugging-face-on-aws.md","2023-02-21-hugging-face-and-aws-partner-to-make-ai-more-accessible",{"id":7757,"data":7759,"filePath":7765,"digest":7766,"rendered":7767,"legacyId":7775},{"title":7760,"description":25,"summary":7761,"pubDate":7762,"source":2720,"url":7763,"thumbnail":7764},"Hugging Face and AWS partner to make AI more accessible","Hugging Face and AWS partner to make AI more accessible It’s time to make AI open and accessible to ...",["Date","2023-02-21T00:00:00.000Z"],"https://huggingface.co/blog/aws-partnership","https://huggingface.co/blog/assets/131_aws-partnership/aws-partnership-thumbnail.png","src/content/posts/2023-02-21-hugging-face-and-aws-partner-to-make-ai-more-accessible.md","ba6ce95914263c2d",{"html":25,"metadata":7768},{"headings":7769,"localImagePaths":7770,"remoteImagePaths":7771,"frontmatter":7772,"imagePaths":7774},[],[],[],{"title":7760,"description":25,"summary":7761,"pubDate":7773,"source":2720,"url":7763,"thumbnail":7764},"Tue, 21 Feb 2023 00:00:00 GMT",[],"2023-02-21-hugging-face-and-aws-partner-to-make-ai-more-accessible.md","2023-02-24-planning-for-agi-and-beyond",{"id":7776,"data":7778,"filePath":7783,"digest":7784,"rendered":7785,"legacyId":7793},{"title":7779,"description":7780,"summary":7780,"pubDate":7781,"source":19,"url":7782,"thumbnail":21},"Planning for AGI and beyond","Our mission is to ensure that artificial general intelligence—AI systems that are generally smarter than humans—benefits all of humanity.",["Date","2023-02-24T08:00:00.000Z"],"https://openai.com/blog/planning-for-agi-and-beyond","src/content/posts/2023-02-24-planning-for-agi-and-beyond.md","91727120662b07c4",{"html":25,"metadata":7786},{"headings":7787,"localImagePaths":7788,"remoteImagePaths":7789,"frontmatter":7790,"imagePaths":7792},[],[],[],{"title":7779,"description":7780,"summary":7780,"pubDate":7791,"source":19,"url":7782,"thumbnail":21},"Fri, 24 Feb 2023 08:00:00 GMT",[],"2023-02-24-planning-for-agi-and-beyond.md","2023-02-24-red-teaming-large-language-models",{"id":7794,"data":7796,"filePath":7802,"digest":7803,"rendered":7804,"legacyId":7812},{"title":7797,"description":25,"summary":7798,"pubDate":7799,"source":2720,"url":7800,"thumbnail":7801},"Red-Teaming Large Language Models","Red-Teaming Large Language Models Warning: This article is about red-teaming and as such contains ex...",["Date","2023-02-24T00:00:00.000Z"],"https://huggingface.co/blog/red-teaming","https://huggingface.co/blog/assets/red-teaming/thumbnail.png","src/content/posts/2023-02-24-red-teaming-large-language-models.md","1efe44942a995431",{"html":25,"metadata":7805},{"headings":7806,"localImagePaths":7807,"remoteImagePaths":7808,"frontmatter":7809,"imagePaths":7811},[],[],[],{"title":7797,"description":25,"summary":7798,"pubDate":7810,"source":2720,"url":7800,"thumbnail":7801},"Fri, 24 Feb 2023 00:00:00 GMT",[],"2023-02-24-red-teaming-large-language-models.md","2023-02-24-swift-diffusers-fast-stable-diffusion-for-mac",{"id":7813,"data":7815,"filePath":7821,"digest":7822,"rendered":7823,"legacyId":7830},{"title":7816,"description":25,"summary":7817,"pubDate":7818,"source":2720,"url":7819,"thumbnail":7820},"Swift Diffusers: Fast Stable Diffusion for Mac","Swift 🧨Diffusers: Fast Stable Diffusion for Mac Transform your text into stunning images with ease u...",["Date","2023-02-24T00:00:00.000Z"],"https://huggingface.co/blog/fast-mac-diffusers","https://huggingface.co/blog/assets/fast-mac-diffusers/thumbnail.png","src/content/posts/2023-02-24-swift-diffusers-fast-stable-diffusion-for-mac.md","ca2c12b217ee1838",{"html":25,"metadata":7824},{"headings":7825,"localImagePaths":7826,"remoteImagePaths":7827,"frontmatter":7828,"imagePaths":7829},[],[],[],{"title":7816,"description":25,"summary":7817,"pubDate":7810,"source":2720,"url":7819,"thumbnail":7820},[],"2023-02-24-swift-diffusers-fast-stable-diffusion-for-mac.md","2023-03-01-how-hugging-face-accelerated-development-of-witty-works-writing-assistant",{"id":7831,"data":7833,"filePath":7839,"digest":7840,"rendered":7841,"legacyId":7849},{"title":7834,"description":25,"summary":7835,"pubDate":7836,"source":2720,"url":7837,"thumbnail":7838},"How Hugging Face Accelerated Development of Witty Works Writing Assistant","How Hugging Face Accelerated Development of Witty Works Writing Assistant The Success Story of Witty...",["Date","2023-03-01T00:00:00.000Z"],"https://huggingface.co/blog/classification-use-cases","https://huggingface.co/blog/assets/78_ml_director_insights/witty-works.png","src/content/posts/2023-03-01-how-hugging-face-accelerated-development-of-witty-works-writing-assistant.md","55573eaac2a8fc09",{"html":25,"metadata":7842},{"headings":7843,"localImagePaths":7844,"remoteImagePaths":7845,"frontmatter":7846,"imagePaths":7848},[],[],[],{"title":7834,"description":25,"summary":7835,"pubDate":7847,"source":2720,"url":7837,"thumbnail":7838},"Wed, 01 Mar 2023 00:00:00 GMT",[],"2023-03-01-how-hugging-face-accelerated-development-of-witty-works-writing-assistant.md","2023-03-02-ethical-guidelines-for-developing-the-diffusers-library",{"id":7850,"data":7852,"filePath":7858,"digest":7859,"rendered":7860,"legacyId":7868},{"title":7853,"description":25,"summary":7854,"pubDate":7855,"source":2720,"url":7856,"thumbnail":7857},"Ethical guidelines for developing the Diffusers library","Ethical guidelines for developing the Diffusers library We are on a journey to make our libraries mo...",["Date","2023-03-02T00:00:00.000Z"],"https://huggingface.co/blog/ethics-diffusers","https://huggingface.co/blog/assets/ethics-diffusers/thumbnail.png","src/content/posts/2023-03-02-ethical-guidelines-for-developing-the-diffusers-library.md","61842f2fd0325fee",{"html":25,"metadata":7861},{"headings":7862,"localImagePaths":7863,"remoteImagePaths":7864,"frontmatter":7865,"imagePaths":7867},[],[],[],{"title":7853,"description":25,"summary":7854,"pubDate":7866,"source":2720,"url":7856,"thumbnail":7857},"Thu, 02 Mar 2023 00:00:00 GMT",[],"2023-03-02-ethical-guidelines-for-developing-the-diffusers-library.md","2023-03-03-controlnet-in-diffusers",{"id":7869,"data":7871,"filePath":7877,"digest":7878,"rendered":7879,"legacyId":7887},{"title":7872,"description":25,"summary":7873,"pubDate":7874,"source":2720,"url":7875,"thumbnail":7876},"ControlNet in Diffusers 🧨","Ultra fast ControlNet with 🧨 Diffusers Ever since Stable Diffusion took the world by storm, people h...",["Date","2023-03-03T00:00:00.000Z"],"https://huggingface.co/blog/controlnet","https://huggingface.co/blog/assets/controlnet/thumbnail.png","src/content/posts/2023-03-03-controlnet-in-diffusers.md","1e1e4aeacdad7bc1",{"html":25,"metadata":7880},{"headings":7881,"localImagePaths":7882,"remoteImagePaths":7883,"frontmatter":7884,"imagePaths":7886},[],[],[],{"title":7872,"description":25,"summary":7873,"pubDate":7885,"source":2720,"url":7875,"thumbnail":7876},"Fri, 03 Mar 2023 00:00:00 GMT",[],"2023-03-03-controlnet-in-diffusers.md","2023-03-03-using-machine-learning-to-aid-survivors-and-race-through-time",{"id":7888,"data":7890,"filePath":7896,"digest":7897,"rendered":7898,"legacyId":7905},{"title":7891,"description":25,"summary":7892,"pubDate":7893,"source":2720,"url":7894,"thumbnail":7895},"Using Machine Learning to Aid Survivors and Race through Time","Using Machine Learning to Aid Survivors and Race through Time On February 6, 2023, earthquakes measu...",["Date","2023-03-03T00:00:00.000Z"],"https://huggingface.co/blog/using-ml-for-disasters","https://huggingface.co/blog/assets/using-ml-for-disasters/thumbnail.png","src/content/posts/2023-03-03-using-machine-learning-to-aid-survivors-and-race-through-time.md","f47b475fcaac793f",{"html":25,"metadata":7899},{"headings":7900,"localImagePaths":7901,"remoteImagePaths":7902,"frontmatter":7903,"imagePaths":7904},[],[],[],{"title":7891,"description":25,"summary":7892,"pubDate":7885,"source":2720,"url":7894,"thumbnail":7895},[],"2023-03-03-using-machine-learning-to-aid-survivors-and-race-through-time.md","2023-03-06-new-vit-and-align-models-from-kakao-brain",{"id":7906,"data":7908,"filePath":7914,"digest":7915,"rendered":7916,"legacyId":7924},{"title":7909,"description":25,"summary":7910,"pubDate":7911,"source":2720,"url":7912,"thumbnail":7913},"New ViT and ALIGN Models From Kakao Brain","Kakao Brain’s Open Source ViT, ALIGN, and the New COYO Text-Image Dataset Kakao Brain and Hugging Fa...",["Date","2023-03-06T00:00:00.000Z"],"https://huggingface.co/blog/vit-align","https://huggingface.co/blog//assets/132_vit_align/thumbnail.png","src/content/posts/2023-03-06-new-vit-and-align-models-from-kakao-brain.md","edf26652a687a73a",{"html":25,"metadata":7917},{"headings":7918,"localImagePaths":7919,"remoteImagePaths":7920,"frontmatter":7921,"imagePaths":7923},[],[],[],{"title":7909,"description":25,"summary":7910,"pubDate":7922,"source":2720,"url":7912,"thumbnail":7913},"Mon, 06 Mar 2023 00:00:00 GMT",[],"2023-03-06-new-vit-and-align-models-from-kakao-brain.md","2023-03-09-fine-tuning-20b-llms-with-rlhf-on-a-24gb-consumer-gpu",{"id":7925,"data":7927,"filePath":7933,"digest":7934,"rendered":7935,"legacyId":7943},{"title":7928,"description":25,"summary":7929,"pubDate":7930,"source":2720,"url":7931,"thumbnail":7932},"Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU","Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU We are excited to officially release the integ...",["Date","2023-03-09T00:00:00.000Z"],"https://huggingface.co/blog/trl-peft","https://huggingface.co/blog/trl-peft/assets/133_trl_peft/thumbnail.png","src/content/posts/2023-03-09-fine-tuning-20b-llms-with-rlhf-on-a-24gb-consumer-gpu.md","47c46fb3a5780f38",{"html":25,"metadata":7936},{"headings":7937,"localImagePaths":7938,"remoteImagePaths":7939,"frontmatter":7940,"imagePaths":7942},[],[],[],{"title":7928,"description":25,"summary":7929,"pubDate":7941,"source":2720,"url":7931,"thumbnail":7932},"Thu, 09 Mar 2023 00:00:00 GMT",[],"2023-03-09-fine-tuning-20b-llms-with-rlhf-on-a-24gb-consumer-gpu.md","2023-03-10-multivariate-probabilistic-time-series-forecasting-with-informer",{"id":7944,"data":7946,"filePath":7952,"digest":7953,"rendered":7954,"legacyId":7962},{"title":7947,"description":25,"summary":7948,"pubDate":7949,"source":2720,"url":7950,"thumbnail":7951},"Multivariate Probabilistic Time Series Forecasting with Informer","Multivariate Probabilistic Time Series Forecasting with Informer Introduction A few months ago we in...",["Date","2023-03-10T00:00:00.000Z"],"https://huggingface.co/blog/informer","https://huggingface.co/blog/assets/134_informer/thumbnail.png","src/content/posts/2023-03-10-multivariate-probabilistic-time-series-forecasting-with-informer.md","22c9204e651cf958",{"html":25,"metadata":7955},{"headings":7956,"localImagePaths":7957,"remoteImagePaths":7958,"frontmatter":7959,"imagePaths":7961},[],[],[],{"title":7947,"description":25,"summary":7948,"pubDate":7960,"source":2720,"url":7950,"thumbnail":7951},"Fri, 10 Mar 2023 00:00:00 GMT",[],"2023-03-10-multivariate-probabilistic-time-series-forecasting-with-informer.md","2023-03-14-filling-crucial-language-learning-gaps",{"id":7963,"data":7965,"filePath":7970,"digest":7971,"rendered":7972,"legacyId":7980},{"title":7966,"description":7967,"summary":7967,"pubDate":7968,"source":19,"url":7969,"thumbnail":21},"Filling crucial language learning gaps","GPT-4 deepens the conversation on Duolingo.",["Date","2023-03-14T07:00:00.000Z"],"https://openai.com/blog/duolingo","src/content/posts/2023-03-14-filling-crucial-language-learning-gaps.md","b941fb7e9f0bfde1",{"html":25,"metadata":7973},{"headings":7974,"localImagePaths":7975,"remoteImagePaths":7976,"frontmatter":7977,"imagePaths":7979},[],[],[],{"title":7966,"description":7967,"summary":7967,"pubDate":7978,"source":19,"url":7969,"thumbnail":21},"Tue, 14 Mar 2023 07:00:00 GMT",[],"2023-03-14-filling-crucial-language-learning-gaps.md","2023-03-14-gpt-4",{"id":7981,"data":7983,"filePath":7988,"digest":7989,"rendered":7990,"legacyId":7997},{"title":7984,"description":7985,"summary":7985,"pubDate":7986,"source":19,"url":7987,"thumbnail":21},"GPT-4","We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks.",["Date","2023-03-14T07:00:00.000Z"],"https://openai.com/blog/gpt-4-research","src/content/posts/2023-03-14-gpt-4.md","06119c4e0b9cd0c0",{"html":25,"metadata":7991},{"headings":7992,"localImagePaths":7993,"remoteImagePaths":7994,"frontmatter":7995,"imagePaths":7996},[],[],[],{"title":7984,"description":7985,"summary":7985,"pubDate":7978,"source":19,"url":7987,"thumbnail":21},[],"2023-03-14-gpt-4.md","2023-03-14-powering-virtual-education-for-the-classroom",{"id":7998,"data":8000,"filePath":8005,"digest":8006,"rendered":8007,"legacyId":8014},{"title":8001,"description":8002,"summary":8002,"pubDate":8003,"source":19,"url":8004,"thumbnail":21},"Powering virtual education for the classroom","Khan Academy explores the potential for GPT-4 in a limited pilot program.",["Date","2023-03-14T07:00:00.000Z"],"https://openai.com/blog/khan-academy","src/content/posts/2023-03-14-powering-virtual-education-for-the-classroom.md","79fe91a5e1d77bcb",{"html":25,"metadata":8008},{"headings":8009,"localImagePaths":8010,"remoteImagePaths":8011,"frontmatter":8012,"imagePaths":8013},[],[],[],{"title":8001,"description":8002,"summary":8002,"pubDate":7978,"source":19,"url":8004,"thumbnail":21},[],"2023-03-14-powering-virtual-education-for-the-classroom.md","2023-03-14-preserving-languages-for-the-future",{"id":8015,"data":8017,"filePath":8022,"digest":8023,"rendered":8024,"legacyId":8031},{"title":8018,"description":8019,"summary":8019,"pubDate":8020,"source":19,"url":8021,"thumbnail":21},"Preserving languages for the future","How Iceland is using GPT-4 to preserve its language.",["Date","2023-03-14T07:00:00.000Z"],"https://openai.com/blog/government-of-iceland","src/content/posts/2023-03-14-preserving-languages-for-the-future.md","41b0e1f6f437d20c",{"html":25,"metadata":8025},{"headings":8026,"localImagePaths":8027,"remoteImagePaths":8028,"frontmatter":8029,"imagePaths":8030},[],[],[],{"title":8018,"description":8019,"summary":8019,"pubDate":7978,"source":19,"url":8021,"thumbnail":21},[],"2023-03-14-preserving-languages-for-the-future.md","2023-03-14-streamlining-financial-solutions-for-safety-and-growth",{"id":8032,"data":8034,"filePath":8039,"digest":8040,"rendered":8041,"legacyId":8048},{"title":8035,"description":8036,"summary":8036,"pubDate":8037,"source":19,"url":8038,"thumbnail":21},"Streamlining financial solutions for safety and growth","Stripe leverages GPT-4 to streamline user experience and combat fraud.",["Date","2023-03-14T07:00:00.000Z"],"https://openai.com/blog/stripe","src/content/posts/2023-03-14-streamlining-financial-solutions-for-safety-and-growth.md","36f1e1c10551a7ad",{"html":25,"metadata":8042},{"headings":8043,"localImagePaths":8044,"remoteImagePaths":8045,"frontmatter":8046,"imagePaths":8047},[],[],[],{"title":8035,"description":8036,"summary":8036,"pubDate":7978,"source":19,"url":8038,"thumbnail":21},[],"2023-03-14-streamlining-financial-solutions-for-safety-and-growth.md","2023-03-14-transforming-visual-accessibility",{"id":8049,"data":8051,"filePath":8056,"digest":8057,"rendered":8058,"legacyId":8065},{"title":8052,"description":8053,"summary":8053,"pubDate":8054,"source":19,"url":8055,"thumbnail":21},"Transforming visual accessibility","Be My Eyes uses GPT-4 to transform visual accessibility.",["Date","2023-03-14T07:00:00.000Z"],"https://openai.com/blog/be-my-eyes","src/content/posts/2023-03-14-transforming-visual-accessibility.md","d71b9e927b90a505",{"html":25,"metadata":8059},{"headings":8060,"localImagePaths":8061,"remoteImagePaths":8062,"frontmatter":8063,"imagePaths":8064},[],[],[],{"title":8052,"description":8053,"summary":8053,"pubDate":7978,"source":19,"url":8055,"thumbnail":21},[],"2023-03-14-transforming-visual-accessibility.md","2023-03-17-gpts-are-gpts-an-early-look-at-the-labor-market-impact-potential-of-large-language-models",{"id":8066,"data":8068,"filePath":8072,"digest":8073,"rendered":8074,"legacyId":8082},{"title":8069,"description":25,"summary":25,"pubDate":8070,"source":19,"url":8071,"thumbnail":21},"GPTs are GPTs: An early look at the labor market impact potential of large language models",["Date","2023-03-17T07:00:00.000Z"],"https://openai.com/blog/gpts-are-gpts","src/content/posts/2023-03-17-gpts-are-gpts-an-early-look-at-the-labor-market-impact-potential-of-large-language-models.md","635239c94c8fd27a",{"html":25,"metadata":8075},{"headings":8076,"localImagePaths":8077,"remoteImagePaths":8078,"frontmatter":8079,"imagePaths":8081},[],[],[],{"title":8069,"description":25,"summary":25,"pubDate":8080,"source":19,"url":8071,"thumbnail":21},"Fri, 17 Mar 2023 07:00:00 GMT",[],"2023-03-17-gpts-are-gpts-an-early-look-at-the-labor-market-impact-potential-of-large-language-models.md","2023-03-23-chatgpt-plugins",{"id":8083,"data":8085,"filePath":8090,"digest":8091,"rendered":8092,"legacyId":8100},{"title":8086,"description":8087,"summary":8087,"pubDate":8088,"source":19,"url":8089,"thumbnail":21},"ChatGPT plugins","We’ve implemented initial support for plugins in ChatGPT. Plugins are tools designed specifically for language models with safety as a core principle, and help ChatGPT access up-to-date information, run computations, or use third-party services.",["Date","2023-03-23T07:00:00.000Z"],"https://openai.com/blog/chatgpt-plugins","src/content/posts/2023-03-23-chatgpt-plugins.md","beb581d5379aa62a",{"html":25,"metadata":8093},{"headings":8094,"localImagePaths":8095,"remoteImagePaths":8096,"frontmatter":8097,"imagePaths":8099},[],[],[],{"title":8086,"description":8087,"summary":8087,"pubDate":8098,"source":19,"url":8089,"thumbnail":21},"Thu, 23 Mar 2023 07:00:00 GMT",[],"2023-03-23-chatgpt-plugins.md","2023-03-23-jupyter-x-hugging-face",{"id":8101,"data":8103,"filePath":8109,"digest":8110,"rendered":8111,"legacyId":8119},{"title":8104,"description":25,"summary":8105,"pubDate":8106,"source":2720,"url":8107,"thumbnail":8108},"Jupyter X Hugging Face","Jupyter X Hugging Face We’re excited to announce improved support for Jupyter notebooks hosted on th...",["Date","2023-03-23T00:00:00.000Z"],"https://huggingface.co/blog/notebooks-hub","https://huggingface.co/blog/assets/135_notebooks-hub/before_after_notebook_rendering.png","src/content/posts/2023-03-23-jupyter-x-hugging-face.md","3171be6c8582ca7f",{"html":25,"metadata":8112},{"headings":8113,"localImagePaths":8114,"remoteImagePaths":8115,"frontmatter":8116,"imagePaths":8118},[],[],[],{"title":8104,"description":25,"summary":8105,"pubDate":8117,"source":2720,"url":8107,"thumbnail":8108},"Thu, 23 Mar 2023 00:00:00 GMT",[],"2023-03-23-jupyter-x-hugging-face.md","2023-03-24-march-20-chatgpt-outage-heres-what-happened",{"id":8120,"data":8122,"filePath":8127,"digest":8128,"rendered":8129,"legacyId":8137},{"title":8123,"description":8124,"summary":8124,"pubDate":8125,"source":19,"url":8126,"thumbnail":21},"March 20 ChatGPT outage: Here’s what happened","An update on our findings, the actions we’ve taken, and technical details of the bug.",["Date","2023-03-24T07:00:00.000Z"],"https://openai.com/blog/march-20-chatgpt-outage","src/content/posts/2023-03-24-march-20-chatgpt-outage-heres-what-happened.md","21e653dfc0cdfe32",{"html":25,"metadata":8130},{"headings":8131,"localImagePaths":8132,"remoteImagePaths":8133,"frontmatter":8134,"imagePaths":8136},[],[],[],{"title":8123,"description":8124,"summary":8124,"pubDate":8135,"source":19,"url":8126,"thumbnail":21},"Fri, 24 Mar 2023 07:00:00 GMT",[],"2023-03-24-march-20-chatgpt-outage-heres-what-happened.md","2023-03-24-train-your-controlnet-with-diffusers",{"id":8138,"data":8140,"filePath":8146,"digest":8147,"rendered":8148,"legacyId":8156},{"title":8141,"description":25,"summary":8142,"pubDate":8143,"source":2720,"url":8144,"thumbnail":8145},"Train your ControlNet with diffusers","Train your ControlNet with diffusers 🧨 Introduction ControlNet is a neural network structure that al...",["Date","2023-03-24T00:00:00.000Z"],"https://huggingface.co/blog/train-your-controlnet","https://huggingface.co/blog/assets/136_train-your-controlnet/thumbnail.png","src/content/posts/2023-03-24-train-your-controlnet-with-diffusers.md","47cfc5b518df76b9",{"html":25,"metadata":8149},{"headings":8150,"localImagePaths":8151,"remoteImagePaths":8152,"frontmatter":8153,"imagePaths":8155},[],[],[],{"title":8141,"description":25,"summary":8142,"pubDate":8154,"source":2720,"url":8144,"thumbnail":8145},"Fri, 24 Mar 2023 00:00:00 GMT",[],"2023-03-24-train-your-controlnet-with-diffusers.md","2023-03-27-federated-learning-using-hugging-face-and-flower",{"id":8157,"data":8159,"filePath":8165,"digest":8166,"rendered":8167,"legacyId":8175},{"title":8160,"description":25,"summary":8161,"pubDate":8162,"source":2720,"url":8163,"thumbnail":8164},"Federated Learning using Hugging Face and Flower","Federated Learning using Hugging Face and Flower This tutorial will show how to leverage Hugging Fac...",["Date","2023-03-27T00:00:00.000Z"],"https://huggingface.co/blog/fl-with-flower","https://huggingface.co/blog/assets/fl-with-flower/thumbnail.png","src/content/posts/2023-03-27-federated-learning-using-hugging-face-and-flower.md","87f4fffbce42fd17",{"html":25,"metadata":8168},{"headings":8169,"localImagePaths":8170,"remoteImagePaths":8171,"frontmatter":8172,"imagePaths":8174},[],[],[],{"title":8160,"description":25,"summary":8161,"pubDate":8173,"source":2720,"url":8163,"thumbnail":8164},"Mon, 27 Mar 2023 00:00:00 GMT",[],"2023-03-27-federated-learning-using-hugging-face-and-flower.md","2023-03-28-accelerating-stable-diffusion-inference-on-intel-cpus",{"id":8176,"data":8178,"filePath":8184,"digest":8185,"rendered":8186,"legacyId":8194},{"title":8179,"description":25,"summary":8180,"pubDate":8181,"source":2720,"url":8182,"thumbnail":8183},"Accelerating Stable Diffusion Inference on Intel CPUs","Accelerating Stable Diffusion Inference on Intel CPUs Recently, we introduced the latest generation ...",["Date","2023-03-28T00:00:00.000Z"],"https://huggingface.co/blog/stable-diffusion-inference-intel","https://huggingface.co/blog/assets/136_stable_diffusion_inference_intel/01.png","src/content/posts/2023-03-28-accelerating-stable-diffusion-inference-on-intel-cpus.md","40512688d98e4a1b",{"html":25,"metadata":8187},{"headings":8188,"localImagePaths":8189,"remoteImagePaths":8190,"frontmatter":8191,"imagePaths":8193},[],[],[],{"title":8179,"description":25,"summary":8180,"pubDate":8192,"source":2720,"url":8182,"thumbnail":8183},"Tue, 28 Mar 2023 00:00:00 GMT",[],"2023-03-28-accelerating-stable-diffusion-inference-on-intel-cpus.md","2023-03-28-fast-inference-on-large-language-models-bloomz-on-habana-gaudi2-accelerator",{"id":8195,"data":8197,"filePath":8203,"digest":8204,"rendered":8205,"legacyId":8212},{"title":8198,"description":25,"summary":8199,"pubDate":8200,"source":2720,"url":8201,"thumbnail":8202},"Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator","Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator This article will show ...",["Date","2023-03-28T00:00:00.000Z"],"https://huggingface.co/blog/habana-gaudi-2-bloom","https://huggingface.co/blog/assets/habana-gaudi-2-bloom/thumbnail.png","src/content/posts/2023-03-28-fast-inference-on-large-language-models-bloomz-on-habana-gaudi2-accelerator.md","758167c449818c10",{"html":25,"metadata":8206},{"headings":8207,"localImagePaths":8208,"remoteImagePaths":8209,"frontmatter":8210,"imagePaths":8211},[],[],[],{"title":8198,"description":25,"summary":8199,"pubDate":8192,"source":2720,"url":8201,"thumbnail":8202},[],"2023-03-28-fast-inference-on-large-language-models-bloomz-on-habana-gaudi2-accelerator.md","2023-03-30-ethics-and-society-newsletter-3-ethical-openness-at-hugging-face",{"id":8213,"data":8215,"filePath":8221,"digest":8222,"rendered":8223,"legacyId":8231},{"title":8216,"description":25,"summary":8217,"pubDate":8218,"source":2720,"url":8219,"thumbnail":8220},"Ethics and Society Newsletter #3: Ethical Openness at Hugging Face","Ethics and Society Newsletter #3: Ethical Openness at Hugging Face Mission: Open and Good ML In our ...",["Date","2023-03-30T00:00:00.000Z"],"https://huggingface.co/blog/ethics-soc-3","https://huggingface.co/blog/assets/137_ethics_soc_3/ethics_3_thumbnail.png","src/content/posts/2023-03-30-ethics-and-society-newsletter-3-ethical-openness-at-hugging-face.md","eb22f3b31370dda1",{"html":25,"metadata":8224},{"headings":8225,"localImagePaths":8226,"remoteImagePaths":8227,"frontmatter":8228,"imagePaths":8230},[],[],[],{"title":8216,"description":25,"summary":8217,"pubDate":8229,"source":2720,"url":8219,"thumbnail":8220},"Thu, 30 Mar 2023 00:00:00 GMT",[],"2023-03-30-ethics-and-society-newsletter-3-ethical-openness-at-hugging-face.md","2023-04-05-our-approach-to-ai-safety",{"id":8232,"data":8234,"filePath":8239,"digest":8240,"rendered":8241,"legacyId":8249},{"title":8235,"description":8236,"summary":8236,"pubDate":8237,"source":19,"url":8238,"thumbnail":21},"Our approach to AI safety","Ensuring that AI systems are built, deployed, and used safely is critical to our mission.",["Date","2023-04-05T07:00:00.000Z"],"https://openai.com/blog/our-approach-to-ai-safety","src/content/posts/2023-04-05-our-approach-to-ai-safety.md","dd8a9880033ca899",{"html":25,"metadata":8242},{"headings":8243,"localImagePaths":8244,"remoteImagePaths":8245,"frontmatter":8246,"imagePaths":8248},[],[],[],{"title":8235,"description":8236,"summary":8236,"pubDate":8247,"source":19,"url":8238,"thumbnail":21},"Wed, 05 Apr 2023 07:00:00 GMT",[],"2023-04-05-our-approach-to-ai-safety.md","2023-04-05-stackllama-a-hands-on-guide-to-train-llama-with-rlhf",{"id":8250,"data":8252,"filePath":8258,"digest":8259,"rendered":8260,"legacyId":8268},{"title":8253,"description":25,"summary":8254,"pubDate":8255,"source":2720,"url":8256,"thumbnail":8257},"StackLLaMA: A hands-on guide to train LLaMA with RLHF","StackLLaMA: A hands-on guide to train LLaMA with RLHF Models such as ChatGPT, GPT-4, and Claude are ...",["Date","2023-04-05T00:00:00.000Z"],"https://huggingface.co/blog/stackllama","https://huggingface.co/blog/assets/138_stackllama/thumbnail.png","src/content/posts/2023-04-05-stackllama-a-hands-on-guide-to-train-llama-with-rlhf.md","bb1f2361c3c4ec56",{"html":25,"metadata":8261},{"headings":8262,"localImagePaths":8263,"remoteImagePaths":8264,"frontmatter":8265,"imagePaths":8267},[],[],[],{"title":8253,"description":25,"summary":8254,"pubDate":8266,"source":2720,"url":8256,"thumbnail":8257},"Wed, 05 Apr 2023 00:00:00 GMT",[],"2023-04-05-stackllama-a-hands-on-guide-to-train-llama-with-rlhf.md","2023-04-06-snorkel-ai-x-hugging-face-unlock-foundation-models-for-enterprises",{"id":8269,"data":8271,"filePath":8277,"digest":8278,"rendered":8279,"legacyId":8287},{"title":8272,"description":25,"summary":8273,"pubDate":8274,"source":2720,"url":8275,"thumbnail":8276},"Snorkel AI x Hugging Face: unlock foundation models for enterprises","Snorkel AI x Hugging Face: unlock foundation models for enterprises This article is a cross-post fro...",["Date","2023-04-06T00:00:00.000Z"],"https://huggingface.co/blog/snorkel-case-study","https://huggingface.co/blog/assets/78_ml_director_insights/snorkel.png","src/content/posts/2023-04-06-snorkel-ai-x-hugging-face-unlock-foundation-models-for-enterprises.md","f3b739e300f07603",{"html":25,"metadata":8280},{"headings":8281,"localImagePaths":8282,"remoteImagePaths":8283,"frontmatter":8284,"imagePaths":8286},[],[],[],{"title":8272,"description":25,"summary":8273,"pubDate":8285,"source":2720,"url":8275,"thumbnail":8276},"Thu, 06 Apr 2023 00:00:00 GMT",[],"2023-04-06-snorkel-ai-x-hugging-face-unlock-foundation-models-for-enterprises.md","2023-04-11-announcing-openais-bug-bounty-program",{"id":8288,"data":8290,"filePath":8295,"digest":8296,"rendered":8297,"legacyId":8305},{"title":8291,"description":8292,"summary":8292,"pubDate":8293,"source":19,"url":8294,"thumbnail":21},"Announcing OpenAI’s Bug Bounty Program","This initiative is essential to our commitment to develop safe and advanced AI. As we create technology and services that are secure, reliable, and trustworthy, we need your help.",["Date","2023-04-11T07:00:00.000Z"],"https://openai.com/blog/bug-bounty-program","src/content/posts/2023-04-11-announcing-openais-bug-bounty-program.md","b74be086403e31e2",{"html":25,"metadata":8298},{"headings":8299,"localImagePaths":8300,"remoteImagePaths":8301,"frontmatter":8302,"imagePaths":8304},[],[],[],{"title":8291,"description":8292,"summary":8292,"pubDate":8303,"source":19,"url":8294,"thumbnail":21},"Tue, 11 Apr 2023 07:00:00 GMT",[],"2023-04-11-announcing-openais-bug-bounty-program.md","2023-04-12-creating-privacy-preserving-ai-with-substra",{"id":8306,"data":8308,"filePath":8314,"digest":8315,"rendered":8316,"legacyId":8324},{"title":8309,"description":25,"summary":8310,"pubDate":8311,"source":2720,"url":8312,"thumbnail":8313},"Creating Privacy Preserving AI with Substra","Creating Privacy Preserving AI with Substra With the recent rise of generative techniques, machine l...",["Date","2023-04-12T00:00:00.000Z"],"https://huggingface.co/blog/owkin-substra","https://huggingface.co/blog/assets/139_owkin-substra/thumbnail.png","src/content/posts/2023-04-12-creating-privacy-preserving-ai-with-substra.md","53f7139d1ae655ef",{"html":25,"metadata":8317},{"headings":8318,"localImagePaths":8319,"remoteImagePaths":8320,"frontmatter":8321,"imagePaths":8323},[],[],[],{"title":8309,"description":25,"summary":8310,"pubDate":8322,"source":2720,"url":8312,"thumbnail":8313},"Wed, 12 Apr 2023 00:00:00 GMT",[],"2023-04-12-creating-privacy-preserving-ai-with-substra.md","2023-04-17-accelerating-hugging-face-transformers-with-aws-inferentia2",{"id":8325,"data":8327,"filePath":8333,"digest":8334,"rendered":8335,"legacyId":8343},{"title":8328,"description":25,"summary":8329,"pubDate":8330,"source":2720,"url":8331,"thumbnail":8332},"Accelerating Hugging Face Transformers with AWS Inferentia2","Accelerating Hugging Face Transformers with AWS Inferentia2 In the last five years, Transformer mode...",["Date","2023-04-17T00:00:00.000Z"],"https://huggingface.co/blog/accelerate-transformers-with-inferentia2","https://huggingface.co/blog/assets/140_accelerate_transformers_with_inferentia2/thumbnail.png","src/content/posts/2023-04-17-accelerating-hugging-face-transformers-with-aws-inferentia2.md","926fbb7144c287a2",{"html":25,"metadata":8336},{"headings":8337,"localImagePaths":8338,"remoteImagePaths":8339,"frontmatter":8340,"imagePaths":8342},[],[],[],{"title":8328,"description":25,"summary":8329,"pubDate":8341,"source":2720,"url":8331,"thumbnail":8332},"Mon, 17 Apr 2023 00:00:00 GMT",[],"2023-04-17-accelerating-hugging-face-transformers-with-aws-inferentia2.md","2023-04-14-graph-classification-with-transformers",{"id":8344,"data":8346,"filePath":8352,"digest":8353,"rendered":8354,"legacyId":8362},{"title":8347,"description":25,"summary":8348,"pubDate":8349,"source":2720,"url":8350,"thumbnail":8351},"Graph Classification with Transformers","Graph classification with Transformers In the previous blog, we explored some of the theoretical asp...",["Date","2023-04-14T00:00:00.000Z"],"https://huggingface.co/blog/graphml-classification","https://huggingface.co/blog/assets/125_intro-to-graphml/thumbnail_classification.png","src/content/posts/2023-04-14-graph-classification-with-transformers.md","3fe63b7007cb6dd1",{"html":25,"metadata":8355},{"headings":8356,"localImagePaths":8357,"remoteImagePaths":8358,"frontmatter":8359,"imagePaths":8361},[],[],[],{"title":8347,"description":25,"summary":8348,"pubDate":8360,"source":2720,"url":8350,"thumbnail":8351},"Fri, 14 Apr 2023 00:00:00 GMT",[],"2023-04-14-graph-classification-with-transformers.md","2023-04-20-announcing-google-deepmind",{"id":8363,"data":8365,"filePath":8371,"digest":8372,"rendered":8373,"legacyId":8381},{"title":8366,"description":8367,"summary":8367,"pubDate":8368,"source":6423,"url":8369,"thumbnail":8370},"Announcing Google DeepMind","DeepMind and the Brain team from Google Research will join forces to accelerate progress towards a world in which AI helps solve the biggest challenges facing humanity.",["Date","2023-04-20T00:00:00.000Z"],"https://deepmind.google/discover/blog/announcing-google-deepmind/","https://lh3.googleusercontent.com/MNdJdEO1VpepmU25h9OTpnMr9hxe6NScc1ZWlerWf5WtOYMnHETsPEWKqvG36zQv5CGflTOHAKG_JbADpmLrh8Mrpa91B95U6bs0isMSbTUerT-qT38=w1200-h630-n-nu","src/content/posts/2023-04-20-announcing-google-deepmind.md","283c061d897cf1a9",{"html":25,"metadata":8374},{"headings":8375,"localImagePaths":8376,"remoteImagePaths":8377,"frontmatter":8378,"imagePaths":8380},[],[],[],{"title":8366,"description":8367,"summary":8367,"pubDate":8379,"source":6423,"url":8369,"thumbnail":8370},"Thu, 20 Apr 2023 00:00:00 +0000",[],"2023-04-20-announcing-google-deepmind.md","2023-04-21-how-to-host-a-unity-game-in-a-space",{"id":8382,"data":8384,"filePath":8390,"digest":8391,"rendered":8392,"legacyId":8400},{"title":8385,"description":25,"summary":8386,"pubDate":8387,"source":2720,"url":8388,"thumbnail":8389},"How to host a Unity game in a Space","How to host a Unity game in a Space Did you know you can host a Unity game in a Hugging Face Space? ...",["Date","2023-04-21T00:00:00.000Z"],"https://huggingface.co/blog/unity-in-spaces","https://huggingface.co/blog/assets/124_ml-for-games/unity-in-spaces-thumbnail.png","src/content/posts/2023-04-21-how-to-host-a-unity-game-in-a-space.md","d76396ba56384979",{"html":25,"metadata":8393},{"headings":8394,"localImagePaths":8395,"remoteImagePaths":8396,"frontmatter":8397,"imagePaths":8399},[],[],[],{"title":8385,"description":25,"summary":8386,"pubDate":8398,"source":2720,"url":8388,"thumbnail":8389},"Fri, 21 Apr 2023 00:00:00 GMT",[],"2023-04-21-how-to-host-a-unity-game-in-a-space.md","2023-04-24-introducing-huggingface-blog-for-chinese-speakers-fostering-collaboration-with-the-chinese-ai-community",{"id":8401,"data":8403,"filePath":8409,"digest":8410,"rendered":8411,"legacyId":8419},{"title":8404,"description":25,"summary":8405,"pubDate":8406,"source":2720,"url":8407,"thumbnail":8408},"Introducing HuggingFace blog for Chinese speakers: Fostering Collaboration with the Chinese AI community","Introducing HuggingFace blog for Chinese speakers: Fostering Collaboration with the Chinese AI commu...",["Date","2023-04-24T00:00:00.000Z"],"https://huggingface.co/blog/chinese-language-blog","https://huggingface.co/blog/assets/chinese-language-blog/thumbnail.png","src/content/posts/2023-04-24-introducing-huggingface-blog-for-chinese-speakers-fostering-collaboration-with-the-chinese-ai-community.md","ef5bf059bc4c9696",{"html":25,"metadata":8412},{"headings":8413,"localImagePaths":8414,"remoteImagePaths":8415,"frontmatter":8416,"imagePaths":8418},[],[],[],{"title":8404,"description":25,"summary":8405,"pubDate":8417,"source":2720,"url":8407,"thumbnail":8408},"Mon, 24 Apr 2023 00:00:00 GMT",[],"2023-04-24-introducing-huggingface-blog-for-chinese-speakers-fostering-collaboration-with-the-chinese-ai-community.md","2023-04-24-how-can-we-build-human-values-into-ai",{"id":8420,"data":8422,"filePath":8428,"digest":8429,"rendered":8430,"legacyId":8438},{"title":8423,"description":8424,"summary":8424,"pubDate":8425,"source":6423,"url":8426,"thumbnail":8427},"How can we build human values into AI?","Drawing from philosophy to identify fair principles for ethical AI...",["Date","2023-04-24T00:00:00.000Z"],"https://deepmind.google/discover/blog/how-can-we-build-human-values-into-ai/","https://lh3.googleusercontent.com/jXiO9PpMnNRhxz3kyDP97SVi5c68dQie9V4AHbH_I0Py0EJoOl0fyPhoVljUGETrNmj3BhbAEahqmsq4r-33IgLgGhsuUhN2p384-d8B_vc4asHWB6Q=w1200-h630-n-nu","src/content/posts/2023-04-24-how-can-we-build-human-values-into-ai.md","e6f48b5e718862af",{"html":25,"metadata":8431},{"headings":8432,"localImagePaths":8433,"remoteImagePaths":8434,"frontmatter":8435,"imagePaths":8437},[],[],[],{"title":8423,"description":8424,"summary":8424,"pubDate":8436,"source":6423,"url":8426,"thumbnail":8427},"Mon, 24 Apr 2023 00:00:00 +0000",[],"2023-04-24-how-can-we-build-human-values-into-ai.md","2023-04-25-new-ways-to-manage-your-data-in-chatgpt",{"id":8439,"data":8441,"filePath":8446,"digest":8447,"rendered":8448,"legacyId":8456},{"title":8442,"description":8443,"summary":8443,"pubDate":8444,"source":19,"url":8445,"thumbnail":21},"New ways to manage your data in ChatGPT","ChatGPT users can now turn off chat history, allowing you to choose which conversations can be used to train our models.",["Date","2023-04-25T07:00:00.000Z"],"https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt","src/content/posts/2023-04-25-new-ways-to-manage-your-data-in-chatgpt.md","392b6df47cc92461",{"html":25,"metadata":8449},{"headings":8450,"localImagePaths":8451,"remoteImagePaths":8452,"frontmatter":8453,"imagePaths":8455},[],[],[],{"title":8442,"description":8443,"summary":8443,"pubDate":8454,"source":19,"url":8445,"thumbnail":21},"Tue, 25 Apr 2023 07:00:00 GMT",[],"2023-04-25-new-ways-to-manage-your-data-in-chatgpt.md","2023-04-26-running-if-with-diffusers-on-a-free-tier-google-colab",{"id":8457,"data":8459,"filePath":8465,"digest":8466,"rendered":8467,"legacyId":8475},{"title":8460,"description":25,"summary":8461,"pubDate":8462,"source":2720,"url":8463,"thumbnail":8464},"Running IF with 🧨 diffusers on a Free Tier Google Colab","Running IF with 🧨 diffusers on a Free Tier Google Colab TL;DR: We show how to run one of the most po...",["Date","2023-04-26T00:00:00.000Z"],"https://huggingface.co/blog/if","https://huggingface.co/blog/assets/if/thumbnail.jpg","src/content/posts/2023-04-26-running-if-with-diffusers-on-a-free-tier-google-colab.md","d7385b6bddde580f",{"html":25,"metadata":8468},{"headings":8469,"localImagePaths":8470,"remoteImagePaths":8471,"frontmatter":8472,"imagePaths":8474},[],[],[],{"title":8460,"description":25,"summary":8461,"pubDate":8473,"source":2720,"url":8463,"thumbnail":8464},"Wed, 26 Apr 2023 00:00:00 GMT",[],"2023-04-26-running-if-with-diffusers-on-a-free-tier-google-colab.md","2023-04-27-deepminds-latest-research-at-iclr-2023",{"id":8476,"data":8478,"filePath":8484,"digest":8485,"rendered":8486,"legacyId":8494},{"title":8479,"description":8480,"summary":8480,"pubDate":8481,"source":6423,"url":8482,"thumbnail":8483},"DeepMind’s latest research at ICLR 2023","Next week marks the start of the 11th International Conference on Learning Representations (ICLR), taking place 1-5 May in Kigali, Rwanda. This will be the first major artificial intelligence (AI) conference to be hosted in Africa and the first in-person event since the start of the pandemic. Researchers from around the world will gather to share their cutting-edge work in deep learning spanning the fields of AI, statistics and data science, and applications including machine vision, gaming and robotics. We’re proud to support the conference as a Diamond sponsor and DEI champion.",["Date","2023-04-27T00:00:00.000Z"],"https://deepmind.google/discover/blog/deepminds-latest-research-at-iclr-2023/","https://lh3.googleusercontent.com/PH30vPBZLwZXlSFrALk6AT507Qn70LSPLW5a89vsRhDdkje_xaPGvNE2UrhOBy8Gkaasn-FVRuDWlPhEPntzw02gxSAEPygt7djS4URtQZJuaLPw3w=w1200-h630-n-nu","src/content/posts/2023-04-27-deepminds-latest-research-at-iclr-2023.md","98da470332edb467",{"html":25,"metadata":8487},{"headings":8488,"localImagePaths":8489,"remoteImagePaths":8490,"frontmatter":8491,"imagePaths":8493},[],[],[],{"title":8479,"description":8480,"summary":8480,"pubDate":8492,"source":6423,"url":8482,"thumbnail":8483},"Thu, 27 Apr 2023 00:00:00 +0000",[],"2023-04-27-deepminds-latest-research-at-iclr-2023.md","2023-04-27-training-a-language-model-with-transformers-using-tensorflow-and-tpus",{"id":8495,"data":8497,"filePath":8503,"digest":8504,"rendered":8505,"legacyId":8513},{"title":8498,"description":25,"summary":8499,"pubDate":8500,"source":2720,"url":8501,"thumbnail":8502},"Training a language model with 🤗 Transformers using TensorFlow and TPUs","Training a language model with 🤗 Transformers using TensorFlow and TPUs Introduction TPU training is...",["Date","2023-04-27T00:00:00.000Z"],"https://huggingface.co/blog/tf_tpu","https://huggingface.co/blog/assets/tf_tpu_training/thumbnail.png","src/content/posts/2023-04-27-training-a-language-model-with-transformers-using-tensorflow-and-tpus.md","e273cda219851d72",{"html":25,"metadata":8506},{"headings":8507,"localImagePaths":8508,"remoteImagePaths":8509,"frontmatter":8510,"imagePaths":8512},[],[],[],{"title":8498,"description":25,"summary":8499,"pubDate":8511,"source":2720,"url":8501,"thumbnail":8502},"Thu, 27 Apr 2023 00:00:00 GMT",[],"2023-04-27-training-a-language-model-with-transformers-using-tensorflow-and-tpus.md","2023-04-26-databricks-hugging-face-up-to-40-faster-training-and-tuning-of-large-language-models",{"id":8514,"data":8516,"filePath":8522,"digest":8523,"rendered":8524,"legacyId":8531},{"title":8517,"description":25,"summary":8518,"pubDate":8519,"source":2720,"url":8520,"thumbnail":8521},"Databricks ❤️ Hugging Face: up to 40% faster training and tuning of Large Language Models","Databricks ❤️ Hugging Face: up to 40% faster training and tuning of Large Language Models Generative...",["Date","2023-04-26T00:00:00.000Z"],"https://huggingface.co/blog/databricks-case-study","https://huggingface.co/blog/assets/78_ml_director_insights/databricks.png","src/content/posts/2023-04-26-databricks-hugging-face-up-to-40-faster-training-and-tuning-of-large-language-models.md","1eb4e79391eb9bcc",{"html":25,"metadata":8525},{"headings":8526,"localImagePaths":8527,"remoteImagePaths":8528,"frontmatter":8529,"imagePaths":8530},[],[],[],{"title":8517,"description":25,"summary":8518,"pubDate":8473,"source":2720,"url":8520,"thumbnail":8521},[],"2023-04-26-databricks-hugging-face-up-to-40-faster-training-and-tuning-of-large-language-models.md","2023-05-04-starcoder-a-state-of-the-art-llm-for-code",{"id":8532,"data":8534,"filePath":8540,"digest":8541,"rendered":8542,"legacyId":8550},{"title":8535,"description":25,"summary":8536,"pubDate":8537,"source":2720,"url":8538,"thumbnail":8539},"StarCoder: A State-of-the-Art LLM for Code","StarCoder: A State-of-the-Art LLM for Code Introducing StarCoder StarCoder and StarCoderBase are Lar...",["Date","2023-05-04T00:00:00.000Z"],"https://huggingface.co/blog/starcoder","https://huggingface.co/blog/assets/141_starcoder/starcoder_thumbnail.png","src/content/posts/2023-05-04-starcoder-a-state-of-the-art-llm-for-code.md","ef4f859c4e6f002d",{"html":25,"metadata":8543},{"headings":8544,"localImagePaths":8545,"remoteImagePaths":8546,"frontmatter":8547,"imagePaths":8549},[],[],[],{"title":8535,"description":25,"summary":8536,"pubDate":8548,"source":2720,"url":8538,"thumbnail":8539},"Thu, 04 May 2023 00:00:00 GMT",[],"2023-05-04-starcoder-a-state-of-the-art-llm-for-code.md","2023-05-01-how-to-install-and-use-the-hugging-face-unity-api",{"id":8551,"data":8553,"filePath":8559,"digest":8560,"rendered":8561,"legacyId":8569},{"title":8554,"description":25,"summary":8555,"pubDate":8556,"source":2720,"url":8557,"thumbnail":8558},"How to Install and Use the Hugging Face Unity API","How to Install and Use the Hugging Face Unity API The Hugging Face Unity API is an easy-to-use integ...",["Date","2023-05-01T00:00:00.000Z"],"https://huggingface.co/blog/unity-api","https://huggingface.co/blog/assets/124_ml-for-games/unity-api-thumbnail.png","src/content/posts/2023-05-01-how-to-install-and-use-the-hugging-face-unity-api.md","4b971b894d6b9d3f",{"html":25,"metadata":8562},{"headings":8563,"localImagePaths":8564,"remoteImagePaths":8565,"frontmatter":8566,"imagePaths":8568},[],[],[],{"title":8554,"description":25,"summary":8555,"pubDate":8567,"source":2720,"url":8557,"thumbnail":8558},"Mon, 01 May 2023 00:00:00 GMT",[],"2023-05-01-how-to-install-and-use-the-hugging-face-unity-api.md","2023-05-08-a-dive-into-text-to-video-models",{"id":8570,"data":8572,"filePath":8578,"digest":8579,"rendered":8580,"legacyId":8588},{"title":8573,"description":25,"summary":8574,"pubDate":8575,"source":2720,"url":8576,"thumbnail":8577},"A Dive into Text-to-Video Models","Text-to-Video: The Task, Challenges and the Current State Video samples generated with ModelScope. T...",["Date","2023-05-08T00:00:00.000Z"],"https://huggingface.co/blog/text-to-video","https://huggingface.co/blog/assets/140_text-to-video/thumbnail.png","src/content/posts/2023-05-08-a-dive-into-text-to-video-models.md","61edc3d6e7f1e877",{"html":25,"metadata":8581},{"headings":8582,"localImagePaths":8583,"remoteImagePaths":8584,"frontmatter":8585,"imagePaths":8587},[],[],[],{"title":8573,"description":25,"summary":8574,"pubDate":8586,"source":2720,"url":8576,"thumbnail":8577},"Mon, 08 May 2023 00:00:00 GMT",[],"2023-05-08-a-dive-into-text-to-video-models.md","2023-05-09-creating-a-coding-assistant-with-starcoder",{"id":8589,"data":8591,"filePath":8597,"digest":8598,"rendered":8599,"legacyId":8607},{"title":8592,"description":25,"summary":8593,"pubDate":8594,"source":2720,"url":8595,"thumbnail":8596},"Creating a Coding Assistant with StarCoder","Creating a Coding Assistant with StarCoder If you’re a software developer, chances are that you’ve u...",["Date","2023-05-09T00:00:00.000Z"],"https://huggingface.co/blog/starchat-alpha","https://huggingface.co/blog/assets/starchat_alpha/thumbnail.png","src/content/posts/2023-05-09-creating-a-coding-assistant-with-starcoder.md","29d4c25242ee5bdf",{"html":25,"metadata":8600},{"headings":8601,"localImagePaths":8602,"remoteImagePaths":8603,"frontmatter":8604,"imagePaths":8606},[],[],[],{"title":8592,"description":25,"summary":8593,"pubDate":8605,"source":2720,"url":8595,"thumbnail":8596},"Tue, 09 May 2023 00:00:00 GMT",[],"2023-05-09-creating-a-coding-assistant-with-starcoder.md","2023-05-11-assisted-generation-a-new-direction-toward-low-latency-text-generation",{"id":8608,"data":8610,"filePath":8616,"digest":8617,"rendered":8618,"legacyId":8626},{"title":8611,"description":25,"summary":8612,"pubDate":8613,"source":2720,"url":8614,"thumbnail":8615},"Assisted Generation: a new direction toward low-latency text generation","Assisted Generation: a new direction toward low-latency text generation Large language models are al...",["Date","2023-05-11T00:00:00.000Z"],"https://huggingface.co/blog/assisted-generation","https://huggingface.co/blog/assets/assisted-generation/thumbnail.png","src/content/posts/2023-05-11-assisted-generation-a-new-direction-toward-low-latency-text-generation.md","bfa4e3b6583457b4",{"html":25,"metadata":8619},{"headings":8620,"localImagePaths":8621,"remoteImagePaths":8622,"frontmatter":8623,"imagePaths":8625},[],[],[],{"title":8611,"description":25,"summary":8612,"pubDate":8624,"source":2720,"url":8614,"thumbnail":8615},"Thu, 11 May 2023 00:00:00 GMT",[],"2023-05-11-assisted-generation-a-new-direction-toward-low-latency-text-generation.md","2023-05-09-language-models-can-explain-neurons-in-language-models",{"id":8627,"data":8629,"filePath":8634,"digest":8635,"rendered":8636,"legacyId":8644},{"title":8630,"description":8631,"summary":8631,"pubDate":8632,"source":19,"url":8633,"thumbnail":21},"Language models can explain neurons in language models","We use GPT-4 to automatically write explanations for the behavior of neurons in large language models and to score those explanations. We release a dataset of these (imperfect) explanations and scores for every neuron in GPT-2.",["Date","2023-05-09T07:00:00.000Z"],"https://openai.com/blog/language-models-can-explain-neurons-in-language-models","src/content/posts/2023-05-09-language-models-can-explain-neurons-in-language-models.md","e93d1997da5710fa",{"html":25,"metadata":8637},{"headings":8638,"localImagePaths":8639,"remoteImagePaths":8640,"frontmatter":8641,"imagePaths":8643},[],[],[],{"title":8630,"description":8631,"summary":8631,"pubDate":8642,"source":19,"url":8633,"thumbnail":21},"Tue, 09 May 2023 07:00:00 GMT",[],"2023-05-09-language-models-can-explain-neurons-in-language-models.md","2023-05-15-hugging-face-selected-for-the-french-data-protection-agency-enhanced-support-program",{"id":8645,"data":8647,"filePath":8653,"digest":8654,"rendered":8655,"legacyId":8663},{"title":8648,"description":25,"summary":8649,"pubDate":8650,"source":2720,"url":8651,"thumbnail":8652},"Hugging Face Selected for the French Data Protection Agency Enhanced Support Program","Hugging Face Selected for the French Data Protection Agency Enhanced Support Program This blog post ...",["Date","2023-05-15T00:00:00.000Z"],"https://huggingface.co/blog/cnil","https://huggingface.co/blog/assets/146_cnil-accompaniment/logo.png","src/content/posts/2023-05-15-hugging-face-selected-for-the-french-data-protection-agency-enhanced-support-program.md","5ff0c50d0e7c821c",{"html":25,"metadata":8656},{"headings":8657,"localImagePaths":8658,"remoteImagePaths":8659,"frontmatter":8660,"imagePaths":8662},[],[],[],{"title":8648,"description":25,"summary":8649,"pubDate":8661,"source":2720,"url":8651,"thumbnail":8652},"Mon, 15 May 2023 00:00:00 GMT",[],"2023-05-15-hugging-face-selected-for-the-french-data-protection-agency-enhanced-support-program.md","2023-05-15-introducing-rwkv-an-rnn-with-the-advantages-of-a-transformer",{"id":8664,"data":8666,"filePath":8672,"digest":8673,"rendered":8674,"legacyId":8681},{"title":8667,"description":25,"summary":8668,"pubDate":8669,"source":2720,"url":8670,"thumbnail":8671},"Introducing RWKV — An RNN with the advantages of a transformer","Introducing RWKV - An RNN with the advantages of a transformer ChatGPT and chatbot-powered applicati...",["Date","2023-05-15T00:00:00.000Z"],"https://huggingface.co/blog/rwkv","https://huggingface.co/blog/assets/142_rwkv/rwkv_thumbnail.png","src/content/posts/2023-05-15-introducing-rwkv-an-rnn-with-the-advantages-of-a-transformer.md","79c9d7673ef9bfe2",{"html":25,"metadata":8675},{"headings":8676,"localImagePaths":8677,"remoteImagePaths":8678,"frontmatter":8679,"imagePaths":8680},[],[],[],{"title":8667,"description":25,"summary":8668,"pubDate":8661,"source":2720,"url":8670,"thumbnail":8671},[],"2023-05-15-introducing-rwkv-an-rnn-with-the-advantages-of-a-transformer.md","2023-05-15-run-a-chatgpt-like-chatbot-on-a-single-gpu-with-rocm",{"id":8682,"data":8684,"filePath":8690,"digest":8691,"rendered":8692,"legacyId":8699},{"title":8685,"description":25,"summary":8686,"pubDate":8687,"source":2720,"url":8688,"thumbnail":8689},"Run a Chatgpt-like Chatbot on a Single GPU with ROCm","Run a Chatgpt-like Chatbot on a Single GPU with ROCm Introduction ChatGPT, OpenAI's groundbreaking l...",["Date","2023-05-15T00:00:00.000Z"],"https://huggingface.co/blog/chatbot-amd-gpu","https://huggingface.co/blog/assets/chatbot-amd-gpu/thumbnail.png","src/content/posts/2023-05-15-run-a-chatgpt-like-chatbot-on-a-single-gpu-with-rocm.md","a324a7295b09751c",{"html":25,"metadata":8693},{"headings":8694,"localImagePaths":8695,"remoteImagePaths":8696,"frontmatter":8697,"imagePaths":8698},[],[],[],{"title":8685,"description":25,"summary":8686,"pubDate":8661,"source":2720,"url":8688,"thumbnail":8689},[],"2023-05-15-run-a-chatgpt-like-chatbot-on-a-single-gpu-with-rocm.md","2023-05-16-large-scale-near-deduplication-behind-bigcode",{"id":8700,"data":8702,"filePath":8708,"digest":8709,"rendered":8710,"legacyId":8718},{"title":8703,"description":25,"summary":8704,"pubDate":8705,"source":2720,"url":8706,"thumbnail":8707},"Large-scale Near-deduplication Behind BigCode","Large-scale Near-deduplication Behind BigCode Intended Audience People who are interested in documen...",["Date","2023-05-16T00:00:00.000Z"],"https://huggingface.co/blog/dedup","https://huggingface.co/blog/assets/dedup/thumbnail.png","src/content/posts/2023-05-16-large-scale-near-deduplication-behind-bigcode.md","dfadba6570a9e105",{"html":25,"metadata":8711},{"headings":8712,"localImagePaths":8713,"remoteImagePaths":8714,"frontmatter":8715,"imagePaths":8717},[],[],[],{"title":8703,"description":25,"summary":8704,"pubDate":8716,"source":2720,"url":8706,"thumbnail":8707},"Tue, 16 May 2023 00:00:00 GMT",[],"2023-05-16-large-scale-near-deduplication-behind-bigcode.md","2023-05-16-smaller-is-better-q8-chat-an-efficient-generative-ai-experience-on-xeon",{"id":8719,"data":8721,"filePath":8727,"digest":8728,"rendered":8729,"legacyId":8736},{"title":8722,"description":25,"summary":8723,"pubDate":8724,"source":2720,"url":8725,"thumbnail":8726},"Smaller is better: Q8-Chat, an efficient generative AI experience on Xeon","Smaller is better: Q8-Chat, an efficient generative AI experience on Xeon Large language models (LLM...",["Date","2023-05-16T00:00:00.000Z"],"https://huggingface.co/blog/generative-ai-models-on-intel-cpu","https://huggingface.co/blog/assets/143_q8chat/thumbnail.png","src/content/posts/2023-05-16-smaller-is-better-q8-chat-an-efficient-generative-ai-experience-on-xeon.md","47def022a2693d00",{"html":25,"metadata":8730},{"headings":8731,"localImagePaths":8732,"remoteImagePaths":8733,"frontmatter":8734,"imagePaths":8735},[],[],[],{"title":8722,"description":25,"summary":8723,"pubDate":8716,"source":2720,"url":8725,"thumbnail":8726},[],"2023-05-16-smaller-is-better-q8-chat-an-efficient-generative-ai-experience-on-xeon.md","2023-05-18-introducing-the-chatgpt-app-for-ios",{"id":8737,"data":8739,"filePath":8744,"digest":8745,"rendered":8746,"legacyId":8754},{"title":8740,"description":8741,"summary":8741,"pubDate":8742,"source":19,"url":8743,"thumbnail":21},"Introducing the ChatGPT app for iOS","The ChatGPT app syncs your conversations, supports voice input, and brings our latest model improvements to your fingertips.",["Date","2023-05-18T07:00:00.000Z"],"https://openai.com/blog/introducing-the-chatgpt-app-for-ios","src/content/posts/2023-05-18-introducing-the-chatgpt-app-for-ios.md","6d7ef2b30d78ebaa",{"html":25,"metadata":8747},{"headings":8748,"localImagePaths":8749,"remoteImagePaths":8750,"frontmatter":8751,"imagePaths":8753},[],[],[],{"title":8740,"description":8741,"summary":8741,"pubDate":8752,"source":19,"url":8743,"thumbnail":21},"Thu, 18 May 2023 07:00:00 GMT",[],"2023-05-18-introducing-the-chatgpt-app-for-ios.md","2023-05-22-governance-of-superintelligence",{"id":8755,"data":8757,"filePath":8762,"digest":8763,"rendered":8764,"legacyId":8772},{"title":8758,"description":8759,"summary":8759,"pubDate":8760,"source":19,"url":8761,"thumbnail":21},"Governance of superintelligence","Now is a good time to start thinking about the governance of superintelligence—future AI systems dramatically more capable than even AGI.",["Date","2023-05-22T07:00:00.000Z"],"https://openai.com/blog/governance-of-superintelligence","src/content/posts/2023-05-22-governance-of-superintelligence.md","b08a5f7052c12a36",{"html":25,"metadata":8765},{"headings":8766,"localImagePaths":8767,"remoteImagePaths":8768,"frontmatter":8769,"imagePaths":8771},[],[],[],{"title":8758,"description":8759,"summary":8759,"pubDate":8770,"source":19,"url":8761,"thumbnail":21},"Mon, 22 May 2023 07:00:00 GMT",[],"2023-05-22-governance-of-superintelligence.md","2023-05-23-hugging-face-and-ibm-partner-on-watsonxai-the-next-generation-enterprise-studio-for-ai-builders",{"id":8773,"data":8775,"filePath":8781,"digest":8782,"rendered":8783,"legacyId":8791},{"title":8776,"description":25,"summary":8777,"pubDate":8778,"source":2720,"url":8779,"thumbnail":8780},"Hugging Face and IBM partner on watsonx.ai, the next-generation enterprise studio for AI builders","Hugging Face and IBM partner on watsonx.ai, the next-generation enterprise studio for AI builders Al...",["Date","2023-05-23T00:00:00.000Z"],"https://huggingface.co/blog/huggingface-and-ibm","https://huggingface.co/blog/assets/144_ibm/01.png","src/content/posts/2023-05-23-hugging-face-and-ibm-partner-on-watsonxai-the-next-generation-enterprise-studio-for-ai-builders.md","9f57c7bf67785fd6",{"html":25,"metadata":8784},{"headings":8785,"localImagePaths":8786,"remoteImagePaths":8787,"frontmatter":8788,"imagePaths":8790},[],[],[],{"title":8776,"description":25,"summary":8777,"pubDate":8789,"source":2720,"url":8779,"thumbnail":8780},"Tue, 23 May 2023 00:00:00 GMT",[],"2023-05-23-hugging-face-and-ibm-partner-on-watsonxai-the-next-generation-enterprise-studio-for-ai-builders.md","2023-05-23-safetensors-audited-as-really-safe-and-becoming-the-default",{"id":8792,"data":8794,"filePath":8800,"digest":8801,"rendered":8802,"legacyId":8809},{"title":8795,"description":25,"summary":8796,"pubDate":8797,"source":2720,"url":8798,"thumbnail":8799},"Safetensors audited as really safe and becoming the default","Audit shows that safetensors is safe and ready to become the default Hugging Face, in close collabor...",["Date","2023-05-23T00:00:00.000Z"],"https://huggingface.co/blog/safetensors-security-audit","https://huggingface.co/blog/assets/142_safetensors_official/thumbnail.png","src/content/posts/2023-05-23-safetensors-audited-as-really-safe-and-becoming-the-default.md","8fd0a6af31175dab",{"html":25,"metadata":8803},{"headings":8804,"localImagePaths":8805,"remoteImagePaths":8806,"frontmatter":8807,"imagePaths":8808},[],[],[],{"title":8795,"description":25,"summary":8796,"pubDate":8789,"source":2720,"url":8798,"thumbnail":8799},[],"2023-05-23-safetensors-audited-as-really-safe-and-becoming-the-default.md","2023-05-23-instruction-tuning-stable-diffusion-with-instructpix2pix",{"id":8810,"data":8812,"filePath":8818,"digest":8819,"rendered":8820,"legacyId":8827},{"title":8813,"description":25,"summary":8814,"pubDate":8815,"source":2720,"url":8816,"thumbnail":8817},"Instruction-tuning Stable Diffusion with InstructPix2Pix","Instruction-tuning Stable Diffusion with InstructPix2Pix This post explores instruction-tuning to te...",["Date","2023-05-23T00:00:00.000Z"],"https://huggingface.co/blog/instruction-tuning-sd","https://huggingface.co/blog/instruction-tuning-sd/assets/instruction_tuning_sd/thumbnail.png","src/content/posts/2023-05-23-instruction-tuning-stable-diffusion-with-instructpix2pix.md","5977aa7a5fa3468d",{"html":25,"metadata":8821},{"headings":8822,"localImagePaths":8823,"remoteImagePaths":8824,"frontmatter":8825,"imagePaths":8826},[],[],[],{"title":8813,"description":25,"summary":8814,"pubDate":8789,"source":2720,"url":8816,"thumbnail":8817},[],"2023-05-23-instruction-tuning-stable-diffusion-with-instructpix2pix.md","2023-05-24-making-llms-even-more-accessible-with-bitsandbytes-4-bit-quantization-and-qlora",{"id":8828,"data":8830,"filePath":8835,"digest":8836,"rendered":8837,"legacyId":8845},{"title":8831,"description":25,"summary":8832,"pubDate":8833,"source":2720,"url":8834,"thumbnail":5974},"Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA","Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA LLMs are known to b...",["Date","2023-05-24T00:00:00.000Z"],"https://huggingface.co/blog/4bit-transformers-bitsandbytes","src/content/posts/2023-05-24-making-llms-even-more-accessible-with-bitsandbytes-4-bit-quantization-and-qlora.md","24ae6222ab423132",{"html":25,"metadata":8838},{"headings":8839,"localImagePaths":8840,"remoteImagePaths":8841,"frontmatter":8842,"imagePaths":8844},[],[],[],{"title":8831,"description":25,"summary":8832,"pubDate":8843,"source":2720,"url":8834,"thumbnail":5974},"Wed, 24 May 2023 00:00:00 GMT",[],"2023-05-24-making-llms-even-more-accessible-with-bitsandbytes-4-bit-quantization-and-qlora.md","2023-05-24-hugging-face-collaborates-with-microsoft-to-launch-hugging-face-model-catalog-on-azure",{"id":8846,"data":8848,"filePath":8854,"digest":8855,"rendered":8856,"legacyId":8863},{"title":8849,"description":25,"summary":8850,"pubDate":8851,"source":2720,"url":8852,"thumbnail":8853},"Hugging Face Collaborates with Microsoft to Launch Hugging Face Model Catalog on Azure","Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure Today, we are...",["Date","2023-05-24T00:00:00.000Z"],"https://huggingface.co/blog/hugging-face-endpoints-on-azure","https://huggingface.co/blog/assets/75_hugging_face_endpoints_on_azure/01.jpg","src/content/posts/2023-05-24-hugging-face-collaborates-with-microsoft-to-launch-hugging-face-model-catalog-on-azure.md","fa1ae59f9e36cc7b",{"html":25,"metadata":8857},{"headings":8858,"localImagePaths":8859,"remoteImagePaths":8860,"frontmatter":8861,"imagePaths":8862},[],[],[],{"title":8849,"description":25,"summary":8850,"pubDate":8843,"source":2720,"url":8852,"thumbnail":8853},[],"2023-05-24-hugging-face-collaborates-with-microsoft-to-launch-hugging-face-model-catalog-on-azure.md","2023-05-25-an-early-warning-system-for-novel-ai-risks",{"id":8864,"data":8866,"filePath":8872,"digest":8873,"rendered":8874,"legacyId":8882},{"title":8867,"description":8868,"summary":8868,"pubDate":8869,"source":6423,"url":8870,"thumbnail":8871},"An early warning system for novel AI risks","New research proposes a framework for evaluating general-purpose models against novel threats",["Date","2023-05-25T00:00:00.000Z"],"https://deepmind.google/discover/blog/an-early-warning-system-for-novel-ai-risks/","https://lh3.googleusercontent.com/REkFCC8KEOAocMWBwcHOxKM6K2zRs_qpMeUhnmHYkkGSbPPCLRhPDluhoZzx2k6_b4XvgZmhUqeuko9BXZZIPLmGR1q4BycDjLuDFQ5G5FDYPKD0x08=w1200-h630-n-nu","src/content/posts/2023-05-25-an-early-warning-system-for-novel-ai-risks.md","6474562951fa33b1",{"html":25,"metadata":8875},{"headings":8876,"localImagePaths":8877,"remoteImagePaths":8878,"frontmatter":8879,"imagePaths":8881},[],[],[],{"title":8867,"description":8868,"summary":8868,"pubDate":8880,"source":6423,"url":8870,"thumbnail":8871},"Thu, 25 May 2023 00:00:00 +0000",[],"2023-05-25-an-early-warning-system-for-novel-ai-risks.md","2023-05-25-democratic-inputs-to-ai",{"id":8883,"data":8885,"filePath":8890,"digest":8891,"rendered":8892,"legacyId":8900},{"title":8886,"description":8887,"summary":8887,"pubDate":8888,"source":19,"url":8889,"thumbnail":21},"Democratic inputs to AI","Our nonprofit organization, OpenAI, Inc., is launching a program to award ten $100,000 grants to fund experiments in setting up a democratic process for deciding what rules AI systems should follow, within the bounds defined by the law.",["Date","2023-05-25T07:00:00.000Z"],"https://openai.com/blog/democratic-inputs-to-ai","src/content/posts/2023-05-25-democratic-inputs-to-ai.md","a52ef1bd638a4cc8",{"html":25,"metadata":8893},{"headings":8894,"localImagePaths":8895,"remoteImagePaths":8896,"frontmatter":8897,"imagePaths":8899},[],[],[],{"title":8886,"description":8887,"summary":8887,"pubDate":8898,"source":19,"url":8889,"thumbnail":21},"Thu, 25 May 2023 07:00:00 GMT",[],"2023-05-25-democratic-inputs-to-ai.md","2023-05-25-optimizing-stable-diffusion-for-intel-cpus-with-nncf-and-optimum",{"id":8901,"data":8903,"filePath":8909,"digest":8910,"rendered":8911,"legacyId":8919},{"title":8904,"description":25,"summary":8905,"pubDate":8906,"source":2720,"url":8907,"thumbnail":8908},"Optimizing Stable Diffusion for Intel CPUs with NNCF and 🤗 Optimum","Optimizing Stable Diffusion for Intel CPUs with NNCF and 🤗 Optimum Latent Diffusion models are game ...",["Date","2023-05-25T00:00:00.000Z"],"https://huggingface.co/blog/train-optimize-sd-intel","https://huggingface.co/blog/assets/train_optimize_sd_intel/thumbnail.png","src/content/posts/2023-05-25-optimizing-stable-diffusion-for-intel-cpus-with-nncf-and-optimum.md","95346b0c01537001",{"html":25,"metadata":8912},{"headings":8913,"localImagePaths":8914,"remoteImagePaths":8915,"frontmatter":8916,"imagePaths":8918},[],[],[],{"title":8904,"description":25,"summary":8905,"pubDate":8917,"source":2720,"url":8907,"thumbnail":8908},"Thu, 25 May 2023 00:00:00 GMT",[],"2023-05-25-optimizing-stable-diffusion-for-intel-cpus-with-nncf-and-optimum.md","2023-05-31-improving-mathematical-reasoning-with-process-supervision",{"id":8920,"data":8922,"filePath":8927,"digest":8928,"rendered":8929,"legacyId":8937},{"title":8923,"description":8924,"summary":8924,"pubDate":8925,"source":19,"url":8926,"thumbnail":21},"Improving mathematical reasoning with process supervision","We've trained a model to achieve a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (“process supervision”) instead of simply rewarding the correct final answer (“outcome supervision”). In addition to boosting performance relative to outcome supervision, process supervision also has an important alignment benefit: it directly trains the model to produce a chain-of-thought that is endorsed by humans.",["Date","2023-05-31T07:00:00.000Z"],"https://openai.com/blog/improving-mathematical-reasoning-with-process-supervision","src/content/posts/2023-05-31-improving-mathematical-reasoning-with-process-supervision.md","7afd7c2a6285deb1",{"html":25,"metadata":8930},{"headings":8931,"localImagePaths":8932,"remoteImagePaths":8933,"frontmatter":8934,"imagePaths":8936},[],[],[],{"title":8923,"description":8924,"summary":8924,"pubDate":8935,"source":19,"url":8926,"thumbnail":21},"Wed, 31 May 2023 07:00:00 GMT",[],"2023-05-31-improving-mathematical-reasoning-with-process-supervision.md","2023-05-31-introducing-bertopic-integration-with-hugging-face-hub",{"id":8938,"data":8940,"filePath":8946,"digest":8947,"rendered":8948,"legacyId":8956},{"title":8941,"description":25,"summary":8942,"pubDate":8943,"source":2720,"url":8944,"thumbnail":8945},"Introducing BERTopic Integration with Hugging Face Hub","Introducing BERTopic Integration with the Hugging Face Hub We are thrilled to announce a significant...",["Date","2023-05-31T00:00:00.000Z"],"https://huggingface.co/blog/bertopic","https://huggingface.co/blog/assets/145_bertopic/logo.png","src/content/posts/2023-05-31-introducing-bertopic-integration-with-hugging-face-hub.md","a3222bad6a8817f0",{"html":25,"metadata":8949},{"headings":8950,"localImagePaths":8951,"remoteImagePaths":8952,"frontmatter":8953,"imagePaths":8955},[],[],[],{"title":8941,"description":25,"summary":8942,"pubDate":8954,"source":2720,"url":8944,"thumbnail":8945},"Wed, 31 May 2023 00:00:00 GMT",[],"2023-05-31-introducing-bertopic-integration-with-hugging-face-hub.md","2023-06-01-openai-cybersecurity-grant-program",{"id":8957,"data":8959,"filePath":8964,"digest":8965,"rendered":8966,"legacyId":8974},{"title":8960,"description":8961,"summary":8961,"pubDate":8962,"source":19,"url":8963,"thumbnail":21},"OpenAI Cybersecurity Grant Program","Our goal is to facilitate the development of AI-powered cybersecurity capabilities for defenders through grants and other support.",["Date","2023-06-01T07:00:00.000Z"],"https://openai.com/blog/openai-cybersecurity-grant-program","src/content/posts/2023-06-01-openai-cybersecurity-grant-program.md","81df4e4d16c0234a",{"html":25,"metadata":8967},{"headings":8968,"localImagePaths":8969,"remoteImagePaths":8970,"frontmatter":8971,"imagePaths":8973},[],[],[],{"title":8960,"description":8961,"summary":8961,"pubDate":8972,"source":19,"url":8963,"thumbnail":21},"Thu, 01 Jun 2023 07:00:00 GMT",[],"2023-06-01-openai-cybersecurity-grant-program.md","2023-06-01-announcing-the-open-source-ai-game-jam",{"id":8975,"data":8977,"filePath":8983,"digest":8984,"rendered":8985,"legacyId":8993},{"title":8978,"description":25,"summary":8979,"pubDate":8980,"source":2720,"url":8981,"thumbnail":8982},"Announcing the Open Source AI Game Jam 🎮","Announcing the Open Source AI Game Jam 🎮 Unleash Your Creativity with AI Tools and make a game in a ...",["Date","2023-06-01T00:00:00.000Z"],"https://huggingface.co/blog/game-jam","https://huggingface.co/blog/assets/145_gamejam/thumbnail.png","src/content/posts/2023-06-01-announcing-the-open-source-ai-game-jam.md","8bc14fcdb90a0667",{"html":25,"metadata":8986},{"headings":8987,"localImagePaths":8988,"remoteImagePaths":8989,"frontmatter":8990,"imagePaths":8992},[],[],[],{"title":8978,"description":25,"summary":8979,"pubDate":8991,"source":2720,"url":8981,"thumbnail":8982},"Thu, 01 Jun 2023 00:00:00 GMT",[],"2023-06-01-announcing-the-open-source-ai-game-jam.md","2023-05-31-introducing-the-hugging-face-llm-inference-container-for-amazon-sagemaker",{"id":8994,"data":8996,"filePath":9002,"digest":9003,"rendered":9004,"legacyId":9011},{"title":8997,"description":25,"summary":8998,"pubDate":8999,"source":2720,"url":9000,"thumbnail":9001},"Introducing the Hugging Face LLM Inference Container for Amazon SageMaker","Introducing the Hugging Face LLM Inference Container for Amazon SageMaker This is an example on how ...",["Date","2023-05-31T00:00:00.000Z"],"https://huggingface.co/blog/sagemaker-huggingface-llm","https://huggingface.co/blog/assets/145_sagemaker-huggingface-llm/thumbnail.jpg","src/content/posts/2023-05-31-introducing-the-hugging-face-llm-inference-container-for-amazon-sagemaker.md","61267c32f603a47e",{"html":25,"metadata":9005},{"headings":9006,"localImagePaths":9007,"remoteImagePaths":9008,"frontmatter":9009,"imagePaths":9010},[],[],[],{"title":8997,"description":25,"summary":8998,"pubDate":8954,"source":2720,"url":9000,"thumbnail":9001},[],"2023-05-31-introducing-the-hugging-face-llm-inference-container-for-amazon-sagemaker.md","2023-06-02-ai-speech-recognition-in-unity",{"id":9012,"data":9014,"filePath":9020,"digest":9021,"rendered":9022,"legacyId":9030},{"title":9015,"description":25,"summary":9016,"pubDate":9017,"source":2720,"url":9018,"thumbnail":9019},"AI Speech Recognition in Unity","AI Speech Recognition in Unity Introduction This tutorial guides you through the process of implemen...",["Date","2023-06-02T00:00:00.000Z"],"https://huggingface.co/blog/unity-asr","https://huggingface.co/blog/assets/124_ml-for-games/unity-asr-thumbnail.png","src/content/posts/2023-06-02-ai-speech-recognition-in-unity.md","de6279f076947694",{"html":25,"metadata":9023},{"headings":9024,"localImagePaths":9025,"remoteImagePaths":9026,"frontmatter":9027,"imagePaths":9029},[],[],[],{"title":9015,"description":25,"summary":9016,"pubDate":9028,"source":2720,"url":9018,"thumbnail":9019},"Fri, 02 Jun 2023 00:00:00 GMT",[],"2023-06-02-ai-speech-recognition-in-unity.md","2023-06-05-the-falcon-has-landed-in-the-hugging-face-ecosystem",{"id":9031,"data":9033,"filePath":9039,"digest":9040,"rendered":9041,"legacyId":9049},{"title":9034,"description":25,"summary":9035,"pubDate":9036,"source":2720,"url":9037,"thumbnail":9038},"The Falcon has landed in the Hugging Face ecosystem","The Falcon has landed in the Hugging Face ecosystem Falcon is a new family of state-of-the-art langu...",["Date","2023-06-05T00:00:00.000Z"],"https://huggingface.co/blog/falcon","https://huggingface.co/blog/assets/147_falcon/falcon_thumbnail.jpg","src/content/posts/2023-06-05-the-falcon-has-landed-in-the-hugging-face-ecosystem.md","f335e46d66185c38",{"html":25,"metadata":9042},{"headings":9043,"localImagePaths":9044,"remoteImagePaths":9045,"frontmatter":9046,"imagePaths":9048},[],[],[],{"title":9034,"description":25,"summary":9035,"pubDate":9047,"source":2720,"url":9037,"thumbnail":9038},"Mon, 05 Jun 2023 00:00:00 GMT",[],"2023-06-05-the-falcon-has-landed-in-the-hugging-face-ecosystem.md","2023-06-06-welcome-fasttext-to-the-hub",{"id":9050,"data":9052,"filePath":9058,"digest":9059,"rendered":9060,"legacyId":9068},{"title":9053,"description":25,"summary":9054,"pubDate":9055,"source":2720,"url":9056,"thumbnail":9057},"Welcome fastText to the 🤗 Hub","Welcome fastText to the Hugging Face Hub fastText is a library for efficient learning of text repres...",["Date","2023-06-06T00:00:00.000Z"],"https://huggingface.co/blog/fasttext","https://huggingface.co/blog/assets/147_fasttext/thumbnail.png","src/content/posts/2023-06-06-welcome-fasttext-to-the-hub.md","7fc79184f27574f7",{"html":25,"metadata":9061},{"headings":9062,"localImagePaths":9063,"remoteImagePaths":9064,"frontmatter":9065,"imagePaths":9067},[],[],[],{"title":9053,"description":25,"summary":9054,"pubDate":9066,"source":2720,"url":9056,"thumbnail":9057},"Tue, 06 Jun 2023 00:00:00 GMT",[],"2023-06-06-welcome-fasttext-to-the-hub.md","2023-06-07-alphadev-discovers-faster-sorting-algorithms",{"id":9069,"data":9071,"filePath":9077,"digest":9078,"rendered":9079,"legacyId":9087},{"title":9072,"description":9073,"summary":9073,"pubDate":9074,"source":6423,"url":9075,"thumbnail":9076},"AlphaDev discovers faster sorting algorithms","New algorithms will transform the foundations of computing",["Date","2023-06-07T00:00:00.000Z"],"https://deepmind.google/discover/blog/alphadev-discovers-faster-sorting-algorithms/","https://lh3.googleusercontent.com/kYAs9KTHdhYZE0BeKMKlphVqU3eQS8oXP_GNrrWBjFbl8r4YFv2FWlRbe6x9L4Q_L-eKZeE7E__GtKVJTLXvW_zGTTzplSJCplN02n_8cz7No815L5M=w1200-h630-n-nu","src/content/posts/2023-06-07-alphadev-discovers-faster-sorting-algorithms.md","22da1a30d76e9716",{"html":25,"metadata":9080},{"headings":9081,"localImagePaths":9082,"remoteImagePaths":9083,"frontmatter":9084,"imagePaths":9086},[],[],[],{"title":9072,"description":9073,"summary":9073,"pubDate":9085,"source":6423,"url":9075,"thumbnail":9076},"Wed, 07 Jun 2023 00:00:00 +0000",[],"2023-06-07-alphadev-discovers-faster-sorting-algorithms.md","2023-06-07-duckdb-run-sql-queries-on-50000-datasets-on-the-hugging-face-hub",{"id":9088,"data":9090,"filePath":9096,"digest":9097,"rendered":9098,"legacyId":9106},{"title":9091,"description":25,"summary":9092,"pubDate":9093,"source":2720,"url":9094,"thumbnail":9095},"DuckDB: run SQL queries on 50,000+ datasets on the Hugging Face Hub","DuckDB: run SQL queries on 50,000+ datasets on the Hugging Face Hub The Hugging Face Hub is dedicate...",["Date","2023-06-07T00:00:00.000Z"],"https://huggingface.co/blog/hub-duckdb","https://huggingface.co/blog/assets/hub_duckdb/hub_duckdb.png","src/content/posts/2023-06-07-duckdb-run-sql-queries-on-50000-datasets-on-the-hugging-face-hub.md","dd042119fd289ba2",{"html":25,"metadata":9099},{"headings":9100,"localImagePaths":9101,"remoteImagePaths":9102,"frontmatter":9103,"imagePaths":9105},[],[],[],{"title":9091,"description":25,"summary":9092,"pubDate":9104,"source":2720,"url":9094,"thumbnail":9095},"Wed, 07 Jun 2023 00:00:00 GMT",[],"2023-06-07-duckdb-run-sql-queries-on-50000-datasets-on-the-hugging-face-hub.md","2023-06-12-can-foundation-models-label-data-like-humans",{"id":9107,"data":9109,"filePath":9115,"digest":9116,"rendered":9117,"legacyId":9125},{"title":9110,"description":25,"summary":9111,"pubDate":9112,"source":2720,"url":9113,"thumbnail":9114},"Can foundation models label data like humans?","Can foundation models label data like humans? Since the advent of ChatGPT, we have seen unprecedente...",["Date","2023-06-12T00:00:00.000Z"],"https://huggingface.co/blog/open-llm-leaderboard-rlhf","https://huggingface.co/blog/assets/llm-leaderboard/leaderboard-thumbnail.png","src/content/posts/2023-06-12-can-foundation-models-label-data-like-humans.md","201b72ad99c45abc",{"html":25,"metadata":9118},{"headings":9119,"localImagePaths":9120,"remoteImagePaths":9121,"frontmatter":9122,"imagePaths":9124},[],[],[],{"title":9110,"description":25,"summary":9111,"pubDate":9123,"source":2720,"url":9113,"thumbnail":9114},"Mon, 12 Jun 2023 00:00:00 GMT",[],"2023-06-12-can-foundation-models-label-data-like-humans.md","2023-06-12-comment-on-ntia-ai-accountability-policy",{"id":9126,"data":9128,"filePath":9133,"digest":9134,"rendered":9135,"legacyId":9142},{"title":9129,"description":9130,"summary":9130,"pubDate":9131,"source":19,"url":9132,"thumbnail":21},"Comment on NTIA AI Accountability Policy","The National Telecommunications and Information Administration (NTIA) request for comments on AI Accountability policy.",["Date","2023-06-12T00:00:00.000Z"],"https://openai.com/global-affairs/comment-on-ntia-ai-accountability-policy","src/content/posts/2023-06-12-comment-on-ntia-ai-accountability-policy.md","03ed01fab9a43eaa",{"html":25,"metadata":9136},{"headings":9137,"localImagePaths":9138,"remoteImagePaths":9139,"frontmatter":9140,"imagePaths":9141},[],[],[],{"title":9129,"description":9130,"summary":9130,"pubDate":9123,"source":19,"url":9132,"thumbnail":21},[],"2023-06-12-comment-on-ntia-ai-accountability-policy.md","2023-06-12-muzero-alphazero-and-alphadev-optimizing-computer-systems",{"id":9143,"data":9145,"filePath":9151,"digest":9152,"rendered":9153,"legacyId":9161},{"title":9146,"description":9147,"summary":9147,"pubDate":9148,"source":6423,"url":9149,"thumbnail":9150},"MuZero, AlphaZero, and AlphaDev: Optimizing computer systems","How MuZero, AlphaZero, and AlphaDev are optimizing the computing ecosystem that powers our world of devices.",["Date","2023-06-12T14:41:00.000Z"],"https://deepmind.google/discover/blog/muzero-alphazero-and-alphadev-optimizing-computer-systems/","https://lh3.googleusercontent.com/6tSxHgEgSLR8FSELf3If1M1QBbXTtpsfH6w2ocuruWGnFDTdogbyNA8sHOyKpFYCja4hT7fGCVwl2xyI9biVB1bFNcnTxvYptuVdcT0XHMjn-TzG=w1200-h630-n-nu","src/content/posts/2023-06-12-muzero-alphazero-and-alphadev-optimizing-computer-systems.md","0d99881049fb05b7",{"html":25,"metadata":9154},{"headings":9155,"localImagePaths":9156,"remoteImagePaths":9157,"frontmatter":9158,"imagePaths":9160},[],[],[],{"title":9146,"description":9147,"summary":9147,"pubDate":9159,"source":6423,"url":9149,"thumbnail":9150},"Mon, 12 Jun 2023 14:41:00 +0000",[],"2023-06-12-muzero-alphazero-and-alphadev-optimizing-computer-systems.md","2023-06-12-the-hugging-face-hub-for-galleries-libraries-archives-and-museums",{"id":9162,"data":9164,"filePath":9170,"digest":9171,"rendered":9172,"legacyId":9179},{"title":9165,"description":25,"summary":9166,"pubDate":9167,"source":2720,"url":9168,"thumbnail":9169},"The Hugging Face Hub for Galleries, Libraries, Archives and Museums","The Hugging Face Hub for Galleries, Libraries, Archives and Museums The Hugging Face Hub for Galleri...",["Date","2023-06-12T00:00:00.000Z"],"https://huggingface.co/blog/hf-hub-glam-guide","https://huggingface.co/blog/assets/144_hf_hub_glam_guide/thumbnail.png","src/content/posts/2023-06-12-the-hugging-face-hub-for-galleries-libraries-archives-and-museums.md","ffe0fb84f7ea9716",{"html":25,"metadata":9173},{"headings":9174,"localImagePaths":9175,"remoteImagePaths":9176,"frontmatter":9177,"imagePaths":9178},[],[],[],{"title":9165,"description":25,"summary":9166,"pubDate":9123,"source":2720,"url":9168,"thumbnail":9169},[],"2023-06-12-the-hugging-face-hub-for-galleries-libraries-archives-and-museums.md","2023-06-13-function-calling-and-other-api-updates",{"id":9180,"data":9182,"filePath":9187,"digest":9188,"rendered":9189,"legacyId":9197},{"title":9183,"description":9184,"summary":9184,"pubDate":9185,"source":19,"url":9186,"thumbnail":21},"Function calling and other API updates","We’re announcing updates including more steerable API models, function calling capabilities, longer context, and lower prices.",["Date","2023-06-13T07:00:00.000Z"],"https://openai.com/blog/function-calling-and-other-api-updates","src/content/posts/2023-06-13-function-calling-and-other-api-updates.md","2ae3a8f34868a3dd",{"html":25,"metadata":9190},{"headings":9191,"localImagePaths":9192,"remoteImagePaths":9193,"frontmatter":9194,"imagePaths":9196},[],[],[],{"title":9183,"description":9184,"summary":9184,"pubDate":9195,"source":19,"url":9186,"thumbnail":21},"Tue, 13 Jun 2023 07:00:00 GMT",[],"2023-06-13-function-calling-and-other-api-updates.md","2023-06-13-hugging-face-and-amd-partner-on-accelerating-state-of-the-art-models-for-cpu-and-gpu-platforms",{"id":9198,"data":9200,"filePath":9206,"digest":9207,"rendered":9208,"legacyId":9216},{"title":9201,"description":25,"summary":9202,"pubDate":9203,"source":2720,"url":9204,"thumbnail":9205},"Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU platforms","Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU platforms Wheth...",["Date","2023-06-13T00:00:00.000Z"],"https://huggingface.co/blog/huggingface-and-amd","https://huggingface.co/blog/assets/148_huggingface_amd/01.png","src/content/posts/2023-06-13-hugging-face-and-amd-partner-on-accelerating-state-of-the-art-models-for-cpu-and-gpu-platforms.md","5b869964350e23d9",{"html":25,"metadata":9209},{"headings":9210,"localImagePaths":9211,"remoteImagePaths":9212,"frontmatter":9213,"imagePaths":9215},[],[],[],{"title":9201,"description":25,"summary":9202,"pubDate":9214,"source":2720,"url":9204,"thumbnail":9205},"Tue, 13 Jun 2023 00:00:00 GMT",[],"2023-06-13-hugging-face-and-amd-partner-on-accelerating-state-of-the-art-models-for-cpu-and-gpu-platforms.md","2023-06-15-deploy-livebook-notebooks-as-apps-to-hugging-face-spaces",{"id":9217,"data":9219,"filePath":9224,"digest":9225,"rendered":9226,"legacyId":9234},{"title":9220,"description":25,"summary":9221,"pubDate":9222,"source":2720,"url":9223,"thumbnail":7023},"Deploy Livebook notebooks as apps to Hugging Face Spaces","Deploy Livebook notebooks as apps to Hugging Face Spaces The Elixir community has been making great ...",["Date","2023-06-15T00:00:00.000Z"],"https://huggingface.co/blog/livebook-app-deployment","src/content/posts/2023-06-15-deploy-livebook-notebooks-as-apps-to-hugging-face-spaces.md","4fe8e4cb8fd3a574",{"html":25,"metadata":9227},{"headings":9228,"localImagePaths":9229,"remoteImagePaths":9230,"frontmatter":9231,"imagePaths":9233},[],[],[],{"title":9220,"description":25,"summary":9221,"pubDate":9232,"source":2720,"url":9223,"thumbnail":7023},"Thu, 15 Jun 2023 00:00:00 GMT",[],"2023-06-15-deploy-livebook-notebooks-as-apps-to-hugging-face-spaces.md","2023-06-14-google-cloud-driving-digital-transformation",{"id":9235,"data":9237,"filePath":9243,"digest":9244,"rendered":9245,"legacyId":9253},{"title":9238,"description":9239,"summary":9239,"pubDate":9240,"source":6423,"url":9241,"thumbnail":9242},"Google Cloud: Driving digital transformation","Google Cloud empowers organizations to digitally transform themselves into smarter businesses. It offers cloud computing, data analytics, and the latest artificial intelligence (AI) and machine learning tools.",["Date","2023-06-14T14:51:00.000Z"],"https://deepmind.google/discover/blog/google-cloud-driving-digital-transformation/","https://lh3.googleusercontent.com/xIps-6-tV3GGWQjVrHYTkLGnXAdZwmjG6jOAgECP5aynUXKeAfUhWv7fFfjPaV8Jmn3B3IabKBeDzBtB491hJAozuAhdQ-TUtZ5dzy9dmE1zWC-J=w1200-h630-n-nu","src/content/posts/2023-06-14-google-cloud-driving-digital-transformation.md","fa4e661a9a88e003",{"html":25,"metadata":9246},{"headings":9247,"localImagePaths":9248,"remoteImagePaths":9249,"frontmatter":9250,"imagePaths":9252},[],[],[],{"title":9238,"description":9239,"summary":9239,"pubDate":9251,"source":6423,"url":9241,"thumbnail":9242},"Wed, 14 Jun 2023 14:51:00 +0000",[],"2023-06-14-google-cloud-driving-digital-transformation.md","2023-06-15-announcing-our-new-content-guidelines-and-policy",{"id":9254,"data":9256,"filePath":9262,"digest":9263,"rendered":9264,"legacyId":9271},{"title":9257,"description":25,"summary":9258,"pubDate":9259,"source":2720,"url":9260,"thumbnail":9261},"Announcing our new Content Guidelines and Policy","Announcing our new Community Policy As a community-driven platform that aims to advance Open, Collab...",["Date","2023-06-15T00:00:00.000Z"],"https://huggingface.co/blog/content-guidelines-update","https://huggingface.co/blog/assets/content-guidelines-blogpost/thumbnail.png","src/content/posts/2023-06-15-announcing-our-new-content-guidelines-and-policy.md","2a1c2a745aa46eb3",{"html":25,"metadata":9265},{"headings":9266,"localImagePaths":9267,"remoteImagePaths":9268,"frontmatter":9269,"imagePaths":9270},[],[],[],{"title":9257,"description":25,"summary":9258,"pubDate":9232,"source":2720,"url":9260,"thumbnail":9261},[],"2023-06-15-announcing-our-new-content-guidelines-and-policy.md","2023-06-15-faster-stable-diffusion-with-core-ml-on-iphone-ipad-and-mac",{"id":9272,"data":9274,"filePath":9280,"digest":9281,"rendered":9282,"legacyId":9289},{"title":9275,"description":25,"summary":9276,"pubDate":9277,"source":2720,"url":9278,"thumbnail":9279},"Faster Stable Diffusion with Core ML on iPhone, iPad, and Mac","Faster Stable Diffusion with Core ML on iPhone, iPad, and Mac WWDC’23 (Apple Worldwide Developers Co...",["Date","2023-06-15T00:00:00.000Z"],"https://huggingface.co/blog/fast-diffusers-coreml","https://huggingface.co/blog/assets/149_fast_diffusers_coreml/thumbnail.png","src/content/posts/2023-06-15-faster-stable-diffusion-with-core-ml-on-iphone-ipad-and-mac.md","a8813747eba4b4e8",{"html":25,"metadata":9283},{"headings":9284,"localImagePaths":9285,"remoteImagePaths":9286,"frontmatter":9287,"imagePaths":9288},[],[],[],{"title":9275,"description":25,"summary":9276,"pubDate":9232,"source":2720,"url":9278,"thumbnail":9279},[],"2023-06-15-faster-stable-diffusion-with-core-ml-on-iphone-ipad-and-mac.md","2023-06-16-yes-transformers-are-effective-for-time-series-forecasting-autoformer",{"id":9290,"data":9292,"filePath":9298,"digest":9299,"rendered":9300,"legacyId":9308},{"title":9293,"description":25,"summary":9294,"pubDate":9295,"source":2720,"url":9296,"thumbnail":9297},"Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)","Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer) Introduction A few months...",["Date","2023-06-16T00:00:00.000Z"],"https://huggingface.co/blog/autoformer","https://huggingface.co/blog/assets/150_autoformer/thumbnail.png","src/content/posts/2023-06-16-yes-transformers-are-effective-for-time-series-forecasting-autoformer.md","6cf41318d0500356",{"html":25,"metadata":9301},{"headings":9302,"localImagePaths":9303,"remoteImagePaths":9304,"frontmatter":9305,"imagePaths":9307},[],[],[],{"title":9293,"description":25,"summary":9294,"pubDate":9306,"source":2720,"url":9296,"thumbnail":9297},"Fri, 16 Jun 2023 00:00:00 GMT",[],"2023-06-16-yes-transformers-are-effective-for-time-series-forecasting-autoformer.md","2023-06-16-youtube-enhancing-the-user-experience",{"id":9309,"data":9311,"filePath":9317,"digest":9318,"rendered":9319,"legacyId":9327},{"title":9312,"description":9313,"summary":9313,"pubDate":9314,"source":6423,"url":9315,"thumbnail":9316},"YouTube: Enhancing the user experience","It’s all about using our technology and research to help enrich people’s lives. Like YouTube — and its mission to give everyone a voice and show them the world.",["Date","2023-06-16T14:55:00.000Z"],"https://deepmind.google/discover/blog/youtube-enhancing-the-user-experience/","https://lh3.googleusercontent.com/RAMu-2QAkfHieGDWkYFQOMiATW-wFi6jMLyC-YJ4f6Jj1H5BlhxQBmfQrb4RS6Sc6DFLFJqBahK3_1--XjoFPdGqYsCdSuTNr-pTcLkRO5SqvReblIQ=w1200-h630-n-nu","src/content/posts/2023-06-16-youtube-enhancing-the-user-experience.md","de2c46a4b1d1e75b",{"html":25,"metadata":9320},{"headings":9321,"localImagePaths":9322,"remoteImagePaths":9323,"frontmatter":9324,"imagePaths":9326},[],[],[],{"title":9312,"description":9313,"summary":9313,"pubDate":9325,"source":6423,"url":9315,"thumbnail":9316},"Fri, 16 Jun 2023 14:55:00 +0000",[],"2023-06-16-youtube-enhancing-the-user-experience.md","2023-06-19-fine-tuning-mms-adapter-models-for-multi-lingual-asr",{"id":9328,"data":9330,"filePath":9336,"digest":9337,"rendered":9338,"legacyId":9346},{"title":9331,"description":25,"summary":9332,"pubDate":9333,"source":2720,"url":9334,"thumbnail":9335},"Fine-tuning MMS Adapter Models for Multi-Lingual ASR","Fine-tuning MMS Adapter Models for Multi-Lingual ASR New (06/2023): This blog post is strongly inspi...",["Date","2023-06-19T00:00:00.000Z"],"https://huggingface.co/blog/mms_adapters","https://huggingface.co/blog/assets/151_mms/mms_map.png","src/content/posts/2023-06-19-fine-tuning-mms-adapter-models-for-multi-lingual-asr.md","f99a25d12dd34d30",{"html":25,"metadata":9339},{"headings":9340,"localImagePaths":9341,"remoteImagePaths":9342,"frontmatter":9343,"imagePaths":9345},[],[],[],{"title":9331,"description":25,"summary":9332,"pubDate":9344,"source":2720,"url":9334,"thumbnail":9335},"Mon, 19 Jun 2023 00:00:00 GMT",[],"2023-06-19-fine-tuning-mms-adapter-models-for-multi-lingual-asr.md","2023-06-20-ai-policy-response-to-the-us-ntias-request-for-comment-on-ai-accountability",{"id":9347,"data":9349,"filePath":9355,"digest":9356,"rendered":9357,"legacyId":9365},{"title":9350,"description":25,"summary":9351,"pubDate":9352,"source":2720,"url":9353,"thumbnail":9354},"AI Policy @🤗: Response to the U.S. NTIA's Request for Comment on AI Accountability","AI Policy @🤗: Response to the U.S. National Telecommunications and Information Administration’s (NTI...",["Date","2023-06-20T00:00:00.000Z"],"https://huggingface.co/blog/policy-ntia-rfc","https://huggingface.co/blog/assets/151_policy_ntia_rfc/us_policy_thumbnail.png","src/content/posts/2023-06-20-ai-policy-response-to-the-us-ntias-request-for-comment-on-ai-accountability.md","9d01448856b04ce4",{"html":25,"metadata":9358},{"headings":9359,"localImagePaths":9360,"remoteImagePaths":9361,"frontmatter":9362,"imagePaths":9364},[],[],[],{"title":9350,"description":25,"summary":9351,"pubDate":9363,"source":2720,"url":9353,"thumbnail":9354},"Tue, 20 Jun 2023 00:00:00 GMT",[],"2023-06-20-ai-policy-response-to-the-us-ntias-request-for-comment-on-ai-accountability.md","2023-06-20-robocat-a-self-improving-robotic-agent",{"id":9366,"data":9368,"filePath":9374,"digest":9375,"rendered":9376,"legacyId":9384},{"title":9369,"description":9370,"summary":9370,"pubDate":9371,"source":6423,"url":9372,"thumbnail":9373},"RoboCat: A self-improving robotic agent","Robots are quickly becoming part of our everyday lives, but they’re often only programmed to perform specific tasks well. While harnessing recent advances in AI could lead to robots that could help in many more ways, progress in building general-purpose robots is slower in part because of the time needed to collect real-world training data. Our latest paper introduces a self-improving AI agent for robotics, RoboCat, that learns to perform a variety of tasks across different arms, and then self-generates new training data to improve its technique.",["Date","2023-06-20T00:00:00.000Z"],"https://deepmind.google/discover/blog/robocat-a-self-improving-robotic-agent/","https://lh3.googleusercontent.com/Rz9Xv4TXuTe-eO2UDUD6kDElDB5wDE2b2hEU1liUAi0AyiTwQ81mLMigXg3kueWrHoqeNctRO5-EMprZDRnXcaL8snfqHwDqgQpw_qB3VEvoO_jCCzI=w1200-h630-n-nu","src/content/posts/2023-06-20-robocat-a-self-improving-robotic-agent.md","345146f1152126a8",{"html":25,"metadata":9377},{"headings":9378,"localImagePaths":9379,"remoteImagePaths":9380,"frontmatter":9381,"imagePaths":9383},[],[],[],{"title":9369,"description":9370,"summary":9370,"pubDate":9382,"source":6423,"url":9372,"thumbnail":9373},"Tue, 20 Jun 2023 00:00:00 +0000",[],"2023-06-20-robocat-a-self-improving-robotic-agent.md","2023-06-22-questions-for-the-record",{"id":9385,"data":9387,"filePath":9392,"digest":9393,"rendered":9394,"legacyId":9402},{"title":9388,"description":9389,"summary":9389,"pubDate":9390,"source":19,"url":9391,"thumbnail":21},"Questions for the Record","The following are the Questions for the Record following Sam Altman's testimony before the U.S. Senate Committee on the Judiciary (Subcommittee on Privacy, Technology, & the Law).",["Date","2023-06-22T00:00:00.000Z"],"https://openai.com/global-affairs/sam-altman-senate-questions-for-the-record","src/content/posts/2023-06-22-questions-for-the-record.md","d62c3496b5495c18",{"html":25,"metadata":9395},{"headings":9396,"localImagePaths":9397,"remoteImagePaths":9398,"frontmatter":9399,"imagePaths":9401},[],[],[],{"title":9388,"description":9389,"summary":9389,"pubDate":9400,"source":19,"url":9391,"thumbnail":21},"Thu, 22 Jun 2023 00:00:00 GMT",[],"2023-06-22-questions-for-the-record.md","2023-06-22-panel-on-hugging-face",{"id":9403,"data":9405,"filePath":9411,"digest":9412,"rendered":9413,"legacyId":9420},{"title":9406,"description":25,"summary":9407,"pubDate":9408,"source":2720,"url":9409,"thumbnail":9410},"Panel on Hugging Face","Panel on Hugging Face We are thrilled to announce the collaboration between Panel and Hugging Face! ...",["Date","2023-06-22T00:00:00.000Z"],"https://huggingface.co/blog/panel-on-hugging-face","https://huggingface.co/blog/assets/panel-on-hugging-face/thumbnail.png","src/content/posts/2023-06-22-panel-on-hugging-face.md","350907daf5861e77",{"html":25,"metadata":9414},{"headings":9415,"localImagePaths":9416,"remoteImagePaths":9417,"frontmatter":9418,"imagePaths":9419},[],[],[],{"title":9406,"description":25,"summary":9407,"pubDate":9400,"source":2720,"url":9409,"thumbnail":9410},[],"2023-06-22-panel-on-hugging-face.md","2023-06-22-testimony-before-the-us-senate",{"id":9421,"data":9423,"filePath":9428,"digest":9429,"rendered":9430,"legacyId":9437},{"title":9424,"description":9425,"summary":9425,"pubDate":9426,"source":19,"url":9427,"thumbnail":21},"Testimony before the U.S. Senate","The following is the written testimony of Sam Altman, Chief Executive Officer of OpenAI, before the U.S. Senate Committee on the Judiciary (Subcommittee on Privacy, Technology, & the Law).",["Date","2023-06-22T00:00:00.000Z"],"https://openai.com/global-affairs/testimony-of-sam-altman-before-the-us-senate","src/content/posts/2023-06-22-testimony-before-the-us-senate.md","8f840ca691240d52",{"html":25,"metadata":9431},{"headings":9432,"localImagePaths":9433,"remoteImagePaths":9434,"frontmatter":9435,"imagePaths":9436},[],[],[],{"title":9424,"description":9425,"summary":9425,"pubDate":9400,"source":19,"url":9427,"thumbnail":21},[],"2023-06-22-testimony-before-the-us-senate.md","2023-06-23-whats-going-on-with-the-open-llm-leaderboard",{"id":9438,"data":9440,"filePath":9446,"digest":9447,"rendered":9448,"legacyId":9456},{"title":9441,"description":25,"summary":9442,"pubDate":9443,"source":2720,"url":9444,"thumbnail":9445},"What's going on with the Open LLM Leaderboard?","What's going on with the Open LLM Leaderboard? Recently an interesting discussion arose on Twitter f...",["Date","2023-06-23T00:00:00.000Z"],"https://huggingface.co/blog/open-llm-leaderboard-mmlu","https://huggingface.co/blog/assets/evaluating-mmlu-leaderboard/thumbnail.png","src/content/posts/2023-06-23-whats-going-on-with-the-open-llm-leaderboard.md","1a1b2145eda901f6",{"html":25,"metadata":9449},{"headings":9450,"localImagePaths":9451,"remoteImagePaths":9452,"frontmatter":9453,"imagePaths":9455},[],[],[],{"title":9441,"description":25,"summary":9442,"pubDate":9454,"source":2720,"url":9444,"thumbnail":9445},"Fri, 23 Jun 2023 00:00:00 GMT",[],"2023-06-23-whats-going-on-with-the-open-llm-leaderboard.md","2023-06-28-introducing-openai-london",{"id":9457,"data":9459,"filePath":9464,"digest":9465,"rendered":9466,"legacyId":9474},{"title":9460,"description":9461,"summary":9461,"pubDate":9462,"source":19,"url":9463,"thumbnail":21},"Introducing OpenAI London","We are excited to announce OpenAI’s first international expansion with a new office in London, United Kingdom.",["Date","2023-06-28T07:00:00.000Z"],"https://openai.com/blog/introducing-openai-london","src/content/posts/2023-06-28-introducing-openai-london.md","e8764153dc65b476",{"html":25,"metadata":9467},{"headings":9468,"localImagePaths":9469,"remoteImagePaths":9470,"frontmatter":9471,"imagePaths":9473},[],[],[],{"title":9460,"description":9461,"summary":9461,"pubDate":9472,"source":19,"url":9463,"thumbnail":21},"Wed, 28 Jun 2023 07:00:00 GMT",[],"2023-06-28-introducing-openai-london.md","2023-06-26-ethics-and-society-newsletter-4-bias-in-text-to-image-models",{"id":9475,"data":9477,"filePath":9483,"digest":9484,"rendered":9485,"legacyId":9493},{"title":9478,"description":25,"summary":9479,"pubDate":9480,"source":2720,"url":9481,"thumbnail":9482},"Ethics and Society Newsletter #4: Bias in Text-to-Image Models","Ethics and Society Newsletter #4: Bias in Text-to-Image Models TL;DR: We need better ways of evaluat...",["Date","2023-06-26T00:00:00.000Z"],"https://huggingface.co/blog/ethics-soc-4","https://huggingface.co/blog/assets/152_ethics_soc_4/ethics_4_thumbnail.png","src/content/posts/2023-06-26-ethics-and-society-newsletter-4-bias-in-text-to-image-models.md","093961401916544b",{"html":25,"metadata":9486},{"headings":9487,"localImagePaths":9488,"remoteImagePaths":9489,"frontmatter":9490,"imagePaths":9492},[],[],[],{"title":9478,"description":25,"summary":9479,"pubDate":9491,"source":2720,"url":9481,"thumbnail":9482},"Mon, 26 Jun 2023 00:00:00 GMT",[],"2023-06-26-ethics-and-society-newsletter-4-bias-in-text-to-image-models.md","2023-06-29-accelerating-vision-language-models-bridgetower-on-habana-gaudi2",{"id":9494,"data":9496,"filePath":9502,"digest":9503,"rendered":9504,"legacyId":9512},{"title":9497,"description":25,"summary":9498,"pubDate":9499,"source":2720,"url":9500,"thumbnail":9501},"Accelerating Vision-Language Models: BridgeTower on Habana Gaudi2","Accelerating Vision-Language Models: BridgeTower on Habana Gaudi2 Update (29/08/2023): A benchmark o...",["Date","2023-06-29T00:00:00.000Z"],"https://huggingface.co/blog/bridgetower","https://huggingface.co/blog/assets/bridgetower/thumbnail.png","src/content/posts/2023-06-29-accelerating-vision-language-models-bridgetower-on-habana-gaudi2.md","59934f8757e9bff7",{"html":25,"metadata":9505},{"headings":9506,"localImagePaths":9507,"remoteImagePaths":9508,"frontmatter":9509,"imagePaths":9511},[],[],[],{"title":9497,"description":25,"summary":9498,"pubDate":9510,"source":2720,"url":9500,"thumbnail":9501},"Thu, 29 Jun 2023 00:00:00 GMT",[],"2023-06-29-accelerating-vision-language-models-bridgetower-on-habana-gaudi2.md","2023-06-29-insights-from-global-conversations",{"id":9513,"data":9515,"filePath":9520,"digest":9521,"rendered":9522,"legacyId":9530},{"title":9516,"description":9517,"summary":9517,"pubDate":9518,"source":19,"url":9519,"thumbnail":21},"Insights from global conversations","We are sharing what we learned from our conversations across 22 countries, and how we will be incorporating those insights moving forward.",["Date","2023-06-29T07:00:00.000Z"],"https://openai.com/blog/insights-from-global-conversations","src/content/posts/2023-06-29-insights-from-global-conversations.md","ce211352a9802d30",{"html":25,"metadata":9523},{"headings":9524,"localImagePaths":9525,"remoteImagePaths":9526,"frontmatter":9527,"imagePaths":9529},[],[],[],{"title":9516,"description":9517,"summary":9517,"pubDate":9528,"source":19,"url":9519,"thumbnail":21},"Thu, 29 Jun 2023 07:00:00 GMT",[],"2023-06-29-insights-from-global-conversations.md","2023-07-01-leveraging-hugging-face-for-complex-generative-ai-use-cases",{"id":9531,"data":9533,"filePath":9539,"digest":9540,"rendered":9541,"legacyId":9549},{"title":9534,"description":25,"summary":9535,"pubDate":9536,"source":2720,"url":9537,"thumbnail":9538},"Leveraging Hugging Face for complex generative AI use cases","Leveraging Hugging Face for complex generative AI use casess Published July 1, 2023 Update on GitHub...",["Date","2023-07-01T00:00:00.000Z"],"https://huggingface.co/blog/writer-case-study","https://huggingface.co/blog/assets/78_ml_director_insights/writer.png","src/content/posts/2023-07-01-leveraging-hugging-face-for-complex-generative-ai-use-cases.md","0a113f5077ca90d8",{"html":25,"metadata":9542},{"headings":9543,"localImagePaths":9544,"remoteImagePaths":9545,"frontmatter":9546,"imagePaths":9548},[],[],[],{"title":9534,"description":25,"summary":9535,"pubDate":9547,"source":2720,"url":9537,"thumbnail":9538},"Sat, 01 Jul 2023 00:00:00 GMT",[],"2023-07-01-leveraging-hugging-face-for-complex-generative-ai-use-cases.md","2023-07-03-making-a-web-app-generator-with-open-ml-models",{"id":9550,"data":9552,"filePath":9558,"digest":9559,"rendered":9560,"legacyId":9568},{"title":9553,"description":25,"summary":9554,"pubDate":9555,"source":2720,"url":9556,"thumbnail":9557},"Making a web app generator with open ML models","Making a web app generator with open ML models As more code generation models become publicly availa...",["Date","2023-07-03T00:00:00.000Z"],"https://huggingface.co/blog/text-to-webapp","https://huggingface.co/blog/assets/153_text_to_webapp/thumbnail.jpg","src/content/posts/2023-07-03-making-a-web-app-generator-with-open-ml-models.md","9d82120af41bcb57",{"html":25,"metadata":9561},{"headings":9562,"localImagePaths":9563,"remoteImagePaths":9564,"frontmatter":9565,"imagePaths":9567},[],[],[],{"title":9553,"description":25,"summary":9554,"pubDate":9566,"source":2720,"url":9556,"thumbnail":9557},"Mon, 03 Jul 2023 00:00:00 GMT",[],"2023-07-03-making-a-web-app-generator-with-open-ml-models.md","2023-07-04-deploy-llms-with-hugging-face-inference-endpoints",{"id":9569,"data":9571,"filePath":9577,"digest":9578,"rendered":9579,"legacyId":9587},{"title":9572,"description":25,"summary":9573,"pubDate":9574,"source":2720,"url":9575,"thumbnail":9576},"Deploy LLMs with Hugging Face Inference Endpoints","Deploy LLMs with Hugging Face Inference Endpoints Open-source LLMs like Falcon, (Open-)LLaMA, X-Gen,...",["Date","2023-07-04T00:00:00.000Z"],"https://huggingface.co/blog/inference-endpoints-llm","https://huggingface.co/blog/assets/155_inference_endpoints_llm/thumbnail.jpg","src/content/posts/2023-07-04-deploy-llms-with-hugging-face-inference-endpoints.md","9382daae70393c77",{"html":25,"metadata":9580},{"headings":9581,"localImagePaths":9582,"remoteImagePaths":9583,"frontmatter":9584,"imagePaths":9586},[],[],[],{"title":9572,"description":25,"summary":9573,"pubDate":9585,"source":2720,"url":9575,"thumbnail":9576},"Tue, 04 Jul 2023 00:00:00 GMT",[],"2023-07-04-deploy-llms-with-hugging-face-inference-endpoints.md","2023-07-05-making-ml-powered-web-games-with-transformersjs",{"id":9588,"data":9590,"filePath":9596,"digest":9597,"rendered":9598,"legacyId":9606},{"title":9591,"description":25,"summary":9592,"pubDate":9593,"source":2720,"url":9594,"thumbnail":9595},"Making ML-powered web games with Transformers.js","Making ML-powered web games with Transformers.js In this blog post, I'll show you how I made Doodle ...",["Date","2023-07-05T00:00:00.000Z"],"https://huggingface.co/blog/ml-web-games","https://huggingface.co/blog/assets/ml-web-games/thumbnail.png","src/content/posts/2023-07-05-making-ml-powered-web-games-with-transformersjs.md","6e6afb92abd90c9c",{"html":25,"metadata":9599},{"headings":9600,"localImagePaths":9601,"remoteImagePaths":9602,"frontmatter":9603,"imagePaths":9605},[],[],[],{"title":9591,"description":25,"summary":9592,"pubDate":9604,"source":2720,"url":9594,"thumbnail":9595},"Wed, 05 Jul 2023 00:00:00 GMT",[],"2023-07-05-making-ml-powered-web-games-with-transformersjs.md","2023-07-06-frontier-ai-regulation-managing-emerging-risks-to-public-safety",{"id":9607,"data":9609,"filePath":9613,"digest":9614,"rendered":9615,"legacyId":9623},{"title":9610,"description":25,"summary":25,"pubDate":9611,"source":19,"url":9612,"thumbnail":21},"Frontier AI regulation: Managing emerging risks to public safety",["Date","2023-07-06T07:00:00.000Z"],"https://openai.com/blog/frontier-ai-regulation","src/content/posts/2023-07-06-frontier-ai-regulation-managing-emerging-risks-to-public-safety.md","f60d7dd0c9614b70",{"html":25,"metadata":9616},{"headings":9617,"localImagePaths":9618,"remoteImagePaths":9619,"frontmatter":9620,"imagePaths":9622},[],[],[],{"title":9610,"description":25,"summary":25,"pubDate":9621,"source":19,"url":9612,"thumbnail":21},"Thu, 06 Jul 2023 07:00:00 GMT",[],"2023-07-06-frontier-ai-regulation-managing-emerging-risks-to-public-safety.md","2023-07-07-accurately-analyzing-large-scale-qualitative-data",{"id":9624,"data":9626,"filePath":9631,"digest":9632,"rendered":9633,"legacyId":9641},{"title":9627,"description":9628,"summary":9628,"pubDate":9629,"source":19,"url":9630,"thumbnail":21},"Accurately analyzing large scale qualitative data","Viable uses GPT-4 to analyze qualitative data at a revolutionary scale with unparalleled accuracy.",["Date","2023-07-07T07:00:00.000Z"],"https://openai.com/blog/viable","src/content/posts/2023-07-07-accurately-analyzing-large-scale-qualitative-data.md","8ee88c1f4702f5aa",{"html":25,"metadata":9634},{"headings":9635,"localImagePaths":9636,"remoteImagePaths":9637,"frontmatter":9638,"imagePaths":9640},[],[],[],{"title":9627,"description":9628,"summary":9628,"pubDate":9639,"source":19,"url":9630,"thumbnail":21},"Fri, 07 Jul 2023 07:00:00 GMT",[],"2023-07-07-accurately-analyzing-large-scale-qualitative-data.md","2023-07-11-exploring-institutions-for-global-ai-governance",{"id":9642,"data":9644,"filePath":9650,"digest":9651,"rendered":9652,"legacyId":9660},{"title":9645,"description":9646,"summary":9646,"pubDate":9647,"source":6423,"url":9648,"thumbnail":9649},"Exploring institutions for global AI governance","New white paper investigates models and functions of international institutions that could help manage opportunities and mitigate risks of advanced AI.",["Date","2023-07-11T00:00:00.000Z"],"https://deepmind.google/discover/blog/exploring-institutions-for-global-ai-governance/","https://lh3.googleusercontent.com/Y9dCJWt3ky1gjizSCHb17S3iHZ_Q2v6hoC8SaBgq9f7e5yW15pzg7BGNoCIaklP6f34uioxwHY0gbzehAMe5HhXBvBBKBKNIcOo7ugjFeLENTWMqNQ=w1200-h630-n-nu","src/content/posts/2023-07-11-exploring-institutions-for-global-ai-governance.md","e13c067495c9ca59",{"html":25,"metadata":9653},{"headings":9654,"localImagePaths":9655,"remoteImagePaths":9656,"frontmatter":9657,"imagePaths":9659},[],[],[],{"title":9645,"description":9646,"summary":9646,"pubDate":9658,"source":6423,"url":9648,"thumbnail":9649},"Tue, 11 Jul 2023 00:00:00 +0000",[],"2023-07-11-exploring-institutions-for-global-ai-governance.md","2023-07-14-fine-tuning-stable-diffusion-models-on-intel-cpus",{"id":9661,"data":9663,"filePath":9669,"digest":9670,"rendered":9671,"legacyId":9679},{"title":9664,"description":25,"summary":9665,"pubDate":9666,"source":2720,"url":9667,"thumbnail":9668},"Fine-tuning Stable Diffusion models on Intel CPUs","Fine-tuning Stable Diffusion Models on Intel CPUs Diffusion models helped popularize generative AI t...",["Date","2023-07-14T00:00:00.000Z"],"https://huggingface.co/blog/stable-diffusion-finetuning-intel","https://huggingface.co/blog/assets/stable-diffusion-finetuning-intel/01.png","src/content/posts/2023-07-14-fine-tuning-stable-diffusion-models-on-intel-cpus.md","ff22cf2a29c5e94f",{"html":25,"metadata":9672},{"headings":9673,"localImagePaths":9674,"remoteImagePaths":9675,"frontmatter":9676,"imagePaths":9678},[],[],[],{"title":9664,"description":25,"summary":9665,"pubDate":9677,"source":2720,"url":9667,"thumbnail":9668},"Fri, 14 Jul 2023 00:00:00 GMT",[],"2023-07-14-fine-tuning-stable-diffusion-models-on-intel-cpus.md","2023-07-17-building-an-ai-webtv",{"id":9680,"data":9682,"filePath":9688,"digest":9689,"rendered":9690,"legacyId":9698},{"title":9683,"description":25,"summary":9684,"pubDate":9685,"source":2720,"url":9686,"thumbnail":9687},"Building an AI WebTV","Building an AI WebTV The AI WebTV is an experimental demo to showcase the latest advancements in aut...",["Date","2023-07-17T00:00:00.000Z"],"https://huggingface.co/blog/ai-webtv","https://huggingface.co/blog/assets/156_ai_webtv/thumbnail.gif","src/content/posts/2023-07-17-building-an-ai-webtv.md","f01926096a55106f",{"html":25,"metadata":9691},{"headings":9692,"localImagePaths":9693,"remoteImagePaths":9694,"frontmatter":9695,"imagePaths":9697},[],[],[],{"title":9683,"description":25,"summary":9684,"pubDate":9696,"source":2720,"url":9686,"thumbnail":9687},"Mon, 17 Jul 2023 00:00:00 GMT",[],"2023-07-17-building-an-ai-webtv.md","2023-07-17-developing-reliable-ai-tools-for-healthcare",{"id":9699,"data":9701,"filePath":9707,"digest":9708,"rendered":9709,"legacyId":9717},{"title":9702,"description":9703,"summary":9703,"pubDate":9704,"source":6423,"url":9705,"thumbnail":9706},"Developing reliable AI tools for healthcare","We’ve published our joint paper with Google Research in Nature Medicine, which proposes CoDoC (Complementarity-driven Deferral-to-Clinical Workflow), an AI system that learns when to rely on predictive AI tools or defer to a clinician for the most accurate interpretation of medical images.",["Date","2023-07-17T00:00:00.000Z"],"https://deepmind.google/discover/blog/codoc-developing-reliable-ai-tools-for-healthcare/","https://lh3.googleusercontent.com/JCyH0sgVtuYFCB0n7g6f2NMV19yeAgvxQBqcfy9H_-DP_aW3k5h4i0bcZ9_9KCExs7rXRrCaC6s21uK5Udap6tX3zy96zOdn8YcF5WIxAFzUgru6Nw=w1200-h630-n-nu","src/content/posts/2023-07-17-developing-reliable-ai-tools-for-healthcare.md","40478f6b487ffec5",{"html":25,"metadata":9710},{"headings":9711,"localImagePaths":9712,"remoteImagePaths":9713,"frontmatter":9714,"imagePaths":9716},[],[],[],{"title":9702,"description":9703,"summary":9703,"pubDate":9715,"source":6423,"url":9705,"thumbnail":9706},"Mon, 17 Jul 2023 00:00:00 +0000",[],"2023-07-17-developing-reliable-ai-tools-for-healthcare.md","2023-07-17-open-source-text-generation-llm-ecosystem-at-hugging-face",{"id":9718,"data":9720,"filePath":9726,"digest":9727,"rendered":9728,"legacyId":9735},{"title":9721,"description":25,"summary":9722,"pubDate":9723,"source":2720,"url":9724,"thumbnail":9725},"Open-Source Text Generation & LLM Ecosystem at Hugging Face","Open-Source Text Generation & LLM Ecosystem at Hugging Face [Updated on July 24, 2023: Added Llama 2...",["Date","2023-07-17T00:00:00.000Z"],"https://huggingface.co/blog/os-llms","https://huggingface.co/blog/assets/os_llms/thumbnail.png","src/content/posts/2023-07-17-open-source-text-generation-llm-ecosystem-at-hugging-face.md","6361f9961aed95a4",{"html":25,"metadata":9729},{"headings":9730,"localImagePaths":9731,"remoteImagePaths":9732,"frontmatter":9733,"imagePaths":9734},[],[],[],{"title":9721,"description":25,"summary":9722,"pubDate":9696,"source":2720,"url":9724,"thumbnail":9725},[],"2023-07-17-open-source-text-generation-llm-ecosystem-at-hugging-face.md","2023-07-18-llama-2-is-here---get-it-on-hugging-face",{"id":9736,"data":9738,"filePath":9744,"digest":9745,"rendered":9746,"legacyId":9754},{"title":9739,"description":25,"summary":9740,"pubDate":9741,"source":2720,"url":9742,"thumbnail":9743},"Llama 2 is here - get it on Hugging Face","Llama 2 is here - get it on Hugging Face Introduction Llama 2 is a family of state-of-the-art open-a...",["Date","2023-07-18T00:00:00.000Z"],"https://huggingface.co/blog/llama2","https://huggingface.co/blog/assets/llama2/thumbnail.jpg","src/content/posts/2023-07-18-llama-2-is-here---get-it-on-hugging-face.md","6853da337f3742da",{"html":25,"metadata":9747},{"headings":9748,"localImagePaths":9749,"remoteImagePaths":9750,"frontmatter":9751,"imagePaths":9753},[],[],[],{"title":9739,"description":25,"summary":9740,"pubDate":9752,"source":2720,"url":9742,"thumbnail":9743},"Tue, 18 Jul 2023 00:00:00 GMT",[],"2023-07-18-llama-2-is-here---get-it-on-hugging-face.md","2023-07-18-partnership-with-american-journalism-project-to-support-local-news",{"id":9755,"data":9757,"filePath":9762,"digest":9763,"rendered":9764,"legacyId":9772},{"title":9758,"description":9759,"summary":9759,"pubDate":9760,"source":19,"url":9761,"thumbnail":21},"Partnership with American Journalism Project to support local news","A new $5+ million partnership aims to explore ways the development of artificial intelligence (AI) can support a thriving, innovative local news field, and ensure local news organizations shape the future of this emerging technology.",["Date","2023-07-18T07:00:00.000Z"],"https://openai.com/blog/partnership-with-american-journalism-project-to-support-local-news","src/content/posts/2023-07-18-partnership-with-american-journalism-project-to-support-local-news.md","f92b6738853997ee",{"html":25,"metadata":9765},{"headings":9766,"localImagePaths":9767,"remoteImagePaths":9768,"frontmatter":9769,"imagePaths":9771},[],[],[],{"title":9758,"description":9759,"summary":9759,"pubDate":9770,"source":19,"url":9761,"thumbnail":21},"Tue, 18 Jul 2023 07:00:00 GMT",[],"2023-07-18-partnership-with-american-journalism-project-to-support-local-news.md","2023-07-20-custom-instructions-for-chatgpt",{"id":9773,"data":9775,"filePath":9780,"digest":9781,"rendered":9782,"legacyId":9790},{"title":9776,"description":9777,"summary":9777,"pubDate":9778,"source":19,"url":9779,"thumbnail":21},"Custom instructions for ChatGPT","We’re rolling out custom instructions to give you more control over how ChatGPT responds. Set your preferences, and ChatGPT will keep them in mind for all future conversations.",["Date","2023-07-20T07:00:00.000Z"],"https://openai.com/blog/custom-instructions-for-chatgpt","src/content/posts/2023-07-20-custom-instructions-for-chatgpt.md","7a11456635581b99",{"html":25,"metadata":9783},{"headings":9784,"localImagePaths":9785,"remoteImagePaths":9786,"frontmatter":9787,"imagePaths":9789},[],[],[],{"title":9776,"description":9777,"summary":9777,"pubDate":9788,"source":19,"url":9779,"thumbnail":21},"Thu, 20 Jul 2023 07:00:00 GMT",[],"2023-07-20-custom-instructions-for-chatgpt.md","2023-07-20-happy-1st-anniversary-diffusers",{"id":9791,"data":9793,"filePath":9799,"digest":9800,"rendered":9801,"legacyId":9809},{"title":9794,"description":25,"summary":9795,"pubDate":9796,"source":2720,"url":9797,"thumbnail":9798},"Happy 1st anniversary 🤗 Diffusers!","Happy 1st anniversary 🤗 Diffusers! 🤗 Diffusers is happy to celebrate its first anniversary! It has b...",["Date","2023-07-20T00:00:00.000Z"],"https://huggingface.co/blog/diffusers-turns-1","https://huggingface.co/blog/assets/diffusers-turns-1/diffusers-turns-1.png","src/content/posts/2023-07-20-happy-1st-anniversary-diffusers.md","149d66cb743d8e28",{"html":25,"metadata":9802},{"headings":9803,"localImagePaths":9804,"remoteImagePaths":9805,"frontmatter":9806,"imagePaths":9808},[],[],[],{"title":9794,"description":25,"summary":9795,"pubDate":9807,"source":2720,"url":9797,"thumbnail":9798},"Thu, 20 Jul 2023 00:00:00 GMT",[],"2023-07-20-happy-1st-anniversary-diffusers.md","2023-07-21-moving-ai-governance-forward",{"id":9810,"data":9812,"filePath":9817,"digest":9818,"rendered":9819,"legacyId":9827},{"title":9813,"description":9814,"summary":9814,"pubDate":9815,"source":19,"url":9816,"thumbnail":21},"Moving AI governance forward","OpenAI and other leading labs reinforce AI safety, security and trustworthiness through voluntary commitments.",["Date","2023-07-21T07:00:00.000Z"],"https://openai.com/blog/moving-ai-governance-forward","src/content/posts/2023-07-21-moving-ai-governance-forward.md","0b56e9e5d6df6c2a",{"html":25,"metadata":9820},{"headings":9821,"localImagePaths":9822,"remoteImagePaths":9823,"frontmatter":9824,"imagePaths":9826},[],[],[],{"title":9813,"description":9814,"summary":9814,"pubDate":9825,"source":19,"url":9816,"thumbnail":21},"Fri, 21 Jul 2023 07:00:00 GMT",[],"2023-07-21-moving-ai-governance-forward.md","2023-07-21-results-of-the-open-source-ai-game-jam",{"id":9828,"data":9830,"filePath":9836,"digest":9837,"rendered":9838,"legacyId":9846},{"title":9831,"description":25,"summary":9832,"pubDate":9833,"source":2720,"url":9834,"thumbnail":9835},"Results of the Open Source AI Game Jam","Results of the Open Source AI Game Jam From July 7th to July 11th, we hosted our first Open Source A...",["Date","2023-07-21T00:00:00.000Z"],"https://huggingface.co/blog/game-jam-first-edition-results","https://huggingface.co/blog/assets/game-jam-first-edition-results/thumbnail.jpg","src/content/posts/2023-07-21-results-of-the-open-source-ai-game-jam.md","9f1beaa893b2e44c",{"html":25,"metadata":9839},{"headings":9840,"localImagePaths":9841,"remoteImagePaths":9842,"frontmatter":9843,"imagePaths":9845},[],[],[],{"title":9831,"description":25,"summary":9832,"pubDate":9844,"source":2720,"url":9834,"thumbnail":9835},"Fri, 21 Jul 2023 00:00:00 GMT",[],"2023-07-21-results-of-the-open-source-ai-game-jam.md","2023-07-20-google-deepminds-latest-research-at-icml-2023",{"id":9847,"data":9849,"filePath":9855,"digest":9856,"rendered":9857,"legacyId":9865},{"title":9850,"description":9851,"summary":9851,"pubDate":9852,"source":6423,"url":9853,"thumbnail":9854},"Google DeepMind’s latest research at ICML 2023","Exploring AI safety, adaptability, and efficiency for the real world",["Date","2023-07-20T00:00:00.000Z"],"https://deepmind.google/discover/blog/google-deepmind-research-at-icml-2023/","https://lh3.googleusercontent.com/5UyUwX8KGovLbTZ0Q8Ynf5Nepy-1zyFaVIIwB7ty0Cp1F5wrKrv24aOT91PDo1vpH3T4P0cwtUn1WxxvtU5vqd4J7cBwEK6UsvnTMNL_qramtFbsX28=w1200-h630-n-nu","src/content/posts/2023-07-20-google-deepminds-latest-research-at-icml-2023.md","efd686f50f0f43af",{"html":25,"metadata":9858},{"headings":9859,"localImagePaths":9860,"remoteImagePaths":9861,"frontmatter":9862,"imagePaths":9864},[],[],[],{"title":9850,"description":9851,"summary":9851,"pubDate":9863,"source":6423,"url":9853,"thumbnail":9854},"Thu, 20 Jul 2023 00:00:00 +0000",[],"2023-07-20-google-deepminds-latest-research-at-icml-2023.md","2023-07-21-using-ai-to-fight-climate-change",{"id":9866,"data":9868,"filePath":9874,"digest":9875,"rendered":9876,"legacyId":9884},{"title":9869,"description":9870,"summary":9870,"pubDate":9871,"source":6423,"url":9872,"thumbnail":9873},"Using AI to fight climate change","AI is a powerful technology that will transform our future, so how can we best apply it to help combat climate change and find sustainable solutions?",["Date","2023-07-21T00:00:00.000Z"],"https://deepmind.google/discover/blog/using-ai-to-fight-climate-change/","https://lh3.googleusercontent.com/_7lNDyMo0JuzMRu0wVUtaJuXaEPDy8ay20vcsv08JvF3fMkEbk20mGBWdI09Wg0USIinNH5urB5nudEGZWRvTeUNOz_WOAwcduNdQQQNGx-JgtQE1aE=w1200-h630-n-nu","src/content/posts/2023-07-21-using-ai-to-fight-climate-change.md","bf7fb1bdc1b73ee5",{"html":25,"metadata":9877},{"headings":9878,"localImagePaths":9879,"remoteImagePaths":9880,"frontmatter":9881,"imagePaths":9883},[],[],[],{"title":9869,"description":9870,"summary":9870,"pubDate":9882,"source":6423,"url":9872,"thumbnail":9873},"Fri, 21 Jul 2023 00:00:00 +0000",[],"2023-07-21-using-ai-to-fight-climate-change.md","2023-07-24-ai-policy-open-ml-considerations-in-the-eu-ai-act",{"id":9885,"data":9887,"filePath":9893,"digest":9894,"rendered":9895,"legacyId":9903},{"title":9888,"description":25,"summary":9889,"pubDate":9890,"source":2720,"url":9891,"thumbnail":9892},"AI Policy @🤗: Open ML Considerations in the EU AI Act","AI Policy @🤗: Open ML Considerations in the EU AI Act Like everyone else in Machine Learning, we’ve ...",["Date","2023-07-24T00:00:00.000Z"],"https://huggingface.co/blog/eu-ai-act-oss","https://huggingface.co/blog/assets/eu_ai_act_oss/thumbnailEU.png","src/content/posts/2023-07-24-ai-policy-open-ml-considerations-in-the-eu-ai-act.md","390fa65d131e85d0",{"html":25,"metadata":9896},{"headings":9897,"localImagePaths":9898,"remoteImagePaths":9899,"frontmatter":9900,"imagePaths":9902},[],[],[],{"title":9888,"description":25,"summary":9889,"pubDate":9901,"source":2720,"url":9891,"thumbnail":9892},"Mon, 24 Jul 2023 00:00:00 GMT",[],"2023-07-24-ai-policy-open-ml-considerations-in-the-eu-ai-act.md","2023-07-27-stable-diffusion-xl-on-mac-with-advanced-core-ml-quantization",{"id":9904,"data":9906,"filePath":9912,"digest":9913,"rendered":9914,"legacyId":9922},{"title":9907,"description":25,"summary":9908,"pubDate":9909,"source":2720,"url":9910,"thumbnail":9911},"Stable Diffusion XL on Mac with Advanced Core ML Quantization","Stable Diffusion XL on Mac with Advanced Core ML Quantization Stable Diffusion XL was released yeste...",["Date","2023-07-27T00:00:00.000Z"],"https://huggingface.co/blog/stable-diffusion-xl-coreml","https://huggingface.co/blog/assets/stable-diffusion-xl-coreml/thumbnail.png","src/content/posts/2023-07-27-stable-diffusion-xl-on-mac-with-advanced-core-ml-quantization.md","eabcf255ccc0bddf",{"html":25,"metadata":9915},{"headings":9916,"localImagePaths":9917,"remoteImagePaths":9918,"frontmatter":9919,"imagePaths":9921},[],[],[],{"title":9907,"description":25,"summary":9908,"pubDate":9920,"source":2720,"url":9910,"thumbnail":9911},"Thu, 27 Jul 2023 00:00:00 GMT",[],"2023-07-27-stable-diffusion-xl-on-mac-with-advanced-core-ml-quantization.md","2023-07-26-frontier-model-forum",{"id":9923,"data":9925,"filePath":9930,"digest":9931,"rendered":9932,"legacyId":9940},{"title":9926,"description":9927,"summary":9927,"pubDate":9928,"source":19,"url":9929,"thumbnail":21},"Frontier Model Forum","We’re forming a new industry body to promote the safe and responsible development of frontier AI systems: advancing AI safety research, identifying best practices and standards, and facilitating information sharing among policymakers and industry.",["Date","2023-07-26T07:00:00.000Z"],"https://openai.com/blog/frontier-model-forum","src/content/posts/2023-07-26-frontier-model-forum.md","f5c483b9c2ca37c5",{"html":25,"metadata":9933},{"headings":9934,"localImagePaths":9935,"remoteImagePaths":9936,"frontmatter":9937,"imagePaths":9939},[],[],[],{"title":9926,"description":9927,"summary":9927,"pubDate":9938,"source":19,"url":9929,"thumbnail":21},"Wed, 26 Jul 2023 07:00:00 GMT",[],"2023-07-26-frontier-model-forum.md","2023-07-24-introducing-agentsjs-give-tools-to-your-llms-using-javascript",{"id":9941,"data":9943,"filePath":9949,"digest":9950,"rendered":9951,"legacyId":9958},{"title":9944,"description":25,"summary":9945,"pubDate":9946,"source":2720,"url":9947,"thumbnail":9948},"Introducing Agents.js: Give tools to your LLMs using JavaScript","Introducing Agents.js: Give tools to your LLMs using JavaScript We have recently been working on Age...",["Date","2023-07-24T00:00:00.000Z"],"https://huggingface.co/blog/agents-js","https://huggingface.co/blog/assets/agents-js/thumbnail.png","src/content/posts/2023-07-24-introducing-agentsjs-give-tools-to-your-llms-using-javascript.md","03a676e35c80fbd8",{"html":25,"metadata":9952},{"headings":9953,"localImagePaths":9954,"remoteImagePaths":9955,"frontmatter":9956,"imagePaths":9957},[],[],[],{"title":9944,"description":25,"summary":9945,"pubDate":9901,"source":2720,"url":9947,"thumbnail":9948},[],"2023-07-24-introducing-agentsjs-give-tools-to-your-llms-using-javascript.md","2023-07-28-rt-2-new-model-translates-vision-and-language-into-action",{"id":9959,"data":9961,"filePath":9967,"digest":9968,"rendered":9969,"legacyId":9977},{"title":9962,"description":9963,"summary":9963,"pubDate":9964,"source":6423,"url":9965,"thumbnail":9966},"RT-2: New model translates vision and language into action","Robotic Transformer 2 (RT-2) is a novel vision-language-action (VLA) model that learns from both web and robotics data, and translates this knowledge into generalised instructions for robotic control.",["Date","2023-07-28T00:00:00.000Z"],"https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/","https://lh3.googleusercontent.com/ZduBtQRn2mrvSfNqkixe2XktBREieIhekS7NcboCn0E76gFVckUwNLZw74EJ5jIndzxbRoCqCY47iW1-eGi5c_JJV1DFyTmkS91vMnRalgT0rih125s=w1200-h630-n-nu","src/content/posts/2023-07-28-rt-2-new-model-translates-vision-and-language-into-action.md","d45212dd20b8e59d",{"html":25,"metadata":9970},{"headings":9971,"localImagePaths":9972,"remoteImagePaths":9973,"frontmatter":9974,"imagePaths":9976},[],[],[],{"title":9962,"description":9963,"summary":9963,"pubDate":9975,"source":6423,"url":9965,"thumbnail":9966},"Fri, 28 Jul 2023 00:00:00 +0000",[],"2023-07-28-rt-2-new-model-translates-vision-and-language-into-action.md","2023-08-01-confidence-building-measures-for-artificial-intelligence-workshop-proceedings",{"id":9978,"data":9980,"filePath":9984,"digest":9985,"rendered":9986,"legacyId":9994},{"title":9981,"description":25,"summary":25,"pubDate":9982,"source":19,"url":9983,"thumbnail":21},"Confidence-Building Measures for Artificial Intelligence: Workshop proceedings",["Date","2023-08-01T07:00:00.000Z"],"https://openai.com/blog/confidence-building-measures-for-artificial-intelligence","src/content/posts/2023-08-01-confidence-building-measures-for-artificial-intelligence-workshop-proceedings.md","289ecc3eb79af044",{"html":25,"metadata":9987},{"headings":9988,"localImagePaths":9989,"remoteImagePaths":9990,"frontmatter":9991,"imagePaths":9993},[],[],[],{"title":9981,"description":25,"summary":25,"pubDate":9992,"source":19,"url":9983,"thumbnail":21},"Tue, 01 Aug 2023 07:00:00 GMT",[],"2023-08-01-confidence-building-measures-for-artificial-intelligence-workshop-proceedings.md","2023-08-01-open-sourcing-knowledge-distillation-code-and-weights-of-sd-small-and-sd-tiny",{"id":9995,"data":9997,"filePath":10003,"digest":10004,"rendered":10005,"legacyId":10013},{"title":9998,"description":25,"summary":9999,"pubDate":10000,"source":2720,"url":10001,"thumbnail":10002},"Open-sourcing Knowledge Distillation Code and Weights of SD-Small and SD-Tiny","Open-sourcing Knowledge Distillation Code and Weights of SD-Small and SD-Tiny In recent times, the A...",["Date","2023-08-01T00:00:00.000Z"],"https://huggingface.co/blog/sd_distillation","https://huggingface.co/blog/assets/distill_sd/thumbnail.png","src/content/posts/2023-08-01-open-sourcing-knowledge-distillation-code-and-weights-of-sd-small-and-sd-tiny.md","08a3dd3a222f811d",{"html":25,"metadata":10006},{"headings":10007,"localImagePaths":10008,"remoteImagePaths":10009,"frontmatter":10010,"imagePaths":10012},[],[],[],{"title":9998,"description":25,"summary":9999,"pubDate":10011,"source":2720,"url":10001,"thumbnail":10002},"Tue, 01 Aug 2023 00:00:00 GMT",[],"2023-08-01-open-sourcing-knowledge-distillation-code-and-weights-of-sd-small-and-sd-tiny.md","2023-08-01-practical-3d-asset-generation-a-step-by-step-guide",{"id":10014,"data":10016,"filePath":10022,"digest":10023,"rendered":10024,"legacyId":10031},{"title":10017,"description":25,"summary":10018,"pubDate":10019,"source":2720,"url":10020,"thumbnail":10021},"Practical 3D Asset Generation: A Step-by-Step Guide","Practical 3D Asset Generation: A Step-by-Step Guide Introduction Generative AI has become an instrum...",["Date","2023-08-01T00:00:00.000Z"],"https://huggingface.co/blog/3d-assets","https://huggingface.co/blog/assets/124_ml-for-games/thumbnail-3d.jpg","src/content/posts/2023-08-01-practical-3d-asset-generation-a-step-by-step-guide.md","6bc09a3d2108749d",{"html":25,"metadata":10025},{"headings":10026,"localImagePaths":10027,"remoteImagePaths":10028,"frontmatter":10029,"imagePaths":10030},[],[],[],{"title":10017,"description":25,"summary":10018,"pubDate":10011,"source":2720,"url":10020,"thumbnail":10021},[],"2023-08-01-practical-3d-asset-generation-a-step-by-step-guide.md","2023-08-02-huggy-lingo-using-machine-learning-to-improve-language-metadata-on-the-hugging-face-hub",{"id":10032,"data":10034,"filePath":10040,"digest":10041,"rendered":10042,"legacyId":10050},{"title":10035,"description":25,"summary":10036,"pubDate":10037,"source":2720,"url":10038,"thumbnail":10039},"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub","Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub Huggy Lingo...",["Date","2023-08-02T00:00:00.000Z"],"https://huggingface.co/blog/huggy-lingo","https://huggingface.co/blog/huggy-lingo/blog/assets/156_huggylingo/Huggy_Lingo.png","src/content/posts/2023-08-02-huggy-lingo-using-machine-learning-to-improve-language-metadata-on-the-hugging-face-hub.md","be2594734217766b",{"html":25,"metadata":10043},{"headings":10044,"localImagePaths":10045,"remoteImagePaths":10046,"frontmatter":10047,"imagePaths":10049},[],[],[],{"title":10035,"description":25,"summary":10036,"pubDate":10048,"source":2720,"url":10038,"thumbnail":10039},"Wed, 02 Aug 2023 00:00:00 GMT",[],"2023-08-02-huggy-lingo-using-machine-learning-to-improve-language-metadata-on-the-hugging-face-hub.md","2023-08-02-towards-encrypted-large-language-models-with-fhe",{"id":10051,"data":10053,"filePath":10059,"digest":10060,"rendered":10061,"legacyId":10068},{"title":10054,"description":25,"summary":10055,"pubDate":10056,"source":2720,"url":10057,"thumbnail":10058},"Towards Encrypted Large Language Models with FHE","Towards Encrypted Large Language Models with FHE Large Language Models (LLM) have recently been prov...",["Date","2023-08-02T00:00:00.000Z"],"https://huggingface.co/blog/encrypted-llm","https://huggingface.co/blog/assets/encrypted-llm/thumbnail.png","src/content/posts/2023-08-02-towards-encrypted-large-language-models-with-fhe.md","3ebde7ce55218e48",{"html":25,"metadata":10062},{"headings":10063,"localImagePaths":10064,"remoteImagePaths":10065,"frontmatter":10066,"imagePaths":10067},[],[],[],{"title":10054,"description":25,"summary":10055,"pubDate":10048,"source":2720,"url":10057,"thumbnail":10058},[],"2023-08-02-towards-encrypted-large-language-models-with-fhe.md","2023-08-04-deploy-musicgen-in-no-time-with-inference-endpoints",{"id":10069,"data":10071,"filePath":10077,"digest":10078,"rendered":10079,"legacyId":10087},{"title":10072,"description":25,"summary":10073,"pubDate":10074,"source":2720,"url":10075,"thumbnail":10076},"Deploy MusicGen in no time with Inference Endpoints","Deploy MusicGen in no time with Inference Endpoints MusicGen is a powerful music generation model th...",["Date","2023-08-04T00:00:00.000Z"],"https://huggingface.co/blog/run-musicgen-as-an-api","https://huggingface.co/blog/assets/run-musicgen-as-an-api/thumbnail.png","src/content/posts/2023-08-04-deploy-musicgen-in-no-time-with-inference-endpoints.md","691475b9dd53ded6",{"html":25,"metadata":10080},{"headings":10081,"localImagePaths":10082,"remoteImagePaths":10083,"frontmatter":10084,"imagePaths":10086},[],[],[],{"title":10072,"description":25,"summary":10073,"pubDate":10085,"source":2720,"url":10075,"thumbnail":10076},"Fri, 04 Aug 2023 00:00:00 GMT",[],"2023-08-04-deploy-musicgen-in-no-time-with-inference-endpoints.md","2023-08-08-fine-tune-llama-2-with-dpo",{"id":10088,"data":10090,"filePath":10096,"digest":10097,"rendered":10098,"legacyId":10106},{"title":10091,"description":25,"summary":10092,"pubDate":10093,"source":2720,"url":10094,"thumbnail":10095},"Fine-tune Llama 2 with DPO","Fine-tune Llama 2 with DPO Introduction Reinforcement Learning from Human Feedback (RLHF) has become...",["Date","2023-08-08T00:00:00.000Z"],"https://huggingface.co/blog/dpo-trl","https://huggingface.co/blog/assets/157_dpo_trl/dpo_thumbnail.png","src/content/posts/2023-08-08-fine-tune-llama-2-with-dpo.md","a4ff4a0377edd4aa",{"html":25,"metadata":10099},{"headings":10100,"localImagePaths":10101,"remoteImagePaths":10102,"frontmatter":10103,"imagePaths":10105},[],[],[],{"title":10091,"description":25,"summary":10092,"pubDate":10104,"source":2720,"url":10094,"thumbnail":10095},"Tue, 08 Aug 2023 00:00:00 GMT",[],"2023-08-08-fine-tune-llama-2-with-dpo.md","2023-08-08-releasing-swift-transformers-run-on-device-llms-in-apple-devices",{"id":10107,"data":10109,"filePath":10115,"digest":10116,"rendered":10117,"legacyId":10124},{"title":10110,"description":25,"summary":10111,"pubDate":10112,"source":2720,"url":10113,"thumbnail":10114},"Releasing Swift Transformers: Run On-Device LLMs in Apple Devices","Releasing Swift Transformers: Run On-Device LLMs in Apple Devices I have a lot of respect for iOS/Ma...",["Date","2023-08-08T00:00:00.000Z"],"https://huggingface.co/blog/swift-coreml-llm","https://huggingface.co/blog/assets/swift-coreml-llm/thumbnail.png","src/content/posts/2023-08-08-releasing-swift-transformers-run-on-device-llms-in-apple-devices.md","4197116b530168d5",{"html":25,"metadata":10118},{"headings":10119,"localImagePaths":10120,"remoteImagePaths":10121,"frontmatter":10122,"imagePaths":10123},[],[],[],{"title":10110,"description":25,"summary":10111,"pubDate":10104,"source":2720,"url":10113,"thumbnail":10114},[],"2023-08-08-releasing-swift-transformers-run-on-device-llms-in-apple-devices.md","2023-08-09-deploying-hugging-face-models-with-bentoml-deepfloyd-if-in-action",{"id":10125,"data":10127,"filePath":10133,"digest":10134,"rendered":10135,"legacyId":10143},{"title":10128,"description":25,"summary":10129,"pubDate":10130,"source":2720,"url":10131,"thumbnail":10132},"Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action","Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action Hugging Face provides a Hub platf...",["Date","2023-08-09T00:00:00.000Z"],"https://huggingface.co/blog/deploy-deepfloydif-using-bentoml","https://huggingface.co/blog/assets/deploy-deepfloydif-using-bentoml/thumbnail.png","src/content/posts/2023-08-09-deploying-hugging-face-models-with-bentoml-deepfloyd-if-in-action.md","52e05eb853654dbf",{"html":25,"metadata":10136},{"headings":10137,"localImagePaths":10138,"remoteImagePaths":10139,"frontmatter":10140,"imagePaths":10142},[],[],[],{"title":10128,"description":25,"summary":10129,"pubDate":10141,"source":2720,"url":10131,"thumbnail":10132},"Wed, 09 Aug 2023 00:00:00 GMT",[],"2023-08-09-deploying-hugging-face-models-with-bentoml-deepfloyd-if-in-action.md","2023-08-09-optimizing-bark-using-transformers",{"id":10144,"data":10146,"filePath":10152,"digest":10153,"rendered":10154,"legacyId":10161},{"title":10147,"description":25,"summary":10148,"pubDate":10149,"source":2720,"url":10150,"thumbnail":10151},"Optimizing Bark using 🤗 Transformers","Optimizing a Text-To-Speech model using 🤗 Transformers 🤗 Transformers provides many of the latest st...",["Date","2023-08-09T00:00:00.000Z"],"https://huggingface.co/blog/optimizing-bark","https://huggingface.co/blog/assets/bark_optimization/thumbnail.png","src/content/posts/2023-08-09-optimizing-bark-using-transformers.md","e0096531d3532b41",{"html":25,"metadata":10155},{"headings":10156,"localImagePaths":10157,"remoteImagePaths":10158,"frontmatter":10159,"imagePaths":10160},[],[],[],{"title":10147,"description":25,"summary":10148,"pubDate":10141,"source":2720,"url":10150,"thumbnail":10151},[],"2023-08-09-optimizing-bark-using-transformers.md","2023-08-10-hugging-face-platform-on-the-aws-marketplace-pay-with-your-aws-account",{"id":10162,"data":10164,"filePath":10170,"digest":10171,"rendered":10172,"legacyId":10180},{"title":10165,"description":25,"summary":10166,"pubDate":10167,"source":2720,"url":10168,"thumbnail":10169},"Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account","Hugging Face Hub on the AWS Marketplace: Pay with your AWS Account The Hugging Face Hub has landed o...",["Date","2023-08-10T00:00:00.000Z"],"https://huggingface.co/blog/aws-marketplace","https://huggingface.co/blog/assets/158_aws_marketplace/thumbnail.jpg","src/content/posts/2023-08-10-hugging-face-platform-on-the-aws-marketplace-pay-with-your-aws-account.md","8edf5d81588d44c4",{"html":25,"metadata":10173},{"headings":10174,"localImagePaths":10175,"remoteImagePaths":10176,"frontmatter":10177,"imagePaths":10179},[],[],[],{"title":10165,"description":25,"summary":10166,"pubDate":10178,"source":2720,"url":10168,"thumbnail":10169},"Thu, 10 Aug 2023 00:00:00 GMT",[],"2023-08-10-hugging-face-platform-on-the-aws-marketplace-pay-with-your-aws-account.md","2023-08-15-using-gpt-4-for-content-moderation",{"id":10181,"data":10183,"filePath":10188,"digest":10189,"rendered":10190,"legacyId":10198},{"title":10184,"description":10185,"summary":10185,"pubDate":10186,"source":19,"url":10187,"thumbnail":21},"Using GPT-4 for content moderation","We use GPT-4 for content policy development and content moderation decisions, enabling more consistent labeling, a faster feedback loop for policy refinement, and less involvement from human moderators.",["Date","2023-08-15T07:00:00.000Z"],"https://openai.com/blog/using-gpt-4-for-content-moderation","src/content/posts/2023-08-15-using-gpt-4-for-content-moderation.md","28806a31a8286e44",{"html":25,"metadata":10191},{"headings":10192,"localImagePaths":10193,"remoteImagePaths":10194,"frontmatter":10195,"imagePaths":10197},[],[],[],{"title":10184,"description":10185,"summary":10185,"pubDate":10196,"source":19,"url":10187,"thumbnail":21},"Tue, 15 Aug 2023 07:00:00 GMT",[],"2023-08-15-using-gpt-4-for-content-moderation.md","2023-08-16-openai-acquires-global-illumination",{"id":10199,"data":10201,"filePath":10206,"digest":10207,"rendered":10208,"legacyId":10216},{"title":10202,"description":10203,"summary":10203,"pubDate":10204,"source":19,"url":10205,"thumbnail":21},"OpenAI acquires Global Illumination","The entire team has joined OpenAI.",["Date","2023-08-16T07:00:00.000Z"],"https://openai.com/blog/openai-acquires-global-illumination","src/content/posts/2023-08-16-openai-acquires-global-illumination.md","b09e72cb960a1a8e",{"html":25,"metadata":10209},{"headings":10210,"localImagePaths":10211,"remoteImagePaths":10212,"frontmatter":10213,"imagePaths":10215},[],[],[],{"title":10202,"description":10203,"summary":10203,"pubDate":10214,"source":19,"url":10205,"thumbnail":21},"Wed, 16 Aug 2023 07:00:00 GMT",[],"2023-08-16-openai-acquires-global-illumination.md","2023-08-22-gpt-35-turbo-fine-tuning-and-api-updates",{"id":10217,"data":10219,"filePath":10224,"digest":10225,"rendered":10226,"legacyId":10234},{"title":10220,"description":10221,"summary":10221,"pubDate":10222,"source":19,"url":10223,"thumbnail":21},"GPT-3.5 Turbo fine-tuning and API updates","Developers can now bring their own data to customize GPT-3.5 Turbo for their use cases.",["Date","2023-08-22T07:00:00.000Z"],"https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates","src/content/posts/2023-08-22-gpt-35-turbo-fine-tuning-and-api-updates.md","8f07cfe18bfc1a6e",{"html":25,"metadata":10227},{"headings":10228,"localImagePaths":10229,"remoteImagePaths":10230,"frontmatter":10231,"imagePaths":10233},[],[],[],{"title":10220,"description":10221,"summary":10221,"pubDate":10232,"source":19,"url":10223,"thumbnail":21},"Tue, 22 Aug 2023 07:00:00 GMT",[],"2023-08-22-gpt-35-turbo-fine-tuning-and-api-updates.md","2023-08-22-introducing-idefics-an-open-reproduction-of-state-of-the-art-visual-language-model",{"id":10235,"data":10237,"filePath":10243,"digest":10244,"rendered":10245,"legacyId":10253},{"title":10238,"description":25,"summary":10239,"pubDate":10240,"source":2720,"url":10241,"thumbnail":10242},"Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Language Model","Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model We are excited t...",["Date","2023-08-22T00:00:00.000Z"],"https://huggingface.co/blog/idefics","https://huggingface.co/blog/assets/idefics/thumbnail.png","src/content/posts/2023-08-22-introducing-idefics-an-open-reproduction-of-state-of-the-art-visual-language-model.md","d7a66936a0f5a27a",{"html":25,"metadata":10246},{"headings":10247,"localImagePaths":10248,"remoteImagePaths":10249,"frontmatter":10250,"imagePaths":10252},[],[],[],{"title":10238,"description":25,"summary":10239,"pubDate":10251,"source":2720,"url":10241,"thumbnail":10242},"Tue, 22 Aug 2023 00:00:00 GMT",[],"2023-08-22-introducing-idefics-an-open-reproduction-of-state-of-the-art-visual-language-model.md","2023-08-22-introducing-safecoder",{"id":10254,"data":10256,"filePath":10262,"digest":10263,"rendered":10264,"legacyId":10271},{"title":10257,"description":25,"summary":10258,"pubDate":10259,"source":2720,"url":10260,"thumbnail":10261},"Introducing SafeCoder","Introducing SafeCoder Today we are excited to announce SafeCoder - a code assistant solution built f...",["Date","2023-08-22T00:00:00.000Z"],"https://huggingface.co/blog/safecoder","https://huggingface.co/blog/assets/159_safecoder/thumbnail.jpg","src/content/posts/2023-08-22-introducing-safecoder.md","f57d318913a75dc6",{"html":25,"metadata":10265},{"headings":10266,"localImagePaths":10267,"remoteImagePaths":10268,"frontmatter":10269,"imagePaths":10270},[],[],[],{"title":10257,"description":25,"summary":10258,"pubDate":10251,"source":2720,"url":10260,"thumbnail":10261},[],"2023-08-22-introducing-safecoder.md","2023-08-23-making-llms-lighter-with-autogptq-and-transformers",{"id":10272,"data":10274,"filePath":10280,"digest":10281,"rendered":10282,"legacyId":10290},{"title":10275,"description":25,"summary":10276,"pubDate":10277,"source":2720,"url":10278,"thumbnail":10279},"Making LLMs lighter with AutoGPTQ and transformers","Making LLMs lighter with AutoGPTQ and transformers Large language models have demonstrated remarkabl...",["Date","2023-08-23T00:00:00.000Z"],"https://huggingface.co/blog/gptq-integration","https://huggingface.co/blog/assets/159_autogptq_transformers/thumbnail.jpg","src/content/posts/2023-08-23-making-llms-lighter-with-autogptq-and-transformers.md","fcb1268c22f6a72e",{"html":25,"metadata":10283},{"headings":10284,"localImagePaths":10285,"remoteImagePaths":10286,"frontmatter":10287,"imagePaths":10289},[],[],[],{"title":10275,"description":25,"summary":10276,"pubDate":10288,"source":2720,"url":10278,"thumbnail":10279},"Wed, 23 Aug 2023 00:00:00 GMT",[],"2023-08-23-making-llms-lighter-with-autogptq-and-transformers.md","2023-08-25-code-llama-llama-2-learns-to-code",{"id":10291,"data":10293,"filePath":10299,"digest":10300,"rendered":10301,"legacyId":10309},{"title":10294,"description":25,"summary":10295,"pubDate":10296,"source":2720,"url":10297,"thumbnail":10298},"Code Llama: Llama 2 learns to code","Code Llama: Llama 2 learns to code Introduction Code Llama is a family of state-of-the-art, open-acc...",["Date","2023-08-25T00:00:00.000Z"],"https://huggingface.co/blog/codellama","https://huggingface.co/blog/assets/160_codellama/thumbnail.jpg","src/content/posts/2023-08-25-code-llama-llama-2-learns-to-code.md","de950ed111308359",{"html":25,"metadata":10302},{"headings":10303,"localImagePaths":10304,"remoteImagePaths":10305,"frontmatter":10306,"imagePaths":10308},[],[],[],{"title":10294,"description":25,"summary":10295,"pubDate":10307,"source":2720,"url":10297,"thumbnail":10298},"Fri, 25 Aug 2023 00:00:00 GMT",[],"2023-08-25-code-llama-llama-2-learns-to-code.md","2023-08-24-openai-partners-with-scale-to-provide-support-for-enterprises-fine-tuning-models",{"id":10310,"data":10312,"filePath":10317,"digest":10318,"rendered":10319,"legacyId":10327},{"title":10313,"description":10314,"summary":10314,"pubDate":10315,"source":19,"url":10316,"thumbnail":21},"OpenAI partners with Scale to provide support for enterprises fine-tuning models","OpenAI’s customers can leverage Scale’s AI expertise to customize our most advanced models.",["Date","2023-08-24T07:00:00.000Z"],"https://openai.com/blog/openai-partners-with-scale-to-provide-support-for-enterprises-fine-tuning-models","src/content/posts/2023-08-24-openai-partners-with-scale-to-provide-support-for-enterprises-fine-tuning-models.md","8009188cf96d2e1b",{"html":25,"metadata":10320},{"headings":10321,"localImagePaths":10322,"remoteImagePaths":10323,"frontmatter":10324,"imagePaths":10326},[],[],[],{"title":10313,"description":10314,"summary":10314,"pubDate":10325,"source":19,"url":10316,"thumbnail":21},"Thu, 24 Aug 2023 07:00:00 GMT",[],"2023-08-24-openai-partners-with-scale-to-provide-support-for-enterprises-fine-tuning-models.md","2023-08-25-deprecation-of-git-authentication-using-password",{"id":10328,"data":10330,"filePath":10336,"digest":10337,"rendered":10338,"legacyId":10345},{"title":10331,"description":25,"summary":10332,"pubDate":10333,"source":2720,"url":10334,"thumbnail":10335},"Deprecation of Git Authentication using password","Hugging Face Hub: Important Git Authentication Changes Because we are committed to improving the sec...",["Date","2023-08-25T00:00:00.000Z"],"https://huggingface.co/blog/password-git-deprecation","https://huggingface.co/blog/assets/password-git-deprecation/thumbnail.png","src/content/posts/2023-08-25-deprecation-of-git-authentication-using-password.md","f4fed3b492412c9d",{"html":25,"metadata":10339},{"headings":10340,"localImagePaths":10341,"remoteImagePaths":10342,"frontmatter":10343,"imagePaths":10344},[],[],[],{"title":10331,"description":25,"summary":10332,"pubDate":10307,"source":2720,"url":10334,"thumbnail":10335},[],"2023-08-25-deprecation-of-git-authentication-using-password.md","2023-08-28-introducing-chatgpt-enterprise",{"id":10346,"data":10348,"filePath":10353,"digest":10354,"rendered":10355,"legacyId":10363},{"title":10349,"description":10350,"summary":10350,"pubDate":10351,"source":19,"url":10352,"thumbnail":21},"Introducing ChatGPT Enterprise","Get enterprise-grade security & privacy and the most powerful version of ChatGPT yet.",["Date","2023-08-28T07:00:00.000Z"],"https://openai.com/blog/introducing-chatgpt-enterprise","src/content/posts/2023-08-28-introducing-chatgpt-enterprise.md","1e097cb2fa67769a",{"html":25,"metadata":10356},{"headings":10357,"localImagePaths":10358,"remoteImagePaths":10359,"frontmatter":10360,"imagePaths":10362},[],[],[],{"title":10349,"description":10350,"summary":10350,"pubDate":10361,"source":19,"url":10352,"thumbnail":21},"Mon, 28 Aug 2023 07:00:00 GMT",[],"2023-08-28-introducing-chatgpt-enterprise.md","2023-08-29-identifying-ai-generated-images-with-synthid",{"id":10364,"data":10366,"filePath":10372,"digest":10373,"rendered":10374,"legacyId":10382},{"title":10367,"description":10368,"summary":10368,"pubDate":10369,"source":6423,"url":10370,"thumbnail":10371},"Identifying AI-generated images with SynthID","New tool helps watermark and identify synthetic images created by Imagen",["Date","2023-08-29T00:00:00.000Z"],"https://deepmind.google/discover/blog/identifying-ai-generated-images-with-synthid/","https://lh3.googleusercontent.com/wUgtK2GBt2yZJ3dJkXtvAK84G93j6idOOalyihOMfwBxY0lR650fZZYIi3bXdgkKbBcitbUZ0ILbaIPg_-vDTgAJLlP1DO3h_UnyoZ27wl3mYSzKtw=w1200-h630-n-nu","src/content/posts/2023-08-29-identifying-ai-generated-images-with-synthid.md","bee9c80fc05a2255",{"html":25,"metadata":10375},{"headings":10376,"localImagePaths":10377,"remoteImagePaths":10378,"frontmatter":10379,"imagePaths":10381},[],[],[],{"title":10367,"description":10368,"summary":10368,"pubDate":10380,"source":6423,"url":10370,"thumbnail":10371},"Tue, 29 Aug 2023 00:00:00 +0000",[],"2023-08-29-identifying-ai-generated-images-with-synthid.md","2023-08-30-audioldm-2-but-faster",{"id":10383,"data":10385,"filePath":10391,"digest":10392,"rendered":10393,"legacyId":10401},{"title":10386,"description":25,"summary":10387,"pubDate":10388,"source":2720,"url":10389,"thumbnail":10390},"AudioLDM 2, but faster ⚡️","AudioLDM 2, but faster ⚡️ AudioLDM 2 was proposed in AudioLDM 2: Learning Holistic Audio Generation ...",["Date","2023-08-30T00:00:00.000Z"],"https://huggingface.co/blog/audioldm2","https://huggingface.co/blog/assets/161_audioldm2/thumbnail.png","src/content/posts/2023-08-30-audioldm-2-but-faster.md","2bba284d62638b82",{"html":25,"metadata":10394},{"headings":10395,"localImagePaths":10396,"remoteImagePaths":10397,"frontmatter":10398,"imagePaths":10400},[],[],[],{"title":10386,"description":25,"summary":10387,"pubDate":10399,"source":2720,"url":10389,"thumbnail":10390},"Wed, 30 Aug 2023 00:00:00 GMT",[],"2023-08-30-audioldm-2-but-faster.md","2023-08-31-teaching-with-ai",{"id":10402,"data":10404,"filePath":10409,"digest":10410,"rendered":10411,"legacyId":10419},{"title":10405,"description":10406,"summary":10406,"pubDate":10407,"source":19,"url":10408,"thumbnail":21},"Teaching with AI","We’re releasing a guide for teachers using ChatGPT in their classroom—including suggested prompts, an explanation of how ChatGPT works and its limitations, the efficacy of AI detectors, and bias.",["Date","2023-08-31T07:00:00.000Z"],"https://openai.com/blog/teaching-with-ai","src/content/posts/2023-08-31-teaching-with-ai.md","8b68b8fe8cb94cfa",{"html":25,"metadata":10412},{"headings":10413,"localImagePaths":10414,"remoteImagePaths":10415,"frontmatter":10416,"imagePaths":10418},[],[],[],{"title":10405,"description":10406,"summary":10406,"pubDate":10417,"source":19,"url":10408,"thumbnail":21},"Thu, 31 Aug 2023 07:00:00 GMT",[],"2023-08-31-teaching-with-ai.md","2023-09-01-fetch-cuts-ml-processing-latency-by-50-using-amazon-sagemaker-hugging-face",{"id":10420,"data":10422,"filePath":10428,"digest":10429,"rendered":10430,"legacyId":10438},{"title":10423,"description":25,"summary":10424,"pubDate":10425,"source":2720,"url":10426,"thumbnail":10427},"Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker & Hugging Face","Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker & Hugging Face This article is a cros...",["Date","2023-09-01T00:00:00.000Z"],"https://huggingface.co/blog/fetch-case-study","https://huggingface.co/blog/assets/78_ml_director_insights/fetch.png","src/content/posts/2023-09-01-fetch-cuts-ml-processing-latency-by-50-using-amazon-sagemaker-hugging-face.md","0adf3ade5e871e48",{"html":25,"metadata":10431},{"headings":10432,"localImagePaths":10433,"remoteImagePaths":10434,"frontmatter":10435,"imagePaths":10437},[],[],[],{"title":10423,"description":25,"summary":10424,"pubDate":10436,"source":2720,"url":10426,"thumbnail":10427},"Fri, 01 Sep 2023 00:00:00 GMT",[],"2023-09-01-fetch-cuts-ml-processing-latency-by-50-using-amazon-sagemaker-hugging-face.md","2023-09-06-join-us-for-openais-first-developer-conference-on-november-6-in-san-francisco",{"id":10439,"data":10441,"filePath":10446,"digest":10447,"rendered":10448,"legacyId":10456},{"title":10442,"description":10443,"summary":10443,"pubDate":10444,"source":19,"url":10445,"thumbnail":21},"Join us for OpenAI’s first developer conference on November 6 in San Francisco","Developer registration for in-person attendance will open in the coming weeks and developers everywhere will be able to livestream the keynote.",["Date","2023-09-06T07:00:00.000Z"],"https://openai.com/blog/announcing-openai-devday","src/content/posts/2023-09-06-join-us-for-openais-first-developer-conference-on-november-6-in-san-francisco.md","85f6a7cde72425ca",{"html":25,"metadata":10449},{"headings":10450,"localImagePaths":10451,"remoteImagePaths":10452,"frontmatter":10453,"imagePaths":10455},[],[],[],{"title":10442,"description":10443,"summary":10443,"pubDate":10454,"source":19,"url":10445,"thumbnail":21},"Wed, 06 Sep 2023 07:00:00 GMT",[],"2023-09-06-join-us-for-openais-first-developer-conference-on-november-6-in-san-francisco.md","2023-09-06-spread-your-wings-falcon-180b-is-here",{"id":10457,"data":10459,"filePath":10465,"digest":10466,"rendered":10467,"legacyId":10475},{"title":10460,"description":25,"summary":10461,"pubDate":10462,"source":2720,"url":10463,"thumbnail":10464},"Spread Your Wings: Falcon 180B is here","Spread Your Wings: Falcon 180B is here Introduction Today, we're excited to welcome TII's Falcon 180...",["Date","2023-09-06T00:00:00.000Z"],"https://huggingface.co/blog/falcon-180b","https://huggingface.co/blog/assets/162_falcon_180b/thumbnail.jpg","src/content/posts/2023-09-06-spread-your-wings-falcon-180b-is-here.md","b98bec44a1c8e6e4",{"html":25,"metadata":10468},{"headings":10469,"localImagePaths":10470,"remoteImagePaths":10471,"frontmatter":10472,"imagePaths":10474},[],[],[],{"title":10460,"description":25,"summary":10461,"pubDate":10473,"source":2720,"url":10463,"thumbnail":10464},"Wed, 06 Sep 2023 00:00:00 GMT",[],"2023-09-06-spread-your-wings-falcon-180b-is-here.md","2023-09-08-efficient-controllable-generation-for-sdxl-with-t2i-adapters",{"id":10476,"data":10478,"filePath":10484,"digest":10485,"rendered":10486,"legacyId":10494},{"title":10479,"description":25,"summary":10480,"pubDate":10481,"source":2720,"url":10482,"thumbnail":10483},"Efficient Controllable Generation for SDXL with T2I-Adapters","Efficient Controllable Generation for SDXL with T2I-Adapters T2I-Adapter is an efficient plug-and-pl...",["Date","2023-09-08T00:00:00.000Z"],"https://huggingface.co/blog/t2i-sdxl-adapters","https://huggingface.co/blog/assets/t2i-sdxl-adapters/thumbnail.png","src/content/posts/2023-09-08-efficient-controllable-generation-for-sdxl-with-t2i-adapters.md","bd003c429231f767",{"html":25,"metadata":10487},{"headings":10488,"localImagePaths":10489,"remoteImagePaths":10490,"frontmatter":10491,"imagePaths":10493},[],[],[],{"title":10479,"description":25,"summary":10480,"pubDate":10492,"source":2720,"url":10482,"thumbnail":10483},"Fri, 08 Sep 2023 00:00:00 GMT",[],"2023-09-08-efficient-controllable-generation-for-sdxl-with-t2i-adapters.md","2023-09-11-safecoder-vs-closed-source-code-assistants",{"id":10495,"data":10497,"filePath":10503,"digest":10504,"rendered":10505,"legacyId":10513},{"title":10498,"description":25,"summary":10499,"pubDate":10500,"source":2720,"url":10501,"thumbnail":10502},"SafeCoder vs. Closed-source Code Assistants","SafeCoder vs. Closed-source Code Assistants For decades, software developers have designed methodolo...",["Date","2023-09-11T00:00:00.000Z"],"https://huggingface.co/blog/safecoder-vs-closed-source-code-assistants","https://huggingface.co/blog/assets/safecoder-vs-closed-source-code-assistants/image.png","src/content/posts/2023-09-11-safecoder-vs-closed-source-code-assistants.md","10eff381a1ab5b7d",{"html":25,"metadata":10506},{"headings":10507,"localImagePaths":10508,"remoteImagePaths":10509,"frontmatter":10510,"imagePaths":10512},[],[],[],{"title":10498,"description":25,"summary":10499,"pubDate":10511,"source":2720,"url":10501,"thumbnail":10502},"Mon, 11 Sep 2023 00:00:00 GMT",[],"2023-09-11-safecoder-vs-closed-source-code-assistants.md","2023-09-12-overview-of-natively-supported-quantization-schemes-in-transformers",{"id":10514,"data":10516,"filePath":10522,"digest":10523,"rendered":10524,"legacyId":10532},{"title":10517,"description":25,"summary":10518,"pubDate":10519,"source":2720,"url":10520,"thumbnail":10521},"Overview of natively supported quantization schemes in 🤗 Transformers","Overview of natively supported quantization schemes in 🤗 Transformers We aim to give a clear overvie...",["Date","2023-09-12T00:00:00.000Z"],"https://huggingface.co/blog/overview-quantization-transformers","https://huggingface.co/blog/assets/163_overview_quantization_transformers/thumbnail.jpg","src/content/posts/2023-09-12-overview-of-natively-supported-quantization-schemes-in-transformers.md","bf159b5dfccb0617",{"html":25,"metadata":10525},{"headings":10526,"localImagePaths":10527,"remoteImagePaths":10528,"frontmatter":10529,"imagePaths":10531},[],[],[],{"title":10517,"description":25,"summary":10518,"pubDate":10530,"source":2720,"url":10520,"thumbnail":10521},"Tue, 12 Sep 2023 00:00:00 GMT",[],"2023-09-12-overview-of-natively-supported-quantization-schemes-in-transformers.md","2023-09-13-fine-tuning-llama-2-70b-using-pytorch-fsdp",{"id":10533,"data":10535,"filePath":10541,"digest":10542,"rendered":10543,"legacyId":10551},{"title":10536,"description":25,"summary":10537,"pubDate":10538,"source":2720,"url":10539,"thumbnail":10540},"Fine-tuning Llama 2 70B using PyTorch FSDP","Fine-tuning Llama 2 70B using PyTorch FSDP Introduction In this blog post, we will look at how to fi...",["Date","2023-09-13T00:00:00.000Z"],"https://huggingface.co/blog/ram-efficient-pytorch-fsdp","https://huggingface.co/blog/assets/160_fsdp_llama/thumbnail.jpg","src/content/posts/2023-09-13-fine-tuning-llama-2-70b-using-pytorch-fsdp.md","edf0a080f6ffa08d",{"html":25,"metadata":10544},{"headings":10545,"localImagePaths":10546,"remoteImagePaths":10547,"frontmatter":10548,"imagePaths":10550},[],[],[],{"title":10536,"description":25,"summary":10537,"pubDate":10549,"source":2720,"url":10539,"thumbnail":10540},"Wed, 13 Sep 2023 00:00:00 GMT",[],"2023-09-13-fine-tuning-llama-2-70b-using-pytorch-fsdp.md","2023-09-13-introducing-openai-dublin",{"id":10552,"data":10554,"filePath":10559,"digest":10560,"rendered":10561,"legacyId":10569},{"title":10555,"description":10556,"summary":10556,"pubDate":10557,"source":19,"url":10558,"thumbnail":21},"Introducing OpenAI Dublin","We’re growing our presence in Europe with an office in Dublin, Ireland.",["Date","2023-09-13T07:00:00.000Z"],"https://openai.com/blog/introducing-openai-dublin","src/content/posts/2023-09-13-introducing-openai-dublin.md","6c191c9cb03c8f39",{"html":25,"metadata":10562},{"headings":10563,"localImagePaths":10564,"remoteImagePaths":10565,"frontmatter":10566,"imagePaths":10568},[],[],[],{"title":10555,"description":10556,"summary":10556,"pubDate":10567,"source":19,"url":10558,"thumbnail":21},"Wed, 13 Sep 2023 07:00:00 GMT",[],"2023-09-13-introducing-openai-dublin.md","2023-09-13-introducing-würstchen-fast-diffusion-for-image-generation",{"id":10570,"data":10572,"filePath":10578,"digest":10579,"rendered":10580,"legacyId":10587},{"title":10573,"description":25,"summary":10574,"pubDate":10575,"source":2720,"url":10576,"thumbnail":10577},"Introducing Würstchen: Fast Diffusion for Image Generation","Introducing Würstchen: Fast Diffusion for Image Generation What is Würstchen? Würstchen is a diffusi...",["Date","2023-09-13T00:00:00.000Z"],"https://huggingface.co/blog/wuerstchen","https://huggingface.co/blog/assets/wuerstchen/thumbnail.jpg","src/content/posts/2023-09-13-introducing-würstchen-fast-diffusion-for-image-generation.md","9c090818b6df38d7",{"html":25,"metadata":10581},{"headings":10582,"localImagePaths":10583,"remoteImagePaths":10584,"frontmatter":10585,"imagePaths":10586},[],[],[],{"title":10573,"description":25,"summary":10574,"pubDate":10549,"source":2720,"url":10576,"thumbnail":10577},[],"2023-09-13-introducing-würstchen-fast-diffusion-for-image-generation.md","2023-09-15-optimizing-your-llm-in-production",{"id":10588,"data":10590,"filePath":10596,"digest":10597,"rendered":10598,"legacyId":10606},{"title":10591,"description":25,"summary":10592,"pubDate":10593,"source":2720,"url":10594,"thumbnail":10595},"Optimizing your LLM in production","Optimizing your LLM in production Note: This blog post is also available as a documentation page on ...",["Date","2023-09-15T00:00:00.000Z"],"https://huggingface.co/blog/optimize-llm","https://huggingface.co/blog/assets/163_optimize_llm/optimize_llm.png","src/content/posts/2023-09-15-optimizing-your-llm-in-production.md","a7bab120bf0168e7",{"html":25,"metadata":10599},{"headings":10600,"localImagePaths":10601,"remoteImagePaths":10602,"frontmatter":10603,"imagePaths":10605},[],[],[],{"title":10591,"description":25,"summary":10592,"pubDate":10604,"source":2720,"url":10594,"thumbnail":10595},"Fri, 15 Sep 2023 00:00:00 GMT",[],"2023-09-15-optimizing-your-llm-in-production.md","2023-09-18-introduction-to-3d-gaussian-splatting",{"id":10607,"data":10609,"filePath":10615,"digest":10616,"rendered":10617,"legacyId":10625},{"title":10610,"description":25,"summary":10611,"pubDate":10612,"source":2720,"url":10613,"thumbnail":10614},"Introduction to 3D Gaussian Splatting","Introduction to 3D Gaussian Splatting 3D Gaussian Splatting is a rasterization technique described i...",["Date","2023-09-18T00:00:00.000Z"],"https://huggingface.co/blog/gaussian-splatting","https://huggingface.co/blog/assets/124_ml-for-games/thumbnail-gaussian-splatting.png","src/content/posts/2023-09-18-introduction-to-3d-gaussian-splatting.md","41c011400894c62f",{"html":25,"metadata":10618},{"headings":10619,"localImagePaths":10620,"remoteImagePaths":10621,"frontmatter":10622,"imagePaths":10624},[],[],[],{"title":10610,"description":25,"summary":10611,"pubDate":10623,"source":2720,"url":10613,"thumbnail":10614},"Mon, 18 Sep 2023 00:00:00 GMT",[],"2023-09-18-introduction-to-3d-gaussian-splatting.md","2023-09-18-object-detection-leaderboard",{"id":10626,"data":10628,"filePath":10634,"digest":10635,"rendered":10636,"legacyId":10643},{"title":10629,"description":25,"summary":10630,"pubDate":10631,"source":2720,"url":10632,"thumbnail":10633},"Object Detection Leaderboard","Object Detection Leaderboard: Decoding Metrics and Their Potential Pitfalls Welcome to our latest di...",["Date","2023-09-18T00:00:00.000Z"],"https://huggingface.co/blog/object-detection-leaderboard","https://huggingface.co/blog/assets/object-detection-leaderboard/thumbnail.png","src/content/posts/2023-09-18-object-detection-leaderboard.md","4bf273b91e1ca3a9",{"html":25,"metadata":10637},{"headings":10638,"localImagePaths":10639,"remoteImagePaths":10640,"frontmatter":10641,"imagePaths":10642},[],[],[],{"title":10629,"description":25,"summary":10630,"pubDate":10623,"source":2720,"url":10632,"thumbnail":10633},[],"2023-09-18-object-detection-leaderboard.md","2023-09-19-a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases",{"id":10644,"data":10646,"filePath":10652,"digest":10653,"rendered":10654,"legacyId":10662},{"title":10647,"description":10648,"summary":10648,"pubDate":10649,"source":6423,"url":10650,"thumbnail":10651},"A catalogue of genetic mutations to help pinpoint the cause of diseases","New AI tool classifies the effects of 71 million ‘missense’ mutations.",["Date","2023-09-19T13:37:00.000Z"],"https://deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/","https://lh3.googleusercontent.com/JySjDTZvGqEzUfDic5QOU6Rne3r6RWpiw5JZQ9VdzK1O5C20EbAkSPURGoCmAhea_U-gyyRu4KdCZmeWSCtYjGHHMvM0jVK5fWiqOwa0rpcC5uzM=w1200-h630-n-nu","src/content/posts/2023-09-19-a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases.md","8aa31c5cd1334f3c",{"html":25,"metadata":10655},{"headings":10656,"localImagePaths":10657,"remoteImagePaths":10658,"frontmatter":10659,"imagePaths":10661},[],[],[],{"title":10647,"description":10648,"summary":10648,"pubDate":10660,"source":6423,"url":10650,"thumbnail":10651},"Tue, 19 Sep 2023 13:37:00 +0000",[],"2023-09-19-a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases.md","2023-09-19-openai-red-teaming-network",{"id":10663,"data":10665,"filePath":10670,"digest":10671,"rendered":10672,"legacyId":10680},{"title":10666,"description":10667,"summary":10667,"pubDate":10668,"source":19,"url":10669,"thumbnail":21},"OpenAI Red Teaming Network","We’re announcing an open call for the OpenAI Red Teaming Network and invite domain experts interested in improving the safety of OpenAI’s models to join our efforts.",["Date","2023-09-19T07:00:00.000Z"],"https://openai.com/blog/red-teaming-network","src/content/posts/2023-09-19-openai-red-teaming-network.md","75244e1fbb9396d9",{"html":25,"metadata":10673},{"headings":10674,"localImagePaths":10675,"remoteImagePaths":10676,"frontmatter":10677,"imagePaths":10679},[],[],[],{"title":10666,"description":10667,"summary":10667,"pubDate":10678,"source":19,"url":10669,"thumbnail":21},"Tue, 19 Sep 2023 07:00:00 GMT",[],"2023-09-19-openai-red-teaming-network.md","2023-09-19-rocket-money-x-hugging-face-scaling-volatile-ml-models-in-production",{"id":10681,"data":10683,"filePath":10689,"digest":10690,"rendered":10691,"legacyId":10699},{"title":10684,"description":25,"summary":10685,"pubDate":10686,"source":2720,"url":10687,"thumbnail":10688},"Rocket Money x Hugging Face: Scaling Volatile ML Models in Production","Rocket Money x Hugging Face: Scaling Volatile ML Models in Production 'We discovered that they were ...",["Date","2023-09-19T00:00:00.000Z"],"https://huggingface.co/blog/rocketmoney-case-study","https://huggingface.co/blog/assets/78_ml_director_insights/rocketmoney.png","src/content/posts/2023-09-19-rocket-money-x-hugging-face-scaling-volatile-ml-models-in-production.md","044cfe7968906117",{"html":25,"metadata":10692},{"headings":10693,"localImagePaths":10694,"remoteImagePaths":10695,"frontmatter":10696,"imagePaths":10698},[],[],[],{"title":10684,"description":25,"summary":10685,"pubDate":10697,"source":2720,"url":10687,"thumbnail":10688},"Tue, 19 Sep 2023 00:00:00 GMT",[],"2023-09-19-rocket-money-x-hugging-face-scaling-volatile-ml-models-in-production.md","2023-09-22-inference-for-pros",{"id":10700,"data":10702,"filePath":10708,"digest":10709,"rendered":10710,"legacyId":10718},{"title":10703,"description":25,"summary":10704,"pubDate":10705,"source":2720,"url":10706,"thumbnail":10707},"Inference for PROs","Inference for PROs Today, we're introducing Inference for PRO users - a community offering that give...",["Date","2023-09-22T00:00:00.000Z"],"https://huggingface.co/blog/inference-pro","https://huggingface.co/blog/assets/inference_pro/thumbnail.png","src/content/posts/2023-09-22-inference-for-pros.md","d0c868b6257349a3",{"html":25,"metadata":10711},{"headings":10712,"localImagePaths":10713,"remoteImagePaths":10714,"frontmatter":10715,"imagePaths":10717},[],[],[],{"title":10703,"description":25,"summary":10704,"pubDate":10716,"source":2720,"url":10706,"thumbnail":10707},"Fri, 22 Sep 2023 00:00:00 GMT",[],"2023-09-22-inference-for-pros.md","2023-09-25-chatgpt-can-now-see-hear-and-speak",{"id":10719,"data":10721,"filePath":10726,"digest":10727,"rendered":10728,"legacyId":10736},{"title":10722,"description":10723,"summary":10723,"pubDate":10724,"source":19,"url":10725,"thumbnail":21},"ChatGPT can now see, hear, and speak","We are beginning to roll out new voice and image capabilities in ChatGPT. They offer a new, more intuitive type of interface by allowing you to have a voice conversation or show ChatGPT what you’re talking about.",["Date","2023-09-25T07:00:00.000Z"],"https://openai.com/blog/chatgpt-can-now-see-hear-and-speak","src/content/posts/2023-09-25-chatgpt-can-now-see-hear-and-speak.md","65f673eace2ad0f6",{"html":25,"metadata":10729},{"headings":10730,"localImagePaths":10731,"remoteImagePaths":10732,"frontmatter":10733,"imagePaths":10735},[],[],[],{"title":10722,"description":10723,"summary":10723,"pubDate":10734,"source":19,"url":10725,"thumbnail":21},"Mon, 25 Sep 2023 07:00:00 GMT",[],"2023-09-25-chatgpt-can-now-see-hear-and-speak.md","2023-09-25-gpt-4vision-system-card",{"id":10737,"data":10739,"filePath":10743,"digest":10744,"rendered":10745,"legacyId":10752},{"title":10740,"description":25,"summary":25,"pubDate":10741,"source":19,"url":10742,"thumbnail":21},"GPT-4V(ision) system card",["Date","2023-09-25T07:00:00.000Z"],"https://openai.com/blog/gpt-4v-system-card","src/content/posts/2023-09-25-gpt-4vision-system-card.md","a977472fd78c8e85",{"html":25,"metadata":10746},{"headings":10747,"localImagePaths":10748,"remoteImagePaths":10749,"frontmatter":10750,"imagePaths":10751},[],[],[],{"title":10740,"description":25,"summary":25,"pubDate":10734,"source":19,"url":10742,"thumbnail":21},[],"2023-09-25-gpt-4vision-system-card.md","2023-09-26-llama-2-on-amazon-sagemaker-a-benchmark",{"id":10753,"data":10755,"filePath":10761,"digest":10762,"rendered":10763,"legacyId":10771},{"title":10756,"description":25,"summary":10757,"pubDate":10758,"source":2720,"url":10759,"thumbnail":10760},"Llama 2 on Amazon SageMaker a Benchmark","Llama 2 on Amazon SageMaker a Benchmark Deploying large language models (LLMs) and other generative ...",["Date","2023-09-26T00:00:00.000Z"],"https://huggingface.co/blog/llama-sagemaker-benchmark","https://huggingface.co/blog/assets/llama_sagemaker_benchmark/thumbnail.jpg","src/content/posts/2023-09-26-llama-2-on-amazon-sagemaker-a-benchmark.md","900ca9730b847a55",{"html":25,"metadata":10764},{"headings":10765,"localImagePaths":10766,"remoteImagePaths":10767,"frontmatter":10768,"imagePaths":10770},[],[],[],{"title":10756,"description":25,"summary":10757,"pubDate":10769,"source":2720,"url":10759,"thumbnail":10760},"Tue, 26 Sep 2023 00:00:00 GMT",[],"2023-09-26-llama-2-on-amazon-sagemaker-a-benchmark.md","2023-09-28-non-engineers-guide-train-a-llama-2-chatbot",{"id":10772,"data":10774,"filePath":10780,"digest":10781,"rendered":10782,"legacyId":10790},{"title":10775,"description":25,"summary":10776,"pubDate":10777,"source":2720,"url":10778,"thumbnail":10779},"Non-engineers guide: Train a LLaMA 2 chatbot","Non-engineers guide: Train a LLaMA 2 chatbot Introduction In this tutorial we will show you how anyo...",["Date","2023-09-28T00:00:00.000Z"],"https://huggingface.co/blog/Llama2-for-non-engineers","https://huggingface.co/blog/assets/78_ml_director_insights/tuto.png","src/content/posts/2023-09-28-non-engineers-guide-train-a-llama-2-chatbot.md","46f69c06cd7490c3",{"html":25,"metadata":10783},{"headings":10784,"localImagePaths":10785,"remoteImagePaths":10786,"frontmatter":10787,"imagePaths":10789},[],[],[],{"title":10775,"description":25,"summary":10776,"pubDate":10788,"source":2720,"url":10778,"thumbnail":10779},"Thu, 28 Sep 2023 00:00:00 GMT",[],"2023-09-28-non-engineers-guide-train-a-llama-2-chatbot.md","2023-09-29-ethics-and-society-newsletter-5-hugging-face-goes-to-washington-and-other-summer-2023-musings",{"id":10791,"data":10793,"filePath":10799,"digest":10800,"rendered":10801,"legacyId":10809},{"title":10794,"description":25,"summary":10795,"pubDate":10796,"source":2720,"url":10797,"thumbnail":10798},"Ethics and Society Newsletter #5: Hugging Face Goes To Washington and Other Summer 2023 Musings","Ethics and Society Newsletter #5: Hugging Face Goes To Washington and Other Summer 2023 Musings One ...",["Date","2023-09-29T00:00:00.000Z"],"https://huggingface.co/blog/ethics-soc-5","https://huggingface.co/blog/assets/164_ethics-soc-5/thumbnail.png","src/content/posts/2023-09-29-ethics-and-society-newsletter-5-hugging-face-goes-to-washington-and-other-summer-2023-musings.md","99c491bca980d1fe",{"html":25,"metadata":10802},{"headings":10803,"localImagePaths":10804,"remoteImagePaths":10805,"frontmatter":10806,"imagePaths":10808},[],[],[],{"title":10794,"description":25,"summary":10795,"pubDate":10807,"source":2720,"url":10797,"thumbnail":10798},"Fri, 29 Sep 2023 00:00:00 GMT",[],"2023-09-29-ethics-and-society-newsletter-5-hugging-face-goes-to-washington-and-other-summer-2023-musings.md","2023-09-29-finetune-stable-diffusion-models-with-ddpo-via-trl",{"id":10810,"data":10812,"filePath":10818,"digest":10819,"rendered":10820,"legacyId":10827},{"title":10813,"description":25,"summary":10814,"pubDate":10815,"source":2720,"url":10816,"thumbnail":10817},"Finetune Stable Diffusion Models with DDPO via TRL","Finetune Stable Diffusion Models with DDPO via TRL Introduction Diffusion models (e.g., DALL-E 2, St...",["Date","2023-09-29T00:00:00.000Z"],"https://huggingface.co/blog/trl-ddpo","https://huggingface.co/blog/assets/166_trl_ddpo/thumbnail.png","src/content/posts/2023-09-29-finetune-stable-diffusion-models-with-ddpo-via-trl.md","5d99716d2d51b935",{"html":25,"metadata":10821},{"headings":10822,"localImagePaths":10823,"remoteImagePaths":10824,"frontmatter":10825,"imagePaths":10826},[],[],[],{"title":10813,"description":25,"summary":10814,"pubDate":10807,"source":2720,"url":10816,"thumbnail":10817},[],"2023-09-29-finetune-stable-diffusion-models-with-ddpo-via-trl.md","2023-10-02-deploying-the-ai-comic-factory-using-the-inference-api",{"id":10828,"data":10830,"filePath":10836,"digest":10837,"rendered":10838,"legacyId":10846},{"title":10831,"description":25,"summary":10832,"pubDate":10833,"source":2720,"url":10834,"thumbnail":10835},"Deploying the AI Comic Factory using the Inference API","Deploying the AI Comic Factory using the Inference API We recently announced Inference for PROs, our...",["Date","2023-10-02T00:00:00.000Z"],"https://huggingface.co/blog/ai-comic-factory","https://huggingface.co/blog/assets/165_ai_comic_factory/thumbnail.jpg","src/content/posts/2023-10-02-deploying-the-ai-comic-factory-using-the-inference-api.md","c9cc60da257c1d71",{"html":25,"metadata":10839},{"headings":10840,"localImagePaths":10841,"remoteImagePaths":10842,"frontmatter":10843,"imagePaths":10845},[],[],[],{"title":10831,"description":25,"summary":10832,"pubDate":10844,"source":2720,"url":10834,"thumbnail":10835},"Mon, 02 Oct 2023 00:00:00 GMT",[],"2023-10-02-deploying-the-ai-comic-factory-using-the-inference-api.md","2023-10-03-accelerating-stable-diffusion-xl-inference-with-jax-on-cloud-tpu-v5e",{"id":10847,"data":10849,"filePath":10855,"digest":10856,"rendered":10857,"legacyId":10865},{"title":10850,"description":25,"summary":10851,"pubDate":10852,"source":2720,"url":10853,"thumbnail":10854},"Accelerating Stable Diffusion XL Inference with JAX on Cloud TPU v5e","Accelerating Stable Diffusion XL Inference with JAX on Cloud TPU v5e Generative AI models, such as S...",["Date","2023-10-03T00:00:00.000Z"],"https://huggingface.co/blog/sdxl_jax","https://huggingface.co/blog/assets/sdxl-jax/thumbnail.jpg","src/content/posts/2023-10-03-accelerating-stable-diffusion-xl-inference-with-jax-on-cloud-tpu-v5e.md","29f91f4883704598",{"html":25,"metadata":10858},{"headings":10859,"localImagePaths":10860,"remoteImagePaths":10861,"frontmatter":10862,"imagePaths":10864},[],[],[],{"title":10850,"description":25,"summary":10851,"pubDate":10863,"source":2720,"url":10853,"thumbnail":10854},"Tue, 03 Oct 2023 00:00:00 GMT",[],"2023-10-03-accelerating-stable-diffusion-xl-inference-with-jax-on-cloud-tpu-v5e.md","2023-10-03-chat-templates-an-end-to-the-silent-performance-killer",{"id":10866,"data":10868,"filePath":10874,"digest":10875,"rendered":10876,"legacyId":10883},{"title":10869,"description":25,"summary":10870,"pubDate":10871,"source":2720,"url":10872,"thumbnail":10873},"Chat Templates: An End to the Silent Performance Killer","Chat Templates A spectre is haunting chat models - the spectre of incorrect formatting! tl;dr Chat m...",["Date","2023-10-03T00:00:00.000Z"],"https://huggingface.co/blog/chat-templates","https://huggingface.co/blog/assets/chat-templates/thumbnail.png","src/content/posts/2023-10-03-chat-templates-an-end-to-the-silent-performance-killer.md","6d4fc7ef68e7c455",{"html":25,"metadata":10877},{"headings":10878,"localImagePaths":10879,"remoteImagePaths":10880,"frontmatter":10881,"imagePaths":10882},[],[],[],{"title":10869,"description":25,"summary":10870,"pubDate":10863,"source":2720,"url":10872,"thumbnail":10873},[],"2023-10-03-chat-templates-an-end-to-the-silent-performance-killer.md","2023-10-03-dalle-3-system-card",{"id":10884,"data":10886,"filePath":10890,"digest":10891,"rendered":10892,"legacyId":10900},{"title":10887,"description":25,"summary":25,"pubDate":10888,"source":19,"url":10889,"thumbnail":21},"DALL·E 3 system card",["Date","2023-10-03T07:00:00.000Z"],"https://openai.com/blog/dall-e-3-system-card","src/content/posts/2023-10-03-dalle-3-system-card.md","d06acca7bddcc3cc",{"html":25,"metadata":10893},{"headings":10894,"localImagePaths":10895,"remoteImagePaths":10896,"frontmatter":10897,"imagePaths":10899},[],[],[],{"title":10887,"description":25,"summary":25,"pubDate":10898,"source":19,"url":10889,"thumbnail":21},"Tue, 03 Oct 2023 07:00:00 GMT",[],"2023-10-03-dalle-3-system-card.md","2023-10-03-scaling-up-learning-across-many-different-robot-types",{"id":10901,"data":10903,"filePath":10909,"digest":10910,"rendered":10911,"legacyId":10919},{"title":10904,"description":10905,"summary":10905,"pubDate":10906,"source":6423,"url":10907,"thumbnail":10908},"Scaling up learning across many different robot types","Robots are great specialists, but poor generalists. Typically, you have to train a model for each task, robot, and environment. Changing a single variable often requires starting from scratch. But what if we could combine the knowledge across robotics and create a way to train a general-purpose robot?",["Date","2023-10-03T15:00:00.000Z"],"https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/","https://lh3.googleusercontent.com/KiNtKw6sX-3WmNln5pnEZjPMfM7VJLg0qe4VshEj_H_oXCI9hb6iGWl1DPx79WBb4EVds8mq2wUq_n9s2Lk8kkWazPtootwAUYBKxBEp64WTcEmXa6U=w1200-h630-n-nu","src/content/posts/2023-10-03-scaling-up-learning-across-many-different-robot-types.md","626467dfd295f399",{"html":25,"metadata":10912},{"headings":10913,"localImagePaths":10914,"remoteImagePaths":10915,"frontmatter":10916,"imagePaths":10918},[],[],[],{"title":10904,"description":10905,"summary":10905,"pubDate":10917,"source":6423,"url":10907,"thumbnail":10908},"Tue, 03 Oct 2023 15:00:00 +0000",[],"2023-10-03-scaling-up-learning-across-many-different-robot-types.md","2023-10-04-accelerating-over-130000-hugging-face-models-with-onnx-runtime",{"id":10920,"data":10922,"filePath":10928,"digest":10929,"rendered":10930,"legacyId":10938},{"title":10923,"description":25,"summary":10924,"pubDate":10925,"source":2720,"url":10926,"thumbnail":10927},"Accelerating over 130,000 Hugging Face models with ONNX Runtime","Accelerating over 130,000 Hugging Face models with ONNX Runtime What is ONNX Runtime? ONNX Runtime i...",["Date","2023-10-04T00:00:00.000Z"],"https://huggingface.co/blog/ort-accelerating-hf-models","https://huggingface.co/blog/assets/ort_accelerating_hf_models/thumbnail.png","src/content/posts/2023-10-04-accelerating-over-130000-hugging-face-models-with-onnx-runtime.md","81f635640f229df8",{"html":25,"metadata":10931},{"headings":10932,"localImagePaths":10933,"remoteImagePaths":10934,"frontmatter":10935,"imagePaths":10937},[],[],[],{"title":10923,"description":25,"summary":10924,"pubDate":10936,"source":2720,"url":10926,"thumbnail":10927},"Wed, 04 Oct 2023 00:00:00 GMT",[],"2023-10-04-accelerating-over-130000-hugging-face-models-with-onnx-runtime.md","2023-10-11-building-ai-powered-apps-for-business",{"id":10939,"data":10941,"filePath":10946,"digest":10947,"rendered":10948,"legacyId":10956},{"title":10942,"description":10943,"summary":10943,"pubDate":10944,"source":19,"url":10945,"thumbnail":21},"Building AI-powered apps for business","Retool uses GPT-4 to give businesses a fast, secure way to build AI-powered apps.",["Date","2023-10-11T07:00:00.000Z"],"https://openai.com/blog/retool","src/content/posts/2023-10-11-building-ai-powered-apps-for-business.md","5b183831f37c6012",{"html":25,"metadata":10949},{"headings":10950,"localImagePaths":10951,"remoteImagePaths":10952,"frontmatter":10953,"imagePaths":10955},[],[],[],{"title":10942,"description":10943,"summary":10943,"pubDate":10954,"source":19,"url":10945,"thumbnail":21},"Wed, 11 Oct 2023 07:00:00 GMT",[],"2023-10-11-building-ai-powered-apps-for-business.md","2023-10-11-evolving-online-forms-into-dynamic-data",{"id":10957,"data":10959,"filePath":10964,"digest":10965,"rendered":10966,"legacyId":10973},{"title":10960,"description":10961,"summary":10961,"pubDate":10962,"source":19,"url":10963,"thumbnail":21},"Evolving online forms into dynamic data","Typeform evolves online forms into dynamic and conversational data collection experiences with GPT-3.5 and GPT-4.",["Date","2023-10-11T07:00:00.000Z"],"https://openai.com/blog/typeform","src/content/posts/2023-10-11-evolving-online-forms-into-dynamic-data.md","d58e3a6ba72c80e1",{"html":25,"metadata":10967},{"headings":10968,"localImagePaths":10969,"remoteImagePaths":10970,"frontmatter":10971,"imagePaths":10972},[],[],[],{"title":10960,"description":10961,"summary":10961,"pubDate":10954,"source":19,"url":10963,"thumbnail":21},[],"2023-10-11-evolving-online-forms-into-dynamic-data.md","2023-10-11-openais-technology-explained",{"id":10974,"data":10976,"filePath":10980,"digest":10981,"rendered":10982,"legacyId":10989},{"title":10977,"description":25,"summary":25,"pubDate":10978,"source":19,"url":10979,"thumbnail":21},"OpenAI’s technology explained",["Date","2023-10-11T07:00:00.000Z"],"https://openai.com/global-affairs/openai-technology-explained","src/content/posts/2023-10-11-openais-technology-explained.md","18c54a4e8201a3f9",{"html":25,"metadata":10983},{"headings":10984,"localImagePaths":10985,"remoteImagePaths":10986,"frontmatter":10987,"imagePaths":10988},[],[],[],{"title":10977,"description":25,"summary":25,"pubDate":10954,"source":19,"url":10979,"thumbnail":21},[],"2023-10-11-openais-technology-explained.md","2023-10-11-simplifying-contract-reviews-with-ai",{"id":10990,"data":10992,"filePath":10997,"digest":10998,"rendered":10999,"legacyId":11006},{"title":10993,"description":10994,"summary":10994,"pubDate":10995,"source":19,"url":10996,"thumbnail":21},"Simplifying contract reviews with AI","Ironclad uses GPT-4 to simplify the contract review process.",["Date","2023-10-11T07:00:00.000Z"],"https://openai.com/blog/ironclad","src/content/posts/2023-10-11-simplifying-contract-reviews-with-ai.md","19d040a6c3a09d1a",{"html":25,"metadata":11000},{"headings":11001,"localImagePaths":11002,"remoteImagePaths":11003,"frontmatter":11004,"imagePaths":11005},[],[],[],{"title":10993,"description":10994,"summary":10994,"pubDate":10954,"source":19,"url":10996,"thumbnail":21},[],"2023-10-11-simplifying-contract-reviews-with-ai.md","2023-10-19-dalle-3-is-now-available-in-chatgpt-plus-and-enterprise",{"id":11007,"data":11009,"filePath":11014,"digest":11015,"rendered":11016,"legacyId":11024},{"title":11010,"description":11011,"summary":11011,"pubDate":11012,"source":19,"url":11013,"thumbnail":21},"DALL·E 3 is now available in ChatGPT Plus and Enterprise","We developed a safety mitigation stack to ready DALL·E 3 for wider release and are sharing updates on our provenance research.",["Date","2023-10-19T07:00:00.000Z"],"https://openai.com/blog/dall-e-3-is-now-available-in-chatgpt-plus-and-enterprise","src/content/posts/2023-10-19-dalle-3-is-now-available-in-chatgpt-plus-and-enterprise.md","2e25f7edba83b369",{"html":25,"metadata":11017},{"headings":11018,"localImagePaths":11019,"remoteImagePaths":11020,"frontmatter":11021,"imagePaths":11023},[],[],[],{"title":11010,"description":11011,"summary":11011,"pubDate":11022,"source":19,"url":11013,"thumbnail":21},"Thu, 19 Oct 2023 07:00:00 GMT",[],"2023-10-19-dalle-3-is-now-available-in-chatgpt-plus-and-enterprise.md","2023-10-19-evaluating-social-and-ethical-risks-from-generative-ai",{"id":11025,"data":11027,"filePath":11033,"digest":11034,"rendered":11035,"legacyId":11043},{"title":11028,"description":11029,"summary":11029,"pubDate":11030,"source":6423,"url":11031,"thumbnail":11032},"Evaluating social and ethical risks from generative AI","Introducing a context-based framework for comprehensively evaluating the social and ethical risks of AI systems",["Date","2023-10-19T15:00:00.000Z"],"https://deepmind.google/discover/blog/evaluating-social-and-ethical-risks-from-generative-ai/","https://lh3.googleusercontent.com/LAqM0ZkFzkDefB5oVEEPoq6p--7XcfBWEDPjl6OdcfvwN9q3leY2qWCf30_MquTn5RfpcPswiAoRns2jOKjB5_8u-vl6TqueSwamEM6U-qyJHOiujkI=w1200-h630-n-nu","src/content/posts/2023-10-19-evaluating-social-and-ethical-risks-from-generative-ai.md","b2d6d55dd9af27c8",{"html":25,"metadata":11036},{"headings":11037,"localImagePaths":11038,"remoteImagePaths":11039,"frontmatter":11040,"imagePaths":11042},[],[],[],{"title":11028,"description":11029,"summary":11029,"pubDate":11041,"source":6423,"url":11031,"thumbnail":11032},"Thu, 19 Oct 2023 15:00:00 +0000",[],"2023-10-19-evaluating-social-and-ethical-risks-from-generative-ai.md","2023-10-19-gradio-lite-serverless-gradio-running-entirely-in-your-browser",{"id":11044,"data":11046,"filePath":11052,"digest":11053,"rendered":11054,"legacyId":11062},{"title":11047,"description":25,"summary":11048,"pubDate":11049,"source":2720,"url":11050,"thumbnail":11051},"Gradio-Lite: Serverless Gradio Running Entirely in Your Browser","Gradio-Lite: Serverless Gradio Running Entirely in Your Browser Gradio is a popular Python library f...",["Date","2023-10-19T00:00:00.000Z"],"https://huggingface.co/blog/gradio-lite","https://huggingface.co/blog/assets/167_gradio_lite/thumbnail.png","src/content/posts/2023-10-19-gradio-lite-serverless-gradio-running-entirely-in-your-browser.md","bc7f585e33daeb6e",{"html":25,"metadata":11055},{"headings":11056,"localImagePaths":11057,"remoteImagePaths":11058,"frontmatter":11059,"imagePaths":11061},[],[],[],{"title":11047,"description":25,"summary":11048,"pubDate":11060,"source":2720,"url":11050,"thumbnail":11051},"Thu, 19 Oct 2023 00:00:00 GMT",[],"2023-10-19-gradio-lite-serverless-gradio-running-entirely-in-your-browser.md","2023-10-24-deploy-embedding-models-with-hugging-face-inference-endpoints",{"id":11063,"data":11065,"filePath":11071,"digest":11072,"rendered":11073,"legacyId":11081},{"title":11066,"description":25,"summary":11067,"pubDate":11068,"source":2720,"url":11069,"thumbnail":11070},"Deploy Embedding Models with Hugging Face Inference Endpoints","Deploy Embedding Models with Hugging Face Inference Endpoints The rise of Generative AI and LLMs lik...",["Date","2023-10-24T00:00:00.000Z"],"https://huggingface.co/blog/inference-endpoints-embeddings","https://huggingface.co/blog/assets/168_inference_endpoints_embeddings/thumbnail.jpg","src/content/posts/2023-10-24-deploy-embedding-models-with-hugging-face-inference-endpoints.md","a261f5021322b95b",{"html":25,"metadata":11074},{"headings":11075,"localImagePaths":11076,"remoteImagePaths":11077,"frontmatter":11078,"imagePaths":11080},[],[],[],{"title":11066,"description":25,"summary":11067,"pubDate":11079,"source":2720,"url":11069,"thumbnail":11070},"Tue, 24 Oct 2023 00:00:00 GMT",[],"2023-10-24-deploy-embedding-models-with-hugging-face-inference-endpoints.md","2023-10-24-exploring-simple-optimizations-for-sdxl",{"id":11082,"data":11084,"filePath":11090,"digest":11091,"rendered":11092,"legacyId":11099},{"title":11085,"description":25,"summary":11086,"pubDate":11087,"source":2720,"url":11088,"thumbnail":11089},"Exploring simple optimizations for SDXL","Exploring simple optimizations for SDXL Stable Diffusion XL (SDXL) is the latest latent diffusion mo...",["Date","2023-10-24T00:00:00.000Z"],"https://huggingface.co/blog/simple_sdxl_optimizations","https://huggingface.co/blog/assets/simple_sdxl_optimizations/thumbnail.png","src/content/posts/2023-10-24-exploring-simple-optimizations-for-sdxl.md","ac271641d774bb96",{"html":25,"metadata":11093},{"headings":11094,"localImagePaths":11095,"remoteImagePaths":11096,"frontmatter":11097,"imagePaths":11098},[],[],[],{"title":11085,"description":25,"summary":11086,"pubDate":11079,"source":2720,"url":11088,"thumbnail":11089},[],"2023-10-24-exploring-simple-optimizations-for-sdxl.md","2023-10-25-frontier-model-forum-updates",{"id":11100,"data":11102,"filePath":11107,"digest":11108,"rendered":11109,"legacyId":11117},{"title":11103,"description":11104,"summary":11104,"pubDate":11105,"source":19,"url":11106,"thumbnail":21},"Frontier Model Forum updates","Together with Anthropic, Google, and Microsoft, we’re announcing the new Executive Director of the Frontier Model Forum and a new $10 million AI Safety Fund.",["Date","2023-10-25T07:00:00.000Z"],"https://openai.com/blog/frontier-model-forum-updates","src/content/posts/2023-10-25-frontier-model-forum-updates.md","0ed1ec1bad354ca4",{"html":25,"metadata":11110},{"headings":11111,"localImagePaths":11112,"remoteImagePaths":11113,"frontmatter":11114,"imagePaths":11116},[],[],[],{"title":11103,"description":11104,"summary":11104,"pubDate":11115,"source":19,"url":11106,"thumbnail":21},"Wed, 25 Oct 2023 07:00:00 GMT",[],"2023-10-25-frontier-model-forum-updates.md","2023-10-24-the-n-implementation-details-of-rlhf-with-ppo",{"id":11118,"data":11120,"filePath":11126,"digest":11127,"rendered":11128,"legacyId":11135},{"title":11121,"description":25,"summary":11122,"pubDate":11123,"source":2720,"url":11124,"thumbnail":11125},"The N Implementation Details of RLHF with PPO","The N Implementation Details of RLHF with PPO RLHF / ChatGPT has been a popular research topic these...",["Date","2023-10-24T00:00:00.000Z"],"https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo","https://huggingface.co/blog/assets/167_the_n_implementation_details_of_rlhf_with_ppo/thumbnail.png","src/content/posts/2023-10-24-the-n-implementation-details-of-rlhf-with-ppo.md","abcf08756c906a20",{"html":25,"metadata":11129},{"headings":11130,"localImagePaths":11131,"remoteImagePaths":11132,"frontmatter":11133,"imagePaths":11134},[],[],[],{"title":11121,"description":25,"summary":11122,"pubDate":11079,"source":2720,"url":11124,"thumbnail":11125},[],"2023-10-24-the-n-implementation-details-of-rlhf-with-ppo.md","2023-10-25-interactively-explore-your-huggingface-dataset-with-one-line-of-code",{"id":11136,"data":11138,"filePath":11144,"digest":11145,"rendered":11146,"legacyId":11154},{"title":11139,"description":25,"summary":11140,"pubDate":11141,"source":2720,"url":11142,"thumbnail":11143},"Interactively explore your Huggingface dataset with one line of code","Interactively explore your Huggingface dataset with one line of code The Hugging Face datasets libra...",["Date","2023-10-25T00:00:00.000Z"],"https://huggingface.co/blog/scalable-data-inspection","https://huggingface.co/blog/assets/scalable-data-inspection/thumbnail.png","src/content/posts/2023-10-25-interactively-explore-your-huggingface-dataset-with-one-line-of-code.md","645e7d839bce712a",{"html":25,"metadata":11147},{"headings":11148,"localImagePaths":11149,"remoteImagePaths":11150,"frontmatter":11151,"imagePaths":11153},[],[],[],{"title":11139,"description":25,"summary":11140,"pubDate":11152,"source":2720,"url":11142,"thumbnail":11143},"Wed, 25 Oct 2023 00:00:00 GMT",[],"2023-10-25-interactively-explore-your-huggingface-dataset-with-one-line-of-code.md","2023-10-26-frontier-risk-and-preparedness",{"id":11155,"data":11157,"filePath":11162,"digest":11163,"rendered":11164,"legacyId":11172},{"title":11158,"description":11159,"summary":11159,"pubDate":11160,"source":19,"url":11161,"thumbnail":21},"Frontier risk and preparedness","To support the safety of highly-capable AI systems, we are developing our approach to catastrophic risk preparedness, including building a Preparedness team and launching a challenge.",["Date","2023-10-26T07:00:00.000Z"],"https://openai.com/blog/frontier-risk-and-preparedness","src/content/posts/2023-10-26-frontier-risk-and-preparedness.md","f8bb25f36c8d352a",{"html":25,"metadata":11165},{"headings":11166,"localImagePaths":11167,"remoteImagePaths":11168,"frontmatter":11169,"imagePaths":11171},[],[],[],{"title":11158,"description":11159,"summary":11159,"pubDate":11170,"source":19,"url":11161,"thumbnail":21},"Thu, 26 Oct 2023 07:00:00 GMT",[],"2023-10-26-frontier-risk-and-preparedness.md","2023-10-26-openais-approach-to-frontier-risk",{"id":11173,"data":11175,"filePath":11180,"digest":11181,"rendered":11182,"legacyId":11189},{"title":11176,"description":11177,"summary":11177,"pubDate":11178,"source":19,"url":11179,"thumbnail":21},"OpenAI’s Approach to Frontier Risk","An Update for the UK AI Safety Summit",["Date","2023-10-26T07:00:00.000Z"],"https://openai.com/global-affairs/our-approach-to-frontier-risk","src/content/posts/2023-10-26-openais-approach-to-frontier-risk.md","4614287820df0cb1",{"html":25,"metadata":11183},{"headings":11184,"localImagePaths":11185,"remoteImagePaths":11186,"frontmatter":11187,"imagePaths":11188},[],[],[],{"title":11176,"description":11177,"summary":11177,"pubDate":11170,"source":19,"url":11179,"thumbnail":21},[],"2023-10-26-openais-approach-to-frontier-risk.md","2023-10-27-personal-copilot-train-your-own-coding-assistant",{"id":11190,"data":11192,"filePath":11198,"digest":11199,"rendered":11200,"legacyId":11208},{"title":11193,"description":25,"summary":11194,"pubDate":11195,"source":2720,"url":11196,"thumbnail":11197},"Personal Copilot: Train Your Own Coding Assistant","Personal Copilot: Train Your Own Coding Assistant In the ever-evolving landscape of programming and ...",["Date","2023-10-27T00:00:00.000Z"],"https://huggingface.co/blog/personal-copilot","https://huggingface.co/blog/assets/170_personal_copilot/thumbnail.png","src/content/posts/2023-10-27-personal-copilot-train-your-own-coding-assistant.md","c696a2a4cfc5c160",{"html":25,"metadata":11201},{"headings":11202,"localImagePaths":11203,"remoteImagePaths":11204,"frontmatter":11205,"imagePaths":11207},[],[],[],{"title":11193,"description":25,"summary":11194,"pubDate":11206,"source":2720,"url":11196,"thumbnail":11197},"Fri, 27 Oct 2023 00:00:00 GMT",[],"2023-10-27-personal-copilot-train-your-own-coding-assistant.md","2023-10-31-a-glimpse-of-the-next-generation-of-alphafold",{"id":11209,"data":11211,"filePath":11217,"digest":11218,"rendered":11219,"legacyId":11227},{"title":11212,"description":11213,"summary":11213,"pubDate":11214,"source":6423,"url":11215,"thumbnail":11216},"A glimpse of the next generation of AlphaFold","Progress update: Our latest AlphaFold model shows significantly improved accuracy and expands coverage beyond proteins to other biological molecules, including ligands.",["Date","2023-10-31T13:00:00.000Z"],"https://deepmind.google/discover/blog/a-glimpse-of-the-next-generation-of-alphafold/","https://lh3.googleusercontent.com/1xoO5BAUUU8kLns4myMNnKw6RRQyUk1JdlWL1M0aDiagMgaBeDA9O8Y4rYFAo9hfnzmb0cnUMrT_-cStBqnyp_zW59F5Edwbvxcy3EVmfeKS-PNgVw=w1200-h630-n-nu","src/content/posts/2023-10-31-a-glimpse-of-the-next-generation-of-alphafold.md","997803f8c8dd10aa",{"html":25,"metadata":11220},{"headings":11221,"localImagePaths":11222,"remoteImagePaths":11223,"frontmatter":11224,"imagePaths":11226},[],[],[],{"title":11212,"description":11213,"summary":11213,"pubDate":11225,"source":6423,"url":11215,"thumbnail":11216},"Tue, 31 Oct 2023 13:00:00 +0000",[],"2023-10-31-a-glimpse-of-the-next-generation-of-alphafold.md","2023-11-03-introducing-storage-regions-on-the-hf-hub",{"id":11228,"data":11230,"filePath":11236,"digest":11237,"rendered":11238,"legacyId":11246},{"title":11231,"description":25,"summary":11232,"pubDate":11233,"source":2720,"url":11234,"thumbnail":11235},"Introducing Storage Regions on the HF Hub","Introducing Storage Regions on the Hub As part of our Enterprise Hub plan, we recently released supp...",["Date","2023-11-03T00:00:00.000Z"],"https://huggingface.co/blog/regions","https://huggingface.co/blog/assets/172_regions/thumbnail.png","src/content/posts/2023-11-03-introducing-storage-regions-on-the-hf-hub.md","cf7c3875805cd837",{"html":25,"metadata":11239},{"headings":11240,"localImagePaths":11241,"remoteImagePaths":11242,"frontmatter":11243,"imagePaths":11245},[],[],[],{"title":11231,"description":25,"summary":11232,"pubDate":11244,"source":2720,"url":11234,"thumbnail":11235},"Fri, 03 Nov 2023 00:00:00 GMT",[],"2023-11-03-introducing-storage-regions-on-the-hf-hub.md","2023-11-06-introducing-gpts",{"id":11247,"data":11249,"filePath":11254,"digest":11255,"rendered":11256,"legacyId":11264},{"title":11250,"description":11251,"summary":11251,"pubDate":11252,"source":19,"url":11253,"thumbnail":21},"Introducing GPTs","You can now create custom versions of ChatGPT that combine instructions, extra knowledge, and any combination of skills.",["Date","2023-11-06T08:00:00.000Z"],"https://openai.com/blog/introducing-gpts","src/content/posts/2023-11-06-introducing-gpts.md","70680e42a0440d1e",{"html":25,"metadata":11257},{"headings":11258,"localImagePaths":11259,"remoteImagePaths":11260,"frontmatter":11261,"imagePaths":11263},[],[],[],{"title":11250,"description":11251,"summary":11251,"pubDate":11262,"source":19,"url":11253,"thumbnail":21},"Mon, 06 Nov 2023 08:00:00 GMT",[],"2023-11-06-introducing-gpts.md","2023-11-06-new-models-and-developer-products-announced-at-devday",{"id":11265,"data":11267,"filePath":11272,"digest":11273,"rendered":11274,"legacyId":11281},{"title":11268,"description":11269,"summary":11269,"pubDate":11270,"source":19,"url":11271,"thumbnail":21},"New models and developer products announced at DevDay","GPT-4 Turbo with 128K context and lower prices, the new Assistants API, GPT-4 Turbo with Vision, DALL·E 3 API, and more.",["Date","2023-11-06T08:00:00.000Z"],"https://openai.com/blog/new-models-and-developer-products-announced-at-devday","src/content/posts/2023-11-06-new-models-and-developer-products-announced-at-devday.md","9ab50a65aba645d5",{"html":25,"metadata":11275},{"headings":11276,"localImagePaths":11277,"remoteImagePaths":11278,"frontmatter":11279,"imagePaths":11280},[],[],[],{"title":11268,"description":11269,"summary":11269,"pubDate":11262,"source":19,"url":11271,"thumbnail":21},[],"2023-11-06-new-models-and-developer-products-announced-at-devday.md","2023-11-07-comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-2-and-mistral-for-disaster-tweets-analysis-with-lora",{"id":11282,"data":11284,"filePath":11290,"digest":11291,"rendered":11292,"legacyId":11300},{"title":11285,"description":25,"summary":11286,"pubDate":11287,"source":2720,"url":11288,"thumbnail":11289},"Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora","Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweet...",["Date","2023-11-07T00:00:00.000Z"],"https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral","https://huggingface.co/blog/assets/Lora-for-sequence-classification-with-Roberta-Llama-Mistral/Thumbnail.png","src/content/posts/2023-11-07-comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-2-and-mistral-for-disaster-tweets-analysis-with-lora.md","b75ae7e917323a0e",{"html":25,"metadata":11293},{"headings":11294,"localImagePaths":11295,"remoteImagePaths":11296,"frontmatter":11297,"imagePaths":11299},[],[],[],{"title":11285,"description":25,"summary":11286,"pubDate":11298,"source":2720,"url":11288,"thumbnail":11289},"Tue, 07 Nov 2023 00:00:00 GMT",[],"2023-11-07-comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-2-and-mistral-for-disaster-tweets-analysis-with-lora.md","2023-11-07-introducing-prodigy-hf-a-direct-integration-with-hugging-face",{"id":11301,"data":11303,"filePath":11309,"digest":11310,"rendered":11311,"legacyId":11318},{"title":11304,"description":25,"summary":11305,"pubDate":11306,"source":2720,"url":11307,"thumbnail":11308},"Introducing Prodigy-HF: a direct integration with Hugging Face","Introducing Prodigy-HF Prodigy is an annotation tool made by Explosion, a company well known as the ...",["Date","2023-11-07T00:00:00.000Z"],"https://huggingface.co/blog/prodigy-hf","https://huggingface.co/blog/assets/171_prodigy_hf/thumbnail.png","src/content/posts/2023-11-07-introducing-prodigy-hf-a-direct-integration-with-hugging-face.md","2dfe458b34947185",{"html":25,"metadata":11312},{"headings":11313,"localImagePaths":11314,"remoteImagePaths":11315,"frontmatter":11316,"imagePaths":11317},[],[],[],{"title":11304,"description":25,"summary":11305,"pubDate":11298,"source":2720,"url":11307,"thumbnail":11308},[],"2023-11-07-introducing-prodigy-hf-a-direct-integration-with-hugging-face.md","2023-11-07-make-your-llama-generation-time-fly-with-aws-inferentia2",{"id":11319,"data":11321,"filePath":11327,"digest":11328,"rendered":11329,"legacyId":11336},{"title":11322,"description":25,"summary":11323,"pubDate":11324,"source":2720,"url":11325,"thumbnail":11326},"Make your llama generation time fly with AWS Inferentia2","Make your llama generation time fly with AWS Inferentia2 Update (02/2024): Performance has improved ...",["Date","2023-11-07T00:00:00.000Z"],"https://huggingface.co/blog/inferentia-llama2","https://huggingface.co/blog/assets/inferentia-llama2/thumbnail.png","src/content/posts/2023-11-07-make-your-llama-generation-time-fly-with-aws-inferentia2.md","b2211a0787c6c0c3",{"html":25,"metadata":11330},{"headings":11331,"localImagePaths":11332,"remoteImagePaths":11333,"frontmatter":11334,"imagePaths":11335},[],[],[],{"title":11322,"description":25,"summary":11323,"pubDate":11298,"source":2720,"url":11325,"thumbnail":11326},[],"2023-11-07-make-your-llama-generation-time-fly-with-aws-inferentia2.md","2023-11-09-openai-data-partnerships",{"id":11337,"data":11339,"filePath":11344,"digest":11345,"rendered":11346,"legacyId":11354},{"title":11340,"description":11341,"summary":11341,"pubDate":11342,"source":19,"url":11343,"thumbnail":21},"OpenAI Data Partnerships","Working together to create open-source and private datasets for AI training.",["Date","2023-11-09T08:00:00.000Z"],"https://openai.com/blog/data-partnerships","src/content/posts/2023-11-09-openai-data-partnerships.md","69db7720b03e870c",{"html":25,"metadata":11347},{"headings":11348,"localImagePaths":11349,"remoteImagePaths":11350,"frontmatter":11351,"imagePaths":11353},[],[],[],{"title":11340,"description":11341,"summary":11341,"pubDate":11352,"source":19,"url":11343,"thumbnail":21},"Thu, 09 Nov 2023 08:00:00 GMT",[],"2023-11-09-openai-data-partnerships.md","2023-11-09-sdxl-in-4-steps-with-latent-consistency-loras",{"id":11355,"data":11357,"filePath":11363,"digest":11364,"rendered":11365,"legacyId":11373},{"title":11358,"description":25,"summary":11359,"pubDate":11360,"source":2720,"url":11361,"thumbnail":11362},"SDXL in 4 steps with Latent Consistency LoRAs","SDXL in 4 steps with Latent Consistency LoRAs Latent Consistency Models (LCM) are a way to decrease ...",["Date","2023-11-09T00:00:00.000Z"],"https://huggingface.co/blog/lcm_lora","https://huggingface.co/blog/assets/lcm_sdxl/lcm_thumbnail.png","src/content/posts/2023-11-09-sdxl-in-4-steps-with-latent-consistency-loras.md","c0557a2272651cd6",{"html":25,"metadata":11366},{"headings":11367,"localImagePaths":11368,"remoteImagePaths":11369,"frontmatter":11370,"imagePaths":11372},[],[],[],{"title":11358,"description":25,"summary":11359,"pubDate":11371,"source":2720,"url":11361,"thumbnail":11362},"Thu, 09 Nov 2023 00:00:00 GMT",[],"2023-11-09-sdxl-in-4-steps-with-latent-consistency-loras.md","2023-11-14-graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting",{"id":11374,"data":11376,"filePath":11382,"digest":11383,"rendered":11384,"legacyId":11392},{"title":11377,"description":11378,"summary":11378,"pubDate":11379,"source":6423,"url":11380,"thumbnail":11381},"GraphCast: AI model for faster and more accurate global weather forecasting","We introduce GraphCast, a state-of-the-art AI model able to make medium-range weather forecasts with unprecedented accuracy",["Date","2023-11-14T15:00:00.000Z"],"https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/","https://lh3.googleusercontent.com/5dL0Cm8RLhoDdfPzVy5MlKB5JDcfYucbgxzNLJVFdtqRe15-bFTvfdOrpqnrM4m5XMEEboWtvyCLQgSCvHEH62QqZZI0V_zuBAz71fghXgU5UNFFwg=w1200-h630-n-nu","src/content/posts/2023-11-14-graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting.md","825a1c80b580a3c6",{"html":25,"metadata":11385},{"headings":11386,"localImagePaths":11387,"remoteImagePaths":11388,"frontmatter":11389,"imagePaths":11391},[],[],[],{"title":11377,"description":11378,"summary":11378,"pubDate":11390,"source":6423,"url":11380,"thumbnail":11381},"Tue, 14 Nov 2023 15:00:00 +0000",[],"2023-11-14-graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting.md","2023-11-15-empowering-the-next-generation-for-an-ai-enabled-world",{"id":11393,"data":11395,"filePath":11401,"digest":11402,"rendered":11403,"legacyId":11411},{"title":11396,"description":11397,"summary":11397,"pubDate":11398,"source":6423,"url":11399,"thumbnail":11400},"Empowering the next generation for an AI-enabled world","Experience AI's course and resources are expanding on a global scale",["Date","2023-11-15T10:00:00.000Z"],"https://deepmind.google/discover/blog/empowering-the-next-generation-for-an-ai-enabled-world/","https://lh3.googleusercontent.com/XJ10IR5bv5MmygxoFC0hrepTZtjq_Bwz69bL7d7jBy06fnEFodAa0tbIWKOwV7gW2Im3JY2GGda-xZKtVQhqcaozz6r_vdHXsgVu0CzyIhIz4VGs=w1200-h630-n-nu","src/content/posts/2023-11-15-empowering-the-next-generation-for-an-ai-enabled-world.md","018c4cfa40c632e5",{"html":25,"metadata":11404},{"headings":11405,"localImagePaths":11406,"remoteImagePaths":11407,"frontmatter":11408,"imagePaths":11410},[],[],[],{"title":11396,"description":11397,"summary":11397,"pubDate":11409,"source":6423,"url":11399,"thumbnail":11400},"Wed, 15 Nov 2023 10:00:00 +0000",[],"2023-11-15-empowering-the-next-generation-for-an-ai-enabled-world.md","2023-11-16-transforming-the-future-of-music-creation",{"id":11412,"data":11414,"filePath":11420,"digest":11421,"rendered":11422,"legacyId":11430},{"title":11415,"description":11416,"summary":11416,"pubDate":11417,"source":6423,"url":11418,"thumbnail":11419},"Transforming the future of music creation","Announcing our most advanced music generation model and two new AI experiments, designed to open a new playground for creativity",["Date","2023-11-16T07:20:00.000Z"],"https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/","https://lh3.googleusercontent.com/msr-Fc99rrkeoQkZ6rLTKnof3RTqo5oo9D2_xPyqtpp0mAMqqkn-x3mPy2dD0My1g7w-cysBQzHU_iWF4mlblU4EgQRcMNKoBUgPdmdmEoyekFJEnA=w1200-h630-n-nu","src/content/posts/2023-11-16-transforming-the-future-of-music-creation.md","83ed9589ee9b1c38",{"html":25,"metadata":11423},{"headings":11424,"localImagePaths":11425,"remoteImagePaths":11426,"frontmatter":11427,"imagePaths":11429},[],[],[],{"title":11415,"description":11416,"summary":11416,"pubDate":11428,"source":6423,"url":11418,"thumbnail":11419},"Thu, 16 Nov 2023 07:20:00 +0000",[],"2023-11-16-transforming-the-future-of-music-creation.md","2023-11-17-openai-announces-leadership-transition",{"id":11431,"data":11433,"filePath":11437,"digest":11438,"rendered":11439,"legacyId":11447},{"title":11434,"description":25,"summary":25,"pubDate":11435,"source":19,"url":11436,"thumbnail":21},"OpenAI announces leadership transition",["Date","2023-11-17T08:00:00.000Z"],"https://openai.com/blog/openai-announces-leadership-transition","src/content/posts/2023-11-17-openai-announces-leadership-transition.md","f68052a287702332",{"html":25,"metadata":11440},{"headings":11441,"localImagePaths":11442,"remoteImagePaths":11443,"frontmatter":11444,"imagePaths":11446},[],[],[],{"title":11434,"description":25,"summary":25,"pubDate":11445,"source":19,"url":11436,"thumbnail":21},"Fri, 17 Nov 2023 08:00:00 GMT",[],"2023-11-17-openai-announces-leadership-transition.md","2023-11-29-millions-of-new-materials-discovered-with-deep-learning",{"id":11448,"data":11450,"filePath":11456,"digest":11457,"rendered":11458,"legacyId":11466},{"title":11451,"description":11452,"summary":11452,"pubDate":11453,"source":6423,"url":11454,"thumbnail":11455},"Millions of new materials discovered with deep learning","We share the discovery of 2.2 million new crystals  –  equivalent to nearly 800 years’ worth of knowledge. We introduce Graph Networks for Materials Exploration (GNoME), our new deep learning tool that dramatically increases the speed and efficiency of discovery by predicting the stability of new materials.",["Date","2023-11-29T16:04:00.000Z"],"https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/","https://lh3.googleusercontent.com/mq3mFiVHSVuszhJMt-Nz4jckN5qy3cAckEIdNYDPhy8UHjxk4VkGFriqo8sA76teioNQ2fC3qgMH7FJfPc0L5JJPppXiZzHP7Rl3UodlU4IC4TWw=w1200-h630-n-nu","src/content/posts/2023-11-29-millions-of-new-materials-discovered-with-deep-learning.md","a203ad4cd3c05f37",{"html":25,"metadata":11459},{"headings":11460,"localImagePaths":11461,"remoteImagePaths":11462,"frontmatter":11463,"imagePaths":11465},[],[],[],{"title":11451,"description":11452,"summary":11452,"pubDate":11464,"source":6423,"url":11454,"thumbnail":11455},"Wed, 29 Nov 2023 16:04:00 +0000",[],"2023-11-29-millions-of-new-materials-discovered-with-deep-learning.md","2023-11-29-sam-altman-returns-as-ceo-openai-has-a-new-initial-board",{"id":11467,"data":11469,"filePath":11474,"digest":11475,"rendered":11476,"legacyId":11484},{"title":11470,"description":11471,"summary":11471,"pubDate":11472,"source":19,"url":11473,"thumbnail":21},"Sam Altman returns as CEO, OpenAI has a new initial board","Mira Murati as CTO, Greg Brockman returns as President. Read messages from CEO Sam Altman and board chair Bret Taylor.",["Date","2023-11-29T08:00:00.000Z"],"https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board","src/content/posts/2023-11-29-sam-altman-returns-as-ceo-openai-has-a-new-initial-board.md","9ecb1a7be5487df2",{"html":25,"metadata":11477},{"headings":11478,"localImagePaths":11479,"remoteImagePaths":11480,"frontmatter":11481,"imagePaths":11483},[],[],[],{"title":11470,"description":11471,"summary":11471,"pubDate":11482,"source":19,"url":11473,"thumbnail":21},"Wed, 29 Nov 2023 08:00:00 GMT",[],"2023-11-29-sam-altman-returns-as-ceo-openai-has-a-new-initial-board.md","2023-12-01-open-llm-leaderboard-drop-deep-dive",{"id":11485,"data":11487,"filePath":11492,"digest":11493,"rendered":11494,"legacyId":11502},{"title":11488,"description":25,"summary":11489,"pubDate":11490,"source":2720,"url":11491,"thumbnail":9445},"Open LLM Leaderboard: DROP deep dive","Open LLM Leaderboard: DROP deep dive Recently, three new benchmarks were added to the Open LLM Leade...",["Date","2023-12-01T00:00:00.000Z"],"https://huggingface.co/blog/open-llm-leaderboard-drop","src/content/posts/2023-12-01-open-llm-leaderboard-drop-deep-dive.md","afdb955cd6b25e90",{"html":25,"metadata":11495},{"headings":11496,"localImagePaths":11497,"remoteImagePaths":11498,"frontmatter":11499,"imagePaths":11501},[],[],[],{"title":11488,"description":25,"summary":11489,"pubDate":11500,"source":2720,"url":11491,"thumbnail":9445},"Fri, 01 Dec 2023 00:00:00 GMT",[],"2023-12-01-open-llm-leaderboard-drop-deep-dive.md","2023-12-05-amd-large-language-models-out-of-the-box-acceleration-with-amd-gpu",{"id":11503,"data":11505,"filePath":11511,"digest":11512,"rendered":11513,"legacyId":11521},{"title":11506,"description":25,"summary":11507,"pubDate":11508,"source":2720,"url":11509,"thumbnail":11510},"AMD + 🤗: Large Language Models Out-of-the-Box Acceleration with AMD GPU","AMD + 🤗: Large Language Models Out-of-the-Box Acceleration with AMD GPU Earlier this year, AMD and H...",["Date","2023-12-05T00:00:00.000Z"],"https://huggingface.co/blog/huggingface-and-optimum-amd","https://huggingface.co/blog/assets/optimum_amd/amd_hf_logo_fixed.png","src/content/posts/2023-12-05-amd-large-language-models-out-of-the-box-acceleration-with-amd-gpu.md","ce8c1c592d26a672",{"html":25,"metadata":11514},{"headings":11515,"localImagePaths":11516,"remoteImagePaths":11517,"frontmatter":11518,"imagePaths":11520},[],[],[],{"title":11506,"description":25,"summary":11507,"pubDate":11519,"source":2720,"url":11509,"thumbnail":11510},"Tue, 05 Dec 2023 00:00:00 GMT",[],"2023-12-05-amd-large-language-models-out-of-the-box-acceleration-with-amd-gpu.md","2023-12-05-goodbye-cold-boot---how-we-made-lora-inference-300-faster",{"id":11522,"data":11524,"filePath":11530,"digest":11531,"rendered":11532,"legacyId":11539},{"title":11525,"description":25,"summary":11526,"pubDate":11527,"source":2720,"url":11528,"thumbnail":11529},"Goodbye cold boot - how we made LoRA inference 300% faster","Goodbye cold boot - how we made LoRA Inference 300% faster tl;dr: We swap the Stable Diffusion LoRA ...",["Date","2023-12-05T00:00:00.000Z"],"https://huggingface.co/blog/lora-adapters-dynamic-loading","https://huggingface.co/blog/assets/171_load_lora_adapters/thumbnail3.png","src/content/posts/2023-12-05-goodbye-cold-boot---how-we-made-lora-inference-300-faster.md","1a9da6e8b3bfe0df",{"html":25,"metadata":11533},{"headings":11534,"localImagePaths":11535,"remoteImagePaths":11536,"frontmatter":11537,"imagePaths":11538},[],[],[],{"title":11525,"description":25,"summary":11526,"pubDate":11519,"source":2720,"url":11528,"thumbnail":11529},[],"2023-12-05-goodbye-cold-boot---how-we-made-lora-inference-300-faster.md","2023-12-05-optimum-nvidia---unlock-blazingly-fast-llm-inference-in-just-1-line-of-code",{"id":11540,"data":11542,"filePath":11548,"digest":11549,"rendered":11550,"legacyId":11557},{"title":11543,"description":25,"summary":11544,"pubDate":11545,"source":2720,"url":11546,"thumbnail":11547},"Optimum-NVIDIA - Unlock blazingly fast LLM inference in just 1 line of code","Optimum-NVIDIA on Hugging Face enables blazingly fast LLM inference in just 1 line of code Large Lan...",["Date","2023-12-05T00:00:00.000Z"],"https://huggingface.co/blog/optimum-nvidia","https://huggingface.co/blog/assets/optimum_nvidia/hf_nvidia_banner.png","src/content/posts/2023-12-05-optimum-nvidia---unlock-blazingly-fast-llm-inference-in-just-1-line-of-code.md","7e1045fc4e0d306e",{"html":25,"metadata":11551},{"headings":11552,"localImagePaths":11553,"remoteImagePaths":11554,"frontmatter":11555,"imagePaths":11556},[],[],[],{"title":11543,"description":25,"summary":11544,"pubDate":11519,"source":2720,"url":11546,"thumbnail":11547},[],"2023-12-05-optimum-nvidia---unlock-blazingly-fast-llm-inference-in-just-1-line-of-code.md","2023-12-06-introducing-gemini-our-largest-and-most-capable-ai-model",{"id":11558,"data":11560,"filePath":11566,"digest":11567,"rendered":11568,"legacyId":11576},{"title":11561,"description":11562,"summary":11562,"pubDate":11563,"source":6423,"url":11564,"thumbnail":11565},"Introducing Gemini: our largest and most capable AI model","Making AI more helpful for everyone",["Date","2023-12-06T15:13:00.000Z"],"https://deepmind.google/discover/blog/introducing-gemini-our-largest-and-most-capable-ai-model/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemini_SS.width-1300.jpg","src/content/posts/2023-12-06-introducing-gemini-our-largest-and-most-capable-ai-model.md","d38c5293bf1bdf81",{"html":25,"metadata":11569},{"headings":11570,"localImagePaths":11571,"remoteImagePaths":11572,"frontmatter":11573,"imagePaths":11575},[],[],[],{"title":11561,"description":11562,"summary":11562,"pubDate":11574,"source":6423,"url":11564,"thumbnail":11565},"Wed, 06 Dec 2023 15:13:00 +0000",[],"2023-12-06-introducing-gemini-our-largest-and-most-capable-ai-model.md","2023-12-06-setfitabsa-few-shot-aspect-based-sentiment-analysis-using-setfit",{"id":11577,"data":11579,"filePath":11585,"digest":11586,"rendered":11587,"legacyId":11595},{"title":11580,"description":25,"summary":11581,"pubDate":11582,"source":2720,"url":11583,"thumbnail":11584},"SetFitABSA: Few-Shot Aspect Based Sentiment Analysis using SetFit","SetFitABSA: Few-Shot Aspect Based Sentiment Analysis using SetFit SetFitABSA is an efficient techniq...",["Date","2023-12-06T00:00:00.000Z"],"https://huggingface.co/blog/setfit-absa","https://huggingface.co/blog/assets/setfit-absa/intel_hf_logo_2.png","src/content/posts/2023-12-06-setfitabsa-few-shot-aspect-based-sentiment-analysis-using-setfit.md","f57f707f83eea199",{"html":25,"metadata":11588},{"headings":11589,"localImagePaths":11590,"remoteImagePaths":11591,"frontmatter":11592,"imagePaths":11594},[],[],[],{"title":11580,"description":25,"summary":11581,"pubDate":11593,"source":2720,"url":11583,"thumbnail":11584},"Wed, 06 Dec 2023 00:00:00 GMT",[],"2023-12-06-setfitabsa-few-shot-aspect-based-sentiment-analysis-using-setfit.md","2023-12-08-google-deepmind-at-neurips-2023",{"id":11596,"data":11598,"filePath":11604,"digest":11605,"rendered":11606,"legacyId":11614},{"title":11599,"description":11600,"summary":11600,"pubDate":11601,"source":6423,"url":11602,"thumbnail":11603},"Google DeepMind at NeurIPS 2023","The Neural Information Processing Systems (NeurIPS) is the largest artificial intelligence (AI) conference in the world. NeurIPS 2023 will be taking place December 10-16 in New Orleans, USA.Teams from across Google DeepMind are presenting more than 150 papers at the main conference and workshops.",["Date","2023-12-08T15:01:00.000Z"],"https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023/","https://lh3.googleusercontent.com/MDme_Q62zVqvTUs5uwaI3Ggy2rWIujPt2elkusnUuCA4wEo79V9mabIg66j9cr9zMso-LObOVcj6_ZnrgSMUKn6fl52kxOUEjcigXtDZ2UMuosX3-2s=w1200-h630-n-nu","src/content/posts/2023-12-08-google-deepmind-at-neurips-2023.md","520cdb712067e20c",{"html":25,"metadata":11607},{"headings":11608,"localImagePaths":11609,"remoteImagePaths":11610,"frontmatter":11611,"imagePaths":11613},[],[],[],{"title":11599,"description":11600,"summary":11600,"pubDate":11612,"source":6423,"url":11602,"thumbnail":11603},"Fri, 08 Dec 2023 15:01:00 +0000",[],"2023-12-08-google-deepmind-at-neurips-2023.md","2023-12-11-mixture-of-experts-explained",{"id":11615,"data":11617,"filePath":11623,"digest":11624,"rendered":11625,"legacyId":11633},{"title":11618,"description":25,"summary":11619,"pubDate":11620,"source":2720,"url":11621,"thumbnail":11622},"Mixture of Experts Explained","Mixture of Experts Explained With the release of Mixtral 8x7B (announcement, model card), a class of...",["Date","2023-12-11T00:00:00.000Z"],"https://huggingface.co/blog/moe","https://huggingface.co/blog/assets/moe/thumbnail.png","src/content/posts/2023-12-11-mixture-of-experts-explained.md","9e76241f49d850de",{"html":25,"metadata":11626},{"headings":11627,"localImagePaths":11628,"remoteImagePaths":11629,"frontmatter":11630,"imagePaths":11632},[],[],[],{"title":11618,"description":25,"summary":11619,"pubDate":11631,"source":2720,"url":11621,"thumbnail":11622},"Mon, 11 Dec 2023 00:00:00 GMT",[],"2023-12-11-mixture-of-experts-explained.md","2023-12-11-welcome-mixtral---a-sota-mixture-of-experts-on-hugging-face",{"id":11634,"data":11636,"filePath":11642,"digest":11643,"rendered":11644,"legacyId":11651},{"title":11637,"description":25,"summary":11638,"pubDate":11639,"source":2720,"url":11640,"thumbnail":11641},"Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face","Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face Mixtral 8x7b is an exciting large langua...",["Date","2023-12-11T00:00:00.000Z"],"https://huggingface.co/blog/mixtral","https://huggingface.co/blog/assets/mixtral/thumbnail.jpg","src/content/posts/2023-12-11-welcome-mixtral---a-sota-mixture-of-experts-on-hugging-face.md","77c990cb928e533b",{"html":25,"metadata":11645},{"headings":11646,"localImagePaths":11647,"remoteImagePaths":11648,"frontmatter":11649,"imagePaths":11650},[],[],[],{"title":11637,"description":25,"summary":11638,"pubDate":11631,"source":2720,"url":11640,"thumbnail":11641},[],"2023-12-11-welcome-mixtral---a-sota-mixture-of-experts-on-hugging-face.md","2023-12-13-partnership-with-axel-springer-to-deepen-beneficial-use-of-ai-in-journalism",{"id":11652,"data":11654,"filePath":11659,"digest":11660,"rendered":11661,"legacyId":11669},{"title":11655,"description":11656,"summary":11656,"pubDate":11657,"source":19,"url":11658,"thumbnail":21},"Partnership with Axel Springer to deepen beneficial use of AI in journalism","Axel Springer is the first publishing house globally to partner with us on a deeper integration of journalism in AI technologies.",["Date","2023-12-13T08:00:00.000Z"],"https://openai.com/blog/axel-springer-partnership","src/content/posts/2023-12-13-partnership-with-axel-springer-to-deepen-beneficial-use-of-ai-in-journalism.md","9e39a1685beeddf1",{"html":25,"metadata":11662},{"headings":11663,"localImagePaths":11664,"remoteImagePaths":11665,"frontmatter":11666,"imagePaths":11668},[],[],[],{"title":11655,"description":11656,"summary":11656,"pubDate":11667,"source":19,"url":11658,"thumbnail":21},"Wed, 13 Dec 2023 08:00:00 GMT",[],"2023-12-13-partnership-with-axel-springer-to-deepen-beneficial-use-of-ai-in-journalism.md","2023-12-14-funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models",{"id":11670,"data":11672,"filePath":11678,"digest":11679,"rendered":11680,"legacyId":11688},{"title":11673,"description":11674,"summary":11674,"pubDate":11675,"source":6423,"url":11676,"thumbnail":11677},"FunSearch: Making new discoveries in mathematical sciences using Large Language Models","In a paper published in Nature, we introduce FunSearch, a method for searching for “functions” written in computer code, and find new solutions in mathematics and computer science. FunSearch works by pairing a pre-trained LLM, whose goal is to provide creative solutions in the form of computer code, with an automated “evaluator”, which guards against hallucinations and incorrect ideas.",["Date","2023-12-14T16:00:00.000Z"],"https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/","https://lh3.googleusercontent.com/GC6SH0u6DyyCT9C1LH6XXmproSod7o5QGp9-Oe8BsuXyPzBlfcxFHX9pxXg69ZftEVU0Joga7tyo0VwQOSBBrugZ8qfl9_X-pgiH527p71S7DC32Jw=w1200-h630-n-nu","src/content/posts/2023-12-14-funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models.md","f930d5bf82f036d0",{"html":25,"metadata":11681},{"headings":11682,"localImagePaths":11683,"remoteImagePaths":11684,"frontmatter":11685,"imagePaths":11687},[],[],[],{"title":11673,"description":11674,"summary":11674,"pubDate":11686,"source":6423,"url":11676,"thumbnail":11677},"Thu, 14 Dec 2023 16:00:00 +0000",[],"2023-12-14-funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models.md","2023-12-14-increasing-accuracy-of-pediatric-visit-notes",{"id":11689,"data":11691,"filePath":11696,"digest":11697,"rendered":11698,"legacyId":11706},{"title":11692,"description":11693,"summary":11693,"pubDate":11694,"source":19,"url":11695,"thumbnail":21},"Increasing accuracy of pediatric visit notes","Summer Health reimagines pediatric doctor’s visits with OpenAI.",["Date","2023-12-14T08:00:00.000Z"],"https://openai.com/blog/summer-health","src/content/posts/2023-12-14-increasing-accuracy-of-pediatric-visit-notes.md","39bfdb2ad8625824",{"html":25,"metadata":11699},{"headings":11700,"localImagePaths":11701,"remoteImagePaths":11702,"frontmatter":11703,"imagePaths":11705},[],[],[],{"title":11692,"description":11693,"summary":11693,"pubDate":11704,"source":19,"url":11695,"thumbnail":21},"Thu, 14 Dec 2023 08:00:00 GMT",[],"2023-12-14-increasing-accuracy-of-pediatric-visit-notes.md","2023-12-14-practices-for-governing-agentic-ai-systems",{"id":11707,"data":11709,"filePath":11713,"digest":11714,"rendered":11715,"legacyId":11722},{"title":11710,"description":25,"summary":25,"pubDate":11711,"source":19,"url":11712,"thumbnail":21},"Practices for Governing Agentic AI Systems",["Date","2023-12-14T08:00:00.000Z"],"https://openai.com/blog/practices-for-governing-agentic-ai-systems","src/content/posts/2023-12-14-practices-for-governing-agentic-ai-systems.md","d1a8d02a31e6bfcd",{"html":25,"metadata":11716},{"headings":11717,"localImagePaths":11718,"remoteImagePaths":11719,"frontmatter":11720,"imagePaths":11721},[],[],[],{"title":11710,"description":25,"summary":25,"pubDate":11704,"source":19,"url":11712,"thumbnail":21},[],"2023-12-14-practices-for-governing-agentic-ai-systems.md","2023-12-14-superalignment-fast-grants",{"id":11723,"data":11725,"filePath":11730,"digest":11731,"rendered":11732,"legacyId":11739},{"title":11726,"description":11727,"summary":11727,"pubDate":11728,"source":19,"url":11729,"thumbnail":21},"Superalignment Fast Grants","We’re launching $10M in grants to support technical research towards the alignment and safety of superhuman AI systems, including weak-to-strong generalization, interpretability, scalable oversight, and more.",["Date","2023-12-14T08:00:00.000Z"],"https://openai.com/blog/superalignment-fast-grants","src/content/posts/2023-12-14-superalignment-fast-grants.md","a52145cdd5a12201",{"html":25,"metadata":11733},{"headings":11734,"localImagePaths":11735,"remoteImagePaths":11736,"frontmatter":11737,"imagePaths":11738},[],[],[],{"title":11726,"description":11727,"summary":11727,"pubDate":11704,"source":19,"url":11729,"thumbnail":21},[],"2023-12-14-superalignment-fast-grants.md","2023-12-14-weak-to-strong-generalization",{"id":11740,"data":11742,"filePath":11747,"digest":11748,"rendered":11749,"legacyId":11757},{"title":11743,"description":11744,"summary":11744,"pubDate":11745,"source":19,"url":11746,"thumbnail":21},"Weak-to-strong generalization","We present a new research direction for superalignment, together with promising initial results: can we leverage the generalization properties of deep learning to control strong models with weak supervisors?",["Date","2023-12-14T00:00:00.000Z"],"https://openai.com/blog/weak-to-strong-generalization","src/content/posts/2023-12-14-weak-to-strong-generalization.md","1cdbdfe1e8219ae2",{"html":25,"metadata":11750},{"headings":11751,"localImagePaths":11752,"remoteImagePaths":11753,"frontmatter":11754,"imagePaths":11756},[],[],[],{"title":11743,"description":11744,"summary":11744,"pubDate":11755,"source":19,"url":11746,"thumbnail":21},"Thu, 14 Dec 2023 00:00:00 GMT",[],"2023-12-14-weak-to-strong-generalization.md","2023-12-18-2023-year-of-open-llms",{"id":11758,"data":11760,"filePath":11765,"digest":11766,"rendered":11767,"legacyId":11775},{"title":11761,"description":25,"summary":11762,"pubDate":11763,"source":2720,"url":11764,"thumbnail":7540},"2023, year of open LLMs","2023, year of open LLMs 2023 has seen a surge of public interest in Large Language Models (LLMs), an...",["Date","2023-12-18T00:00:00.000Z"],"https://huggingface.co/blog/2023-in-llms","src/content/posts/2023-12-18-2023-year-of-open-llms.md","0e9b11b92f4ae587",{"html":25,"metadata":11768},{"headings":11769,"localImagePaths":11770,"remoteImagePaths":11771,"frontmatter":11772,"imagePaths":11774},[],[],[],{"title":11761,"description":25,"summary":11762,"pubDate":11773,"source":2720,"url":11764,"thumbnail":7540},"Mon, 18 Dec 2023 00:00:00 GMT",[],"2023-12-18-2023-year-of-open-llms.md","2023-12-20-speculative-decoding-for-2x-faster-whisper-inference",{"id":11776,"data":11778,"filePath":11784,"digest":11785,"rendered":11786,"legacyId":11794},{"title":11779,"description":25,"summary":11780,"pubDate":11781,"source":2720,"url":11782,"thumbnail":11783},"Speculative Decoding for 2x Faster Whisper Inference","Speculative Decoding for 2x Faster Whisper Inference Open AI's Whisper is a general purpose speech t...",["Date","2023-12-20T00:00:00.000Z"],"https://huggingface.co/blog/whisper-speculative-decoding","https://huggingface.co/blog/assets/whisper-speculative-decoding/thumbnail.png","src/content/posts/2023-12-20-speculative-decoding-for-2x-faster-whisper-inference.md","ea7afa73d5381652",{"html":25,"metadata":11787},{"headings":11788,"localImagePaths":11789,"remoteImagePaths":11790,"frontmatter":11791,"imagePaths":11793},[],[],[],{"title":11779,"description":25,"summary":11780,"pubDate":11792,"source":2720,"url":11782,"thumbnail":11783},"Wed, 20 Dec 2023 00:00:00 GMT",[],"2023-12-20-speculative-decoding-for-2x-faster-whisper-inference.md","2023-12-22-2023-a-year-of-groundbreaking-advances-in-ai-and-computing",{"id":11795,"data":11797,"filePath":11803,"digest":11804,"rendered":11805,"legacyId":11813},{"title":11798,"description":11799,"summary":11799,"pubDate":11800,"source":6423,"url":11801,"thumbnail":11802},"2023: A Year of Groundbreaking Advances in AI and Computing","This has been a year of incredible progress in the field of Artificial Intelligence (AI) research and its practical applications.",["Date","2023-12-22T13:30:00.000Z"],"https://deepmind.google/discover/blog/2023-a-year-of-groundbreaking-advances-in-ai-and-computing/","https://lh3.googleusercontent.com/fkZqqqpfLKvV2E6ebVmYJjR9q9XnczvWtiui5uU-yPkHCQb5mLAB4kBmh3opGqOJLhtaC58td96UtvULI8uGpbB9TmejR82GZ2vWOqTyWZ6HSItIpHg=w1200-h630-n-nu","src/content/posts/2023-12-22-2023-a-year-of-groundbreaking-advances-in-ai-and-computing.md","f7366e8f2457ed49",{"html":25,"metadata":11806},{"headings":11807,"localImagePaths":11808,"remoteImagePaths":11809,"frontmatter":11810,"imagePaths":11812},[],[],[],{"title":11798,"description":11799,"summary":11799,"pubDate":11811,"source":6423,"url":11801,"thumbnail":11802},"Fri, 22 Dec 2023 13:30:00 +0000",[],"2023-12-22-2023-a-year-of-groundbreaking-advances-in-ai-and-computing.md","2024-01-02-images-altered-to-trick-machine-vision-can-influence-humans-too",{"id":11814,"data":11816,"filePath":11821,"digest":11822,"rendered":11823,"legacyId":11831},{"title":11817,"description":11818,"summary":11818,"pubDate":11819,"source":6423,"url":11820,"thumbnail":6742},"Images altered to trick machine vision can influence humans too","In a series of experiments published in Nature Communications, we found evidence that human judgments are indeed systematically influenced by adversarial perturbations.",["Date","2024-01-02T16:00:00.000Z"],"https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-influence-humans-too/","src/content/posts/2024-01-02-images-altered-to-trick-machine-vision-can-influence-humans-too.md","ab15c3bf4412ea70",{"html":25,"metadata":11824},{"headings":11825,"localImagePaths":11826,"remoteImagePaths":11827,"frontmatter":11828,"imagePaths":11830},[],[],[],{"title":11817,"description":11818,"summary":11818,"pubDate":11829,"source":6423,"url":11820,"thumbnail":6742},"Tue, 02 Jan 2024 16:00:00 +0000",[],"2024-01-02-images-altered-to-trick-machine-vision-can-influence-humans-too.md","2024-01-02-lora-training-scripts-of-the-world-unite",{"id":11832,"data":11834,"filePath":11840,"digest":11841,"rendered":11842,"legacyId":11850},{"title":11835,"description":25,"summary":11836,"pubDate":11837,"source":2720,"url":11838,"thumbnail":11839},"LoRA training scripts of the world, unite!","LoRA training scripts of the world, unite! A community derived guide to some of the SOTA practices f...",["Date","2024-01-02T00:00:00.000Z"],"https://huggingface.co/blog/sdxl_lora_advanced_script","https://huggingface.co/blog/assets/dreambooth_lora_sdxl/thumbnail.png","src/content/posts/2024-01-02-lora-training-scripts-of-the-world-unite.md","173f6e68168ac45e",{"html":25,"metadata":11843},{"headings":11844,"localImagePaths":11845,"remoteImagePaths":11846,"frontmatter":11847,"imagePaths":11849},[],[],[],{"title":11835,"description":25,"summary":11836,"pubDate":11848,"source":2720,"url":11838,"thumbnail":11839},"Tue, 02 Jan 2024 00:00:00 GMT",[],"2024-01-02-lora-training-scripts-of-the-world-unite.md","2024-01-04-delivering-llm-powered-health-solutions",{"id":11851,"data":11853,"filePath":11858,"digest":11859,"rendered":11860,"legacyId":11868},{"title":11854,"description":11855,"summary":11855,"pubDate":11856,"source":19,"url":11857,"thumbnail":21},"Delivering LLM-powered health solutions","WHOOP delivers personalized fitness and health coaching with GPT-4.",["Date","2024-01-04T08:00:00.000Z"],"https://openai.com/blog/whoop","src/content/posts/2024-01-04-delivering-llm-powered-health-solutions.md","ef434fa836f1fb5b",{"html":25,"metadata":11861},{"headings":11862,"localImagePaths":11863,"remoteImagePaths":11864,"frontmatter":11865,"imagePaths":11867},[],[],[],{"title":11854,"description":11855,"summary":11855,"pubDate":11866,"source":19,"url":11857,"thumbnail":21},"Thu, 04 Jan 2024 08:00:00 GMT",[],"2024-01-04-delivering-llm-powered-health-solutions.md","2024-01-04-shaping-the-future-of-advanced-robotics",{"id":11869,"data":11871,"filePath":11877,"digest":11878,"rendered":11879,"legacyId":11887},{"title":11872,"description":11873,"summary":11873,"pubDate":11874,"source":6423,"url":11875,"thumbnail":11876},"Shaping the future of advanced robotics","Introducing AutoRT, SARA-RT, and RT-Trajectory",["Date","2024-01-04T11:39:00.000Z"],"https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/","https://lh3.googleusercontent.com/qeWlfSbr0jW0OsZ0dvaQK2V7tYM0HtTtwivx-fUJzK4GivdM6kffvNXlSgqOJyjAQWXBCycqF77zT7XDGxIqGvPiCnTqLX_C3VRmXGJIGGW5GAv7YQ=w1200-h630-n-nu","src/content/posts/2024-01-04-shaping-the-future-of-advanced-robotics.md","98c0854dcbe03fa4",{"html":25,"metadata":11880},{"headings":11881,"localImagePaths":11882,"remoteImagePaths":11883,"frontmatter":11884,"imagePaths":11886},[],[],[],{"title":11872,"description":11873,"summary":11873,"pubDate":11885,"source":6423,"url":11875,"thumbnail":11876},"Thu, 04 Jan 2024 11:39:00 +0000",[],"2024-01-04-shaping-the-future-of-advanced-robotics.md","2024-01-04-welcome-amused-efficient-text-to-image-generation",{"id":11888,"data":11890,"filePath":11896,"digest":11897,"rendered":11898,"legacyId":11906},{"title":11891,"description":25,"summary":11892,"pubDate":11893,"source":2720,"url":11894,"thumbnail":11895},"Welcome aMUSEd: Efficient Text-to-Image Generation","Welcome aMUSEd: Efficient Text-to-Image Generation We’re excited to present an efficient non-diffusi...",["Date","2024-01-04T00:00:00.000Z"],"https://huggingface.co/blog/amused","https://huggingface.co/blog/assets/amused/thumbnail.png","src/content/posts/2024-01-04-welcome-amused-efficient-text-to-image-generation.md","29df0542bccf2561",{"html":25,"metadata":11899},{"headings":11900,"localImagePaths":11901,"remoteImagePaths":11902,"frontmatter":11903,"imagePaths":11905},[],[],[],{"title":11891,"description":25,"summary":11892,"pubDate":11904,"source":2720,"url":11894,"thumbnail":11895},"Thu, 04 Jan 2024 00:00:00 GMT",[],"2024-01-04-welcome-amused-efficient-text-to-image-generation.md","2024-01-08-openai-and-journalism",{"id":11907,"data":11909,"filePath":11914,"digest":11915,"rendered":11916,"legacyId":11924},{"title":11910,"description":11911,"summary":11911,"pubDate":11912,"source":19,"url":11913,"thumbnail":21},"OpenAI and journalism","We support journalism, partner with news organizations, and believe The New York Times lawsuit is without merit.",["Date","2024-01-08T08:00:00.000Z"],"https://openai.com/blog/openai-and-journalism","src/content/posts/2024-01-08-openai-and-journalism.md","111a929eb8ea7644",{"html":25,"metadata":11917},{"headings":11918,"localImagePaths":11919,"remoteImagePaths":11920,"frontmatter":11921,"imagePaths":11923},[],[],[],{"title":11910,"description":11911,"summary":11911,"pubDate":11922,"source":19,"url":11913,"thumbnail":21},"Mon, 08 Jan 2024 08:00:00 GMT",[],"2024-01-08-openai-and-journalism.md","2024-01-10-faster-fine-tuning-using-trl-unsloth",{"id":11925,"data":11927,"filePath":11933,"digest":11934,"rendered":11935,"legacyId":11943},{"title":11928,"description":25,"summary":11929,"pubDate":11930,"source":2720,"url":11931,"thumbnail":11932},"Faster fine-tuning using TRL & Unsloth","Make LLM Fine-tuning 2x faster with Unsloth and 🤗 TRL Pulling your hair out because LLM fine-tuning ...",["Date","2024-01-10T00:00:00.000Z"],"https://huggingface.co/blog/unsloth-trl","https://huggingface.co/blog/assets/hf_unsloth/thumbnail.png","src/content/posts/2024-01-10-faster-fine-tuning-using-trl-unsloth.md","717c585632f4c33f",{"html":25,"metadata":11936},{"headings":11937,"localImagePaths":11938,"remoteImagePaths":11939,"frontmatter":11940,"imagePaths":11942},[],[],[],{"title":11928,"description":25,"summary":11929,"pubDate":11941,"source":2720,"url":11931,"thumbnail":11932},"Wed, 10 Jan 2024 00:00:00 GMT",[],"2024-01-10-faster-fine-tuning-using-trl-unsloth.md","2024-01-10-introducing-chatgpt-team",{"id":11944,"data":11946,"filePath":11951,"digest":11952,"rendered":11953,"legacyId":11961},{"title":11947,"description":11948,"summary":11948,"pubDate":11949,"source":19,"url":11950,"thumbnail":21},"Introducing ChatGPT Team","We’re launching a new ChatGPT plan for teams of all sizes, which provides a secure, collaborative workspace to get the most out of ChatGPT at work.",["Date","2024-01-10T08:00:00.000Z"],"https://openai.com/blog/introducing-chatgpt-team","src/content/posts/2024-01-10-introducing-chatgpt-team.md","20de87ff4503fd1c",{"html":25,"metadata":11954},{"headings":11955,"localImagePaths":11956,"remoteImagePaths":11957,"frontmatter":11958,"imagePaths":11960},[],[],[],{"title":11947,"description":11948,"summary":11948,"pubDate":11959,"source":19,"url":11950,"thumbnail":21},"Wed, 10 Jan 2024 08:00:00 GMT",[],"2024-01-10-introducing-chatgpt-team.md","2024-01-10-introducing-the-gpt-store",{"id":11962,"data":11964,"filePath":11969,"digest":11970,"rendered":11971,"legacyId":11978},{"title":11965,"description":11966,"summary":11966,"pubDate":11967,"source":19,"url":11968,"thumbnail":21},"Introducing the GPT Store","We’re launching the GPT Store to help you find useful and popular custom versions of ChatGPT.",["Date","2024-01-10T08:00:00.000Z"],"https://openai.com/blog/introducing-the-gpt-store","src/content/posts/2024-01-10-introducing-the-gpt-store.md","be4959decd949b4a",{"html":25,"metadata":11972},{"headings":11973,"localImagePaths":11974,"remoteImagePaths":11975,"frontmatter":11976,"imagePaths":11977},[],[],[],{"title":11965,"description":11966,"summary":11966,"pubDate":11959,"source":19,"url":11968,"thumbnail":21},[],"2024-01-10-introducing-the-gpt-store.md","2024-01-12-a-guide-to-setting-up-your-own-hugging-face-leaderboard-an-end-to-end-example-with-vectaras-hallucination-leaderboard",{"id":11979,"data":11981,"filePath":11987,"digest":11988,"rendered":11989,"legacyId":11997},{"title":11982,"description":25,"summary":11983,"pubDate":11984,"source":2720,"url":11985,"thumbnail":11986},"A guide to setting up your own Hugging Face leaderboard: an end-to-end example with Vectara's hallucination leaderboard","A guide to setting up your own Hugging Face leaderboard: an end-to-end example with Vectara's halluc...",["Date","2024-01-12T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-vectara","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail.png","src/content/posts/2024-01-12-a-guide-to-setting-up-your-own-hugging-face-leaderboard-an-end-to-end-example-with-vectaras-hallucination-leaderboard.md","280a4f7d3217b0f5",{"html":25,"metadata":11990},{"headings":11991,"localImagePaths":11992,"remoteImagePaths":11993,"frontmatter":11994,"imagePaths":11996},[],[],[],{"title":11982,"description":25,"summary":11983,"pubDate":11995,"source":2720,"url":11985,"thumbnail":11986},"Fri, 12 Jan 2024 00:00:00 GMT",[],"2024-01-12-a-guide-to-setting-up-your-own-hugging-face-leaderboard-an-end-to-end-example-with-vectaras-hallucination-leaderboard.md","2024-01-12-building-agricultural-database-for-farmers",{"id":11998,"data":12000,"filePath":12005,"digest":12006,"rendered":12007,"legacyId":12015},{"title":12001,"description":12002,"summary":12002,"pubDate":12003,"source":19,"url":12004,"thumbnail":21},"Building agricultural database for farmers","Digital Green uses OpenAI to increase farmer income.",["Date","2024-01-12T08:00:00.000Z"],"https://openai.com/blog/digital-green","src/content/posts/2024-01-12-building-agricultural-database-for-farmers.md","28b60646cb76f996",{"html":25,"metadata":12008},{"headings":12009,"localImagePaths":12010,"remoteImagePaths":12011,"frontmatter":12012,"imagePaths":12014},[],[],[],{"title":12001,"description":12002,"summary":12002,"pubDate":12013,"source":19,"url":12004,"thumbnail":21},"Fri, 12 Jan 2024 08:00:00 GMT",[],"2024-01-12-building-agricultural-database-for-farmers.md","2024-01-14-run-comfyui-workflows-for-free-on-spaces",{"id":12016,"data":12018,"filePath":12024,"digest":12025,"rendered":12026,"legacyId":12034},{"title":12019,"description":25,"summary":12020,"pubDate":12021,"source":2720,"url":12022,"thumbnail":12023},"Run ComfyUI workflows for free on Spaces","Run ComfyUI workflows for free with Gradio on Hugging Face Spaces Index: - Intro - Exporting your Co...",["Date","2024-01-14T00:00:00.000Z"],"https://huggingface.co/blog/run-comfyui-workflows-on-spaces","https://huggingface.co/blog/assets/comfyui-to-gradio/cover.png","src/content/posts/2024-01-14-run-comfyui-workflows-for-free-on-spaces.md","32b9cfe074644ee8",{"html":25,"metadata":12027},{"headings":12028,"localImagePaths":12029,"remoteImagePaths":12030,"frontmatter":12031,"imagePaths":12033},[],[],[],{"title":12019,"description":25,"summary":12020,"pubDate":12032,"source":2720,"url":12022,"thumbnail":12023},"Sun, 14 Jan 2024 00:00:00 GMT",[],"2024-01-14-run-comfyui-workflows-for-free-on-spaces.md","2024-01-15-accelerating-sd-turbo-and-sdxl-turbo-inference-with-onnx-runtime-and-olive",{"id":12035,"data":12037,"filePath":12042,"digest":12043,"rendered":12044,"legacyId":12052},{"title":12038,"description":25,"summary":12039,"pubDate":12040,"source":2720,"url":12041,"thumbnail":7429},"Accelerating SD Turbo and SDXL Turbo Inference with ONNX Runtime and Olive","Accelerating SD Turbo and SDXL Turbo Inference with ONNX Runtime and Olive Introduction SD Turbo and...",["Date","2024-01-15T00:00:00.000Z"],"https://huggingface.co/blog/sdxl_ort_inference","src/content/posts/2024-01-15-accelerating-sd-turbo-and-sdxl-turbo-inference-with-onnx-runtime-and-olive.md","d606cf353cea19cc",{"html":25,"metadata":12045},{"headings":12046,"localImagePaths":12047,"remoteImagePaths":12048,"frontmatter":12049,"imagePaths":12051},[],[],[],{"title":12038,"description":25,"summary":12039,"pubDate":12050,"source":2720,"url":12041,"thumbnail":7429},"Mon, 15 Jan 2024 00:00:00 GMT",[],"2024-01-15-accelerating-sd-turbo-and-sdxl-turbo-inference-with-onnx-runtime-and-olive.md","2024-01-15-how-openai-is-approaching-2024-worldwide-elections",{"id":12053,"data":12055,"filePath":12060,"digest":12061,"rendered":12062,"legacyId":12070},{"title":12056,"description":12057,"summary":12057,"pubDate":12058,"source":19,"url":12059,"thumbnail":21},"How OpenAI is approaching 2024 worldwide elections","We’re working to prevent abuse, provide transparency on AI-generated content, and improve access to accurate voting information.",["Date","2024-01-15T08:00:00.000Z"],"https://openai.com/blog/how-openai-is-approaching-2024-worldwide-elections","src/content/posts/2024-01-15-how-openai-is-approaching-2024-worldwide-elections.md","e483d6aa6d842f77",{"html":25,"metadata":12063},{"headings":12064,"localImagePaths":12065,"remoteImagePaths":12066,"frontmatter":12067,"imagePaths":12069},[],[],[],{"title":12056,"description":12057,"summary":12057,"pubDate":12068,"source":19,"url":12059,"thumbnail":21},"Mon, 15 Jan 2024 08:00:00 GMT",[],"2024-01-15-how-openai-is-approaching-2024-worldwide-elections.md","2024-01-16-democratic-inputs-to-ai-grant-program-lessons-learned-and-implementation-plans",{"id":12071,"data":12073,"filePath":12078,"digest":12079,"rendered":12080,"legacyId":12088},{"title":12074,"description":12075,"summary":12075,"pubDate":12076,"source":19,"url":12077,"thumbnail":21},"Democratic inputs to AI grant program: lessons learned and implementation plans","We funded 10 teams from around the world to design ideas and tools to collectively govern AI. We summarize the innovations, outline our learnings, and call for researchers and engineers to join us as we continue this work.",["Date","2024-01-16T08:00:00.000Z"],"https://openai.com/blog/democratic-inputs-to-ai-grant-program-update","src/content/posts/2024-01-16-democratic-inputs-to-ai-grant-program-lessons-learned-and-implementation-plans.md","314774bcec234827",{"html":25,"metadata":12081},{"headings":12082,"localImagePaths":12083,"remoteImagePaths":12084,"frontmatter":12085,"imagePaths":12087},[],[],[],{"title":12074,"description":12075,"summary":12075,"pubDate":12086,"source":19,"url":12077,"thumbnail":21},"Tue, 16 Jan 2024 08:00:00 GMT",[],"2024-01-16-democratic-inputs-to-ai-grant-program-lessons-learned-and-implementation-plans.md","2024-01-17-alphageometry-an-olympiad-level-ai-system-for-geometry",{"id":12089,"data":12091,"filePath":12097,"digest":12098,"rendered":12099,"legacyId":12107},{"title":12092,"description":12093,"summary":12093,"pubDate":12094,"source":6423,"url":12095,"thumbnail":12096},"AlphaGeometry: An Olympiad-level AI system for geometry","Advancing AI reasoning in mathematics",["Date","2024-01-17T16:00:00.000Z"],"https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/","https://lh3.googleusercontent.com/tVTh_ZCW5Qozy4vOCpMH06B7Ac_eF7fmEULMMTwDellOh6hnOMUtf28toD68N527IHQTlBWfBCHcZykYPMdrS48yvuEcJKMJG8rU3YRM3u5Ojn3JXnc=w1200-h630-n-nu","src/content/posts/2024-01-17-alphageometry-an-olympiad-level-ai-system-for-geometry.md","8c1dba4fcbb0cf7c",{"html":25,"metadata":12100},{"headings":12101,"localImagePaths":12102,"remoteImagePaths":12103,"frontmatter":12104,"imagePaths":12106},[],[],[],{"title":12092,"description":12093,"summary":12093,"pubDate":12105,"source":6423,"url":12095,"thumbnail":12096},"Wed, 17 Jan 2024 16:00:00 +0000",[],"2024-01-17-alphageometry-an-olympiad-level-ai-system-for-geometry.md","2024-01-18-preference-tuning-llms-with-direct-preference-optimization-methods",{"id":12108,"data":12110,"filePath":12116,"digest":12117,"rendered":12118,"legacyId":12126},{"title":12111,"description":25,"summary":12112,"pubDate":12113,"source":2720,"url":12114,"thumbnail":12115},"Preference Tuning LLMs with Direct Preference Optimization Methods","Preference Tuning LLMs with Direct Preference Optimization Methods Addendum After consulting with th...",["Date","2024-01-18T00:00:00.000Z"],"https://huggingface.co/blog/pref-tuning","https://huggingface.co/blog/assets/pref-tuning/thumbnail.jpg","src/content/posts/2024-01-18-preference-tuning-llms-with-direct-preference-optimization-methods.md","5fc515799711b39e",{"html":25,"metadata":12119},{"headings":12120,"localImagePaths":12121,"remoteImagePaths":12122,"frontmatter":12123,"imagePaths":12125},[],[],[],{"title":12111,"description":25,"summary":12112,"pubDate":12124,"source":2720,"url":12114,"thumbnail":12115},"Thu, 18 Jan 2024 00:00:00 GMT",[],"2024-01-18-preference-tuning-llms-with-direct-preference-optimization-methods.md","2024-01-19-fine-tune-w2v2-bert-for-low-resource-asr-with-transformers",{"id":12127,"data":12129,"filePath":12135,"digest":12136,"rendered":12137,"legacyId":12145},{"title":12130,"description":25,"summary":12131,"pubDate":12132,"source":2720,"url":12133,"thumbnail":12134},"Fine-Tune W2V2-Bert for low-resource ASR with 🤗 Transformers","Fine-Tune W2V2-Bert for low-resource ASR with 🤗 Transformers New (01/2024): This blog post is strong...",["Date","2024-01-19T00:00:00.000Z"],"https://huggingface.co/blog/fine-tune-w2v2-bert","https://huggingface.co/blog/assets/fine-tune-w2v2-bert/w2v_thumbnail.png","src/content/posts/2024-01-19-fine-tune-w2v2-bert-for-low-resource-asr-with-transformers.md","96558e2ca9af73c4",{"html":25,"metadata":12138},{"headings":12139,"localImagePaths":12140,"remoteImagePaths":12141,"frontmatter":12142,"imagePaths":12144},[],[],[],{"title":12130,"description":25,"summary":12131,"pubDate":12143,"source":2720,"url":12133,"thumbnail":12134},"Fri, 19 Jan 2024 00:00:00 GMT",[],"2024-01-19-fine-tune-w2v2-bert-for-low-resource-asr-with-transformers.md","2024-01-19-patchtsmixer-in-huggingface",{"id":12146,"data":12148,"filePath":12154,"digest":12155,"rendered":12156,"legacyId":12163},{"title":12149,"description":25,"summary":12150,"pubDate":12151,"source":2720,"url":12152,"thumbnail":12153},"PatchTSMixer in HuggingFace","PatchTSMixer in HuggingFace - Getting Started PatchTSMixer is a lightweight time-series modeling app...",["Date","2024-01-19T00:00:00.000Z"],"https://huggingface.co/blog/patchtsmixer","https://huggingface.co/blog/assets/patchtsmixer/thumbnail.jpeg","src/content/posts/2024-01-19-patchtsmixer-in-huggingface.md","cb2de905076adf14",{"html":25,"metadata":12157},{"headings":12158,"localImagePaths":12159,"remoteImagePaths":12160,"frontmatter":12161,"imagePaths":12162},[],[],[],{"title":12149,"description":25,"summary":12150,"pubDate":12143,"source":2720,"url":12152,"thumbnail":12153},[],"2024-01-19-patchtsmixer-in-huggingface.md","2024-01-24-open-source-llms-as-langchain-agents",{"id":12164,"data":12166,"filePath":12172,"digest":12173,"rendered":12174,"legacyId":12182},{"title":12167,"description":25,"summary":12168,"pubDate":12169,"source":2720,"url":12170,"thumbnail":12171},"Open-source LLMs as LangChain Agents","Open-source LLMs as LangChain Agents TL;DR Open-source LLMs have now reached a performance level tha...",["Date","2024-01-24T00:00:00.000Z"],"https://huggingface.co/blog/open-source-llms-as-agents","https://huggingface.co/blog/assets/open-source-llms-as-agents/thumbnail_open_source_agents.png","src/content/posts/2024-01-24-open-source-llms-as-langchain-agents.md","ccd5055c87810255",{"html":25,"metadata":12175},{"headings":12176,"localImagePaths":12177,"remoteImagePaths":12178,"frontmatter":12179,"imagePaths":12181},[],[],[],{"title":12167,"description":25,"summary":12168,"pubDate":12180,"source":2720,"url":12170,"thumbnail":12171},"Wed, 24 Jan 2024 00:00:00 GMT",[],"2024-01-24-open-source-llms-as-langchain-agents.md","2024-01-25-hugging-face-and-google-partner-for-open-ai-collaboration",{"id":12183,"data":12185,"filePath":12191,"digest":12192,"rendered":12193,"legacyId":12201},{"title":12186,"description":25,"summary":12187,"pubDate":12188,"source":2720,"url":12189,"thumbnail":12190},"Hugging Face and Google partner for open AI collaboration","Hugging Face and Google partner for open AI collaboration At Hugging Face, we want to enable all com...",["Date","2024-01-25T00:00:00.000Z"],"https://huggingface.co/blog/gcp-partnership","https://huggingface.co/blog/assets/173_gcp-partnership/thumbnail.jpg","src/content/posts/2024-01-25-hugging-face-and-google-partner-for-open-ai-collaboration.md","c41bd23d2d2e1314",{"html":25,"metadata":12194},{"headings":12195,"localImagePaths":12196,"remoteImagePaths":12197,"frontmatter":12198,"imagePaths":12200},[],[],[],{"title":12186,"description":25,"summary":12187,"pubDate":12199,"source":2720,"url":12189,"thumbnail":12190},"Thu, 25 Jan 2024 00:00:00 GMT",[],"2024-01-25-hugging-face-and-google-partner-for-open-ai-collaboration.md","2024-01-25-new-embedding-models-and-api-updates",{"id":12202,"data":12204,"filePath":12209,"digest":12210,"rendered":12211,"legacyId":12219},{"title":12205,"description":12206,"summary":12206,"pubDate":12207,"source":19,"url":12208,"thumbnail":21},"New embedding models and API updates","We are launching a new generation of embedding models, new GPT-4 Turbo and moderation models, new API usage management tools, and soon, lower pricing on GPT-3.5 Turbo.",["Date","2024-01-25T08:00:00.000Z"],"https://openai.com/blog/new-embedding-models-and-api-updates","src/content/posts/2024-01-25-new-embedding-models-and-api-updates.md","3718de0d1c56d8c7",{"html":25,"metadata":12212},{"headings":12213,"localImagePaths":12214,"remoteImagePaths":12215,"frontmatter":12216,"imagePaths":12218},[],[],[],{"title":12205,"description":12206,"summary":12206,"pubDate":12217,"source":19,"url":12208,"thumbnail":21},"Thu, 25 Jan 2024 08:00:00 GMT",[],"2024-01-25-new-embedding-models-and-api-updates.md","2024-01-26-an-introduction-to-ai-secure-llm-safety-leaderboard",{"id":12220,"data":12222,"filePath":12228,"digest":12229,"rendered":12230,"legacyId":12238},{"title":12223,"description":25,"summary":12224,"pubDate":12225,"source":2720,"url":12226,"thumbnail":12227},"An Introduction to AI Secure LLM Safety Leaderboard","An Introduction to AI Secure LLM Safety Leaderboard Given the widespread adoption of LLMs, it is cri...",["Date","2024-01-26T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-decodingtrust","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_decodingtrust.png","src/content/posts/2024-01-26-an-introduction-to-ai-secure-llm-safety-leaderboard.md","51e70c499a66dad4",{"html":25,"metadata":12231},{"headings":12232,"localImagePaths":12233,"remoteImagePaths":12234,"frontmatter":12235,"imagePaths":12237},[],[],[],{"title":12223,"description":25,"summary":12224,"pubDate":12236,"source":2720,"url":12226,"thumbnail":12227},"Fri, 26 Jan 2024 00:00:00 GMT",[],"2024-01-26-an-introduction-to-ai-secure-llm-safety-leaderboard.md","2024-01-29-the-hallucinations-leaderboard-an-open-effort-to-measure-hallucinations-in-large-language-models",{"id":12239,"data":12241,"filePath":12246,"digest":12247,"rendered":12248,"legacyId":12256},{"title":12242,"description":25,"summary":12243,"pubDate":12244,"source":2720,"url":12245,"thumbnail":11986},"The Hallucinations Leaderboard, an Open Effort to Measure Hallucinations in Large Language Models","The Hallucinations Leaderboard, an Open Effort to Measure Hallucinations in Large Language Models In...",["Date","2024-01-29T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-hallucinations","src/content/posts/2024-01-29-the-hallucinations-leaderboard-an-open-effort-to-measure-hallucinations-in-large-language-models.md","35e07735c8b6093d",{"html":25,"metadata":12249},{"headings":12250,"localImagePaths":12251,"remoteImagePaths":12252,"frontmatter":12253,"imagePaths":12255},[],[],[],{"title":12242,"description":25,"summary":12243,"pubDate":12254,"source":2720,"url":12245,"thumbnail":11986},"Mon, 29 Jan 2024 00:00:00 GMT",[],"2024-01-29-the-hallucinations-leaderboard-an-open-effort-to-measure-hallucinations-in-large-language-models.md","2024-01-30-accelerate-starcoder-with-optimum-intel-on-xeon-q8q4-and-speculative-decoding",{"id":12257,"data":12259,"filePath":12265,"digest":12266,"rendered":12267,"legacyId":12275},{"title":12260,"description":25,"summary":12261,"pubDate":12262,"source":2720,"url":12263,"thumbnail":12264},"Accelerate StarCoder with 🤗 Optimum Intel on Xeon: Q8/Q4 and Speculative Decoding","Accelerate StarCoder with 🤗 Optimum Intel on Xeon: Q8/Q4 and Speculative Decoding Introduction Recen...",["Date","2024-01-30T00:00:00.000Z"],"https://huggingface.co/blog/intel-starcoder-quantization","https://huggingface.co/blog/assets/optimum_intel/intel_thumbnail.png","src/content/posts/2024-01-30-accelerate-starcoder-with-optimum-intel-on-xeon-q8q4-and-speculative-decoding.md","686bcd8f5031c2c3",{"html":25,"metadata":12268},{"headings":12269,"localImagePaths":12270,"remoteImagePaths":12271,"frontmatter":12272,"imagePaths":12274},[],[],[],{"title":12260,"description":25,"summary":12261,"pubDate":12273,"source":2720,"url":12263,"thumbnail":12264},"Tue, 30 Jan 2024 00:00:00 GMT",[],"2024-01-30-accelerate-starcoder-with-optimum-intel-on-xeon-q8q4-and-speculative-decoding.md","2024-01-31-building-an-early-warning-system-for-llm-aided-biological-threat-creation",{"id":12276,"data":12278,"filePath":12283,"digest":12284,"rendered":12285,"legacyId":12293},{"title":12279,"description":12280,"summary":12280,"pubDate":12281,"source":19,"url":12282,"thumbnail":21},"Building an early warning system for LLM-aided biological threat creation","We’re developing a blueprint for evaluating the risk that a large language model (LLM) could aid someone in creating a biological threat. In an evaluation involving both biology experts and students, we found that GPT-4 provides at most a mild uplift in biological threat creation accuracy. While this uplift is not large enough to be conclusive, our finding is a starting point for continued research and community deliberation.",["Date","2024-01-31T08:00:00.000Z"],"https://openai.com/blog/building-an-early-warning-system-for-llm-aided-biological-threat-creation","src/content/posts/2024-01-31-building-an-early-warning-system-for-llm-aided-biological-threat-creation.md","f2bd5a8d3e7e02de",{"html":25,"metadata":12286},{"headings":12287,"localImagePaths":12288,"remoteImagePaths":12289,"frontmatter":12290,"imagePaths":12292},[],[],[],{"title":12279,"description":12280,"summary":12280,"pubDate":12291,"source":19,"url":12282,"thumbnail":21},"Wed, 31 Jan 2024 08:00:00 GMT",[],"2024-01-31-building-an-early-warning-system-for-llm-aided-biological-threat-creation.md","2024-01-31-introducing-the-enterprise-scenarios-leaderboard-a-leaderboard-for-real-world-use-cases",{"id":12294,"data":12296,"filePath":12302,"digest":12303,"rendered":12304,"legacyId":12312},{"title":12297,"description":25,"summary":12298,"pubDate":12299,"source":2720,"url":12300,"thumbnail":12301},"Introducing the Enterprise Scenarios Leaderboard: a Leaderboard for Real World Use Cases","Introducing the Enterprise Scenarios Leaderboard: a Leaderboard for Real World Use Cases Today, the ...",["Date","2024-01-31T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-patronus","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_patronus.png","src/content/posts/2024-01-31-introducing-the-enterprise-scenarios-leaderboard-a-leaderboard-for-real-world-use-cases.md","2349a5b62f887d91",{"html":25,"metadata":12305},{"headings":12306,"localImagePaths":12307,"remoteImagePaths":12308,"frontmatter":12309,"imagePaths":12311},[],[],[],{"title":12297,"description":25,"summary":12298,"pubDate":12310,"source":2720,"url":12300,"thumbnail":12301},"Wed, 31 Jan 2024 00:00:00 GMT",[],"2024-01-31-introducing-the-enterprise-scenarios-leaderboard-a-leaderboard-for-real-world-use-cases.md","2024-02-01-constitutional-ai-with-open-llms",{"id":12313,"data":12315,"filePath":12321,"digest":12322,"rendered":12323,"legacyId":12331},{"title":12316,"description":25,"summary":12317,"pubDate":12318,"source":2720,"url":12319,"thumbnail":12320},"Constitutional AI with Open LLMs","Constitutional AI with Open LLMs Since the launch of ChatGPT in 2022, we have seen tremendous progre...",["Date","2024-02-01T00:00:00.000Z"],"https://huggingface.co/blog/constitutional_ai","https://huggingface.co/blog/assets/175_constitutional_ai/thumbnail.png","src/content/posts/2024-02-01-constitutional-ai-with-open-llms.md","1cc5e973614ec143",{"html":25,"metadata":12324},{"headings":12325,"localImagePaths":12326,"remoteImagePaths":12327,"frontmatter":12328,"imagePaths":12330},[],[],[],{"title":12316,"description":25,"summary":12317,"pubDate":12329,"source":2720,"url":12319,"thumbnail":12320},"Thu, 01 Feb 2024 00:00:00 GMT",[],"2024-02-01-constitutional-ai-with-open-llms.md","2024-02-01-hugging-face-text-generation-inference-available-for-aws-inferentia2",{"id":12332,"data":12334,"filePath":12340,"digest":12341,"rendered":12342,"legacyId":12349},{"title":12335,"description":25,"summary":12336,"pubDate":12337,"source":2720,"url":12338,"thumbnail":12339},"Hugging Face Text Generation Inference available for AWS Inferentia2","Hugging Face Text Generation Inference available for AWS Inferentia2 We are excited to announce the ...",["Date","2024-02-01T00:00:00.000Z"],"https://huggingface.co/blog/text-generation-inference-on-inferentia2","https://huggingface.co/blog/assets/175_text_generation_inference_on_inferentia2/thumbnail.jpg","src/content/posts/2024-02-01-hugging-face-text-generation-inference-available-for-aws-inferentia2.md","2b8641e60bdb1cdc",{"html":25,"metadata":12343},{"headings":12344,"localImagePaths":12345,"remoteImagePaths":12346,"frontmatter":12347,"imagePaths":12348},[],[],[],{"title":12335,"description":25,"summary":12336,"pubDate":12329,"source":2720,"url":12338,"thumbnail":12339},[],"2024-02-01-hugging-face-text-generation-inference-available-for-aws-inferentia2.md","2024-02-01-patch-time-series-transformer-in-hugging-face",{"id":12350,"data":12352,"filePath":12358,"digest":12359,"rendered":12360,"legacyId":12367},{"title":12353,"description":25,"summary":12354,"pubDate":12355,"source":2720,"url":12356,"thumbnail":12357},"Patch Time Series Transformer in Hugging Face","Patch Time Series Transformer in Hugging Face - Getting Started In this blog, we provide examples of...",["Date","2024-02-01T00:00:00.000Z"],"https://huggingface.co/blog/patchtst","https://huggingface.co/blog/assets/patchtst/thumbnail.png","src/content/posts/2024-02-01-patch-time-series-transformer-in-hugging-face.md","b5fadfac60e454fa",{"html":25,"metadata":12361},{"headings":12362,"localImagePaths":12363,"remoteImagePaths":12364,"frontmatter":12365,"imagePaths":12366},[],[],[],{"title":12353,"description":25,"summary":12354,"pubDate":12329,"source":2720,"url":12356,"thumbnail":12357},[],"2024-02-01-patch-time-series-transformer-in-hugging-face.md","2024-02-02-nphardeval-leaderboard-unveiling-the-reasoning-abilities-of-large-language-models-through-complexity-classes-and-dynamic-updates",{"id":12368,"data":12370,"filePath":12376,"digest":12377,"rendered":12378,"legacyId":12386},{"title":12371,"description":25,"summary":12372,"pubDate":12373,"source":2720,"url":12374,"thumbnail":12375},"NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates","NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexit...",["Date","2024-02-02T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-nphardeval","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_nphardeval.png","src/content/posts/2024-02-02-nphardeval-leaderboard-unveiling-the-reasoning-abilities-of-large-language-models-through-complexity-classes-and-dynamic-updates.md","13d0e8fc2114b78d",{"html":25,"metadata":12379},{"headings":12380,"localImagePaths":12381,"remoteImagePaths":12382,"frontmatter":12383,"imagePaths":12385},[],[],[],{"title":12371,"description":25,"summary":12372,"pubDate":12384,"source":2720,"url":12374,"thumbnail":12375},"Fri, 02 Feb 2024 00:00:00 GMT",[],"2024-02-02-nphardeval-leaderboard-unveiling-the-reasoning-abilities-of-large-language-models-through-complexity-classes-and-dynamic-updates.md","2024-02-02-response-to-nist-executive-order-on-ai",{"id":12387,"data":12389,"filePath":12394,"digest":12395,"rendered":12396,"legacyId":12403},{"title":12390,"description":12391,"summary":12391,"pubDate":12392,"source":19,"url":12393,"thumbnail":21},"Response to NIST Executive Order on AI","The National Institute of Standards and Technology (NIST) request for information related to its assignments under sections 4.1, 4.5, and 11 of the Executive Order Concerning Artificial Intelligence",["Date","2024-02-02T00:00:00.000Z"],"https://openai.com/global-affairs/response-to-nist-executive-order-on-ai","src/content/posts/2024-02-02-response-to-nist-executive-order-on-ai.md","625bd9b844836db7",{"html":25,"metadata":12397},{"headings":12398,"localImagePaths":12399,"remoteImagePaths":12400,"frontmatter":12401,"imagePaths":12402},[],[],[],{"title":12390,"description":12391,"summary":12391,"pubDate":12384,"source":19,"url":12393,"thumbnail":21},[],"2024-02-02-response-to-nist-executive-order-on-ai.md","2024-02-03-segmoe-segmind-mixture-of-diffusion-experts",{"id":12404,"data":12406,"filePath":12412,"digest":12413,"rendered":12414,"legacyId":12422},{"title":12407,"description":25,"summary":12408,"pubDate":12409,"source":2720,"url":12410,"thumbnail":12411},"SegMoE: Segmind Mixture of Diffusion Experts","SegMoE: Segmind Mixture of Diffusion Experts SegMoE is an exciting framework for creating Mixture-of...",["Date","2024-02-03T00:00:00.000Z"],"https://huggingface.co/blog/segmoe","https://huggingface.co/blog/assets/segmoe/thumbnail.png","src/content/posts/2024-02-03-segmoe-segmind-mixture-of-diffusion-experts.md","192434893558f097",{"html":25,"metadata":12415},{"headings":12416,"localImagePaths":12417,"remoteImagePaths":12418,"frontmatter":12419,"imagePaths":12421},[],[],[],{"title":12407,"description":25,"summary":12408,"pubDate":12420,"source":2720,"url":12410,"thumbnail":12411},"Sat, 03 Feb 2024 00:00:00 GMT",[],"2024-02-03-segmoe-segmind-mixture-of-diffusion-experts.md","2024-02-08-from-openai-to-open-llms-with-messages-api",{"id":12423,"data":12425,"filePath":12431,"digest":12432,"rendered":12433,"legacyId":12441},{"title":12426,"description":25,"summary":12427,"pubDate":12428,"source":2720,"url":12429,"thumbnail":12430},"From OpenAI to Open LLMs with Messages API","From OpenAI to Open LLMs with Messages API on Hugging Face We are excited to introduce the Messages ...",["Date","2024-02-08T00:00:00.000Z"],"https://huggingface.co/blog/tgi-messages-api","https://huggingface.co/blog/assets/tgi-messages-api/thumbnail.jpg","src/content/posts/2024-02-08-from-openai-to-open-llms-with-messages-api.md","313450f664489f04",{"html":25,"metadata":12434},{"headings":12435,"localImagePaths":12436,"remoteImagePaths":12437,"frontmatter":12438,"imagePaths":12440},[],[],[],{"title":12426,"description":25,"summary":12427,"pubDate":12439,"source":2720,"url":12429,"thumbnail":12430},"Thu, 08 Feb 2024 00:00:00 GMT",[],"2024-02-08-from-openai-to-open-llms-with-messages-api.md","2024-02-08-the-next-chapter-of-our-gemini-era",{"id":12442,"data":12444,"filePath":12450,"digest":12451,"rendered":12452,"legacyId":12460},{"title":12445,"description":12446,"summary":12446,"pubDate":12447,"source":6423,"url":12448,"thumbnail":12449},"The next chapter of our Gemini era","We're bringing Gemini to more Google products",["Date","2024-02-08T13:00:00.000Z"],"https://deepmind.google/discover/blog/google-gemini-update-sundar-pichai-2024/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Keyword_Social_-_1920x1080.width-1300.png","src/content/posts/2024-02-08-the-next-chapter-of-our-gemini-era.md","fbc1ad330b265fc0",{"html":25,"metadata":12453},{"headings":12454,"localImagePaths":12455,"remoteImagePaths":12456,"frontmatter":12457,"imagePaths":12459},[],[],[],{"title":12445,"description":12446,"summary":12446,"pubDate":12458,"source":6423,"url":12448,"thumbnail":12449},"Thu, 08 Feb 2024 13:00:00 +0000",[],"2024-02-08-the-next-chapter-of-our-gemini-era.md","2024-02-13-memory-and-new-controls-for-chatgpt",{"id":12461,"data":12463,"filePath":12468,"digest":12469,"rendered":12470,"legacyId":12478},{"title":12464,"description":12465,"summary":12465,"pubDate":12466,"source":19,"url":12467,"thumbnail":21},"Memory and new controls for ChatGPT","We’re testing the ability for ChatGPT to remember things you discuss to make future chats more helpful. You’re in control of ChatGPT’s memory.",["Date","2024-02-13T00:00:00.000Z"],"https://openai.com/blog/memory-and-new-controls-for-chatgpt","src/content/posts/2024-02-13-memory-and-new-controls-for-chatgpt.md","4bd97ea26639f80b",{"html":25,"metadata":12471},{"headings":12472,"localImagePaths":12473,"remoteImagePaths":12474,"frontmatter":12475,"imagePaths":12477},[],[],[],{"title":12464,"description":12465,"summary":12465,"pubDate":12476,"source":19,"url":12467,"thumbnail":21},"Tue, 13 Feb 2024 00:00:00 GMT",[],"2024-02-13-memory-and-new-controls-for-chatgpt.md","2024-02-14-amd-pervasive-ai-developer-contest",{"id":12479,"data":12481,"filePath":12487,"digest":12488,"rendered":12489,"legacyId":12497},{"title":12482,"description":25,"summary":12483,"pubDate":12484,"source":2720,"url":12485,"thumbnail":12486},"AMD Pervasive AI Developer Contest!","AMD Pervasive AI Developer Contest AMD and Hugging Face are actively engaged in helping developers s...",["Date","2024-02-14T00:00:00.000Z"],"https://huggingface.co/blog/amd_pervasive_developer_ai_contest","https://huggingface.co/blog/amd_pervasive_developer_ai_contest/assets/amd_pervasive_developer_ai_contest/amd_developer_general_abstract.jpg","src/content/posts/2024-02-14-amd-pervasive-ai-developer-contest.md","03b56af147893d5e",{"html":25,"metadata":12490},{"headings":12491,"localImagePaths":12492,"remoteImagePaths":12493,"frontmatter":12494,"imagePaths":12496},[],[],[],{"title":12482,"description":25,"summary":12483,"pubDate":12495,"source":2720,"url":12485,"thumbnail":12486},"Wed, 14 Feb 2024 00:00:00 GMT",[],"2024-02-14-amd-pervasive-ai-developer-contest.md","2024-02-14-disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors",{"id":12498,"data":12500,"filePath":12505,"digest":12506,"rendered":12507,"legacyId":12515},{"title":12501,"description":12502,"summary":12502,"pubDate":12503,"source":19,"url":12504,"thumbnail":21},"Disrupting malicious uses of AI by state-affiliated threat actors","We terminated accounts associated with state-affiliated threat actors. Our findings show our models offer only limited, incremental capabilities for malicious cybersecurity tasks.",["Date","2024-02-14T08:00:00.000Z"],"https://openai.com/blog/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors","src/content/posts/2024-02-14-disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors.md","a30c9c18d492c1f4",{"html":25,"metadata":12508},{"headings":12509,"localImagePaths":12510,"remoteImagePaths":12511,"frontmatter":12512,"imagePaths":12514},[],[],[],{"title":12501,"description":12502,"summary":12502,"pubDate":12513,"source":19,"url":12504,"thumbnail":21},"Wed, 14 Feb 2024 08:00:00 GMT",[],"2024-02-14-disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors.md","2024-02-15-our-next-generation-model-gemini-15",{"id":12516,"data":12518,"filePath":12524,"digest":12525,"rendered":12526,"legacyId":12534},{"title":12519,"description":12520,"summary":12520,"pubDate":12521,"source":6423,"url":12522,"thumbnail":12523},"Our next-generation model: Gemini 1.5","The model delivers dramatically enhanced performance, with a breakthrough in long-context understanding across modalities.",["Date","2024-02-15T15:00:00.000Z"],"https://deepmind.google/discover/blog/our-next-generation-model-gemini-15/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_gemini_1.5_blog_social_share_800x418.width-1300.png","src/content/posts/2024-02-15-our-next-generation-model-gemini-15.md","c5b5c5dd3c97558e",{"html":25,"metadata":12527},{"headings":12528,"localImagePaths":12529,"remoteImagePaths":12530,"frontmatter":12531,"imagePaths":12533},[],[],[],{"title":12519,"description":12520,"summary":12520,"pubDate":12532,"source":6423,"url":12522,"thumbnail":12523},"Thu, 15 Feb 2024 15:00:00 +0000",[],"2024-02-15-our-next-generation-model-gemini-15.md","2024-02-15-video-generation-models-as-world-simulators",{"id":12535,"data":12537,"filePath":12542,"digest":12543,"rendered":12544,"legacyId":12552},{"title":12538,"description":12539,"summary":12539,"pubDate":12540,"source":19,"url":12541,"thumbnail":21},"Video generation models as world simulators","We explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.",["Date","2024-02-15T08:00:00.000Z"],"https://openai.com/blog/video-generation-models-as-world-simulators","src/content/posts/2024-02-15-video-generation-models-as-world-simulators.md","250c25efa6363e0c",{"html":25,"metadata":12545},{"headings":12546,"localImagePaths":12547,"remoteImagePaths":12548,"frontmatter":12549,"imagePaths":12551},[],[],[],{"title":12538,"description":12539,"summary":12539,"pubDate":12550,"source":19,"url":12541,"thumbnail":21},"Thu, 15 Feb 2024 08:00:00 GMT",[],"2024-02-15-video-generation-models-as-world-simulators.md","2024-02-16-synthetic-data-save-money-time-and-carbon-with-open-source",{"id":12553,"data":12555,"filePath":12561,"digest":12562,"rendered":12563,"legacyId":12571},{"title":12556,"description":25,"summary":12557,"pubDate":12558,"source":2720,"url":12559,"thumbnail":12560},"Synthetic data: save money, time and carbon with open source","Synthetic data: save money, time and carbon with open source tl;dr Should you fine-tune your own mod...",["Date","2024-02-16T00:00:00.000Z"],"https://huggingface.co/blog/synthetic-data-save-costs","https://huggingface.co/blog/assets/176_synthetic-data-save-costs/thumbnail.png","src/content/posts/2024-02-16-synthetic-data-save-money-time-and-carbon-with-open-source.md","a08dc93fe5b54293",{"html":25,"metadata":12564},{"headings":12565,"localImagePaths":12566,"remoteImagePaths":12567,"frontmatter":12568,"imagePaths":12570},[],[],[],{"title":12556,"description":25,"summary":12557,"pubDate":12569,"source":2720,"url":12559,"thumbnail":12560},"Fri, 16 Feb 2024 00:00:00 GMT",[],"2024-02-16-synthetic-data-save-money-time-and-carbon-with-open-source.md","2024-02-19-peft-welcomes-new-merging-methods",{"id":12572,"data":12574,"filePath":12580,"digest":12581,"rendered":12582,"legacyId":12590},{"title":12575,"description":25,"summary":12576,"pubDate":12577,"source":2720,"url":12578,"thumbnail":12579},"🤗 PEFT welcomes new merging methods","🤗 PEFT welcomes new merging methods Model merging has quickly become the de-facto standard of pushin...",["Date","2024-02-19T00:00:00.000Z"],"https://huggingface.co/blog/peft_merging","https://huggingface.co/blog/assets/peft_merging/thumbnail.png","src/content/posts/2024-02-19-peft-welcomes-new-merging-methods.md","e1c8c33f4c41a654",{"html":25,"metadata":12583},{"headings":12584,"localImagePaths":12585,"remoteImagePaths":12586,"frontmatter":12587,"imagePaths":12589},[],[],[],{"title":12575,"description":25,"summary":12576,"pubDate":12588,"source":2720,"url":12578,"thumbnail":12579},"Mon, 19 Feb 2024 00:00:00 GMT",[],"2024-02-19-peft-welcomes-new-merging-methods.md","2024-02-20-introducing-the-open-ko-llm-leaderboard-leading-the-korean-llm-evaluation-ecosystem",{"id":12591,"data":12593,"filePath":12599,"digest":12600,"rendered":12601,"legacyId":12609},{"title":12594,"description":25,"summary":12595,"pubDate":12596,"source":2720,"url":12597,"thumbnail":12598},"Introducing the Open Ko-LLM Leaderboard: Leading the Korean LLM Evaluation Ecosystem","Introducing the Open Ko-LLM Leaderboard: Leading the Korean LLM Evaluation Ecosystem In the fast-evo...",["Date","2024-02-20T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-upstage","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_upstage.png","src/content/posts/2024-02-20-introducing-the-open-ko-llm-leaderboard-leading-the-korean-llm-evaluation-ecosystem.md","0cad40f98e46e6dd",{"html":25,"metadata":12602},{"headings":12603,"localImagePaths":12604,"remoteImagePaths":12605,"frontmatter":12606,"imagePaths":12608},[],[],[],{"title":12594,"description":25,"summary":12595,"pubDate":12607,"source":2720,"url":12597,"thumbnail":12598},"Tue, 20 Feb 2024 00:00:00 GMT",[],"2024-02-20-introducing-the-open-ko-llm-leaderboard-leading-the-korean-llm-evaluation-ecosystem.md","2024-02-21-gemma-introducing-new-state-of-the-art-open-models",{"id":12610,"data":12612,"filePath":12618,"digest":12619,"rendered":12620,"legacyId":12628},{"title":12613,"description":12614,"summary":12614,"pubDate":12615,"source":6423,"url":12616,"thumbnail":12617},"Gemma: Introducing new state-of-the-art open models","Gemma is built for responsible AI development from the same research and technology used to create Gemini models.",["Date","2024-02-21T13:06:00.000Z"],"https://deepmind.google/discover/blog/gemma-introducing-new-state-of-the-art-open-models/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemma-social-share.width-1300.jpg","src/content/posts/2024-02-21-gemma-introducing-new-state-of-the-art-open-models.md","3cd36e1e54eaa81f",{"html":25,"metadata":12621},{"headings":12622,"localImagePaths":12623,"remoteImagePaths":12624,"frontmatter":12625,"imagePaths":12627},[],[],[],{"title":12613,"description":12614,"summary":12614,"pubDate":12626,"source":6423,"url":12616,"thumbnail":12617},"Wed, 21 Feb 2024 13:06:00 +0000",[],"2024-02-21-gemma-introducing-new-state-of-the-art-open-models.md","2024-02-21-welcome-gemma---googles-new-open-llm",{"id":12629,"data":12631,"filePath":12637,"digest":12638,"rendered":12639,"legacyId":12647},{"title":12632,"description":25,"summary":12633,"pubDate":12634,"source":2720,"url":12635,"thumbnail":12636},"Welcome Gemma - Google's new open LLM","Welcome Gemma - Google’s new open LLM An update to the Gemma models was released two months after th...",["Date","2024-02-21T00:00:00.000Z"],"https://huggingface.co/blog/gemma","https://huggingface.co/blog/assets/gemma/thumbnail.jpg","src/content/posts/2024-02-21-welcome-gemma---googles-new-open-llm.md","1962972c6075125a",{"html":25,"metadata":12640},{"headings":12641,"localImagePaths":12642,"remoteImagePaths":12643,"frontmatter":12644,"imagePaths":12646},[],[],[],{"title":12632,"description":25,"summary":12633,"pubDate":12645,"source":2720,"url":12635,"thumbnail":12636},"Wed, 21 Feb 2024 00:00:00 GMT",[],"2024-02-21-welcome-gemma---googles-new-open-llm.md","2024-02-23-fine-tuning-gemma-models-in-hugging-face",{"id":12648,"data":12650,"filePath":12656,"digest":12657,"rendered":12658,"legacyId":12666},{"title":12651,"description":25,"summary":12652,"pubDate":12653,"source":2720,"url":12654,"thumbnail":12655},"Fine-Tuning Gemma Models in Hugging Face","Fine-Tuning Gemma Models in Hugging Face We recently announced that Gemma, the open weights language...",["Date","2024-02-23T00:00:00.000Z"],"https://huggingface.co/blog/gemma-peft","https://huggingface.co/blog/assets/gemma-peft/thumbnail.png","src/content/posts/2024-02-23-fine-tuning-gemma-models-in-hugging-face.md","dc90394f4ce448a6",{"html":25,"metadata":12659},{"headings":12660,"localImagePaths":12661,"remoteImagePaths":12662,"frontmatter":12663,"imagePaths":12665},[],[],[],{"title":12651,"description":25,"summary":12652,"pubDate":12664,"source":2720,"url":12654,"thumbnail":12655},"Fri, 23 Feb 2024 00:00:00 GMT",[],"2024-02-23-fine-tuning-gemma-models-in-hugging-face.md","2024-02-23-introduction-to-matryoshka-embedding-models",{"id":12667,"data":12669,"filePath":12675,"digest":12676,"rendered":12677,"legacyId":12684},{"title":12670,"description":25,"summary":12671,"pubDate":12672,"source":2720,"url":12673,"thumbnail":12674},"🪆 Introduction to Matryoshka Embedding Models","🪆 Introduction to Matryoshka Embedding Models In this blogpost, we will introduce you to the concept...",["Date","2024-02-23T00:00:00.000Z"],"https://huggingface.co/blog/matryoshka","https://huggingface.co/blog/assets/matryoshka/thumbnail.png","src/content/posts/2024-02-23-introduction-to-matryoshka-embedding-models.md","29b3f472c4eb87a8",{"html":25,"metadata":12678},{"headings":12679,"localImagePaths":12680,"remoteImagePaths":12681,"frontmatter":12682,"imagePaths":12683},[],[],[],{"title":12670,"description":25,"summary":12671,"pubDate":12664,"source":2720,"url":12673,"thumbnail":12674},[],"2024-02-23-introduction-to-matryoshka-embedding-models.md","2024-02-23-introducing-the-red-teaming-resistance-leaderboard",{"id":12685,"data":12687,"filePath":12693,"digest":12694,"rendered":12695,"legacyId":12702},{"title":12688,"description":25,"summary":12689,"pubDate":12690,"source":2720,"url":12691,"thumbnail":12692},"Introducing the Red-Teaming Resistance Leaderboard","Introducing the Red-Teaming Resistance Leaderboard Content warning: since this blog post is about a ...",["Date","2024-02-23T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-haizelab","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_haizelab.png","src/content/posts/2024-02-23-introducing-the-red-teaming-resistance-leaderboard.md","503862cd130b397f",{"html":25,"metadata":12696},{"headings":12697,"localImagePaths":12698,"remoteImagePaths":12699,"frontmatter":12700,"imagePaths":12701},[],[],[],{"title":12688,"description":25,"summary":12689,"pubDate":12664,"source":2720,"url":12691,"thumbnail":12692},[],"2024-02-23-introducing-the-red-teaming-resistance-leaderboard.md","2024-02-26-ai-watermarking-101-tools-and-techniques",{"id":12703,"data":12705,"filePath":12711,"digest":12712,"rendered":12713,"legacyId":12721},{"title":12706,"description":25,"summary":12707,"pubDate":12708,"source":2720,"url":12709,"thumbnail":12710},"AI Watermarking 101: Tools and Techniques","AI Watermarking 101: Tools and Techniques In recent months, we've seen multiple news stories involvi...",["Date","2024-02-26T00:00:00.000Z"],"https://huggingface.co/blog/watermarking","https://huggingface.co/blog/assets/watermarking/thumbnail.png","src/content/posts/2024-02-26-ai-watermarking-101-tools-and-techniques.md","38f3b25f35459a79",{"html":25,"metadata":12714},{"headings":12715,"localImagePaths":12716,"remoteImagePaths":12717,"frontmatter":12718,"imagePaths":12720},[],[],[],{"title":12706,"description":25,"summary":12707,"pubDate":12719,"source":2720,"url":12709,"thumbnail":12710},"Mon, 26 Feb 2024 00:00:00 GMT",[],"2024-02-26-ai-watermarking-101-tools-and-techniques.md","2024-02-27-tts-arena-benchmarking-text-to-speech-models-in-the-wild",{"id":12722,"data":12724,"filePath":12730,"digest":12731,"rendered":12732,"legacyId":12740},{"title":12725,"description":25,"summary":12726,"pubDate":12727,"source":2720,"url":12728,"thumbnail":12729},"TTS Arena: Benchmarking Text-to-Speech Models in the Wild","TTS Arena: Benchmarking Text-to-Speech Models in the Wild Automated measurement of the quality of te...",["Date","2024-02-27T00:00:00.000Z"],"https://huggingface.co/blog/arena-tts","https://huggingface.co/blog/assets/arenas-on-the-hub/thumbnail.png","src/content/posts/2024-02-27-tts-arena-benchmarking-text-to-speech-models-in-the-wild.md","44e5dcf65e1500f0",{"html":25,"metadata":12733},{"headings":12734,"localImagePaths":12735,"remoteImagePaths":12736,"frontmatter":12737,"imagePaths":12739},[],[],[],{"title":12725,"description":25,"summary":12726,"pubDate":12738,"source":2720,"url":12728,"thumbnail":12729},"Tue, 27 Feb 2024 00:00:00 GMT",[],"2024-02-27-tts-arena-benchmarking-text-to-speech-models-in-the-wild.md","2024-02-29-text-generation-pipeline-on-intel-gaudi-2-ai-accelerator",{"id":12741,"data":12743,"filePath":12749,"digest":12750,"rendered":12751,"legacyId":12759},{"title":12744,"description":25,"summary":12745,"pubDate":12746,"source":2720,"url":12747,"thumbnail":12748},"Text-Generation Pipeline on Intel® Gaudi® 2 AI Accelerator","Text-Generation Pipeline on Intel® Gaudi® 2 AI Accelerator With the Generative AI (GenAI) revolution...",["Date","2024-02-29T00:00:00.000Z"],"https://huggingface.co/blog/textgen-pipe-gaudi","https://huggingface.co/blog/assets/textgen-pipe-gaudi/thumbnail.png","src/content/posts/2024-02-29-text-generation-pipeline-on-intel-gaudi-2-ai-accelerator.md","8c6b0dba753ac62e",{"html":25,"metadata":12752},{"headings":12753,"localImagePaths":12754,"remoteImagePaths":12755,"frontmatter":12756,"imagePaths":12758},[],[],[],{"title":12744,"description":25,"summary":12745,"pubDate":12757,"source":2720,"url":12747,"thumbnail":12748},"Thu, 29 Feb 2024 00:00:00 GMT",[],"2024-02-29-text-generation-pipeline-on-intel-gaudi-2-ai-accelerator.md","2024-03-04-data-is-better-together",{"id":12760,"data":12762,"filePath":12768,"digest":12769,"rendered":12770,"legacyId":12778},{"title":12763,"description":25,"summary":12764,"pubDate":12765,"source":2720,"url":12766,"thumbnail":12767},"Data is better together","Data is better together: Enabling communities to collectively build better datasets together using A...",["Date","2024-03-04T00:00:00.000Z"],"https://huggingface.co/blog/community-datasets","https://huggingface.co/blog/assets/community-datasets/thumbnail.png","src/content/posts/2024-03-04-data-is-better-together.md","3300ed8ad5e0964f",{"html":25,"metadata":12771},{"headings":12772,"localImagePaths":12773,"remoteImagePaths":12774,"frontmatter":12775,"imagePaths":12777},[],[],[],{"title":12763,"description":25,"summary":12764,"pubDate":12776,"source":2720,"url":12766,"thumbnail":12767},"Mon, 04 Mar 2024 00:00:00 GMT",[],"2024-03-04-data-is-better-together.md","2024-03-05-introducing-contextual-how-well-can-your-multimodal-model-jointly-reason-over-text-and-image-in-text-rich-scenes",{"id":12779,"data":12781,"filePath":12787,"digest":12788,"rendered":12789,"legacyId":12797},{"title":12782,"description":25,"summary":12783,"pubDate":12784,"source":2720,"url":12785,"thumbnail":12786},"Introducing ConTextual: How well can your Multimodal model jointly reason over text and image in text-rich scenes?","Introducing ConTextual: How well can your Multimodal model jointly reason over text and image in tex...",["Date","2024-03-05T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-contextual","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_contextual.png","src/content/posts/2024-03-05-introducing-contextual-how-well-can-your-multimodal-model-jointly-reason-over-text-and-image-in-text-rich-scenes.md","9b3717e7532e0542",{"html":25,"metadata":12790},{"headings":12791,"localImagePaths":12792,"remoteImagePaths":12793,"frontmatter":12794,"imagePaths":12796},[],[],[],{"title":12782,"description":25,"summary":12783,"pubDate":12795,"source":2720,"url":12785,"thumbnail":12786},"Tue, 05 Mar 2024 00:00:00 GMT",[],"2024-03-05-introducing-contextual-how-well-can-your-multimodal-model-jointly-reason-over-text-and-image-in-text-rich-scenes.md","2024-02-28-starcoder2-and-the-stack-v2",{"id":12798,"data":12800,"filePath":12806,"digest":12807,"rendered":12808,"legacyId":12816},{"title":12801,"description":25,"summary":12802,"pubDate":12803,"source":2720,"url":12804,"thumbnail":12805},"StarCoder2 and The Stack v2","StarCoder2 and The Stack v2 BigCode is releasing StarCoder2, the next generation of transparently tr...",["Date","2024-02-28T00:00:00.000Z"],"https://huggingface.co/blog/starcoder2","https://huggingface.co/blog/assets/177_starcoder2/sc2-banner.png","src/content/posts/2024-02-28-starcoder2-and-the-stack-v2.md","6600c8189338aee5",{"html":25,"metadata":12809},{"headings":12810,"localImagePaths":12811,"remoteImagePaths":12812,"frontmatter":12813,"imagePaths":12815},[],[],[],{"title":12801,"description":25,"summary":12802,"pubDate":12814,"source":2720,"url":12804,"thumbnail":12805},"Wed, 28 Feb 2024 00:00:00 GMT",[],"2024-02-28-starcoder2-and-the-stack-v2.md","2024-03-05-openai-and-elon-musk",{"id":12817,"data":12819,"filePath":12824,"digest":12825,"rendered":12826,"legacyId":12834},{"title":12820,"description":12821,"summary":12821,"pubDate":12822,"source":19,"url":12823,"thumbnail":21},"OpenAI and Elon Musk","We are dedicated to the OpenAI mission and have pursued it every step of the way.",["Date","2024-03-05T08:00:00.000Z"],"https://openai.com/blog/openai-elon-musk","src/content/posts/2024-03-05-openai-and-elon-musk.md","38fb45f67ff0dff5",{"html":25,"metadata":12827},{"headings":12828,"localImagePaths":12829,"remoteImagePaths":12830,"frontmatter":12831,"imagePaths":12833},[],[],[],{"title":12820,"description":12821,"summary":12821,"pubDate":12832,"source":19,"url":12823,"thumbnail":21},"Tue, 05 Mar 2024 08:00:00 GMT",[],"2024-03-05-openai-and-elon-musk.md","2024-03-06-improving-health-literacy-and-patient-well-being",{"id":12835,"data":12837,"filePath":12842,"digest":12843,"rendered":12844,"legacyId":12852},{"title":12838,"description":12839,"summary":12839,"pubDate":12840,"source":19,"url":12841,"thumbnail":21},"Improving health literacy and patient well-being","Lifespan uses GPT-4 to radically improve health literacy and patient outcomes.",["Date","2024-03-06T08:00:00.000Z"],"https://openai.com/blog/lifespan","src/content/posts/2024-03-06-improving-health-literacy-and-patient-well-being.md","0840ac936e5ae563",{"html":25,"metadata":12845},{"headings":12846,"localImagePaths":12847,"remoteImagePaths":12848,"frontmatter":12849,"imagePaths":12851},[],[],[],{"title":12838,"description":12839,"summary":12839,"pubDate":12850,"source":19,"url":12841,"thumbnail":21},"Wed, 06 Mar 2024 08:00:00 GMT",[],"2024-03-06-improving-health-literacy-and-patient-well-being.md","2024-03-06-using-ai-to-improve-patient-access-to-clinical-trials",{"id":12853,"data":12855,"filePath":12860,"digest":12861,"rendered":12862,"legacyId":12869},{"title":12856,"description":12857,"summary":12857,"pubDate":12858,"source":19,"url":12859,"thumbnail":21},"Using AI to improve patient access to clinical trials","Paradigm uses OpenAI’s API to improve patient access to clinical trials.",["Date","2024-03-06T08:00:00.000Z"],"https://openai.com/blog/paradigm","src/content/posts/2024-03-06-using-ai-to-improve-patient-access-to-clinical-trials.md","9e5e0347441e67dd",{"html":25,"metadata":12863},{"headings":12864,"localImagePaths":12865,"remoteImagePaths":12866,"frontmatter":12867,"imagePaths":12868},[],[],[],{"title":12856,"description":12857,"summary":12857,"pubDate":12850,"source":19,"url":12859,"thumbnail":21},[],"2024-03-06-using-ai-to-improve-patient-access-to-clinical-trials.md","2024-03-06-sparking-a-more-productive-company-with-chatgpt-enterprise",{"id":12870,"data":12872,"filePath":12877,"digest":12878,"rendered":12879,"legacyId":12886},{"title":12873,"description":12874,"summary":12874,"pubDate":12875,"source":19,"url":12876,"thumbnail":21},"Sparking a more productive company with ChatGPT Enterprise","Match Group uses ChatGPT Enterprise to spark creativity and impact.",["Date","2024-03-06T08:00:00.000Z"],"https://openai.com/blog/match-group","src/content/posts/2024-03-06-sparking-a-more-productive-company-with-chatgpt-enterprise.md","16335bb42646787d",{"html":25,"metadata":12880},{"headings":12881,"localImagePaths":12882,"remoteImagePaths":12883,"frontmatter":12884,"imagePaths":12885},[],[],[],{"title":12873,"description":12874,"summary":12874,"pubDate":12850,"source":19,"url":12876,"thumbnail":21},[],"2024-03-06-sparking-a-more-productive-company-with-chatgpt-enterprise.md","2024-03-08-openai-announces-new-members-to-board-of-directors",{"id":12887,"data":12889,"filePath":12894,"digest":12895,"rendered":12896,"legacyId":12904},{"title":12890,"description":12891,"summary":12891,"pubDate":12892,"source":19,"url":12893,"thumbnail":21},"OpenAI announces new members to board of directors","Dr. Sue Desmond-Hellmann, Nicole Seligman, Fidji Simo join; Sam Altman rejoins board",["Date","2024-03-08T08:00:00.000Z"],"https://openai.com/blog/openai-announces-new-members-to-board-of-directors","src/content/posts/2024-03-08-openai-announces-new-members-to-board-of-directors.md","e3370c659a6c3c57",{"html":25,"metadata":12897},{"headings":12898,"localImagePaths":12899,"remoteImagePaths":12900,"frontmatter":12901,"imagePaths":12903},[],[],[],{"title":12890,"description":12891,"summary":12891,"pubDate":12902,"source":19,"url":12893,"thumbnail":21},"Fri, 08 Mar 2024 08:00:00 GMT",[],"2024-03-08-openai-announces-new-members-to-board-of-directors.md","2024-03-08-review-completed-altman-brockman-to-continue-to-lead-openai",{"id":12905,"data":12907,"filePath":12912,"digest":12913,"rendered":12914,"legacyId":12921},{"title":12908,"description":12909,"summary":12909,"pubDate":12910,"source":19,"url":12911,"thumbnail":21},"Review completed & Altman, Brockman to continue to lead OpenAI","New board members named and enhancements to the governance structure introduced",["Date","2024-03-08T08:00:00.000Z"],"https://openai.com/blog/review-completed-altman-brockman-to-continue-to-lead-openai","src/content/posts/2024-03-08-review-completed-altman-brockman-to-continue-to-lead-openai.md","e78905eb9e405aff",{"html":25,"metadata":12915},{"headings":12916,"localImagePaths":12917,"remoteImagePaths":12918,"frontmatter":12919,"imagePaths":12920},[],[],[],{"title":12908,"description":12909,"summary":12909,"pubDate":12902,"source":19,"url":12911,"thumbnail":21},[],"2024-03-08-review-completed-altman-brockman-to-continue-to-lead-openai.md","2024-03-13-a-generalist-ai-agent-for-3d-virtual-environments",{"id":12922,"data":12924,"filePath":12930,"digest":12931,"rendered":12932,"legacyId":12940},{"title":12925,"description":12926,"summary":12926,"pubDate":12927,"source":6423,"url":12928,"thumbnail":12929},"A generalist AI agent for 3D virtual environments","Introducing SIMA, a Scalable Instructable Multiworld Agent",["Date","2024-03-13T14:00:00.000Z"],"https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/","https://lh3.googleusercontent.com/2GNumOaJCB48RQIFbwJmmZro-AFdBebufxvY_ZkSdUs9RQ-0nSTgBMXuhUdIE5zpPknqevL4ZyP44PLOpJlg0U0ArlOCcJHfoOagzSnZZoXLnq7hdQ=w1200-h630-n-nu","src/content/posts/2024-03-13-a-generalist-ai-agent-for-3d-virtual-environments.md","0cababe14a237cf6",{"html":25,"metadata":12933},{"headings":12934,"localImagePaths":12935,"remoteImagePaths":12936,"frontmatter":12937,"imagePaths":12939},[],[],[],{"title":12925,"description":12926,"summary":12926,"pubDate":12938,"source":6423,"url":12928,"thumbnail":12929},"Wed, 13 Mar 2024 14:00:00 +0000",[],"2024-03-13-a-generalist-ai-agent-for-3d-virtual-environments.md","2024-03-13-global-news-partnerships-le-monde-and-prisa-media",{"id":12941,"data":12943,"filePath":12948,"digest":12949,"rendered":12950,"legacyId":12958},{"title":12944,"description":12945,"summary":12945,"pubDate":12946,"source":19,"url":12947,"thumbnail":21},"Global news partnerships: Le Monde and Prisa Media","We have partnered with international news organizations Le Monde and Prisa Media to bring French and Spanish news content to ChatGPT.",["Date","2024-03-13T07:00:00.000Z"],"https://openai.com/blog/global-news-partnerships-le-monde-and-prisa-media","src/content/posts/2024-03-13-global-news-partnerships-le-monde-and-prisa-media.md","f81837f928ba0aa2",{"html":25,"metadata":12951},{"headings":12952,"localImagePaths":12953,"remoteImagePaths":12954,"frontmatter":12955,"imagePaths":12957},[],[],[],{"title":12944,"description":12945,"summary":12945,"pubDate":12956,"source":19,"url":12947,"thumbnail":21},"Wed, 13 Mar 2024 07:00:00 GMT",[],"2024-03-13-global-news-partnerships-le-monde-and-prisa-media.md","2024-03-13-saving-lives-with-ai-health-coaching",{"id":12959,"data":12961,"filePath":12966,"digest":12967,"rendered":12968,"legacyId":12975},{"title":12962,"description":12963,"summary":12963,"pubDate":12964,"source":19,"url":12965,"thumbnail":21},"Saving lives with AI health coaching","Healthify collaborates with OpenAI to improve millions of lives with sustainable weight loss.",["Date","2024-03-13T07:00:00.000Z"],"https://openai.com/blog/healthify","src/content/posts/2024-03-13-saving-lives-with-ai-health-coaching.md","beb0372ef59d8648",{"html":25,"metadata":12969},{"headings":12970,"localImagePaths":12971,"remoteImagePaths":12972,"frontmatter":12973,"imagePaths":12974},[],[],[],{"title":12962,"description":12963,"summary":12963,"pubDate":12956,"source":19,"url":12965,"thumbnail":21},[],"2024-03-13-saving-lives-with-ai-health-coaching.md","2024-03-15-cpu-optimized-embeddings-with-optimum-intel-and-fastrag",{"id":12976,"data":12978,"filePath":12983,"digest":12984,"rendered":12985,"legacyId":12993},{"title":12979,"description":25,"summary":12980,"pubDate":12981,"source":2720,"url":12982,"thumbnail":12264},"CPU Optimized Embeddings with 🤗 Optimum Intel and fastRAG","CPU Optimized Embeddings with 🤗 Optimum Intel and fastRAG Embedding models are useful for many appli...",["Date","2024-03-15T00:00:00.000Z"],"https://huggingface.co/blog/intel-fast-embedding","src/content/posts/2024-03-15-cpu-optimized-embeddings-with-optimum-intel-and-fastrag.md","eff9737c62b80379",{"html":25,"metadata":12986},{"headings":12987,"localImagePaths":12988,"remoteImagePaths":12989,"frontmatter":12990,"imagePaths":12992},[],[],[],{"title":12979,"description":25,"summary":12980,"pubDate":12991,"source":2720,"url":12982,"thumbnail":12264},"Fri, 15 Mar 2024 00:00:00 GMT",[],"2024-03-15-cpu-optimized-embeddings-with-optimum-intel-and-fastrag.md","2024-03-15-unlocking-the-conversion-of-web-screenshots-into-html-code-with-the-websight-dataset",{"id":12994,"data":12996,"filePath":13002,"digest":13003,"rendered":13004,"legacyId":13011},{"title":12997,"description":25,"summary":12998,"pubDate":12999,"source":2720,"url":13000,"thumbnail":13001},"Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset","From screenshots to HTML code: Introducing the WebSight dataset In the world of web development, tur...",["Date","2024-03-15T00:00:00.000Z"],"https://huggingface.co/blog/websight","https://huggingface.co/blog/assets/websight/thumbnail.png","src/content/posts/2024-03-15-unlocking-the-conversion-of-web-screenshots-into-html-code-with-the-websight-dataset.md","5decb888ed78ad65",{"html":25,"metadata":13005},{"headings":13006,"localImagePaths":13007,"remoteImagePaths":13008,"frontmatter":13009,"imagePaths":13010},[],[],[],{"title":12997,"description":25,"summary":12998,"pubDate":12991,"source":2720,"url":13000,"thumbnail":13001},[],"2024-03-15-unlocking-the-conversion-of-web-screenshots-into-html-code-with-the-websight-dataset.md","2024-03-18-building-a-data-driven-efficient-culture-with-ai",{"id":13012,"data":13014,"filePath":13019,"digest":13020,"rendered":13021,"legacyId":13029},{"title":13015,"description":13016,"summary":13016,"pubDate":13017,"source":19,"url":13018,"thumbnail":21},"Building a data-driven, efficient culture with AI","Holiday Extras rolls out ChatGPT Enterprise across every team, boosting productivity by 500 hours weekly.",["Date","2024-03-18T07:00:00.000Z"],"https://openai.com/blog/holiday-extras","src/content/posts/2024-03-18-building-a-data-driven-efficient-culture-with-ai.md","78f4b9ba128bd53e",{"html":25,"metadata":13022},{"headings":13023,"localImagePaths":13024,"remoteImagePaths":13025,"frontmatter":13026,"imagePaths":13028},[],[],[],{"title":13015,"description":13016,"summary":13016,"pubDate":13027,"source":19,"url":13018,"thumbnail":21},"Mon, 18 Mar 2024 07:00:00 GMT",[],"2024-03-18-building-a-data-driven-efficient-culture-with-ai.md","2024-03-18-easily-train-models-with-h100-gpus-on-nvidia-dgx-cloud",{"id":13030,"data":13032,"filePath":13038,"digest":13039,"rendered":13040,"legacyId":13048},{"title":13033,"description":25,"summary":13034,"pubDate":13035,"source":2720,"url":13036,"thumbnail":13037},"Easily Train Models with H100 GPUs on NVIDIA DGX Cloud","Easily Train Models with H100 GPUs on NVIDIA DGX Cloud Update: This service is deprecated and no lon...",["Date","2024-03-18T00:00:00.000Z"],"https://huggingface.co/blog/train-dgx-cloud","https://huggingface.co/blog/assets/train-dgx-cloud/thumbnail.jpg","src/content/posts/2024-03-18-easily-train-models-with-h100-gpus-on-nvidia-dgx-cloud.md","7bc133875bed23cb",{"html":25,"metadata":13041},{"headings":13042,"localImagePaths":13043,"remoteImagePaths":13044,"frontmatter":13045,"imagePaths":13047},[],[],[],{"title":13033,"description":25,"summary":13034,"pubDate":13046,"source":2720,"url":13036,"thumbnail":13037},"Mon, 18 Mar 2024 00:00:00 GMT",[],"2024-03-18-easily-train-models-with-h100-gpus-on-nvidia-dgx-cloud.md","2024-03-18-enterprise-ready-trust-and-safety",{"id":13049,"data":13051,"filePath":13056,"digest":13057,"rendered":13058,"legacyId":13065},{"title":13052,"description":13053,"summary":13053,"pubDate":13054,"source":19,"url":13055,"thumbnail":21},"Enterprise-ready trust and safety","Salesforce integrates OpenAI’s enterprise-ready LLMs to transform customer applications.",["Date","2024-03-18T07:00:00.000Z"],"https://openai.com/blog/salesforce","src/content/posts/2024-03-18-enterprise-ready-trust-and-safety.md","0df3931afa51b87b",{"html":25,"metadata":13059},{"headings":13060,"localImagePaths":13061,"remoteImagePaths":13062,"frontmatter":13063,"imagePaths":13064},[],[],[],{"title":13052,"description":13053,"summary":13053,"pubDate":13027,"source":19,"url":13055,"thumbnail":21},[],"2024-03-18-enterprise-ready-trust-and-safety.md","2024-03-18-quanto-a-pytorch-quantization-toolkit",{"id":13066,"data":13068,"filePath":13074,"digest":13075,"rendered":13076,"legacyId":13083},{"title":13069,"description":25,"summary":13070,"pubDate":13071,"source":2720,"url":13072,"thumbnail":13073},"quanto: a pytorch quantization toolkit","Quanto: a PyTorch quantization backend for Optimum Quantization is a technique to reduce the computa...",["Date","2024-03-18T00:00:00.000Z"],"https://huggingface.co/blog/quanto-introduction","https://huggingface.co/blog/assets/169_quanto_intro/thumbnail.png","src/content/posts/2024-03-18-quanto-a-pytorch-quantization-toolkit.md","e72211f8a55c1fcb",{"html":25,"metadata":13077},{"headings":13078,"localImagePaths":13079,"remoteImagePaths":13080,"frontmatter":13081,"imagePaths":13082},[],[],[],{"title":13069,"description":25,"summary":13070,"pubDate":13046,"source":2720,"url":13072,"thumbnail":13073},[],"2024-03-18-quanto-a-pytorch-quantization-toolkit.md","2024-03-18-reimagining-the-email-experience-with-ai",{"id":13084,"data":13086,"filePath":13091,"digest":13092,"rendered":13093,"legacyId":13100},{"title":13087,"description":13088,"summary":13088,"pubDate":13089,"source":19,"url":13090,"thumbnail":21},"Reimagining the email experience with AI","Superhuman introduces a new era of email with OpenAI.",["Date","2024-03-18T07:00:00.000Z"],"https://openai.com/blog/superhuman","src/content/posts/2024-03-18-reimagining-the-email-experience-with-ai.md","8860cbeb026c6816",{"html":25,"metadata":13094},{"headings":13095,"localImagePaths":13096,"remoteImagePaths":13097,"frontmatter":13098,"imagePaths":13099},[],[],[],{"title":13087,"description":13088,"summary":13088,"pubDate":13027,"source":19,"url":13090,"thumbnail":21},[],"2024-03-18-reimagining-the-email-experience-with-ai.md","2024-03-19-tacticai-an-ai-assistant-for-football-tactics",{"id":13101,"data":13103,"filePath":13109,"digest":13110,"rendered":13111,"legacyId":13119},{"title":13104,"description":13105,"summary":13105,"pubDate":13106,"source":6423,"url":13107,"thumbnail":13108},"TacticAI: an AI assistant for football tactics","As part of our multi-year collaboration with Liverpool FC, we develop a full AI system that can advise coaches on corner kicks",["Date","2024-03-19T16:03:00.000Z"],"https://deepmind.google/discover/blog/tacticai-ai-assistant-for-football-tactics/","https://lh3.googleusercontent.com/pPa45NPPYrOc4QHbcLIsmueJXi9hKNFdB0rbnRMdiRH0Gf3fgIc_g26-UbFxHVzqUT85QA-N3IvPpQaDevlp3OeF3RIiLQjmuONVRVyX1et0WYEKTQ=w1200-h630-n-nu","src/content/posts/2024-03-19-tacticai-an-ai-assistant-for-football-tactics.md","2f784854aa067c15",{"html":25,"metadata":13112},{"headings":13113,"localImagePaths":13114,"remoteImagePaths":13115,"frontmatter":13116,"imagePaths":13118},[],[],[],{"title":13104,"description":13105,"summary":13105,"pubDate":13117,"source":6423,"url":13107,"thumbnail":13108},"Tue, 19 Mar 2024 16:03:00 +0000",[],"2024-03-19-tacticai-an-ai-assistant-for-football-tactics.md","2024-03-20-a-chatbot-on-your-laptop-phi-2-on-intel-meteor-lake",{"id":13120,"data":13122,"filePath":13128,"digest":13129,"rendered":13130,"legacyId":13138},{"title":13123,"description":25,"summary":13124,"pubDate":13125,"source":2720,"url":13126,"thumbnail":13127},"A Chatbot on your Laptop: Phi-2 on Intel Meteor Lake","A Chatbot on your Laptop: Phi-2 on Intel Meteor Lake Because of their impressive abilities, large la...",["Date","2024-03-20T00:00:00.000Z"],"https://huggingface.co/blog/phi2-intel-meteor-lake","https://huggingface.co/blog/assets/phi2-intel-meteor-lake/02.jpg","src/content/posts/2024-03-20-a-chatbot-on-your-laptop-phi-2-on-intel-meteor-lake.md","9efedc52f93daed6",{"html":25,"metadata":13131},{"headings":13132,"localImagePaths":13133,"remoteImagePaths":13134,"frontmatter":13135,"imagePaths":13137},[],[],[],{"title":13123,"description":25,"summary":13124,"pubDate":13136,"source":2720,"url":13126,"thumbnail":13127},"Wed, 20 Mar 2024 00:00:00 GMT",[],"2024-03-20-a-chatbot-on-your-laptop-phi-2-on-intel-meteor-lake.md","2024-03-20-cosmopedia-how-to-create-large-scale-synthetic-data-for-pre-training-large-language-models",{"id":13139,"data":13141,"filePath":13147,"digest":13148,"rendered":13149,"legacyId":13156},{"title":13142,"description":25,"summary":13143,"pubDate":13144,"source":2720,"url":13145,"thumbnail":13146},"Cosmopedia: how to create large-scale synthetic data for pre-training Large Language Models","Cosmopedia: how to create large-scale synthetic data for pre-training In this blog post, we outline ...",["Date","2024-03-20T00:00:00.000Z"],"https://huggingface.co/blog/cosmopedia","https://huggingface.co/blog/assets/cosmopedia/thumbnail.png","src/content/posts/2024-03-20-cosmopedia-how-to-create-large-scale-synthetic-data-for-pre-training-large-language-models.md","765197934eb3d366",{"html":25,"metadata":13150},{"headings":13151,"localImagePaths":13152,"remoteImagePaths":13153,"frontmatter":13154,"imagePaths":13155},[],[],[],{"title":13142,"description":25,"summary":13143,"pubDate":13136,"source":2720,"url":13145,"thumbnail":13146},[],"2024-03-20-cosmopedia-how-to-create-large-scale-synthetic-data-for-pre-training-large-language-models.md","2024-03-20-galore-advancing-large-model-training-on-consumer-grade-hardware",{"id":13157,"data":13159,"filePath":13164,"digest":13165,"rendered":13166,"legacyId":13173},{"title":13160,"description":25,"summary":13161,"pubDate":13162,"source":2720,"url":13163,"thumbnail":3514},"GaLore: Advancing Large Model Training on Consumer-grade Hardware","GaLore: Advancing Large Model Training on Consumer-grade Hardware The integration of GaLore into the...",["Date","2024-03-20T00:00:00.000Z"],"https://huggingface.co/blog/galore","src/content/posts/2024-03-20-galore-advancing-large-model-training-on-consumer-grade-hardware.md","dd8177a3a401647d",{"html":25,"metadata":13167},{"headings":13168,"localImagePaths":13169,"remoteImagePaths":13170,"frontmatter":13171,"imagePaths":13172},[],[],[],{"title":13160,"description":25,"summary":13161,"pubDate":13136,"source":2720,"url":13163,"thumbnail":3514},[],"2024-03-20-galore-advancing-large-model-training-on-consumer-grade-hardware.md","2024-03-21-embedding-ai-into-developer-software",{"id":13174,"data":13176,"filePath":13181,"digest":13182,"rendered":13183,"legacyId":13191},{"title":13177,"description":13178,"summary":13178,"pubDate":13179,"source":19,"url":13180,"thumbnail":21},"Embedding AI into developer software","JetBrains uses OpenAI’s API to build its fastest-growing product ever.",["Date","2024-03-21T07:00:00.000Z"],"https://openai.com/blog/jetbrains","src/content/posts/2024-03-21-embedding-ai-into-developer-software.md","8c53ebff00c6302c",{"html":25,"metadata":13184},{"headings":13185,"localImagePaths":13186,"remoteImagePaths":13187,"frontmatter":13188,"imagePaths":13190},[],[],[],{"title":13177,"description":13178,"summary":13178,"pubDate":13189,"source":19,"url":13180,"thumbnail":21},"Thu, 21 Mar 2024 07:00:00 GMT",[],"2024-03-21-embedding-ai-into-developer-software.md","2024-03-21-introducing-the-chatbot-guardrails-arena",{"id":13192,"data":13194,"filePath":13200,"digest":13201,"rendered":13202,"legacyId":13210},{"title":13195,"description":25,"summary":13196,"pubDate":13197,"source":2720,"url":13198,"thumbnail":13199},"Introducing the Chatbot Guardrails Arena","Introducing the Chatbot Guardrails Arena With the recent advancements in augmented LLM capabilities,...",["Date","2024-03-21T00:00:00.000Z"],"https://huggingface.co/blog/arena-lighthouz","https://huggingface.co/blog/assets/arenas-on-the-hub/thumbnail_lighthouz.png","src/content/posts/2024-03-21-introducing-the-chatbot-guardrails-arena.md","ea7fa90d8a000c1d",{"html":25,"metadata":13203},{"headings":13204,"localImagePaths":13205,"remoteImagePaths":13206,"frontmatter":13207,"imagePaths":13209},[],[],[],{"title":13195,"description":25,"summary":13196,"pubDate":13208,"source":2720,"url":13198,"thumbnail":13199},"Thu, 21 Mar 2024 00:00:00 GMT",[],"2024-03-21-introducing-the-chatbot-guardrails-arena.md","2024-03-22-binary-and-scalar-embedding-quantization-for-significantly-faster-cheaper-retrieval",{"id":13211,"data":13213,"filePath":13219,"digest":13220,"rendered":13221,"legacyId":13229},{"title":13214,"description":25,"summary":13215,"pubDate":13216,"source":2720,"url":13217,"thumbnail":13218},"Binary and Scalar Embedding Quantization for Significantly Faster & Cheaper Retrieval","Binary and Scalar Embedding Quantization for Significantly Faster & Cheaper Retrieval We introduce t...",["Date","2024-03-22T00:00:00.000Z"],"https://huggingface.co/blog/embedding-quantization","https://huggingface.co/blog/assets/embedding-quantization/thumbnail.png","src/content/posts/2024-03-22-binary-and-scalar-embedding-quantization-for-significantly-faster-cheaper-retrieval.md","2889da6e0750cf8c",{"html":25,"metadata":13222},{"headings":13223,"localImagePaths":13224,"remoteImagePaths":13225,"frontmatter":13226,"imagePaths":13228},[],[],[],{"title":13214,"description":25,"summary":13215,"pubDate":13227,"source":2720,"url":13217,"thumbnail":13218},"Fri, 22 Mar 2024 00:00:00 GMT",[],"2024-03-22-binary-and-scalar-embedding-quantization-for-significantly-faster-cheaper-retrieval.md","2024-03-22-total-noobs-intro-to-hugging-face-transformers",{"id":13230,"data":13232,"filePath":13238,"digest":13239,"rendered":13240,"legacyId":13247},{"title":13233,"description":25,"summary":13234,"pubDate":13235,"source":2720,"url":13236,"thumbnail":13237},"Total noob’s intro to Hugging Face Transformers","Total noob’s intro to Hugging Face Transformers Welcome to 'A Total Noob’s Introduction to Hugging F...",["Date","2024-03-22T00:00:00.000Z"],"https://huggingface.co/blog/noob_intro_transformers","https://huggingface.co/blog/assets/78_ml_director_insights/guide.png","src/content/posts/2024-03-22-total-noobs-intro-to-hugging-face-transformers.md","28ba42728f3d5e38",{"html":25,"metadata":13241},{"headings":13242,"localImagePaths":13243,"remoteImagePaths":13244,"frontmatter":13245,"imagePaths":13246},[],[],[],{"title":13233,"description":25,"summary":13234,"pubDate":13227,"source":2720,"url":13236,"thumbnail":13237},[],"2024-03-22-total-noobs-intro-to-hugging-face-transformers.md","2024-03-25-pollen-vision-unified-interface-for-zero-shot-vision-models-in-robotics",{"id":13248,"data":13250,"filePath":13256,"digest":13257,"rendered":13258,"legacyId":13266},{"title":13251,"description":25,"summary":13252,"pubDate":13253,"source":2720,"url":13254,"thumbnail":13255},"Pollen-Vision: Unified interface for Zero-Shot vision models in robotics","Pollen-Vision: Unified interface for Zero-Shot vision models in robotics This is a guest blog post b...",["Date","2024-03-25T00:00:00.000Z"],"https://huggingface.co/blog/pollen-vision","https://huggingface.co/blog/assets/pollen-vision/thumbnail.jpg","src/content/posts/2024-03-25-pollen-vision-unified-interface-for-zero-shot-vision-models-in-robotics.md","a9542af713ce118c",{"html":25,"metadata":13259},{"headings":13260,"localImagePaths":13261,"remoteImagePaths":13262,"frontmatter":13263,"imagePaths":13265},[],[],[],{"title":13251,"description":25,"summary":13252,"pubDate":13264,"source":2720,"url":13254,"thumbnail":13255},"Mon, 25 Mar 2024 00:00:00 GMT",[],"2024-03-25-pollen-vision-unified-interface-for-zero-shot-vision-models-in-robotics.md","2024-03-25-sora-first-impressions",{"id":13267,"data":13269,"filePath":13274,"digest":13275,"rendered":13276,"legacyId":13283},{"title":13270,"description":13271,"summary":13271,"pubDate":13272,"source":19,"url":13273,"thumbnail":21},"Sora first impressions","Since we introduced Sora to the world last month, we’ve been working with artists to learn how Sora might aid in their creative process.",["Date","2024-03-25T00:00:00.000Z"],"https://openai.com/blog/sora-first-impressions","src/content/posts/2024-03-25-sora-first-impressions.md","4ca859e0a64b34cf",{"html":25,"metadata":13277},{"headings":13278,"localImagePaths":13279,"remoteImagePaths":13280,"frontmatter":13281,"imagePaths":13282},[],[],[],{"title":13270,"description":13271,"summary":13271,"pubDate":13264,"source":19,"url":13273,"thumbnail":21},[],"2024-03-25-sora-first-impressions.md","2024-03-27-openais-comment-to-the-ntia-on-open-model-weights",{"id":13284,"data":13286,"filePath":13291,"digest":13292,"rendered":13293,"legacyId":13301},{"title":13287,"description":13288,"summary":13288,"pubDate":13289,"source":19,"url":13290,"thumbnail":21},"OpenAI’s comment to the NTIA on open model weights","OpenAI’s comment to the NTIA on open model weights This comment was submitted by OpenAI in response to NTIA’s March 2024 Request for Information on Dual-Use Foundation Models with Widely Available Weights.",["Date","2024-03-27T00:00:00.000Z"],"https://openai.com/global-affairs/openai-s-comment-to-the-ntia-on-open-model-weights","src/content/posts/2024-03-27-openais-comment-to-the-ntia-on-open-model-weights.md","53abfdf17dbdc69b",{"html":25,"metadata":13294},{"headings":13295,"localImagePaths":13296,"remoteImagePaths":13297,"frontmatter":13298,"imagePaths":13300},[],[],[],{"title":13287,"description":13288,"summary":13288,"pubDate":13299,"source":19,"url":13290,"thumbnail":21},"Wed, 27 Mar 2024 00:00:00 GMT",[],"2024-03-27-openais-comment-to-the-ntia-on-open-model-weights.md","2024-03-28-making-education-data-accessible",{"id":13302,"data":13304,"filePath":13309,"digest":13310,"rendered":13311,"legacyId":13319},{"title":13305,"description":13306,"summary":13306,"pubDate":13307,"source":19,"url":13308,"thumbnail":21},"Making education data accessible","Zelma uses GPT-4 to make education data accessible.",["Date","2024-03-28T00:00:00.000Z"],"https://openai.com/blog/zelma","src/content/posts/2024-03-28-making-education-data-accessible.md","0425d0a250d3450f",{"html":25,"metadata":13312},{"headings":13313,"localImagePaths":13314,"remoteImagePaths":13315,"frontmatter":13316,"imagePaths":13318},[],[],[],{"title":13305,"description":13306,"summary":13306,"pubDate":13317,"source":19,"url":13308,"thumbnail":21},"Thu, 28 Mar 2024 00:00:00 GMT",[],"2024-03-28-making-education-data-accessible.md","2024-03-29-navigating-the-challenges-and-opportunities-of-synthetic-voices",{"id":13320,"data":13322,"filePath":13327,"digest":13328,"rendered":13329,"legacyId":13337},{"title":13323,"description":13324,"summary":13324,"pubDate":13325,"source":19,"url":13326,"thumbnail":21},"Navigating the challenges and opportunities of synthetic voices","We’re sharing lessons from a small scale preview of Voice Engine, a model for creating custom voices.",["Date","2024-03-29T00:00:00.000Z"],"https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices","src/content/posts/2024-03-29-navigating-the-challenges-and-opportunities-of-synthetic-voices.md","20e3cee67f8f9607",{"html":25,"metadata":13330},{"headings":13331,"localImagePaths":13332,"remoteImagePaths":13333,"frontmatter":13334,"imagePaths":13336},[],[],[],{"title":13323,"description":13324,"summary":13324,"pubDate":13335,"source":19,"url":13326,"thumbnail":21},"Fri, 29 Mar 2024 00:00:00 GMT",[],"2024-03-29-navigating-the-challenges-and-opportunities-of-synthetic-voices.md","2024-04-01-reducing-health-insurance-costs-and-improving-care",{"id":13338,"data":13340,"filePath":13345,"digest":13346,"rendered":13347,"legacyId":13355},{"title":13341,"description":13342,"summary":13342,"pubDate":13343,"source":19,"url":13344,"thumbnail":21},"Reducing health insurance costs and improving care","Oscar brings AI to health insurance, reducing costs and improving patient care.",["Date","2024-04-01T00:00:00.000Z"],"https://openai.com/blog/oscar","src/content/posts/2024-04-01-reducing-health-insurance-costs-and-improving-care.md","5ceb531f06873ccd",{"html":25,"metadata":13348},{"headings":13349,"localImagePaths":13350,"remoteImagePaths":13351,"frontmatter":13352,"imagePaths":13354},[],[],[],{"title":13341,"description":13342,"summary":13342,"pubDate":13353,"source":19,"url":13344,"thumbnail":21},"Mon, 01 Apr 2024 00:00:00 GMT",[],"2024-04-01-reducing-health-insurance-costs-and-improving-care.md","2024-04-01-start-using-chatgpt-instantly",{"id":13356,"data":13358,"filePath":13363,"digest":13364,"rendered":13365,"legacyId":13372},{"title":13359,"description":13360,"summary":13360,"pubDate":13361,"source":19,"url":13362,"thumbnail":21},"Start using ChatGPT instantly","We’re making it easier for people to experience the benefits of AI without needing to sign up",["Date","2024-04-01T00:00:00.000Z"],"https://openai.com/blog/start-using-chatgpt-instantly","src/content/posts/2024-04-01-start-using-chatgpt-instantly.md","53cf6c0c96c21dc5",{"html":25,"metadata":13366},{"headings":13367,"localImagePaths":13368,"remoteImagePaths":13369,"frontmatter":13370,"imagePaths":13371},[],[],[],{"title":13359,"description":13360,"summary":13360,"pubDate":13353,"source":19,"url":13362,"thumbnail":21},[],"2024-04-01-start-using-chatgpt-instantly.md","2024-04-02-bringing-serverless-gpu-inference-to-hugging-face-users",{"id":13373,"data":13375,"filePath":13381,"digest":13382,"rendered":13383,"legacyId":13391},{"title":13376,"description":25,"summary":13377,"pubDate":13378,"source":2720,"url":13379,"thumbnail":13380},"Bringing serverless GPU inference to Hugging Face users","Bringing serverless GPU inference to Hugging Face users Today, we are thrilled to announce the launc...",["Date","2024-04-02T00:00:00.000Z"],"https://huggingface.co/blog/cloudflare-workers-ai","https://huggingface.co/blog/assets/cloudflare-workers-ai/thumbnail.jpg","src/content/posts/2024-04-02-bringing-serverless-gpu-inference-to-hugging-face-users.md","e93009b2c7f064de",{"html":25,"metadata":13384},{"headings":13385,"localImagePaths":13386,"remoteImagePaths":13387,"frontmatter":13388,"imagePaths":13390},[],[],[],{"title":13376,"description":25,"summary":13377,"pubDate":13389,"source":2720,"url":13379,"thumbnail":13380},"Tue, 02 Apr 2024 00:00:00 GMT",[],"2024-04-02-bringing-serverless-gpu-inference-to-hugging-face-users.md","2024-04-02-customizing-models-for-legal-professionals",{"id":13392,"data":13394,"filePath":13399,"digest":13400,"rendered":13401,"legacyId":13408},{"title":13395,"description":13396,"summary":13396,"pubDate":13397,"source":19,"url":13398,"thumbnail":21},"Customizing models for legal professionals","Harvey partners with OpenAI to build a custom-trained model for legal professionals.",["Date","2024-04-02T00:00:00.000Z"],"https://openai.com/blog/harvey","src/content/posts/2024-04-02-customizing-models-for-legal-professionals.md","f97158a61a8a4ca4",{"html":25,"metadata":13402},{"headings":13403,"localImagePaths":13404,"remoteImagePaths":13405,"frontmatter":13406,"imagePaths":13407},[],[],[],{"title":13395,"description":13396,"summary":13396,"pubDate":13389,"source":19,"url":13398,"thumbnail":21},[],"2024-04-02-customizing-models-for-legal-professionals.md","2024-04-03-blazing-fast-setfit-inference-with-optimum-intel-on-xeon",{"id":13409,"data":13411,"filePath":13416,"digest":13417,"rendered":13418,"legacyId":13426},{"title":13412,"description":25,"summary":13413,"pubDate":13414,"source":2720,"url":13415,"thumbnail":12264},"Blazing Fast SetFit Inference with 🤗 Optimum Intel on Xeon","Blazing Fast SetFit Inference with 🤗 Optimum Intel on Xeon SetFit is a promising solution for a comm...",["Date","2024-04-03T00:00:00.000Z"],"https://huggingface.co/blog/setfit-optimum-intel","src/content/posts/2024-04-03-blazing-fast-setfit-inference-with-optimum-intel-on-xeon.md","a99f904dc1e0b79d",{"html":25,"metadata":13419},{"headings":13420,"localImagePaths":13421,"remoteImagePaths":13422,"frontmatter":13423,"imagePaths":13425},[],[],[],{"title":13412,"description":25,"summary":13413,"pubDate":13424,"source":2720,"url":13415,"thumbnail":12264},"Wed, 03 Apr 2024 00:00:00 GMT",[],"2024-04-03-blazing-fast-setfit-inference-with-optimum-intel-on-xeon.md","2024-04-04-hugging-face-partners-with-wiz-research-to-improve-ai-security",{"id":13427,"data":13429,"filePath":13435,"digest":13436,"rendered":13437,"legacyId":13445},{"title":13430,"description":25,"summary":13431,"pubDate":13432,"source":2720,"url":13433,"thumbnail":13434},"Hugging Face partners with Wiz Research to Improve AI Security","Hugging Face partners with Wiz Research to Improve AI Security We are pleased to announce that we ar...",["Date","2024-04-04T00:00:00.000Z"],"https://huggingface.co/blog/hugging-face-wiz-security-blog","https://huggingface.co/blog/assets/wiz_security/security.png","src/content/posts/2024-04-04-hugging-face-partners-with-wiz-research-to-improve-ai-security.md","d5233e6145574088",{"html":25,"metadata":13438},{"headings":13439,"localImagePaths":13440,"remoteImagePaths":13441,"frontmatter":13442,"imagePaths":13444},[],[],[],{"title":13430,"description":25,"summary":13431,"pubDate":13443,"source":2720,"url":13433,"thumbnail":13434},"Thu, 04 Apr 2024 00:00:00 GMT",[],"2024-04-04-hugging-face-partners-with-wiz-research-to-improve-ai-security.md","2024-04-04-introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program",{"id":13446,"data":13448,"filePath":13453,"digest":13454,"rendered":13455,"legacyId":13462},{"title":13449,"description":13450,"summary":13450,"pubDate":13451,"source":19,"url":13452,"thumbnail":21},"Introducing improvements to the fine-tuning API and expanding our custom models program","We’re adding new features to help developers have more control over fine-tuning and announcing new ways to build custom models with OpenAI.",["Date","2024-04-04T00:00:00.000Z"],"https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program","src/content/posts/2024-04-04-introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program.md","2755f96bca4f9b53",{"html":25,"metadata":13456},{"headings":13457,"localImagePaths":13458,"remoteImagePaths":13459,"frontmatter":13460,"imagePaths":13461},[],[],[],{"title":13449,"description":13450,"summary":13450,"pubDate":13443,"source":19,"url":13452,"thumbnail":21},[],"2024-04-04-introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program.md","2024-04-04-text2sql-using-hugging-face-dataset-viewer-api-and-motherduck-duckdb-nsql-7b",{"id":13463,"data":13465,"filePath":13471,"digest":13472,"rendered":13473,"legacyId":13480},{"title":13466,"description":25,"summary":13467,"pubDate":13468,"source":2720,"url":13469,"thumbnail":13470},"Text2SQL using Hugging Face Dataset Viewer API and Motherduck DuckDB-NSQL-7B","Text2SQL using Hugging Face Dataset Viewer API and Motherduck DuckDB-NSQL-7B Today, integrating AI-p...",["Date","2024-04-04T00:00:00.000Z"],"https://huggingface.co/blog/duckdb-nsql-7b","https://huggingface.co/blog/assets/duckdb-nsql-7b/thumbnail.png","src/content/posts/2024-04-04-text2sql-using-hugging-face-dataset-viewer-api-and-motherduck-duckdb-nsql-7b.md","7fc1275ed00cc152",{"html":25,"metadata":13474},{"headings":13475,"localImagePaths":13476,"remoteImagePaths":13477,"frontmatter":13478,"imagePaths":13479},[],[],[],{"title":13466,"description":25,"summary":13467,"pubDate":13443,"source":2720,"url":13469,"thumbnail":13470},[],"2024-04-04-text2sql-using-hugging-face-dataset-viewer-api-and-motherduck-duckdb-nsql-7b.md","2024-04-05-klarnas-ai-assistant-does-the-work-of-700-full-time-agents",{"id":13481,"data":13483,"filePath":13488,"digest":13489,"rendered":13490,"legacyId":13498},{"title":13484,"description":13485,"summary":13485,"pubDate":13486,"source":19,"url":13487,"thumbnail":21},"Klarna's AI assistant does the work of 700 full-time agents","Klarna is using AI to revolutionize personal shopping, customer service, and employee productivity.",["Date","2024-04-05T00:00:00.000Z"],"https://openai.com/blog/klarna","src/content/posts/2024-04-05-klarnas-ai-assistant-does-the-work-of-700-full-time-agents.md","0fe8df05f078223e",{"html":25,"metadata":13491},{"headings":13492,"localImagePaths":13493,"remoteImagePaths":13494,"frontmatter":13495,"imagePaths":13497},[],[],[],{"title":13484,"description":13485,"summary":13485,"pubDate":13496,"source":19,"url":13487,"thumbnail":21},"Fri, 05 Apr 2024 00:00:00 GMT",[],"2024-04-05-klarnas-ai-assistant-does-the-work-of-700-full-time-agents.md","2024-04-08-public-policy-at-hugging-face",{"id":13499,"data":13501,"filePath":13507,"digest":13508,"rendered":13509,"legacyId":13517},{"title":13502,"description":25,"summary":13503,"pubDate":13504,"source":2720,"url":13505,"thumbnail":13506},"Public Policy at Hugging Face","Public Policy at Hugging Face Published April 8, 2024 Update on GitHubAI Policy at Hugging Face is a...",["Date","2024-04-08T00:00:00.000Z"],"https://huggingface.co/blog/policy-blog","https://huggingface.co/blog/assets/policy_docs/policy_blog_thumbnail.png","src/content/posts/2024-04-08-public-policy-at-hugging-face.md","b120b69eee317c6e",{"html":25,"metadata":13510},{"headings":13511,"localImagePaths":13512,"remoteImagePaths":13513,"frontmatter":13514,"imagePaths":13516},[],[],[],{"title":13502,"description":25,"summary":13503,"pubDate":13515,"source":2720,"url":13505,"thumbnail":13506},"Mon, 08 Apr 2024 00:00:00 GMT",[],"2024-04-08-public-policy-at-hugging-face.md","2024-04-09-codegemma---an-official-google-release-for-code-llms",{"id":13518,"data":13520,"filePath":13526,"digest":13527,"rendered":13528,"legacyId":13536},{"title":13521,"description":25,"summary":13522,"pubDate":13523,"source":2720,"url":13524,"thumbnail":13525},"CodeGemma - an official Google release for code LLMs","CodeGemma - an official Google release for code LLMs CodeGemma is a family of open-access versions o...",["Date","2024-04-09T00:00:00.000Z"],"https://huggingface.co/blog/codegemma","https://huggingface.co/blog/assets/codegemma/thumbnail_b.png","src/content/posts/2024-04-09-codegemma---an-official-google-release-for-code-llms.md","e1c621bb57f57fcf",{"html":25,"metadata":13529},{"headings":13530,"localImagePaths":13531,"remoteImagePaths":13532,"frontmatter":13533,"imagePaths":13535},[],[],[],{"title":13521,"description":25,"summary":13522,"pubDate":13534,"source":2720,"url":13524,"thumbnail":13525},"Tue, 09 Apr 2024 00:00:00 GMT",[],"2024-04-09-codegemma---an-official-google-release-for-code-llms.md","2024-04-10-making-thousands-of-open-llms-bloom-in-the-vertex-ai-model-garden",{"id":13537,"data":13539,"filePath":13544,"digest":13545,"rendered":13546,"legacyId":13554},{"title":13540,"description":25,"summary":13541,"pubDate":13542,"source":2720,"url":13543,"thumbnail":12190},"Making thousands of open LLMs bloom in the Vertex AI Model Garden","Making thousands of open LLMs bloom in the Vertex AI Model Garden Today, we are thrilled to announce...",["Date","2024-04-10T00:00:00.000Z"],"https://huggingface.co/blog/google-cloud-model-garden","src/content/posts/2024-04-10-making-thousands-of-open-llms-bloom-in-the-vertex-ai-model-garden.md","a18c2c380ef6ff59",{"html":25,"metadata":13547},{"headings":13548,"localImagePaths":13549,"remoteImagePaths":13550,"frontmatter":13551,"imagePaths":13553},[],[],[],{"title":13540,"description":25,"summary":13541,"pubDate":13552,"source":2720,"url":13543,"thumbnail":12190},"Wed, 10 Apr 2024 00:00:00 GMT",[],"2024-04-10-making-thousands-of-open-llms-bloom-in-the-vertex-ai-model-garden.md","2024-04-11-vision-language-models-explained",{"id":13555,"data":13557,"filePath":13563,"digest":13564,"rendered":13565,"legacyId":13573},{"title":13558,"description":25,"summary":13559,"pubDate":13560,"source":2720,"url":13561,"thumbnail":13562},"Vision Language Models Explained","Vision Language Models Explained This blog post was written on April 2024 and provides a great intro...",["Date","2024-04-11T00:00:00.000Z"],"https://huggingface.co/blog/vlms","https://huggingface.co/blog/assets/vlms_explained/thumbnail.png","src/content/posts/2024-04-11-vision-language-models-explained.md","a224a5d49f6bc92f",{"html":25,"metadata":13566},{"headings":13567,"localImagePaths":13568,"remoteImagePaths":13569,"frontmatter":13570,"imagePaths":13572},[],[],[],{"title":13558,"description":25,"summary":13559,"pubDate":13571,"source":2720,"url":13561,"thumbnail":13562},"Thu, 11 Apr 2024 00:00:00 GMT",[],"2024-04-11-vision-language-models-explained.md","2024-04-14-introducing-openai-japan",{"id":13574,"data":13576,"filePath":13581,"digest":13582,"rendered":13583,"legacyId":13591},{"title":13577,"description":13578,"summary":13578,"pubDate":13579,"source":19,"url":13580,"thumbnail":21},"Introducing OpenAI Japan","We are excited to announce our first office in Asia and we’re releasing a GPT-4 custom model optimized for the Japanese language.",["Date","2024-04-14T00:00:00.000Z"],"https://openai.com/blog/introducing-openai-japan","src/content/posts/2024-04-14-introducing-openai-japan.md","e9c4153382f56ae0",{"html":25,"metadata":13584},{"headings":13585,"localImagePaths":13586,"remoteImagePaths":13587,"frontmatter":13588,"imagePaths":13590},[],[],[],{"title":13577,"description":13578,"summary":13578,"pubDate":13589,"source":19,"url":13580,"thumbnail":21},"Sun, 14 Apr 2024 00:00:00 GMT",[],"2024-04-14-introducing-openai-japan.md","2024-04-15-introducing-idefics2-a-powerful-8b-vision-language-model-for-the-community",{"id":13592,"data":13594,"filePath":13599,"digest":13600,"rendered":13601,"legacyId":13609},{"title":13595,"description":25,"summary":13596,"pubDate":13597,"source":2720,"url":13598,"thumbnail":10242},"Introducing Idefics2: A Powerful 8B Vision-Language Model for the community","Introducing Idefics2: A Powerful 8B Vision-Language Model for the community We are excited to releas...",["Date","2024-04-15T00:00:00.000Z"],"https://huggingface.co/blog/idefics2","src/content/posts/2024-04-15-introducing-idefics2-a-powerful-8b-vision-language-model-for-the-community.md","b89c37e936f64ba8",{"html":25,"metadata":13602},{"headings":13603,"localImagePaths":13604,"remoteImagePaths":13605,"frontmatter":13606,"imagePaths":13608},[],[],[],{"title":13595,"description":25,"summary":13596,"pubDate":13607,"source":2720,"url":13598,"thumbnail":10242},"Mon, 15 Apr 2024 00:00:00 GMT",[],"2024-04-15-introducing-idefics2-a-powerful-8b-vision-language-model-for-the-community.md","2024-04-16-ai-apps-in-a-flash-with-gradios-reload-mode",{"id":13610,"data":13612,"filePath":13618,"digest":13619,"rendered":13620,"legacyId":13628},{"title":13613,"description":25,"summary":13614,"pubDate":13615,"source":2720,"url":13616,"thumbnail":13617},"AI Apps in a Flash with Gradio's Reload Mode","AI Apps in a Flash with Gradio's Reload Mode In this post, I will show you how you can build a funct...",["Date","2024-04-16T00:00:00.000Z"],"https://huggingface.co/blog/gradio-reload","https://huggingface.co/blog/assets/gradio-reload/thumbnail_compressed.png","src/content/posts/2024-04-16-ai-apps-in-a-flash-with-gradios-reload-mode.md","40cdeeb506ebfb62",{"html":25,"metadata":13621},{"headings":13622,"localImagePaths":13623,"remoteImagePaths":13624,"frontmatter":13625,"imagePaths":13627},[],[],[],{"title":13613,"description":25,"summary":13614,"pubDate":13626,"source":2720,"url":13616,"thumbnail":13617},"Tue, 16 Apr 2024 00:00:00 GMT",[],"2024-04-16-ai-apps-in-a-flash-with-gradios-reload-mode.md","2024-04-16-introducing-the-livecodebench-leaderboard---holistic-and-contamination-free-evaluation-of-code-llms",{"id":13629,"data":13631,"filePath":13636,"digest":13637,"rendered":13638,"legacyId":13645},{"title":13632,"description":25,"summary":13633,"pubDate":13634,"source":2720,"url":13635,"thumbnail":11986},"Introducing the LiveCodeBench Leaderboard - Holistic and Contamination-Free Evaluation of Code LLMs","Introducing the LiveCodeBench Leaderboard - Holistic and Contamination-Free Evaluation of Code LLMs ...",["Date","2024-04-16T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-livecodebench","src/content/posts/2024-04-16-introducing-the-livecodebench-leaderboard---holistic-and-contamination-free-evaluation-of-code-llms.md","58785fe256f82a1c",{"html":25,"metadata":13639},{"headings":13640,"localImagePaths":13641,"remoteImagePaths":13642,"frontmatter":13643,"imagePaths":13644},[],[],[],{"title":13632,"description":25,"summary":13633,"pubDate":13626,"source":2720,"url":13635,"thumbnail":11986},[],"2024-04-16-introducing-the-livecodebench-leaderboard---holistic-and-contamination-free-evaluation-of-code-llms.md","2024-04-16-running-privacy-preserving-inference-on-hugging-face-endpoints",{"id":13646,"data":13648,"filePath":13654,"digest":13655,"rendered":13656,"legacyId":13663},{"title":13649,"description":25,"summary":13650,"pubDate":13651,"source":2720,"url":13652,"thumbnail":13653},"Running Privacy-Preserving Inference on Hugging Face Endpoints","Running Privacy-Preserving Inferences on Hugging Face Endpoints This is a guest blog post by the Zam...",["Date","2024-04-16T00:00:00.000Z"],"https://huggingface.co/blog/fhe-endpoints","https://huggingface.co/blog/assets/fhe-endpoints/thumbnail.png","src/content/posts/2024-04-16-running-privacy-preserving-inference-on-hugging-face-endpoints.md","2060c2c2a34c9083",{"html":25,"metadata":13657},{"headings":13658,"localImagePaths":13659,"remoteImagePaths":13660,"frontmatter":13661,"imagePaths":13662},[],[],[],{"title":13649,"description":25,"summary":13650,"pubDate":13626,"source":2720,"url":13652,"thumbnail":13653},[],"2024-04-16-running-privacy-preserving-inference-on-hugging-face-endpoints.md","2024-04-16-ryghts-journey-to-empower-healthcare-and-life-sciences-with-expert-support-from-hugging-face",{"id":13664,"data":13666,"filePath":13672,"digest":13673,"rendered":13674,"legacyId":13681},{"title":13667,"description":25,"summary":13668,"pubDate":13669,"source":2720,"url":13670,"thumbnail":13671},"Ryght’s Journey to Empower Healthcare and Life Sciences with Expert Support from Hugging Face","Ryght’s Journey to Empower Healthcare and Life Sciences with Expert Support from Hugging Face This i...",["Date","2024-04-16T00:00:00.000Z"],"https://huggingface.co/blog/ryght-case-study","https://huggingface.co/blog/assets/ryght-case-study/thumbnail.png","src/content/posts/2024-04-16-ryghts-journey-to-empower-healthcare-and-life-sciences-with-expert-support-from-hugging-face.md","f0e2cc234543a7d4",{"html":25,"metadata":13675},{"headings":13676,"localImagePaths":13677,"remoteImagePaths":13678,"frontmatter":13679,"imagePaths":13680},[],[],[],{"title":13667,"description":25,"summary":13668,"pubDate":13626,"source":2720,"url":13670,"thumbnail":13671},[],"2024-04-16-ryghts-journey-to-empower-healthcare-and-life-sciences-with-expert-support-from-hugging-face.md","2024-04-18-welcome-llama-3---metas-new-open-llm",{"id":13682,"data":13684,"filePath":13690,"digest":13691,"rendered":13692,"legacyId":13700},{"title":13685,"description":25,"summary":13686,"pubDate":13687,"source":2720,"url":13688,"thumbnail":13689},"Welcome Llama 3 - Meta's new open LLM","Welcome Llama 3 - Meta’s new open LLM Introduction Meta’s Llama 3, the next iteration of the open-ac...",["Date","2024-04-18T00:00:00.000Z"],"https://huggingface.co/blog/llama3","https://huggingface.co/blog/assets/llama3/thumbnail.jpg","src/content/posts/2024-04-18-welcome-llama-3---metas-new-open-llm.md","3dcdecd4e61243d0",{"html":25,"metadata":13693},{"headings":13694,"localImagePaths":13695,"remoteImagePaths":13696,"frontmatter":13697,"imagePaths":13699},[],[],[],{"title":13685,"description":25,"summary":13686,"pubDate":13698,"source":2720,"url":13688,"thumbnail":13689},"Thu, 18 Apr 2024 00:00:00 GMT",[],"2024-04-18-welcome-llama-3---metas-new-open-llm.md","2024-04-19-the-ethics-of-advanced-ai-assistants",{"id":13701,"data":13703,"filePath":13709,"digest":13710,"rendered":13711,"legacyId":13719},{"title":13704,"description":13705,"summary":13705,"pubDate":13706,"source":6423,"url":13707,"thumbnail":13708},"The ethics of advanced AI assistants","Exploring the promise and risks of a future with more capable AI",["Date","2024-04-19T10:00:00.000Z"],"https://deepmind.google/discover/blog/the-ethics-of-advanced-ai-assistants/","https://lh3.googleusercontent.com/28MrwSZMny-Gf_FVYJS0z3JbnfLXzRLNAF2BA0YQ7rbcrZWdNNwddfFsWVh_n7C31N8oXBmWexFbyce4jzaX3FSNt3EXG6mSLSlXaSx70Mc7Q0s7FF4=w1200-h630-n-nu","src/content/posts/2024-04-19-the-ethics-of-advanced-ai-assistants.md","fd4f9a183bb012cb",{"html":25,"metadata":13712},{"headings":13713,"localImagePaths":13714,"remoteImagePaths":13715,"frontmatter":13716,"imagePaths":13718},[],[],[],{"title":13704,"description":13705,"summary":13705,"pubDate":13717,"source":6423,"url":13707,"thumbnail":13708},"Fri, 19 Apr 2024 10:00:00 +0000",[],"2024-04-19-the-ethics-of-advanced-ai-assistants.md","2024-04-19-the-open-medical-llm-leaderboard-benchmarking-large-language-models-in-healthcare",{"id":13720,"data":13722,"filePath":13728,"digest":13729,"rendered":13730,"legacyId":13738},{"title":13723,"description":25,"summary":13724,"pubDate":13725,"source":2720,"url":13726,"thumbnail":13727},"The Open Medical-LLM Leaderboard: Benchmarking Large Language Models in Healthcare","The Open Medical-LLM Leaderboard: Benchmarking Large Language Models in Healthcare Over the years, L...",["Date","2024-04-19T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-medicalllm","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_medicalllm.png","src/content/posts/2024-04-19-the-open-medical-llm-leaderboard-benchmarking-large-language-models-in-healthcare.md","1104a22816756287",{"html":25,"metadata":13731},{"headings":13732,"localImagePaths":13733,"remoteImagePaths":13734,"frontmatter":13735,"imagePaths":13737},[],[],[],{"title":13723,"description":25,"summary":13724,"pubDate":13736,"source":2720,"url":13726,"thumbnail":13727},"Fri, 19 Apr 2024 00:00:00 GMT",[],"2024-04-19-the-open-medical-llm-leaderboard-benchmarking-large-language-models-in-healthcare.md","2024-04-19-the-instruction-hierarchy-training-llms-to-prioritize-privileged-instructions",{"id":13739,"data":13741,"filePath":13746,"digest":13747,"rendered":13748,"legacyId":13756},{"title":13742,"description":13743,"summary":13743,"pubDate":13744,"source":19,"url":13745,"thumbnail":21},"The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions","Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts.",["Date","2024-04-19T19:00:00.000Z"],"https://openai.com/blog/the-instruction-hierarchy","src/content/posts/2024-04-19-the-instruction-hierarchy-training-llms-to-prioritize-privileged-instructions.md","5546a3d24387b149",{"html":25,"metadata":13749},{"headings":13750,"localImagePaths":13751,"remoteImagePaths":13752,"frontmatter":13753,"imagePaths":13755},[],[],[],{"title":13742,"description":13743,"summary":13743,"pubDate":13754,"source":19,"url":13745,"thumbnail":21},"Fri, 19 Apr 2024 19:00:00 GMT",[],"2024-04-19-the-instruction-hierarchy-training-llms-to-prioritize-privileged-instructions.md","2024-04-22-jack-of-all-trades-master-of-some-a-multi-purpose-transformer-agent",{"id":13757,"data":13759,"filePath":13765,"digest":13766,"rendered":13767,"legacyId":13775},{"title":13760,"description":25,"summary":13761,"pubDate":13762,"source":2720,"url":13763,"thumbnail":13764},"Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent","Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent Introduction We're excited to ...",["Date","2024-04-22T00:00:00.000Z"],"https://huggingface.co/blog/jat","https://huggingface.co/blog/assets/jat/thumbnail.png","src/content/posts/2024-04-22-jack-of-all-trades-master-of-some-a-multi-purpose-transformer-agent.md","4506ec894f4c3648",{"html":25,"metadata":13768},{"headings":13769,"localImagePaths":13770,"remoteImagePaths":13771,"frontmatter":13772,"imagePaths":13774},[],[],[],{"title":13760,"description":25,"summary":13761,"pubDate":13773,"source":2720,"url":13763,"thumbnail":13764},"Mon, 22 Apr 2024 00:00:00 GMT",[],"2024-04-22-jack-of-all-trades-master-of-some-a-multi-purpose-transformer-agent.md","2024-04-23-introducing-more-enterprise-grade-features-for-api-customers",{"id":13776,"data":13778,"filePath":13783,"digest":13784,"rendered":13785,"legacyId":13793},{"title":13779,"description":13780,"summary":13780,"pubDate":13781,"source":19,"url":13782,"thumbnail":21},"Introducing more enterprise-grade features for API customers","Increasing enterprise support with more security features and controls, updates to our Assistants API, and tools to better manage costs.",["Date","2024-04-23T00:00:00.000Z"],"https://openai.com/blog/more-enterprise-grade-features-for-api-customers","src/content/posts/2024-04-23-introducing-more-enterprise-grade-features-for-api-customers.md","575847baa41b44c8",{"html":25,"metadata":13786},{"headings":13787,"localImagePaths":13788,"remoteImagePaths":13789,"frontmatter":13790,"imagePaths":13792},[],[],[],{"title":13779,"description":13780,"summary":13780,"pubDate":13791,"source":19,"url":13782,"thumbnail":21},"Tue, 23 Apr 2024 00:00:00 GMT",[],"2024-04-23-introducing-more-enterprise-grade-features-for-api-customers.md","2024-04-23-introducing-the-open-chain-of-thought-leaderboard",{"id":13794,"data":13796,"filePath":13802,"digest":13803,"rendered":13804,"legacyId":13811},{"title":13797,"description":25,"summary":13798,"pubDate":13799,"source":2720,"url":13800,"thumbnail":13801},"Introducing the Open Chain of Thought Leaderboard","Introducing the Open Chain of Thought Leaderboard Chain-of-thought prompting is emerging as a powerf...",["Date","2024-04-23T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-cot","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_cot.png","src/content/posts/2024-04-23-introducing-the-open-chain-of-thought-leaderboard.md","5a5f73d3423f54d5",{"html":25,"metadata":13805},{"headings":13806,"localImagePaths":13807,"remoteImagePaths":13808,"frontmatter":13809,"imagePaths":13810},[],[],[],{"title":13797,"description":25,"summary":13798,"pubDate":13791,"source":2720,"url":13800,"thumbnail":13801},[],"2024-04-23-introducing-the-open-chain-of-thought-leaderboard.md","2024-04-23-openais-commitment-to-child-safety-adopting-safety-by-design-principles",{"id":13812,"data":13814,"filePath":13818,"digest":13819,"rendered":13820,"legacyId":13827},{"title":13815,"description":25,"summary":25,"pubDate":13816,"source":19,"url":13817,"thumbnail":21},"OpenAI’s commitment to child safety: adopting safety by design principles",["Date","2024-04-23T00:00:00.000Z"],"https://openai.com/blog/child-safety-adopting-sbd-principles","src/content/posts/2024-04-23-openais-commitment-to-child-safety-adopting-safety-by-design-principles.md","16815ead778d2da0",{"html":25,"metadata":13821},{"headings":13822,"localImagePaths":13823,"remoteImagePaths":13824,"frontmatter":13825,"imagePaths":13826},[],[],[],{"title":13815,"description":25,"summary":25,"pubDate":13791,"source":19,"url":13817,"thumbnail":21},[],"2024-04-23-openais-commitment-to-child-safety-adopting-safety-by-design-principles.md","2024-04-24-accelerating-the-development-of-life-saving-treatments",{"id":13828,"data":13830,"filePath":13835,"digest":13836,"rendered":13837,"legacyId":13845},{"title":13831,"description":13832,"summary":13832,"pubDate":13833,"source":19,"url":13834,"thumbnail":21},"Accelerating the development of life-saving treatments","Accelerating the development of life-saving treatments.",["Date","2024-04-24T00:00:00.000Z"],"https://openai.com/blog/moderna","src/content/posts/2024-04-24-accelerating-the-development-of-life-saving-treatments.md","1e36732794d2c847",{"html":25,"metadata":13838},{"headings":13839,"localImagePaths":13840,"remoteImagePaths":13841,"frontmatter":13842,"imagePaths":13844},[],[],[],{"title":13831,"description":13832,"summary":13832,"pubDate":13843,"source":19,"url":13834,"thumbnail":21},"Wed, 24 Apr 2024 00:00:00 GMT",[],"2024-04-24-accelerating-the-development-of-life-saving-treatments.md","2024-04-24-gpt-4-api-general-availability-and-deprecation-of-older-models-in-the-completions-api",{"id":13846,"data":13848,"filePath":13853,"digest":13854,"rendered":13855,"legacyId":13862},{"title":13849,"description":13850,"summary":13850,"pubDate":13851,"source":19,"url":13852,"thumbnail":21},"GPT-4 API general availability and deprecation of older models in the Completions API","GPT-3.5 Turbo, DALL·E and Whisper APIs are also generally available, and we are releasing a deprecation plan for older models of the Completions API, which will retire at the beginning of 2024.",["Date","2024-04-24T00:00:00.000Z"],"https://openai.com/blog/gpt-4-api-general-availability","src/content/posts/2024-04-24-gpt-4-api-general-availability-and-deprecation-of-older-models-in-the-completions-api.md","f3e13531f0a6da7a",{"html":25,"metadata":13856},{"headings":13857,"localImagePaths":13858,"remoteImagePaths":13859,"frontmatter":13860,"imagePaths":13861},[],[],[],{"title":13849,"description":13850,"summary":13850,"pubDate":13843,"source":19,"url":13852,"thumbnail":21},[],"2024-04-24-gpt-4-api-general-availability-and-deprecation-of-older-models-in-the-completions-api.md","2024-04-24-introducing-chatgpt-and-whisper-apis",{"id":13863,"data":13865,"filePath":13870,"digest":13871,"rendered":13872,"legacyId":13879},{"title":13866,"description":13867,"summary":13867,"pubDate":13868,"source":19,"url":13869,"thumbnail":21},"Introducing ChatGPT and Whisper APIs","Developers can now integrate ChatGPT and Whisper models into their apps and products through our API.",["Date","2024-04-24T00:00:00.000Z"],"https://openai.com/blog/introducing-chatgpt-and-whisper-apis","src/content/posts/2024-04-24-introducing-chatgpt-and-whisper-apis.md","189d571a61de3132",{"html":25,"metadata":13873},{"headings":13874,"localImagePaths":13875,"remoteImagePaths":13876,"frontmatter":13877,"imagePaths":13878},[],[],[],{"title":13866,"description":13867,"summary":13867,"pubDate":13843,"source":19,"url":13869,"thumbnail":21},[],"2024-04-24-introducing-chatgpt-and-whisper-apis.md","2024-04-29-starcoder2-instruct-fully-transparent-and-permissive-self-alignment-for-code-generation",{"id":13880,"data":13882,"filePath":13888,"digest":13889,"rendered":13890,"legacyId":13898},{"title":13883,"description":25,"summary":13884,"pubDate":13885,"source":2720,"url":13886,"thumbnail":13887},"StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation","StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation Instruction...",["Date","2024-04-29T00:00:00.000Z"],"https://huggingface.co/blog/sc2-instruct","https://huggingface.co/blog/assets/sc2-instruct/sc2-instruct-banner.png","src/content/posts/2024-04-29-starcoder2-instruct-fully-transparent-and-permissive-self-alignment-for-code-generation.md","6aad9c150149a91e",{"html":25,"metadata":13891},{"headings":13892,"localImagePaths":13893,"remoteImagePaths":13894,"frontmatter":13895,"imagePaths":13897},[],[],[],{"title":13883,"description":25,"summary":13884,"pubDate":13896,"source":2720,"url":13886,"thumbnail":13887},"Mon, 29 Apr 2024 00:00:00 GMT",[],"2024-04-29-starcoder2-instruct-fully-transparent-and-permissive-self-alignment-for-code-generation.md","2024-04-29-were-bringing-the-financial-times-world-class-journalism-to-chatgpt",{"id":13899,"data":13901,"filePath":13906,"digest":13907,"rendered":13908,"legacyId":13915},{"title":13902,"description":13903,"summary":13903,"pubDate":13904,"source":19,"url":13905,"thumbnail":21},"We’re bringing the Financial Times’ world-class journalism to ChatGPT","We will also collaborate on new AI experiences for FT readers.",["Date","2024-04-29T00:00:00.000Z"],"https://openai.com/blog/content-partnership-with-financial-times","src/content/posts/2024-04-29-were-bringing-the-financial-times-world-class-journalism-to-chatgpt.md","50279394ebc29926",{"html":25,"metadata":13909},{"headings":13910,"localImagePaths":13911,"remoteImagePaths":13912,"frontmatter":13913,"imagePaths":13914},[],[],[],{"title":13902,"description":13903,"summary":13903,"pubDate":13896,"source":19,"url":13905,"thumbnail":21},[],"2024-04-29-were-bringing-the-financial-times-world-class-journalism-to-chatgpt.md","2024-04-30-improving-prompt-consistency-with-structured-generations",{"id":13916,"data":13918,"filePath":13923,"digest":13924,"rendered":13925,"legacyId":13933},{"title":13919,"description":25,"summary":13920,"pubDate":13921,"source":2720,"url":13922,"thumbnail":9445},"Improving Prompt Consistency with Structured Generations","Improving Prompt Consistency with Structured Generations Recently, the Leaderboards and Evals resear...",["Date","2024-04-30T00:00:00.000Z"],"https://huggingface.co/blog/evaluation-structured-outputs","src/content/posts/2024-04-30-improving-prompt-consistency-with-structured-generations.md","de5b5cfe1e7674a6",{"html":25,"metadata":13926},{"headings":13927,"localImagePaths":13928,"remoteImagePaths":13929,"frontmatter":13930,"imagePaths":13932},[],[],[],{"title":13919,"description":25,"summary":13920,"pubDate":13931,"source":2720,"url":13922,"thumbnail":9445},"Tue, 30 Apr 2024 00:00:00 GMT",[],"2024-04-30-improving-prompt-consistency-with-structured-generations.md","2024-05-01-powerful-asr-diarization-speculative-decoding-with-hugging-face-inference-endpoints",{"id":13934,"data":13936,"filePath":13942,"digest":13943,"rendered":13944,"legacyId":13952},{"title":13937,"description":25,"summary":13938,"pubDate":13939,"source":2720,"url":13940,"thumbnail":13941},"Powerful ASR + diarization + speculative decoding with Hugging Face Inference Endpoints","Powerful ASR + diarization + speculative decoding with Hugging Face Inference Endpoints Whisper is o...",["Date","2024-05-01T00:00:00.000Z"],"https://huggingface.co/blog/asr-diarization","https://huggingface.co/blog/assets/asr-diarization/thumbnail.png","src/content/posts/2024-05-01-powerful-asr-diarization-speculative-decoding-with-hugging-face-inference-endpoints.md","e3a62da399970515",{"html":25,"metadata":13945},{"headings":13946,"localImagePaths":13947,"remoteImagePaths":13948,"frontmatter":13949,"imagePaths":13951},[],[],[],{"title":13937,"description":25,"summary":13938,"pubDate":13950,"source":2720,"url":13940,"thumbnail":13941},"Wed, 01 May 2024 00:00:00 GMT",[],"2024-05-01-powerful-asr-diarization-speculative-decoding-with-hugging-face-inference-endpoints.md","2024-05-03-bringing-the-artificial-analysis-llm-performance-leaderboard-to-hugging-face",{"id":13953,"data":13955,"filePath":13961,"digest":13962,"rendered":13963,"legacyId":13971},{"title":13956,"description":25,"summary":13957,"pubDate":13958,"source":2720,"url":13959,"thumbnail":13960},"Bringing the Artificial Analysis LLM Performance Leaderboard to Hugging Face","Bringing the Artificial Analysis LLM Performance Leaderboard to Hugging Face Building applications w...",["Date","2024-05-03T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-artificial-analysis","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_artificialanalysis.png","src/content/posts/2024-05-03-bringing-the-artificial-analysis-llm-performance-leaderboard-to-hugging-face.md","0c08b8bb4c469ad4",{"html":25,"metadata":13964},{"headings":13965,"localImagePaths":13966,"remoteImagePaths":13967,"frontmatter":13968,"imagePaths":13970},[],[],[],{"title":13956,"description":25,"summary":13957,"pubDate":13969,"source":2720,"url":13959,"thumbnail":13960},"Fri, 03 May 2024 00:00:00 GMT",[],"2024-05-03-bringing-the-artificial-analysis-llm-performance-leaderboard-to-hugging-face.md","2024-05-03-google-deepmind-at-iclr-2024",{"id":13972,"data":13974,"filePath":13980,"digest":13981,"rendered":13982,"legacyId":13990},{"title":13975,"description":13976,"summary":13976,"pubDate":13977,"source":6423,"url":13978,"thumbnail":13979},"Google DeepMind at ICLR 2024","Developing next-gen AI agents, exploring new modalities, and pioneering foundational learning",["Date","2024-05-03T13:39:00.000Z"],"https://deepmind.google/discover/blog/google-deepmind-at-iclr-2024/","https://lh3.googleusercontent.com/8PzKGooudBtamqh9keU_q7O0ex5XxGgIIK3BKQNAVEV6WDzIkfadsbNPhU0QCg5PurFGnAOSOClrM9dQHIGvOEe9MPluA5uhyFcun3FvNMBfPI63mWk=w1200-h630-n-nu","src/content/posts/2024-05-03-google-deepmind-at-iclr-2024.md","b51fc14160ad7c5c",{"html":25,"metadata":13983},{"headings":13984,"localImagePaths":13985,"remoteImagePaths":13986,"frontmatter":13987,"imagePaths":13989},[],[],[],{"title":13975,"description":13976,"summary":13976,"pubDate":13988,"source":6423,"url":13978,"thumbnail":13979},"Fri, 03 May 2024 13:39:00 +0000",[],"2024-05-03-google-deepmind-at-iclr-2024.md","2024-05-03-reimagining-secure-infrastructure-for-advanced-ai",{"id":13991,"data":13993,"filePath":13998,"digest":13999,"rendered":14000,"legacyId":14007},{"title":13994,"description":13995,"summary":13995,"pubDate":13996,"source":19,"url":13997,"thumbnail":21},"Reimagining secure infrastructure for advanced AI","Securing advanced AI systems will require an evolution in infrastructure security. We’re calling for research and investment in six security measures that we believe will play key roles in protecting advanced AI. Protecting, exploring, and applying advanced artificial intelligence (AI) is our strategic imperative. OpenAI’s mission is to deliver positive impact of advanced AI to everything from healthcare to science to education – and yes, even to cybersecurity. That work begins with building secure, trustworthy AI systems and protecting the underlying technologies from those who seek to subvert our work to cause harm.",["Date","2024-05-03T00:00:00.000Z"],"https://openai.com/blog/reimagining-secure-infrastructure-for-advanced-ai","src/content/posts/2024-05-03-reimagining-secure-infrastructure-for-advanced-ai.md","d3a570ba5d4a9294",{"html":25,"metadata":14001},{"headings":14002,"localImagePaths":14003,"remoteImagePaths":14004,"frontmatter":14005,"imagePaths":14006},[],[],[],{"title":13994,"description":13995,"summary":13995,"pubDate":13969,"source":19,"url":13997,"thumbnail":21},[],"2024-05-03-reimagining-secure-infrastructure-for-advanced-ai.md","2024-05-05-introducing-the-open-leaderboard-for-hebrew-llms",{"id":14008,"data":14010,"filePath":14016,"digest":14017,"rendered":14018,"legacyId":14026},{"title":14011,"description":25,"summary":14012,"pubDate":14013,"source":2720,"url":14014,"thumbnail":14015},"Introducing the Open Leaderboard for Hebrew LLMs!","Introducing the Open Leaderboard for Hebrew LLMs! This project addresses the critical need for advan...",["Date","2024-05-05T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-hebrew","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_hebrew.png","src/content/posts/2024-05-05-introducing-the-open-leaderboard-for-hebrew-llms.md","bb7cd354b6848009",{"html":25,"metadata":14019},{"headings":14020,"localImagePaths":14021,"remoteImagePaths":14022,"frontmatter":14023,"imagePaths":14025},[],[],[],{"title":14011,"description":25,"summary":14012,"pubDate":14024,"source":2720,"url":14014,"thumbnail":14015},"Sun, 05 May 2024 00:00:00 GMT",[],"2024-05-05-introducing-the-open-leaderboard-for-hebrew-llms.md","2024-05-06-api-partnership-with-stack-overflow",{"id":14027,"data":14029,"filePath":14034,"digest":14035,"rendered":14036,"legacyId":14044},{"title":14030,"description":14031,"summary":14031,"pubDate":14032,"source":19,"url":14033,"thumbnail":21},"API Partnership with Stack Overflow","API Partnership with Stack Overflow Stack Overflow and OpenAI today announced a new API partnership that will empower developers with the collective strengths of the world’s leading knowledge platform for highly technical content with the world’s most popular LLM models for AI development.",["Date","2024-05-06T00:00:00.000Z"],"https://openai.com/blog/api-partnership-with-stack-overflow","src/content/posts/2024-05-06-api-partnership-with-stack-overflow.md","b65625e4c18e4b67",{"html":25,"metadata":14037},{"headings":14038,"localImagePaths":14039,"remoteImagePaths":14040,"frontmatter":14041,"imagePaths":14043},[],[],[],{"title":14030,"description":14031,"summary":14031,"pubDate":14042,"source":19,"url":14033,"thumbnail":21},"Mon, 06 May 2024 00:00:00 GMT",[],"2024-05-06-api-partnership-with-stack-overflow.md","2024-05-07-our-approach-to-data-and-ai",{"id":14045,"data":14047,"filePath":14052,"digest":14053,"rendered":14054,"legacyId":14062},{"title":14048,"description":14049,"summary":14049,"pubDate":14050,"source":19,"url":14051,"thumbnail":21},"Our approach to data and AI","Just over a year after launching ChatGPT, AI is changing how we live, work and learn. It’s also raised important conversations about data in the age of AI. More on our approach, a new Media Manager for creators and content owners, and where we’re headed.",["Date","2024-05-07T00:00:00.000Z"],"https://openai.com/blog/approach-to-data-and-ai","src/content/posts/2024-05-07-our-approach-to-data-and-ai.md","071abd2a39962148",{"html":25,"metadata":14055},{"headings":14056,"localImagePaths":14057,"remoteImagePaths":14058,"frontmatter":14059,"imagePaths":14061},[],[],[],{"title":14048,"description":14049,"summary":14049,"pubDate":14060,"source":19,"url":14051,"thumbnail":21},"Tue, 07 May 2024 00:00:00 GMT",[],"2024-05-07-our-approach-to-data-and-ai.md","2024-05-07-understanding-the-source-of-what-we-see-and-hear-online",{"id":14063,"data":14065,"filePath":14070,"digest":14071,"rendered":14072,"legacyId":14079},{"title":14066,"description":14067,"summary":14067,"pubDate":14068,"source":19,"url":14069,"thumbnail":21},"Understanding the source of what we see and hear online","Today we’re introducing new technology to help researchers identify content created by our tools and joining the Coalition for Content Provenance and Authenticity Steering Committee to promote industry standards.",["Date","2024-05-07T00:00:00.000Z"],"https://openai.com/blog/understanding-the-source-of-what-we-see-and-hear-online","src/content/posts/2024-05-07-understanding-the-source-of-what-we-see-and-hear-online.md","3eedf24b4a586383",{"html":25,"metadata":14073},{"headings":14074,"localImagePaths":14075,"remoteImagePaths":14076,"frontmatter":14077,"imagePaths":14078},[],[],[],{"title":14066,"description":14067,"summary":14067,"pubDate":14060,"source":19,"url":14069,"thumbnail":21},[],"2024-05-07-understanding-the-source-of-what-we-see-and-hear-online.md","2024-05-08-alphafold-3-predicts-the-structure-and-interactions-of-all-of-lifes-molecules",{"id":14080,"data":14082,"filePath":14088,"digest":14089,"rendered":14090,"legacyId":14098},{"title":14083,"description":14084,"summary":14084,"pubDate":14085,"source":6423,"url":14086,"thumbnail":14087},"AlphaFold 3 predicts the structure and interactions of all of life’s molecules","Introducing a new AI model developed by Google DeepMind and Isomorphic Labs.",["Date","2024-05-08T16:00:00.000Z"],"https://deepmind.google/discover/blog/alphafold-3-predicts-the-structure-and-interactions-of-all-lifes-molecules/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_social_share.width-1300.jpg","src/content/posts/2024-05-08-alphafold-3-predicts-the-structure-and-interactions-of-all-of-lifes-molecules.md","aff94bc51688eb56",{"html":25,"metadata":14091},{"headings":14092,"localImagePaths":14093,"remoteImagePaths":14094,"frontmatter":14095,"imagePaths":14097},[],[],[],{"title":14083,"description":14084,"summary":14084,"pubDate":14096,"source":6423,"url":14086,"thumbnail":14087},"Wed, 08 May 2024 16:00:00 +0000",[],"2024-05-08-alphafold-3-predicts-the-structure-and-interactions-of-all-of-lifes-molecules.md","2024-05-08-introducing-the-model-spec",{"id":14099,"data":14101,"filePath":14105,"digest":14106,"rendered":14107,"legacyId":14115},{"title":14102,"description":25,"summary":25,"pubDate":14103,"source":19,"url":14104,"thumbnail":21},"Introducing the Model Spec",["Date","2024-05-08T00:00:00.000Z"],"https://openai.com/blog/introducing-the-model-spec","src/content/posts/2024-05-08-introducing-the-model-spec.md","5d778455f3669036",{"html":25,"metadata":14108},{"headings":14109,"localImagePaths":14110,"remoteImagePaths":14111,"frontmatter":14112,"imagePaths":14114},[],[],[],{"title":14102,"description":25,"summary":25,"pubDate":14113,"source":19,"url":14104,"thumbnail":21},"Wed, 08 May 2024 00:00:00 GMT",[],"2024-05-08-introducing-the-model-spec.md","2024-05-09-building-cost-efficient-enterprise-rag-applications-with-intel-gaudi-2-and-intel-xeon",{"id":14116,"data":14118,"filePath":14124,"digest":14125,"rendered":14126,"legacyId":14134},{"title":14119,"description":25,"summary":14120,"pubDate":14121,"source":2720,"url":14122,"thumbnail":14123},"Building Cost-Efficient Enterprise RAG applications with Intel Gaudi 2 and Intel Xeon","Building Cost-Efficient Enterprise RAG applications with Intel Gaudi 2 and Intel Xeon Retrieval-augm...",["Date","2024-05-09T00:00:00.000Z"],"https://huggingface.co/blog/cost-efficient-rag-applications-with-intel","https://huggingface.co/blog/assets/cost_efficient_rag_applications_with_intel/main.jpg","src/content/posts/2024-05-09-building-cost-efficient-enterprise-rag-applications-with-intel-gaudi-2-and-intel-xeon.md","52d41a994a70d6d8",{"html":25,"metadata":14127},{"headings":14128,"localImagePaths":14129,"remoteImagePaths":14130,"frontmatter":14131,"imagePaths":14133},[],[],[],{"title":14119,"description":25,"summary":14120,"pubDate":14132,"source":2720,"url":14122,"thumbnail":14123},"Thu, 09 May 2024 00:00:00 GMT",[],"2024-05-09-building-cost-efficient-enterprise-rag-applications-with-intel-gaudi-2-and-intel-xeon.md","2024-05-09-subscribe-to-enterprise-hub-with-your-aws-account",{"id":14135,"data":14137,"filePath":14142,"digest":14143,"rendered":14144,"legacyId":14151},{"title":14138,"description":25,"summary":14139,"pubDate":14140,"source":2720,"url":14141,"thumbnail":10169},"Subscribe to Enterprise Hub with your AWS Account","Subscribe to Enterprise Hub with your AWS Account You can now upgrade your Hugging Face Organization...",["Date","2024-05-09T00:00:00.000Z"],"https://huggingface.co/blog/enterprise-hub-aws-marketplace","src/content/posts/2024-05-09-subscribe-to-enterprise-hub-with-your-aws-account.md","3d866df91d9e1bd3",{"html":25,"metadata":14145},{"headings":14146,"localImagePaths":14147,"remoteImagePaths":14148,"frontmatter":14149,"imagePaths":14150},[],[],[],{"title":14138,"description":25,"summary":14139,"pubDate":14132,"source":2720,"url":14141,"thumbnail":10169},[],"2024-05-09-subscribe-to-enterprise-hub-with-your-aws-account.md","2024-05-13-hello-gpt-4o",{"id":14152,"data":14154,"filePath":14159,"digest":14160,"rendered":14161,"legacyId":14169},{"title":14155,"description":14156,"summary":14156,"pubDate":14157,"source":19,"url":14158,"thumbnail":21},"Hello GPT-4o","We’re announcing GPT-4 Omni, our new flagship model which can reason across audio, vision, and text in real time.",["Date","2024-05-13T10:05:00.000Z"],"https://openai.com/blog/hello-gpt-4o","src/content/posts/2024-05-13-hello-gpt-4o.md","0c8735de71234adc",{"html":25,"metadata":14162},{"headings":14163,"localImagePaths":14164,"remoteImagePaths":14165,"frontmatter":14166,"imagePaths":14168},[],[],[],{"title":14155,"description":14156,"summary":14156,"pubDate":14167,"source":19,"url":14158,"thumbnail":21},"Mon, 13 May 2024 10:05:00 GMT",[],"2024-05-13-hello-gpt-4o.md","2024-05-13-introducing-gpt-4o-and-more-tools-to-chatgpt-free-users",{"id":14170,"data":14172,"filePath":14177,"digest":14178,"rendered":14179,"legacyId":14187},{"title":14173,"description":14174,"summary":14174,"pubDate":14175,"source":19,"url":14176,"thumbnail":21},"Introducing GPT-4o and more tools to ChatGPT free users","Introducing GPT-4o and more tools to ChatGPT free users We are launching our newest flagship model and making more capabilities available for free in ChatGPT.",["Date","2024-05-13T10:00:00.000Z"],"https://openai.com/blog/gpt-4o-and-more-tools-to-chatgpt-free","src/content/posts/2024-05-13-introducing-gpt-4o-and-more-tools-to-chatgpt-free-users.md","e6e7a3d18c8f9edf",{"html":25,"metadata":14180},{"headings":14181,"localImagePaths":14182,"remoteImagePaths":14183,"frontmatter":14184,"imagePaths":14186},[],[],[],{"title":14173,"description":14174,"summary":14174,"pubDate":14185,"source":19,"url":14176,"thumbnail":21},"Mon, 13 May 2024 10:00:00 GMT",[],"2024-05-13-introducing-gpt-4o-and-more-tools-to-chatgpt-free-users.md","2024-05-13-license-to-call-introducing-transformers-agents-20",{"id":14188,"data":14190,"filePath":14196,"digest":14197,"rendered":14198,"legacyId":14206},{"title":14191,"description":25,"summary":14192,"pubDate":14193,"source":2720,"url":14194,"thumbnail":14195},"License to Call: Introducing Transformers Agents 2.0","License to Call: Introducing Transformers Agents 2.0 TL;DR We are releasing Transformers Agents 2.0!...",["Date","2024-05-13T00:00:00.000Z"],"https://huggingface.co/blog/agents","https://huggingface.co/blog/assets/agents/thumbnail.png","src/content/posts/2024-05-13-license-to-call-introducing-transformers-agents-20.md","db434e19eb4f63c5",{"html":25,"metadata":14199},{"headings":14200,"localImagePaths":14201,"remoteImagePaths":14202,"frontmatter":14203,"imagePaths":14205},[],[],[],{"title":14191,"description":25,"summary":14192,"pubDate":14204,"source":2720,"url":14194,"thumbnail":14195},"Mon, 13 May 2024 00:00:00 GMT",[],"2024-05-13-license-to-call-introducing-transformers-agents-20.md","2024-05-13-spring-update",{"id":14207,"data":14209,"filePath":14214,"digest":14215,"rendered":14216,"legacyId":14223},{"title":14210,"description":14211,"summary":14211,"pubDate":14212,"source":19,"url":14213,"thumbnail":21},"Spring Update","Introducing GPT-4o and making more capabilities available for free in ChatGPT.",["Date","2024-05-13T10:00:00.000Z"],"https://openai.com/blog/spring-update","src/content/posts/2024-05-13-spring-update.md","0a67bae2b2084fc9",{"html":25,"metadata":14217},{"headings":14218,"localImagePaths":14219,"remoteImagePaths":14220,"frontmatter":14221,"imagePaths":14222},[],[],[],{"title":14210,"description":14211,"summary":14211,"pubDate":14185,"source":19,"url":14213,"thumbnail":21},[],"2024-05-13-spring-update.md","2024-05-14-collaborating-with-carlyle-to-chart-the-future-of-private-equity",{"id":14224,"data":14226,"filePath":14230,"digest":14231,"rendered":14232,"legacyId":14240},{"title":14227,"description":14227,"summary":14227,"pubDate":14228,"source":19,"url":14229,"thumbnail":21},"Collaborating with Carlyle to Chart the Future of Private Equity",["Date","2024-05-14T08:00:00.000Z"],"https://openai.com/blog/collaborating-with-carlyle-to-chart-the-future-of-private-equity","src/content/posts/2024-05-14-collaborating-with-carlyle-to-chart-the-future-of-private-equity.md","b4b8298d9d5db3ce",{"html":25,"metadata":14233},{"headings":14234,"localImagePaths":14235,"remoteImagePaths":14236,"frontmatter":14237,"imagePaths":14239},[],[],[],{"title":14227,"description":14227,"summary":14227,"pubDate":14238,"source":19,"url":14229,"thumbnail":21},"Tue, 14 May 2024 08:00:00 GMT",[],"2024-05-14-collaborating-with-carlyle-to-chart-the-future-of-private-equity.md","2024-05-14-gemini-breaks-new-ground-a-faster-model-longer-context-and-ai-agents",{"id":14241,"data":14243,"filePath":14249,"digest":14250,"rendered":14251,"legacyId":14259},{"title":14244,"description":14245,"summary":14245,"pubDate":14246,"source":6423,"url":14247,"thumbnail":14248},"Gemini breaks new ground: a faster model, longer context and AI agents","We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants.",["Date","2024-05-14T17:58:00.000Z"],"https://deepmind.google/discover/blog/gemini-breaks-new-ground-a-faster-model-longer-context-and-ai-agents/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemini_Blog_Social_Share.width-1300.png","src/content/posts/2024-05-14-gemini-breaks-new-ground-a-faster-model-longer-context-and-ai-agents.md","56847c75e0fa5012",{"html":25,"metadata":14252},{"headings":14253,"localImagePaths":14254,"remoteImagePaths":14255,"frontmatter":14256,"imagePaths":14258},[],[],[],{"title":14244,"description":14245,"summary":14245,"pubDate":14257,"source":6423,"url":14247,"thumbnail":14248},"Tue, 14 May 2024 17:58:00 +0000",[],"2024-05-14-gemini-breaks-new-ground-a-faster-model-longer-context-and-ai-agents.md","2024-05-14-hugging-face-x-langchain-a-new-partner-package-in-langchain",{"id":14260,"data":14262,"filePath":14268,"digest":14269,"rendered":14270,"legacyId":14278},{"title":14263,"description":25,"summary":14264,"pubDate":14265,"source":2720,"url":14266,"thumbnail":14267},"Hugging Face x LangChain : A new partner package in LangChain","Hugging Face x LangChain : A new partner package in LangChain We are thrilled to announce the launch...",["Date","2024-05-14T00:00:00.000Z"],"https://huggingface.co/blog/langchain","https://huggingface.co/blog/assets/langchain_huggingface/thumbnail.png","src/content/posts/2024-05-14-hugging-face-x-langchain-a-new-partner-package-in-langchain.md","92bb9d728ec94e77",{"html":25,"metadata":14271},{"headings":14272,"localImagePaths":14273,"remoteImagePaths":14274,"frontmatter":14275,"imagePaths":14277},[],[],[],{"title":14263,"description":25,"summary":14264,"pubDate":14276,"source":2720,"url":14266,"thumbnail":14267},"Tue, 14 May 2024 00:00:00 GMT",[],"2024-05-14-hugging-face-x-langchain-a-new-partner-package-in-langchain.md","2024-05-14-ilya-sutskever-to-leave-openai-jakub-pachocki-announced-as-chief-scientist",{"id":14279,"data":14281,"filePath":14285,"digest":14286,"rendered":14287,"legacyId":14295},{"title":14282,"description":25,"summary":25,"pubDate":14283,"source":19,"url":14284,"thumbnail":21},"Ilya Sutskever to leave OpenAI, Jakub Pachocki announced as Chief Scientist",["Date","2024-05-14T18:00:00.000Z"],"https://openai.com/blog/jakub-pachocki-announced-as-chief-scientist","src/content/posts/2024-05-14-ilya-sutskever-to-leave-openai-jakub-pachocki-announced-as-chief-scientist.md","3b4d73316aee6877",{"html":25,"metadata":14288},{"headings":14289,"localImagePaths":14290,"remoteImagePaths":14291,"frontmatter":14292,"imagePaths":14294},[],[],[],{"title":14282,"description":25,"summary":25,"pubDate":14293,"source":19,"url":14284,"thumbnail":21},"Tue, 14 May 2024 18:00:00 GMT",[],"2024-05-14-ilya-sutskever-to-leave-openai-jakub-pachocki-announced-as-chief-scientist.md","2024-05-14-introducing-the-open-arabic-llm-leaderboard",{"id":14296,"data":14298,"filePath":14304,"digest":14305,"rendered":14306,"legacyId":14313},{"title":14299,"description":25,"summary":14300,"pubDate":14301,"source":2720,"url":14302,"thumbnail":14303},"Introducing the Open Arabic LLM Leaderboard","Introducing the Open Arabic LLM Leaderboard The Open Arabic LLM Leaderboard (OALL) is designed to ad...",["Date","2024-05-14T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-arabic","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_arabic.png","src/content/posts/2024-05-14-introducing-the-open-arabic-llm-leaderboard.md","4ad241691a48e2f5",{"html":25,"metadata":14307},{"headings":14308,"localImagePaths":14309,"remoteImagePaths":14310,"frontmatter":14311,"imagePaths":14312},[],[],[],{"title":14299,"description":25,"summary":14300,"pubDate":14276,"source":2720,"url":14302,"thumbnail":14303},[],"2024-05-14-introducing-the-open-arabic-llm-leaderboard.md","2024-05-14-new-generative-media-models-and-tools-built-with-and-for-creators",{"id":14314,"data":14316,"filePath":14322,"digest":14323,"rendered":14324,"legacyId":14332},{"title":14317,"description":14318,"summary":14318,"pubDate":14319,"source":6423,"url":14320,"thumbnail":14321},"New generative media models and tools, built with and for creators","We’re introducing Veo, our most capable model for generating high-definition video, and Imagen 3, our highest quality text-to-image model. We’re also sharing new demo recordings created with our Music AI Sandbox.",["Date","2024-05-14T17:57:00.000Z"],"https://deepmind.google/discover/blog/new-generative-media-models-and-tools-built-with-and-for-creators/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO24_Gen_Media_Header_1.width-1300.png","src/content/posts/2024-05-14-new-generative-media-models-and-tools-built-with-and-for-creators.md","d6d10d94f6bc7af1",{"html":25,"metadata":14325},{"headings":14326,"localImagePaths":14327,"remoteImagePaths":14328,"frontmatter":14329,"imagePaths":14331},[],[],[],{"title":14317,"description":14318,"summary":14318,"pubDate":14330,"source":6423,"url":14320,"thumbnail":14321},"Tue, 14 May 2024 17:57:00 +0000",[],"2024-05-14-new-generative-media-models-and-tools-built-with-and-for-creators.md","2024-05-14-paligemma-googles-cutting-edge-open-vision-language-model",{"id":14333,"data":14335,"filePath":14341,"digest":14342,"rendered":14343,"legacyId":14350},{"title":14336,"description":25,"summary":14337,"pubDate":14338,"source":2720,"url":14339,"thumbnail":14340},"PaliGemma – Google's Cutting-Edge Open Vision Language Model","PaliGemma – Google's Cutting-Edge Open Vision Language Model Updated on 23-05-2024: We have introduc...",["Date","2024-05-14T00:00:00.000Z"],"https://huggingface.co/blog/paligemma","https://huggingface.co/blog/assets/paligemma/Paligemma.png","src/content/posts/2024-05-14-paligemma-googles-cutting-edge-open-vision-language-model.md","f1e8f76a20d97a33",{"html":25,"metadata":14344},{"headings":14345,"localImagePaths":14346,"remoteImagePaths":14347,"frontmatter":14348,"imagePaths":14349},[],[],[],{"title":14336,"description":25,"summary":14337,"pubDate":14276,"source":2720,"url":14339,"thumbnail":14340},[],"2024-05-14-paligemma-googles-cutting-edge-open-vision-language-model.md","2024-05-14-watermarking-ai-generated-text-and-video-with-synthid",{"id":14351,"data":14353,"filePath":14359,"digest":14360,"rendered":14361,"legacyId":14369},{"title":14354,"description":14355,"summary":14355,"pubDate":14356,"source":6423,"url":14357,"thumbnail":14358},"Watermarking AI-generated text and video with SynthID","Announcing our novel watermarking method for AI-generated text and video, and how we’re bringing SynthID to key Google products",["Date","2024-05-14T17:56:00.000Z"],"https://deepmind.google/discover/blog/watermarking-ai-generated-text-and-video-with-synthid/","https://lh3.googleusercontent.com/I6bH75hNf57977cub27rFEsgxhcmkLrcINfCmGUaBCr7Q1bFTIl552R_6kuqlSkUjRtsTh929u6NoQmtHcwIG-GnjvPqMeynVLY0Rc9RRvezPQS0=w1200-h630-n-nu","src/content/posts/2024-05-14-watermarking-ai-generated-text-and-video-with-synthid.md","d0cfe05ea2aa844d",{"html":25,"metadata":14362},{"headings":14363,"localImagePaths":14364,"remoteImagePaths":14365,"frontmatter":14366,"imagePaths":14368},[],[],[],{"title":14354,"description":14355,"summary":14355,"pubDate":14367,"source":6423,"url":14357,"thumbnail":14358},"Tue, 14 May 2024 17:56:00 +0000",[],"2024-05-14-watermarking-ai-generated-text-and-video-with-synthid.md","2024-05-16-creating-an-ai-powered-magic-studio",{"id":14370,"data":14372,"filePath":14377,"digest":14378,"rendered":14379,"legacyId":14387},{"title":14373,"description":14374,"summary":14374,"pubDate":14375,"source":19,"url":14376,"thumbnail":21},"Creating an AI-powered Magic Studio","Canva is a visual communication platform, enjoyed by more than 175 million people monthly to make presentations, videos, documents, websites, social media graphics and more. A majority of the world’s knowledge workers lack design training, but Canva’s combination of an easy-to-use interface, vast libraries, and time-saving tools allows anyone to create visually compelling content.",["Date","2024-05-16T00:00:00.000Z"],"https://openai.com/blog/canva","src/content/posts/2024-05-16-creating-an-ai-powered-magic-studio.md","2919d1a650ab9cda",{"html":25,"metadata":14380},{"headings":14381,"localImagePaths":14382,"remoteImagePaths":14383,"frontmatter":14384,"imagePaths":14386},[],[],[],{"title":14373,"description":14374,"summary":14374,"pubDate":14385,"source":19,"url":14376,"thumbnail":21},"Thu, 16 May 2024 00:00:00 GMT",[],"2024-05-16-creating-an-ai-powered-magic-studio.md","2024-05-16-improvements-to-data-analysis-in-chatgpt",{"id":14388,"data":14390,"filePath":14395,"digest":14396,"rendered":14397,"legacyId":14405},{"title":14391,"description":14392,"summary":14392,"pubDate":14393,"source":19,"url":14394,"thumbnail":21},"Improvements to data analysis in ChatGPT","Improvements to data analysis in ChatGPT Interact with tables and charts and add files directly from Google Drive and Microsoft OneDrive.",["Date","2024-05-16T15:00:00.000Z"],"https://openai.com/blog/improvements-to-data-analysis-in-chatgpt","src/content/posts/2024-05-16-improvements-to-data-analysis-in-chatgpt.md","03ff838ce5322a41",{"html":25,"metadata":14398},{"headings":14399,"localImagePaths":14400,"remoteImagePaths":14401,"frontmatter":14402,"imagePaths":14404},[],[],[],{"title":14391,"description":14392,"summary":14392,"pubDate":14403,"source":19,"url":14394,"thumbnail":21},"Thu, 16 May 2024 15:00:00 GMT",[],"2024-05-16-improvements-to-data-analysis-in-chatgpt.md","2024-05-16-openai-and-reddit-partnership",{"id":14406,"data":14408,"filePath":14413,"digest":14414,"rendered":14415,"legacyId":14423},{"title":14409,"description":14410,"summary":14410,"pubDate":14411,"source":19,"url":14412,"thumbnail":21},"OpenAI and Reddit Partnership","OpenAI and Reddit Partnership We’re bringing Reddit’s unique content to ChatGPT and our products.",["Date","2024-05-16T13:30:00.000Z"],"https://openai.com/blog/openai-and-reddit-partnership","src/content/posts/2024-05-16-openai-and-reddit-partnership.md","079aaa3fa73bb97c",{"html":25,"metadata":14416},{"headings":14417,"localImagePaths":14418,"remoteImagePaths":14419,"frontmatter":14420,"imagePaths":14422},[],[],[],{"title":14409,"description":14410,"summary":14410,"pubDate":14421,"source":19,"url":14412,"thumbnail":21},"Thu, 16 May 2024 13:30:00 GMT",[],"2024-05-16-openai-and-reddit-partnership.md","2024-05-16-unlocking-longer-generation-with-key-value-cache-quantization",{"id":14424,"data":14426,"filePath":14432,"digest":14433,"rendered":14434,"legacyId":14441},{"title":14427,"description":25,"summary":14428,"pubDate":14429,"source":2720,"url":14430,"thumbnail":14431},"Unlocking Longer Generation with Key-Value Cache Quantization","Unlocking Longer Generation with Key-Value Cache Quantization At Hugging Face, we are excited to sha...",["Date","2024-05-16T00:00:00.000Z"],"https://huggingface.co/blog/kv-cache-quantization","https://huggingface.co/blog/assets/kv_cache_quantization/thumbnail.png","src/content/posts/2024-05-16-unlocking-longer-generation-with-key-value-cache-quantization.md","2e152b883d979773",{"html":25,"metadata":14435},{"headings":14436,"localImagePaths":14437,"remoteImagePaths":14438,"frontmatter":14439,"imagePaths":14440},[],[],[],{"title":14427,"description":25,"summary":14428,"pubDate":14385,"source":2720,"url":14430,"thumbnail":14431},[],"2024-05-16-unlocking-longer-generation-with-key-value-cache-quantization.md","2024-05-17-introducing-the-frontier-safety-framework",{"id":14442,"data":14444,"filePath":14450,"digest":14451,"rendered":14452,"legacyId":14460},{"title":14445,"description":14446,"summary":14446,"pubDate":14447,"source":6423,"url":14448,"thumbnail":14449},"Introducing the Frontier Safety Framework","Our approach to analyzing and mitigating future risks posed by advanced AI models",["Date","2024-05-17T14:00:00.000Z"],"https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/","https://lh3.googleusercontent.com/_NVnftxEp6r9O9gnZT2_jLPpIn_nGjYp9xgl8hFhg_-fX131_koFcj6znzflexf4-MdfkSTtA060-Hh7RcvVkNkY5kQ-QBulRYDCO1Li1R1jK71G=w1200-h630-n-nu","src/content/posts/2024-05-17-introducing-the-frontier-safety-framework.md","fcb3efa73107316b",{"html":25,"metadata":14453},{"headings":14454,"localImagePaths":14455,"remoteImagePaths":14456,"frontmatter":14457,"imagePaths":14459},[],[],[],{"title":14445,"description":14446,"summary":14446,"pubDate":14458,"source":6423,"url":14448,"thumbnail":14449},"Fri, 17 May 2024 14:00:00 +0000",[],"2024-05-17-introducing-the-frontier-safety-framework.md","2024-05-19-how-the-voices-for-chatgpt-were-chosen",{"id":14461,"data":14463,"filePath":14468,"digest":14469,"rendered":14470,"legacyId":14478},{"title":14464,"description":14465,"summary":14465,"pubDate":14466,"source":19,"url":14467,"thumbnail":21},"How the voices for ChatGPT were chosen","How the voices for ChatGPT were chosen We worked with industry-leading casting and directing professionals to narrow down over 400 submissions before selecting the 5 voices.",["Date","2024-05-19T23:30:00.000Z"],"https://openai.com/blog/how-the-voices-for-chatgpt-were-chosen","src/content/posts/2024-05-19-how-the-voices-for-chatgpt-were-chosen.md","ce221a4f3b6ab9e9",{"html":25,"metadata":14471},{"headings":14472,"localImagePaths":14473,"remoteImagePaths":14474,"frontmatter":14475,"imagePaths":14477},[],[],[],{"title":14464,"description":14465,"summary":14465,"pubDate":14476,"source":19,"url":14467,"thumbnail":21},"Sun, 19 May 2024 23:30:00 GMT",[],"2024-05-19-how-the-voices-for-chatgpt-were-chosen.md","2024-05-20-looking-ahead-to-the-ai-seoul-summit",{"id":14479,"data":14481,"filePath":14487,"digest":14488,"rendered":14489,"legacyId":14497},{"title":14482,"description":14483,"summary":14483,"pubDate":14484,"source":6423,"url":14485,"thumbnail":14486},"Looking ahead to the AI Seoul Summit","How summits in Seoul, France and beyond can galvanize international cooperation on frontier AI safety",["Date","2024-05-20T07:00:00.000Z"],"https://deepmind.google/discover/blog/looking-ahead-to-the-ai-seoul-summit/","https://lh3.googleusercontent.com/LuT46lyRujmyyTlxwixL9_e8LKvzqZOGUyQUAFbTO6POaYlAqWYfEMag39UkZGsZhjs3SmW3V-s0dCjK4_81jpezAzL7c6kXuTY2MhXbv5yR4NDG8Q=w1200-h630-n-nu","src/content/posts/2024-05-20-looking-ahead-to-the-ai-seoul-summit.md","a4453ed550b52cd4",{"html":25,"metadata":14490},{"headings":14491,"localImagePaths":14492,"remoteImagePaths":14493,"frontmatter":14494,"imagePaths":14496},[],[],[],{"title":14482,"description":14483,"summary":14483,"pubDate":14495,"source":6423,"url":14485,"thumbnail":14486},"Mon, 20 May 2024 07:00:00 +0000",[],"2024-05-20-looking-ahead-to-the-ai-seoul-summit.md","2024-05-21-build-ai-on-premise-with-dell-enterprise-hub",{"id":14498,"data":14500,"filePath":14506,"digest":14507,"rendered":14508,"legacyId":14516},{"title":14501,"description":25,"summary":14502,"pubDate":14503,"source":2720,"url":14504,"thumbnail":14505},"Build AI on premise with Dell Enterprise Hub","Build AI on premise with Dell Enterprise Hub Today we announce the Dell Enterprise Hub, a new experi...",["Date","2024-05-21T00:00:00.000Z"],"https://huggingface.co/blog/dell-enterprise-hub","https://huggingface.co/blog/assets/dell-enterprise-hub/thumbnail.jpg","src/content/posts/2024-05-21-build-ai-on-premise-with-dell-enterprise-hub.md","fec5ef5092ed740a",{"html":25,"metadata":14509},{"headings":14510,"localImagePaths":14511,"remoteImagePaths":14512,"frontmatter":14513,"imagePaths":14515},[],[],[],{"title":14501,"description":25,"summary":14502,"pubDate":14514,"source":2720,"url":14504,"thumbnail":14505},"Tue, 21 May 2024 00:00:00 GMT",[],"2024-05-21-build-ai-on-premise-with-dell-enterprise-hub.md","2024-05-21-from-cloud-to-developers-hugging-face-and-microsoft-deepen-collaboration",{"id":14517,"data":14519,"filePath":14525,"digest":14526,"rendered":14527,"legacyId":14534},{"title":14520,"description":25,"summary":14521,"pubDate":14522,"source":2720,"url":14523,"thumbnail":14524},"From cloud to developers: Hugging Face and Microsoft Deepen Collaboration","From cloud to developers: Hugging Face and Microsoft Deepen Collaboration Today at Microsoft Build w...",["Date","2024-05-21T00:00:00.000Z"],"https://huggingface.co/blog/microsoft-collaboration","https://huggingface.co/blog/assets/microsoft-collaboration/thumbnail.jpg","src/content/posts/2024-05-21-from-cloud-to-developers-hugging-face-and-microsoft-deepen-collaboration.md","86155f3f8f46dc19",{"html":25,"metadata":14528},{"headings":14529,"localImagePaths":14530,"remoteImagePaths":14531,"frontmatter":14532,"imagePaths":14533},[],[],[],{"title":14520,"description":25,"summary":14521,"pubDate":14514,"source":2720,"url":14523,"thumbnail":14524},[],"2024-05-21-from-cloud-to-developers-hugging-face-and-microsoft-deepen-collaboration.md","2024-05-21-hugging-face-on-amd-instinct-mi300-gpu",{"id":14535,"data":14537,"filePath":14542,"digest":14543,"rendered":14544,"legacyId":14551},{"title":14538,"description":25,"summary":14539,"pubDate":14540,"source":2720,"url":14541,"thumbnail":11510},"Hugging Face on AMD Instinct MI300 GPU","Hugging Face on AMD Instinct MI300 GPU Join the next Hugging Cast on June 6th to ask questions to th...",["Date","2024-05-21T00:00:00.000Z"],"https://huggingface.co/blog/huggingface-amd-mi300","src/content/posts/2024-05-21-hugging-face-on-amd-instinct-mi300-gpu.md","a86b80968d8bbbb1",{"html":25,"metadata":14545},{"headings":14546,"localImagePaths":14547,"remoteImagePaths":14548,"frontmatter":14549,"imagePaths":14550},[],[],[],{"title":14538,"description":25,"summary":14539,"pubDate":14514,"source":2720,"url":14541,"thumbnail":11510},[],"2024-05-21-hugging-face-on-amd-instinct-mi300-gpu.md","2024-05-21-introducing-spaces-dev-mode-for-a-seamless-developer-experience",{"id":14552,"data":14554,"filePath":14560,"digest":14561,"rendered":14562,"legacyId":14569},{"title":14555,"description":25,"summary":14556,"pubDate":14557,"source":2720,"url":14558,"thumbnail":14559},"Introducing Spaces Dev Mode for a seamless developer experience","Introducing Spaces Dev Mode for a seamless developer experience Hugging Face Spaces makes it easy fo...",["Date","2024-05-21T00:00:00.000Z"],"https://huggingface.co/blog/spaces-dev-mode","https://huggingface.co/blog/assets/spaces-dev-mode/thumbnail.jpg","src/content/posts/2024-05-21-introducing-spaces-dev-mode-for-a-seamless-developer-experience.md","5e7fefb8fa7a0016",{"html":25,"metadata":14563},{"headings":14564,"localImagePaths":14565,"remoteImagePaths":14566,"frontmatter":14567,"imagePaths":14568},[],[],[],{"title":14555,"description":25,"summary":14556,"pubDate":14514,"source":2720,"url":14558,"thumbnail":14559},[],"2024-05-21-introducing-spaces-dev-mode-for-a-seamless-developer-experience.md","2024-05-21-openai-safety-practices",{"id":14570,"data":14572,"filePath":14577,"digest":14578,"rendered":14579,"legacyId":14587},{"title":14573,"description":14574,"summary":14574,"pubDate":14575,"source":19,"url":14576,"thumbnail":21},"OpenAI safety practices","Artificial general intelligence has the potential to benefit nearly every aspect of our lives—so it must be developed and deployed responsibly.",["Date","2024-05-21T06:00:00.000Z"],"https://openai.com/blog/openai-safety-update","src/content/posts/2024-05-21-openai-safety-practices.md","f3444d5e05cc62d0",{"html":25,"metadata":14580},{"headings":14581,"localImagePaths":14582,"remoteImagePaths":14583,"frontmatter":14584,"imagePaths":14586},[],[],[],{"title":14573,"description":14574,"summary":14574,"pubDate":14585,"source":19,"url":14576,"thumbnail":21},"Tue, 21 May 2024 06:00:00 GMT",[],"2024-05-21-openai-safety-practices.md","2024-05-22-a-landmark-multi-year-global-partnership-with-news-corp",{"id":14588,"data":14590,"filePath":14595,"digest":14596,"rendered":14597,"legacyId":14605},{"title":14591,"description":14592,"summary":14592,"pubDate":14593,"source":19,"url":14594,"thumbnail":21},"A landmark multi-year global partnership with News Corp","Companies Join Forces to Enrich OpenAI’s Generative AI Products and Platforms with Premium Journalism",["Date","2024-05-22T13:15:00.000Z"],"https://openai.com/blog/news-corp-and-openai-sign-landmark-multi-year-global-partnership","src/content/posts/2024-05-22-a-landmark-multi-year-global-partnership-with-news-corp.md","3909b0dbf2ae8441",{"html":25,"metadata":14598},{"headings":14599,"localImagePaths":14600,"remoteImagePaths":14601,"frontmatter":14602,"imagePaths":14604},[],[],[],{"title":14591,"description":14592,"summary":14592,"pubDate":14603,"source":19,"url":14594,"thumbnail":21},"Wed, 22 May 2024 13:15:00 GMT",[],"2024-05-22-a-landmark-multi-year-global-partnership-with-news-corp.md","2024-05-22-deploy-models-on-aws-inferentia2-from-hugging-face",{"id":14606,"data":14608,"filePath":14614,"digest":14615,"rendered":14616,"legacyId":14624},{"title":14609,"description":25,"summary":14610,"pubDate":14611,"source":2720,"url":14612,"thumbnail":14613},"Deploy models on AWS Inferentia2 from Hugging Face","Deploy models on AWS Inferentia2 from Hugging Face AWS Inferentia2 is the latest AWS machine learnin...",["Date","2024-05-22T00:00:00.000Z"],"https://huggingface.co/blog/inferentia-inference-endpoints","https://huggingface.co/blog/assets/inferentia-inference-endpoints/thumbnail.jpg","src/content/posts/2024-05-22-deploy-models-on-aws-inferentia2-from-hugging-face.md","73da36e8611f3e67",{"html":25,"metadata":14617},{"headings":14618,"localImagePaths":14619,"remoteImagePaths":14620,"frontmatter":14621,"imagePaths":14623},[],[],[],{"title":14609,"description":25,"summary":14610,"pubDate":14622,"source":2720,"url":14612,"thumbnail":14613},"Wed, 22 May 2024 00:00:00 GMT",[],"2024-05-22-deploy-models-on-aws-inferentia2-from-hugging-face.md","2024-05-24-cyberseceval-2---a-comprehensive-evaluation-framework-for-cybersecurity-risks-and-capabilities-of-large-language-models",{"id":14625,"data":14627,"filePath":14633,"digest":14634,"rendered":14635,"legacyId":14643},{"title":14628,"description":25,"summary":14629,"pubDate":14630,"source":2720,"url":14631,"thumbnail":14632},"CyberSecEval 2 - A Comprehensive Evaluation Framework for Cybersecurity Risks and Capabilities of Large Language Models","CyberSecEval 2 - A Comprehensive Evaluation Framework for Cybersecurity Risks and Capabilities of La...",["Date","2024-05-24T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-llamaguard","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_llamaguard.png","src/content/posts/2024-05-24-cyberseceval-2---a-comprehensive-evaluation-framework-for-cybersecurity-risks-and-capabilities-of-large-language-models.md","185a6788b1abfaee",{"html":25,"metadata":14636},{"headings":14637,"localImagePaths":14638,"remoteImagePaths":14639,"frontmatter":14640,"imagePaths":14642},[],[],[],{"title":14628,"description":25,"summary":14629,"pubDate":14641,"source":2720,"url":14631,"thumbnail":14632},"Fri, 24 May 2024 00:00:00 GMT",[],"2024-05-24-cyberseceval-2---a-comprehensive-evaluation-framework-for-cybersecurity-risks-and-capabilities-of-large-language-models.md","2024-05-24-falcon-2-an-11b-parameter-pretrained-language-model-and-vlm-trained-on-over-5000b-tokens-tokens-and-11-languages",{"id":14644,"data":14646,"filePath":14652,"digest":14653,"rendered":14654,"legacyId":14661},{"title":14647,"description":25,"summary":14648,"pubDate":14649,"source":2720,"url":14650,"thumbnail":14651},"Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens tokens and 11 languages","Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 la...",["Date","2024-05-24T00:00:00.000Z"],"https://huggingface.co/blog/falcon2-11b","https://huggingface.co/blog/assets/179_falcon2-11b/thumbnail.jpg","src/content/posts/2024-05-24-falcon-2-an-11b-parameter-pretrained-language-model-and-vlm-trained-on-over-5000b-tokens-tokens-and-11-languages.md","4cd95d70832ee684",{"html":25,"metadata":14655},{"headings":14656,"localImagePaths":14657,"remoteImagePaths":14658,"frontmatter":14659,"imagePaths":14660},[],[],[],{"title":14647,"description":25,"summary":14648,"pubDate":14641,"source":2720,"url":14650,"thumbnail":14651},[],"2024-05-24-falcon-2-an-11b-parameter-pretrained-language-model-and-vlm-trained-on-over-5000b-tokens-tokens-and-11-languages.md","2024-05-28-openai-board-forms-safety-and-security-committee",{"id":14662,"data":14664,"filePath":14668,"digest":14669,"rendered":14670,"legacyId":14678},{"title":14665,"description":25,"summary":25,"pubDate":14666,"source":19,"url":14667,"thumbnail":21},"OpenAI Board Forms Safety and Security Committee",["Date","2024-05-28T03:00:00.000Z"],"https://openai.com/blog/openai-board-forms-safety-and-security-committee","src/content/posts/2024-05-28-openai-board-forms-safety-and-security-committee.md","76722097b356d6b8",{"html":25,"metadata":14671},{"headings":14672,"localImagePaths":14673,"remoteImagePaths":14674,"frontmatter":14675,"imagePaths":14677},[],[],[],{"title":14665,"description":25,"summary":25,"pubDate":14676,"source":19,"url":14667,"thumbnail":21},"Tue, 28 May 2024 03:00:00 GMT",[],"2024-05-28-openai-board-forms-safety-and-security-committee.md","2024-05-28-training-and-finetuning-embedding-models-with-sentence-transformers-v3",{"id":14679,"data":14681,"filePath":14687,"digest":14688,"rendered":14689,"legacyId":14697},{"title":14682,"description":25,"summary":14683,"pubDate":14684,"source":2720,"url":14685,"thumbnail":14686},"Training and Finetuning Embedding Models with Sentence Transformers v3","Training and Finetuning Embedding Models with Sentence Transformers v3 Sentence Transformers is a Py...",["Date","2024-05-28T00:00:00.000Z"],"https://huggingface.co/blog/train-sentence-transformers","https://huggingface.co/blog/assets/train-sentence-transformers/st-hf-thumbnail.png","src/content/posts/2024-05-28-training-and-finetuning-embedding-models-with-sentence-transformers-v3.md","cda647aeab72967f",{"html":25,"metadata":14690},{"headings":14691,"localImagePaths":14692,"remoteImagePaths":14693,"frontmatter":14694,"imagePaths":14696},[],[],[],{"title":14682,"description":25,"summary":14683,"pubDate":14695,"source":2720,"url":14685,"thumbnail":14686},"Tue, 28 May 2024 00:00:00 GMT",[],"2024-05-28-training-and-finetuning-embedding-models-with-sentence-transformers-v3.md","2024-05-29-a-content-and-product-partnership-with-vox-media",{"id":14698,"data":14700,"filePath":14705,"digest":14706,"rendered":14707,"legacyId":14715},{"title":14701,"description":14702,"summary":14702,"pubDate":14703,"source":19,"url":14704,"thumbnail":21},"A Content and Product Partnership with Vox Media","In a multi-faceted agreement, Vox Media’s content will enhance the output of OpenAI’s ChatGPT, and the company will build on OpenAI’s technology to develop products to better serve its audiences and advertisers.",["Date","2024-05-29T07:00:00.000Z"],"https://openai.com/blog/a-content-and-product-partnership-with-vox-media","src/content/posts/2024-05-29-a-content-and-product-partnership-with-vox-media.md","a09ff2c1c8e2fa5e",{"html":25,"metadata":14708},{"headings":14709,"localImagePaths":14710,"remoteImagePaths":14711,"frontmatter":14712,"imagePaths":14714},[],[],[],{"title":14701,"description":14702,"summary":14702,"pubDate":14713,"source":19,"url":14704,"thumbnail":21},"Wed, 29 May 2024 07:00:00 GMT",[],"2024-05-29-a-content-and-product-partnership-with-vox-media.md","2024-05-29-automating-customer-support-agents",{"id":14716,"data":14718,"filePath":14723,"digest":14724,"rendered":14725,"legacyId":14733},{"title":14719,"description":14720,"summary":14720,"pubDate":14721,"source":19,"url":14722,"thumbnail":21},"Automating customer support agents","MavenAGI is a new software company for the AI era. They recently launched an AI customer service agent, built on the flexibility of GPT-4, which a number of companies like Tripadvisor, Clickup and Rho are already using to save time and better serve their customers.",["Date","2024-05-29T09:00:00.000Z"],"https://openai.com/blog/mavenagi","src/content/posts/2024-05-29-automating-customer-support-agents.md","bb01b1a2ac47abbd",{"html":25,"metadata":14726},{"headings":14727,"localImagePaths":14728,"remoteImagePaths":14729,"frontmatter":14730,"imagePaths":14732},[],[],[],{"title":14719,"description":14720,"summary":14720,"pubDate":14731,"source":19,"url":14722,"thumbnail":21},"Wed, 29 May 2024 09:00:00 GMT",[],"2024-05-29-automating-customer-support-agents.md","2024-05-29-benchmarking-text-generation-inference",{"id":14734,"data":14736,"filePath":14742,"digest":14743,"rendered":14744,"legacyId":14752},{"title":14737,"description":25,"summary":14738,"pubDate":14739,"source":2720,"url":14740,"thumbnail":14741},"Benchmarking Text Generation Inference","Benchmarking Text Generation Inference In this blog we will be exploring Text Generation Inference’s...",["Date","2024-05-29T00:00:00.000Z"],"https://huggingface.co/blog/tgi-benchmarking","https://huggingface.co/blog/assets/tgi-benchmarking/tgi-benchmarking-thumbnail.png","src/content/posts/2024-05-29-benchmarking-text-generation-inference.md","c104fc00dacdb8da",{"html":25,"metadata":14745},{"headings":14746,"localImagePaths":14747,"remoteImagePaths":14748,"frontmatter":14749,"imagePaths":14751},[],[],[],{"title":14737,"description":25,"summary":14738,"pubDate":14750,"source":2720,"url":14740,"thumbnail":14741},"Wed, 29 May 2024 00:00:00 GMT",[],"2024-05-29-benchmarking-text-generation-inference.md","2024-05-29-enhancing-news-in-chatgpt-with-the-atlantic",{"id":14753,"data":14755,"filePath":14760,"digest":14761,"rendered":14762,"legacyId":14770},{"title":14756,"description":14757,"summary":14757,"pubDate":14758,"source":19,"url":14759,"thumbnail":21},"Enhancing news in ChatGPT with The Atlantic","The Atlantic is announcing a strategic content and product partnership with OpenAI, which positions The Atlantic as a premium news source within OpenAI. The Atlantic’s articles will be discoverable within OpenAI’s products, including ChatGPT, and as a partner, The Atlantic will help to shape how news is surfaced and presented in future real-time discovery products.",["Date","2024-05-29T07:30:00.000Z"],"https://openai.com/blog/enhancing-news-in-chatgpt-with-the-atlantic","src/content/posts/2024-05-29-enhancing-news-in-chatgpt-with-the-atlantic.md","61612149155339e0",{"html":25,"metadata":14763},{"headings":14764,"localImagePaths":14765,"remoteImagePaths":14766,"frontmatter":14767,"imagePaths":14769},[],[],[],{"title":14756,"description":14757,"summary":14757,"pubDate":14768,"source":19,"url":14759,"thumbnail":21},"Wed, 29 May 2024 07:30:00 GMT",[],"2024-05-29-enhancing-news-in-chatgpt-with-the-atlantic.md","2024-05-29-the-newsroom-ai-catalyst-a-global-program-with-wan-ifra",{"id":14771,"data":14773,"filePath":14778,"digest":14779,"rendered":14780,"legacyId":14788},{"title":14774,"description":14775,"summary":14775,"pubDate":14776,"source":19,"url":14777,"thumbnail":21},"The Newsroom AI Catalyst: a global program with WAN-IFRA","We’re collaborating with WAN-IFRA, the World Association of News Publishers, to launch a global accelerator program that will assist over 100 news publishers to explore and integrate AI in their newsroom.",["Date","2024-05-29T08:00:00.000Z"],"https://openai.com/blog/newsroom-ai-catalyst-global-program-with-wan-ifra","src/content/posts/2024-05-29-the-newsroom-ai-catalyst-a-global-program-with-wan-ifra.md","e48ef12bf1f55190",{"html":25,"metadata":14781},{"headings":14782,"localImagePaths":14783,"remoteImagePaths":14784,"frontmatter":14785,"imagePaths":14787},[],[],[],{"title":14774,"description":14775,"summary":14775,"pubDate":14786,"source":19,"url":14777,"thumbnail":21},"Wed, 29 May 2024 08:00:00 GMT",[],"2024-05-29-the-newsroom-ai-catalyst-a-global-program-with-wan-ifra.md","2024-05-30-introducing-openai-for-nonprofits",{"id":14789,"data":14791,"filePath":14796,"digest":14797,"rendered":14798,"legacyId":14806},{"title":14792,"description":14793,"summary":14793,"pubDate":14794,"source":19,"url":14795,"thumbnail":21},"Introducing OpenAI for Nonprofits","We’re launching a new initiative to enhance the accessibility of our tools for nonprofit organizations, including discounted rates for ChatGPT Team and Enterprise.",["Date","2024-05-30T07:00:00.000Z"],"https://openai.com/blog/introducing-openai-for-nonprofits","src/content/posts/2024-05-30-introducing-openai-for-nonprofits.md","33580f4ce1bf6679",{"html":25,"metadata":14799},{"headings":14800,"localImagePaths":14801,"remoteImagePaths":14802,"frontmatter":14803,"imagePaths":14805},[],[],[],{"title":14792,"description":14793,"summary":14793,"pubDate":14804,"source":19,"url":14795,"thumbnail":21},"Thu, 30 May 2024 07:00:00 GMT",[],"2024-05-30-introducing-openai-for-nonprofits.md","2024-05-30-disrupting-deceptive-uses-of-ai-by-covert-influence-operations",{"id":14807,"data":14809,"filePath":14814,"digest":14815,"rendered":14816,"legacyId":14824},{"title":14810,"description":14811,"summary":14811,"pubDate":14812,"source":19,"url":14813,"thumbnail":21},"Disrupting deceptive uses of AI by covert influence operations","We’ve terminated accounts linked to covert influence operations; no significant audience increase due to our services.",["Date","2024-05-30T10:00:00.000Z"],"https://openai.com/blog/disrupting-deceptive-uses-of-AI-by-covert-influence-operations","src/content/posts/2024-05-30-disrupting-deceptive-uses-of-ai-by-covert-influence-operations.md","1574b2f947a8cb8b",{"html":25,"metadata":14817},{"headings":14818,"localImagePaths":14819,"remoteImagePaths":14820,"frontmatter":14821,"imagePaths":14823},[],[],[],{"title":14810,"description":14811,"summary":14811,"pubDate":14822,"source":19,"url":14813,"thumbnail":21},"Thu, 30 May 2024 10:00:00 GMT",[],"2024-05-30-disrupting-deceptive-uses-of-ai-by-covert-influence-operations.md","2024-05-30-openai-for-education",{"id":14825,"data":14827,"filePath":14832,"digest":14833,"rendered":14834,"legacyId":14841},{"title":14828,"description":14829,"summary":14829,"pubDate":14830,"source":19,"url":14831,"thumbnail":21},"OpenAI for Education","An affordable offering for universities to responsibly bring AI to campus.",["Date","2024-05-30T07:00:00.000Z"],"https://openai.com/blog/introducing-chatgpt-edu","src/content/posts/2024-05-30-openai-for-education.md","3d69331e788182aa",{"html":25,"metadata":14835},{"headings":14836,"localImagePaths":14837,"remoteImagePaths":14838,"frontmatter":14839,"imagePaths":14840},[],[],[],{"title":14828,"description":14829,"summary":14829,"pubDate":14804,"source":19,"url":14831,"thumbnail":21},[],"2024-05-30-openai-for-education.md","2024-05-31-space-secrets-security-update",{"id":14842,"data":14844,"filePath":14850,"digest":14851,"rendered":14852,"legacyId":14860},{"title":14845,"description":25,"summary":14846,"pubDate":14847,"source":2720,"url":14848,"thumbnail":14849},"Space secrets security update","Space secrets leak disclosure Earlier this week our team detected unauthorized access to our Spaces ...",["Date","2024-05-31T00:00:00.000Z"],"https://huggingface.co/blog/space-secrets-disclosure","https://huggingface.co/blog/assets/space-secrets-security-update/space-secrets-security-update.png","src/content/posts/2024-05-31-space-secrets-security-update.md","1224c68905577e8d",{"html":25,"metadata":14853},{"headings":14854,"localImagePaths":14855,"remoteImagePaths":14856,"frontmatter":14857,"imagePaths":14859},[],[],[],{"title":14845,"description":25,"summary":14846,"pubDate":14858,"source":2720,"url":14848,"thumbnail":14849},"Fri, 31 May 2024 00:00:00 GMT",[],"2024-05-31-space-secrets-security-update.md","2024-06-04-faster-assisted-generation-support-for-intel-gaudi",{"id":14861,"data":14863,"filePath":14869,"digest":14870,"rendered":14871,"legacyId":14879},{"title":14864,"description":25,"summary":14865,"pubDate":14866,"source":2720,"url":14867,"thumbnail":14868},"Faster assisted generation support for Intel Gaudi","Faster assisted generation support for Intel Gaudi As model sizes grow, Generative AI implementation...",["Date","2024-06-04T00:00:00.000Z"],"https://huggingface.co/blog/assisted-generation-support-gaudi","https://huggingface.co/blog/assets/assisted-generation-support-gaudi/thumbnail.png","src/content/posts/2024-06-04-faster-assisted-generation-support-for-intel-gaudi.md","83b4663479040f9f",{"html":25,"metadata":14872},{"headings":14873,"localImagePaths":14874,"remoteImagePaths":14875,"frontmatter":14876,"imagePaths":14878},[],[],[],{"title":14864,"description":25,"summary":14865,"pubDate":14877,"source":2720,"url":14867,"thumbnail":14868},"Tue, 04 Jun 2024 00:00:00 GMT",[],"2024-06-04-faster-assisted-generation-support-for-intel-gaudi.md","2024-06-05-introducing-npc-playground-a-3d-playground-to-interact-with-llm-powered-npcs",{"id":14880,"data":14882,"filePath":14888,"digest":14889,"rendered":14890,"legacyId":14898},{"title":14883,"description":25,"summary":14884,"pubDate":14885,"source":2720,"url":14886,"thumbnail":14887},"Introducing NPC-Playground, a 3D playground to interact with LLM-powered NPCs","Introducing NPC-Playground, a 3D playground to interact with LLM-powered NPCs AI-powered NPCs (Non-P...",["Date","2024-06-05T00:00:00.000Z"],"https://huggingface.co/blog/npc-gigax-cubzh","https://huggingface.co/blog/assets/181_npc-gigax-cubzh/thumbnail.png","src/content/posts/2024-06-05-introducing-npc-playground-a-3d-playground-to-interact-with-llm-powered-npcs.md","00626c8d95aa8a2b",{"html":25,"metadata":14891},{"headings":14892,"localImagePaths":14893,"remoteImagePaths":14894,"frontmatter":14895,"imagePaths":14897},[],[],[],{"title":14883,"description":25,"summary":14884,"pubDate":14896,"source":2720,"url":14886,"thumbnail":14887},"Wed, 05 Jun 2024 00:00:00 GMT",[],"2024-06-05-introducing-npc-playground-a-3d-playground-to-interact-with-llm-powered-npcs.md","2024-06-05-securing-research-infrastructure-for-advanced-ai",{"id":14899,"data":14901,"filePath":14906,"digest":14907,"rendered":14908,"legacyId":14916},{"title":14902,"description":14903,"summary":14903,"pubDate":14904,"source":19,"url":14905,"thumbnail":21},"Securing Research Infrastructure for Advanced AI","We outline our architecture that supports the secure training of frontier models.",["Date","2024-06-05T10:00:00.000Z"],"https://openai.com/blog/securing-research-infrastructure-for-advanced-ai","src/content/posts/2024-06-05-securing-research-infrastructure-for-advanced-ai.md","b2ce654b336a4262",{"html":25,"metadata":14909},{"headings":14910,"localImagePaths":14911,"remoteImagePaths":14912,"frontmatter":14913,"imagePaths":14915},[],[],[],{"title":14902,"description":14903,"summary":14903,"pubDate":14914,"source":19,"url":14905,"thumbnail":21},"Wed, 05 Jun 2024 10:00:00 GMT",[],"2024-06-05-securing-research-infrastructure-for-advanced-ai.md","2024-06-06-extracting-concepts-from-gpt-4",{"id":14917,"data":14919,"filePath":14924,"digest":14925,"rendered":14926,"legacyId":14934},{"title":14920,"description":14921,"summary":14921,"pubDate":14922,"source":19,"url":14923,"thumbnail":21},"Extracting Concepts from GPT-4","Using new techniques for scaling sparse autoencoders, we automatically identified 16 million patterns in GPT-4's computations.",["Date","2024-06-06T00:00:00.000Z"],"https://openai.com/blog/extracting-concepts-from-gpt-4","src/content/posts/2024-06-06-extracting-concepts-from-gpt-4.md","2b521dc02307d49c",{"html":25,"metadata":14927},{"headings":14928,"localImagePaths":14929,"remoteImagePaths":14930,"frontmatter":14931,"imagePaths":14933},[],[],[],{"title":14920,"description":14921,"summary":14921,"pubDate":14932,"source":19,"url":14923,"thumbnail":21},"Thu, 06 Jun 2024 00:00:00 GMT",[],"2024-06-06-extracting-concepts-from-gpt-4.md","2024-06-06-improving-indias-critical-care-infrastructure",{"id":14935,"data":14937,"filePath":14941,"digest":14942,"rendered":14943,"legacyId":14951},{"title":14938,"description":25,"summary":25,"pubDate":14939,"source":19,"url":14940,"thumbnail":21},"Improving India’s critical care infrastructure",["Date","2024-06-06T10:00:00.000Z"],"https://openai.com/blog/10bedicu","src/content/posts/2024-06-06-improving-indias-critical-care-infrastructure.md","d23b0edc40ebfa5c",{"html":25,"metadata":14944},{"headings":14945,"localImagePaths":14946,"remoteImagePaths":14947,"frontmatter":14948,"imagePaths":14950},[],[],[],{"title":14938,"description":25,"summary":25,"pubDate":14949,"source":19,"url":14940,"thumbnail":21},"Thu, 06 Jun 2024 10:00:00 GMT",[],"2024-06-06-improving-indias-critical-care-infrastructure.md","2024-06-06-launching-the-artificial-analysis-text-to-image-leaderboard-arena",{"id":14952,"data":14954,"filePath":14959,"digest":14960,"rendered":14961,"legacyId":14968},{"title":14955,"description":25,"summary":14956,"pubDate":14957,"source":2720,"url":14958,"thumbnail":13960},"Launching the Artificial Analysis Text to Image Leaderboard & Arena","Launching the Artificial Analysis Text to Image Leaderboard & Arena In two short years since the adv...",["Date","2024-06-06T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-artificial-analysis2","src/content/posts/2024-06-06-launching-the-artificial-analysis-text-to-image-leaderboard-arena.md","692da5b5f87433e4",{"html":25,"metadata":14962},{"headings":14963,"localImagePaths":14964,"remoteImagePaths":14965,"frontmatter":14966,"imagePaths":14967},[],[],[],{"title":14955,"description":25,"summary":14956,"pubDate":14932,"source":2720,"url":14958,"thumbnail":13960},[],"2024-06-06-launching-the-artificial-analysis-text-to-image-leaderboard-arena.md","2024-06-07-expanding-on-how-voice-engine-works-and-our-safety-research",{"id":14969,"data":14971,"filePath":14976,"digest":14977,"rendered":14978,"legacyId":14986},{"title":14972,"description":14973,"summary":14973,"pubDate":14974,"source":19,"url":14975,"thumbnail":21},"Expanding on how Voice Engine works and our safety research","Exploring the technology behind our text-to-speech model.",["Date","2024-06-07T17:45:00.000Z"],"https://openai.com/blog/expanding-on-how-voice-engine-works-and-our-safety-research","src/content/posts/2024-06-07-expanding-on-how-voice-engine-works-and-our-safety-research.md","78e4db5c4973e3d5",{"html":25,"metadata":14979},{"headings":14980,"localImagePaths":14981,"remoteImagePaths":14982,"frontmatter":14983,"imagePaths":14985},[],[],[],{"title":14972,"description":14973,"summary":14973,"pubDate":14984,"source":19,"url":14975,"thumbnail":21},"Fri, 07 Jun 2024 17:45:00 GMT",[],"2024-06-07-expanding-on-how-voice-engine-works-and-our-safety-research.md","2024-06-07-introducing-the-hugging-face-embedding-container-for-amazon-sagemaker",{"id":14987,"data":14989,"filePath":14995,"digest":14996,"rendered":14997,"legacyId":15005},{"title":14990,"description":25,"summary":14991,"pubDate":14992,"source":2720,"url":14993,"thumbnail":14994},"Introducing the Hugging Face Embedding Container for Amazon SageMaker","Introducing the Hugging Face Embedding Container for Amazon SageMaker We are excited to announce tha...",["Date","2024-06-07T00:00:00.000Z"],"https://huggingface.co/blog/sagemaker-huggingface-embedding","https://huggingface.co/blog/assets/sagemaker-huggingface-embedding/thumbnail.jpg","src/content/posts/2024-06-07-introducing-the-hugging-face-embedding-container-for-amazon-sagemaker.md","e0dbc31698d236d0",{"html":25,"metadata":14998},{"headings":14999,"localImagePaths":15000,"remoteImagePaths":15001,"frontmatter":15002,"imagePaths":15004},[],[],[],{"title":14990,"description":25,"summary":14991,"pubDate":15003,"source":2720,"url":14993,"thumbnail":14994},"Fri, 07 Jun 2024 00:00:00 GMT",[],"2024-06-07-introducing-the-hugging-face-embedding-container-for-amazon-sagemaker.md","2024-06-07-making-sense-of-this-mess",{"id":15006,"data":15008,"filePath":15014,"digest":15015,"rendered":15016,"legacyId":15023},{"title":15009,"description":25,"summary":15010,"pubDate":15011,"source":2720,"url":15012,"thumbnail":15013},"Making sense of this mess","Making sense of this mess When I joined Hugging Face nearly 3 years ago, the Transformers documentat...",["Date","2024-06-07T00:00:00.000Z"],"https://huggingface.co/blog/transformers-docs-redesign","https://huggingface.co/blog/assets/transformers-docs-redesign/thumbnail.png","src/content/posts/2024-06-07-making-sense-of-this-mess.md","7dab37f42942fb00",{"html":25,"metadata":15017},{"headings":15018,"localImagePaths":15019,"remoteImagePaths":15020,"frontmatter":15021,"imagePaths":15022},[],[],[],{"title":15009,"description":25,"summary":15010,"pubDate":15003,"source":2720,"url":15012,"thumbnail":15013},[],"2024-06-07-making-sense-of-this-mess.md","2024-06-10-openai-and-apple-announce-partnership",{"id":15024,"data":15026,"filePath":15031,"digest":15032,"rendered":15033,"legacyId":15041},{"title":15027,"description":15028,"summary":15028,"pubDate":15029,"source":19,"url":15030,"thumbnail":21},"OpenAI and Apple announce partnership","OpenAI and Apple announce partnership to integrate ChatGPT into Apple experiences.",["Date","2024-06-10T11:55:00.000Z"],"https://openai.com/blog/openai-and-apple-announce-partnership","src/content/posts/2024-06-10-openai-and-apple-announce-partnership.md","483723bd6f4738b9",{"html":25,"metadata":15034},{"headings":15035,"localImagePaths":15036,"remoteImagePaths":15037,"frontmatter":15038,"imagePaths":15040},[],[],[],{"title":15027,"description":15028,"summary":15028,"pubDate":15039,"source":19,"url":15030,"thumbnail":21},"Mon, 10 Jun 2024 11:55:00 GMT",[],"2024-06-10-openai-and-apple-announce-partnership.md","2024-06-10-openai-welcomes-sarah-friar-cfo-and-kevin-weil-cpo",{"id":15042,"data":15044,"filePath":15048,"digest":15049,"rendered":15050,"legacyId":15058},{"title":15045,"description":15045,"summary":15045,"pubDate":15046,"source":19,"url":15047,"thumbnail":21},"OpenAI welcomes Sarah Friar (CFO) and Kevin Weil (CPO)",["Date","2024-06-10T10:30:00.000Z"],"https://openai.com/blog/openai-welcomes-cfo-cpo","src/content/posts/2024-06-10-openai-welcomes-sarah-friar-cfo-and-kevin-weil-cpo.md","c289cb5b4cf234bc",{"html":25,"metadata":15051},{"headings":15052,"localImagePaths":15053,"remoteImagePaths":15054,"frontmatter":15055,"imagePaths":15057},[],[],[],{"title":15045,"description":15045,"summary":15045,"pubDate":15056,"source":19,"url":15047,"thumbnail":21},"Mon, 10 Jun 2024 10:30:00 GMT",[],"2024-06-10-openai-welcomes-sarah-friar-cfo-and-kevin-weil-cpo.md","2024-06-12-diffusers-welcomes-stable-diffusion-3",{"id":15059,"data":15061,"filePath":15067,"digest":15068,"rendered":15069,"legacyId":15077},{"title":15062,"description":25,"summary":15063,"pubDate":15064,"source":2720,"url":15065,"thumbnail":15066},"🧨 Diffusers welcomes Stable Diffusion 3","🧨 Diffusers welcomes Stable Diffusion 3 Stable Diffusion 3 (SD3), Stability AI’s latest iteration of...",["Date","2024-06-12T00:00:00.000Z"],"https://huggingface.co/blog/sd3","https://huggingface.co/blog/assets/sd3/thumbnail.png","src/content/posts/2024-06-12-diffusers-welcomes-stable-diffusion-3.md","2cb78455c40b2d0e",{"html":25,"metadata":15070},{"headings":15071,"localImagePaths":15072,"remoteImagePaths":15073,"frontmatter":15074,"imagePaths":15076},[],[],[],{"title":15062,"description":25,"summary":15063,"pubDate":15075,"source":2720,"url":15065,"thumbnail":15066},"Wed, 12 Jun 2024 00:00:00 GMT",[],"2024-06-12-diffusers-welcomes-stable-diffusion-3.md","2024-06-12-putting-rl-back-in-rlhf",{"id":15078,"data":15080,"filePath":15086,"digest":15087,"rendered":15088,"legacyId":15095},{"title":15081,"description":25,"summary":15082,"pubDate":15083,"source":2720,"url":15084,"thumbnail":15085},"Putting RL back in RLHF","Putting RL back in RLHF We are excited to introduce the RLOO (REINFORCE Leave One-Out) Trainer in TR...",["Date","2024-06-12T00:00:00.000Z"],"https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo","https://huggingface.co/blog/assets/putting_rl_back_in_rlhf_with_rloo/thumbnail.png","src/content/posts/2024-06-12-putting-rl-back-in-rlhf.md","65b0558bdcb1e1c4",{"html":25,"metadata":15089},{"headings":15090,"localImagePaths":15091,"remoteImagePaths":15092,"frontmatter":15093,"imagePaths":15094},[],[],[],{"title":15081,"description":25,"summary":15082,"pubDate":15075,"source":2720,"url":15084,"thumbnail":15085},[],"2024-06-12-putting-rl-back-in-rlhf.md","2024-06-13-from-deepspeed-to-fsdp-and-back-again-with-hugging-face-accelerate",{"id":15096,"data":15098,"filePath":15104,"digest":15105,"rendered":15106,"legacyId":15114},{"title":15099,"description":25,"summary":15100,"pubDate":15101,"source":2720,"url":15102,"thumbnail":15103},"From DeepSpeed to FSDP and Back Again with Hugging Face Accelerate","A Hugging Face Accelerate Story of Multiple Backends: FSDP and DeepSpeed There are two popular imple...",["Date","2024-06-13T00:00:00.000Z"],"https://huggingface.co/blog/deepspeed-to-fsdp-and-back","https://huggingface.co/blog/assets/deepspeed-to-fsdp-and-back/thumbnail.png","src/content/posts/2024-06-13-from-deepspeed-to-fsdp-and-back-again-with-hugging-face-accelerate.md","0be0ef84f5e2a64a",{"html":25,"metadata":15107},{"headings":15108,"localImagePaths":15109,"remoteImagePaths":15110,"frontmatter":15111,"imagePaths":15113},[],[],[],{"title":15099,"description":25,"summary":15100,"pubDate":15112,"source":2720,"url":15102,"thumbnail":15103},"Thu, 13 Jun 2024 00:00:00 GMT",[],"2024-06-13-from-deepspeed-to-fsdp-and-back-again-with-hugging-face-accelerate.md","2024-06-13-openai-appoints-retired-us-army-general-paul-m-nakasone-to-board-of-directors",{"id":15115,"data":15117,"filePath":15122,"digest":15123,"rendered":15124,"legacyId":15132},{"title":15118,"description":15119,"summary":15119,"pubDate":15120,"source":19,"url":15121,"thumbnail":21},"OpenAI appoints Retired U.S. Army General Paul M. Nakasone to Board of Directors","Nakasone brings cybersecurity experience to growing Board of Directors; will join the Board’s Safety and Security Committee",["Date","2024-06-13T14:00:00.000Z"],"https://openai.com/blog/openai-appoints-retired-us-army-general","src/content/posts/2024-06-13-openai-appoints-retired-us-army-general-paul-m-nakasone-to-board-of-directors.md","dc768a726dbbe2b2",{"html":25,"metadata":15125},{"headings":15126,"localImagePaths":15127,"remoteImagePaths":15128,"frontmatter":15129,"imagePaths":15131},[],[],[],{"title":15118,"description":15119,"summary":15119,"pubDate":15130,"source":19,"url":15121,"thumbnail":21},"Thu, 13 Jun 2024 14:00:00 GMT",[],"2024-06-13-openai-appoints-retired-us-army-general-paul-m-nakasone-to-board-of-directors.md","2024-06-17-generating-audio-for-video",{"id":15133,"data":15135,"filePath":15141,"digest":15142,"rendered":15143,"legacyId":15151},{"title":15136,"description":15137,"summary":15137,"pubDate":15138,"source":6423,"url":15139,"thumbnail":15140},"Generating audio for video","Video-to-audio research uses video pixels and text prompts to generate rich soundtracks",["Date","2024-06-17T16:00:00.000Z"],"https://deepmind.google/discover/blog/generating-audio-for-video/","https://lh3.googleusercontent.com/Lzihw4F171DQeSgZ9q0MUONzbt1BkbK1sOgnqvLAV3AUIQQ1UJ4niEXOTgWiiyKZrJaCpE4Q6APwV8RRQj7a86_2yDlbIV6WUzD6S_Gu2mjuZDyVWqo=w1200-h630-n-nu","src/content/posts/2024-06-17-generating-audio-for-video.md","ec419fcb70c801bf",{"html":25,"metadata":15144},{"headings":15145,"localImagePaths":15146,"remoteImagePaths":15147,"frontmatter":15148,"imagePaths":15150},[],[],[],{"title":15136,"description":15137,"summary":15137,"pubDate":15149,"source":6423,"url":15139,"thumbnail":15140},"Mon, 17 Jun 2024 16:00:00 +0000",[],"2024-06-17-generating-audio-for-video.md","2024-06-17-using-gpt-4o-reasoning-to-transform-cancer-care",{"id":15152,"data":15154,"filePath":15159,"digest":15160,"rendered":15161,"legacyId":15169},{"title":15155,"description":15156,"summary":15156,"pubDate":15157,"source":19,"url":15158,"thumbnail":21},"Using GPT-4o reasoning to transform cancer care","Color Health is working with OpenAI to pioneer a new way of accelerating cancer patients’ access to treatment. Their new Cancer Copilot application uses GPT-4o to identify missing diagnostics and create tailored workup plans, enabling healthcare providers to make evidence-based decisions about cancer screening and treatment.",["Date","2024-06-17T04:15:00.000Z"],"https://openai.com/blog/color-health","src/content/posts/2024-06-17-using-gpt-4o-reasoning-to-transform-cancer-care.md","283d45c6d3c7e08f",{"html":25,"metadata":15162},{"headings":15163,"localImagePaths":15164,"remoteImagePaths":15165,"frontmatter":15166,"imagePaths":15168},[],[],[],{"title":15155,"description":15156,"summary":15156,"pubDate":15167,"source":19,"url":15158,"thumbnail":21},"Mon, 17 Jun 2024 04:15:00 GMT",[],"2024-06-17-using-gpt-4o-reasoning-to-transform-cancer-care.md","2024-06-18-achieving-10x-growth-with-agentic-sales-prospecting",{"id":15170,"data":15172,"filePath":15176,"digest":15177,"rendered":15178,"legacyId":15186},{"title":15173,"description":25,"summary":25,"pubDate":15174,"source":19,"url":15175,"thumbnail":21},"Achieving 10x growth with agentic sales prospecting",["Date","2024-06-18T07:00:00.000Z"],"https://openai.com/blog/clay","src/content/posts/2024-06-18-achieving-10x-growth-with-agentic-sales-prospecting.md","e1c023deb2819d63",{"html":25,"metadata":15179},{"headings":15180,"localImagePaths":15181,"remoteImagePaths":15182,"frontmatter":15183,"imagePaths":15185},[],[],[],{"title":15173,"description":25,"summary":25,"pubDate":15184,"source":19,"url":15175,"thumbnail":21},"Tue, 18 Jun 2024 07:00:00 GMT",[],"2024-06-18-achieving-10x-growth-with-agentic-sales-prospecting.md","2024-06-18-bigcodebench-benchmarking-large-language-models-on-solving-practical-and-challenging-programming-tasks",{"id":15187,"data":15189,"filePath":15195,"digest":15196,"rendered":15197,"legacyId":15205},{"title":15190,"description":25,"summary":15191,"pubDate":15192,"source":2720,"url":15193,"thumbnail":15194},"BigCodeBench: Benchmarking Large Language Models on Solving Practical and Challenging Programming Tasks","BigCodeBench: The Next Generation of HumanEval HumanEval is a reference benchmark for evaluating lar...",["Date","2024-06-18T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-bigcodebench","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_bigcode.png","src/content/posts/2024-06-18-bigcodebench-benchmarking-large-language-models-on-solving-practical-and-challenging-programming-tasks.md","8ecfe16eb862dcdb",{"html":25,"metadata":15198},{"headings":15199,"localImagePaths":15200,"remoteImagePaths":15201,"frontmatter":15202,"imagePaths":15204},[],[],[],{"title":15190,"description":25,"summary":15191,"pubDate":15203,"source":2720,"url":15193,"thumbnail":15194},"Tue, 18 Jun 2024 00:00:00 GMT",[],"2024-06-18-bigcodebench-benchmarking-large-language-models-on-solving-practical-and-challenging-programming-tasks.md","2024-06-18-surging-developer-productivity-with-custom-gpts",{"id":15206,"data":15208,"filePath":15213,"digest":15214,"rendered":15215,"legacyId":15223},{"title":15209,"description":15210,"summary":15210,"pubDate":15211,"source":19,"url":15212,"thumbnail":21},"Surging developer productivity with custom GPTs","Paf adopted ChatGPT Enterprise across its entire company, with engineers using custom GPTs on a daily basis to speed up routine development tasks. Paf also integrated ChatGPT Enterprise into the grit:lab coding academy (gritlab.ax), training the next generation of software developers using an AI-augmented, systems-architecture mindset from day one. In addition to the wide range of use cases for developers and grit:lab students, 70% of Paf employees actively use ChatGPT Enterprise, spanning business teams like finance, HR, marketing, and customer support.",["Date","2024-06-18T08:45:00.000Z"],"https://openai.com/blog/paf","src/content/posts/2024-06-18-surging-developer-productivity-with-custom-gpts.md","ff74d84dac000e66",{"html":25,"metadata":15216},{"headings":15217,"localImagePaths":15218,"remoteImagePaths":15219,"frontmatter":15220,"imagePaths":15222},[],[],[],{"title":15209,"description":15210,"summary":15210,"pubDate":15221,"source":19,"url":15212,"thumbnail":21},"Tue, 18 Jun 2024 08:45:00 GMT",[],"2024-06-18-surging-developer-productivity-with-custom-gpts.md","2024-06-19-going-multimodal-how-prezi-is-leveraging-the-hub-and-the-expert-support-program-to-accelerate-their-ml-roadmap",{"id":15224,"data":15226,"filePath":15232,"digest":15233,"rendered":15234,"legacyId":15242},{"title":15227,"description":25,"summary":15228,"pubDate":15229,"source":2720,"url":15230,"thumbnail":15231},"Going multimodal: How Prezi is leveraging the Hub and the Expert Support Program to accelerate their ML roadmap","Going multimodal: How Prezi is leveraging the Hub and the Expert Support Program to accelerate their...",["Date","2024-06-19T00:00:00.000Z"],"https://huggingface.co/blog/prezi-case-study","https://huggingface.co/blog/assets/70_sempre_health/thumbnailprezi.jpg","src/content/posts/2024-06-19-going-multimodal-how-prezi-is-leveraging-the-hub-and-the-expert-support-program-to-accelerate-their-ml-roadmap.md","c40889e247daf53c",{"html":25,"metadata":15235},{"headings":15236,"localImagePaths":15237,"remoteImagePaths":15238,"frontmatter":15239,"imagePaths":15241},[],[],[],{"title":15227,"description":25,"summary":15228,"pubDate":15240,"source":2720,"url":15230,"thumbnail":15231},"Wed, 19 Jun 2024 00:00:00 GMT",[],"2024-06-19-going-multimodal-how-prezi-is-leveraging-the-hub-and-the-expert-support-program-to-accelerate-their-ml-roadmap.md","2024-06-20-a-holistic-approach-to-undesired-content-detection-in-the-real-world",{"id":15243,"data":15245,"filePath":15250,"digest":15251,"rendered":15252,"legacyId":15260},{"title":15246,"description":15247,"summary":15247,"pubDate":15248,"source":19,"url":15249,"thumbnail":21},"A Holistic Approach to Undesired Content Detection in the Real World","We present a holistic approach to building a robust and useful natural language classification system for real-world content moderation.",["Date","2024-06-20T00:00:00.000Z"],"https://openai.com/blog/a-holistic-approach-to-undesired-content-detection-in-the-real-world","src/content/posts/2024-06-20-a-holistic-approach-to-undesired-content-detection-in-the-real-world.md","65f03fdfcc218b55",{"html":25,"metadata":15253},{"headings":15254,"localImagePaths":15255,"remoteImagePaths":15256,"frontmatter":15257,"imagePaths":15259},[],[],[],{"title":15246,"description":15247,"summary":15247,"pubDate":15258,"source":19,"url":15249,"thumbnail":21},"Thu, 20 Jun 2024 00:00:00 GMT",[],"2024-06-20-a-holistic-approach-to-undesired-content-detection-in-the-real-world.md","2024-06-20-consistency-models",{"id":15261,"data":15263,"filePath":15268,"digest":15269,"rendered":15270,"legacyId":15277},{"title":15264,"description":15265,"summary":15265,"pubDate":15266,"source":19,"url":15267,"thumbnail":21},"Consistency Models","Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation.",["Date","2024-06-20T00:00:00.000Z"],"https://openai.com/blog/consistency-models","src/content/posts/2024-06-20-consistency-models.md","463f2d5dbeb7b7ec",{"html":25,"metadata":15271},{"headings":15272,"localImagePaths":15273,"remoteImagePaths":15274,"frontmatter":15275,"imagePaths":15276},[],[],[],{"title":15264,"description":15265,"summary":15265,"pubDate":15258,"source":19,"url":15267,"thumbnail":21},[],"2024-06-20-consistency-models.md","2024-06-20-data-is-better-together-a-look-back-and-forward",{"id":15278,"data":15280,"filePath":15286,"digest":15287,"rendered":15288,"legacyId":15295},{"title":15281,"description":25,"summary":15282,"pubDate":15283,"source":2720,"url":15284,"thumbnail":15285},"Data Is Better Together: A Look Back and Forward","Data Is Better Together: A Look Back and Forward For the past few months, we have been working on th...",["Date","2024-06-20T00:00:00.000Z"],"https://huggingface.co/blog/dibt","https://huggingface.co/blog/assets/dibt/thumbnail.png","src/content/posts/2024-06-20-data-is-better-together-a-look-back-and-forward.md","1ba7e9179e6a7651",{"html":25,"metadata":15289},{"headings":15290,"localImagePaths":15291,"remoteImagePaths":15292,"frontmatter":15293,"imagePaths":15294},[],[],[],{"title":15281,"description":25,"summary":15282,"pubDate":15258,"source":2720,"url":15284,"thumbnail":15285},[],"2024-06-20-data-is-better-together-a-look-back-and-forward.md","2024-06-20-empowering-defenders-through-our-cybersecurity-grant-program",{"id":15296,"data":15298,"filePath":15303,"digest":15304,"rendered":15305,"legacyId":15313},{"title":15299,"description":15300,"summary":15300,"pubDate":15301,"source":19,"url":15302,"thumbnail":21},"Empowering defenders through our Cybersecurity Grant Program","Highlighting innovative research and AI integration in cybersecurity",["Date","2024-06-20T10:00:00.000Z"],"https://openai.com/blog/empowering-defenders-through-our-cybersecurity-grant-program","src/content/posts/2024-06-20-empowering-defenders-through-our-cybersecurity-grant-program.md","7a3aa0f54db9e8c8",{"html":25,"metadata":15306},{"headings":15307,"localImagePaths":15308,"remoteImagePaths":15309,"frontmatter":15310,"imagePaths":15312},[],[],[],{"title":15299,"description":15300,"summary":15300,"pubDate":15311,"source":19,"url":15302,"thumbnail":21},"Thu, 20 Jun 2024 10:00:00 GMT",[],"2024-06-20-empowering-defenders-through-our-cybersecurity-grant-program.md","2024-06-20-improved-techniques-for-training-consistency-models",{"id":15314,"data":15316,"filePath":15321,"digest":15322,"rendered":15323,"legacyId":15330},{"title":15317,"description":15318,"summary":15318,"pubDate":15319,"source":19,"url":15320,"thumbnail":21},"Improved Techniques for Training Consistency Models","Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training.",["Date","2024-06-20T00:00:00.000Z"],"https://openai.com/blog/improved-techniques-for-training-consistency-models","src/content/posts/2024-06-20-improved-techniques-for-training-consistency-models.md","accd9238a3da2b94",{"html":25,"metadata":15324},{"headings":15325,"localImagePaths":15326,"remoteImagePaths":15327,"frontmatter":15328,"imagePaths":15329},[],[],[],{"title":15317,"description":15318,"summary":15318,"pubDate":15258,"source":19,"url":15320,"thumbnail":21},[],"2024-06-20-improved-techniques-for-training-consistency-models.md","2024-06-21-openai-acquires-rockset",{"id":15331,"data":15333,"filePath":15338,"digest":15339,"rendered":15340,"legacyId":15348},{"title":15334,"description":15335,"summary":15335,"pubDate":15336,"source":19,"url":15337,"thumbnail":21},"OpenAI acquires Rockset","OpenAI Acquires Rockset",["Date","2024-06-21T08:00:00.000Z"],"https://openai.com/blog/openai-acquires-rockset","src/content/posts/2024-06-21-openai-acquires-rockset.md","91ea512a1f9403c7",{"html":25,"metadata":15341},{"headings":15342,"localImagePaths":15343,"remoteImagePaths":15344,"frontmatter":15345,"imagePaths":15347},[],[],[],{"title":15334,"description":15335,"summary":15335,"pubDate":15346,"source":19,"url":15337,"thumbnail":21},"Fri, 21 Jun 2024 08:00:00 GMT",[],"2024-06-21-openai-acquires-rockset.md","2024-06-24-ethics-and-society-newsletter-6-building-better-ai-the-importance-of-data-quality",{"id":15349,"data":15351,"filePath":15357,"digest":15358,"rendered":15359,"legacyId":15367},{"title":15352,"description":25,"summary":15353,"pubDate":15354,"source":2720,"url":15355,"thumbnail":15356},"Ethics and Society Newsletter #6: Building Better AI: The Importance of Data Quality","Ethics and Society Newsletter #6: Building Better AI: The Importance of Data Quality In February, Re...",["Date","2024-06-24T00:00:00.000Z"],"https://huggingface.co/blog/ethics-soc-6","https://huggingface.co/blog/assets/182_ethics-soc-6/thumbnail.png","src/content/posts/2024-06-24-ethics-and-society-newsletter-6-building-better-ai-the-importance-of-data-quality.md","9356cb73d6147654",{"html":25,"metadata":15360},{"headings":15361,"localImagePaths":15362,"remoteImagePaths":15363,"frontmatter":15364,"imagePaths":15366},[],[],[],{"title":15352,"description":25,"summary":15353,"pubDate":15365,"source":2720,"url":15355,"thumbnail":15356},"Mon, 24 Jun 2024 00:00:00 GMT",[],"2024-06-24-ethics-and-society-newsletter-6-building-better-ai-the-importance-of-data-quality.md","2024-06-24-fine-tuning-florence-2---microsofts-cutting-edge-vision-language-models",{"id":15368,"data":15370,"filePath":15376,"digest":15377,"rendered":15378,"legacyId":15385},{"title":15371,"description":25,"summary":15372,"pubDate":15373,"source":2720,"url":15374,"thumbnail":15375},"Fine-tuning Florence-2 - Microsoft's Cutting-edge Vision Language Models","Fine-tuning Florence-2 - Microsoft's Cutting-edge Vision Language Models Florence-2, released by Mic...",["Date","2024-06-24T00:00:00.000Z"],"https://huggingface.co/blog/finetune-florence2","https://huggingface.co/blog/assets/182_finetune-florence/thumbnail.png","src/content/posts/2024-06-24-fine-tuning-florence-2---microsofts-cutting-edge-vision-language-models.md","c46b4b59910e9521",{"html":25,"metadata":15379},{"headings":15380,"localImagePaths":15381,"remoteImagePaths":15382,"frontmatter":15383,"imagePaths":15384},[],[],[],{"title":15371,"description":25,"summary":15372,"pubDate":15365,"source":2720,"url":15374,"thumbnail":15375},[],"2024-06-24-fine-tuning-florence-2---microsofts-cutting-edge-vision-language-models.md","2024-06-25-xlscout-unveils-paraembed-20-a-powerful-embedding-model-tailored-for-patents-and-ip-with-expert-support-from-hugging-face",{"id":15386,"data":15388,"filePath":15394,"digest":15395,"rendered":15396,"legacyId":15404},{"title":15389,"description":25,"summary":15390,"pubDate":15391,"source":2720,"url":15392,"thumbnail":15393},"XLSCOUT Unveils ParaEmbed 2.0: a Powerful Embedding Model Tailored for Patents and IP with Expert Support from Hugging Face","XLSCOUT Unveils ParaEmbed 2.0: a Powerful Embedding Model Tailored for Patents and IP with Expert Su...",["Date","2024-06-25T00:00:00.000Z"],"https://huggingface.co/blog/xlscout-case-study","https://huggingface.co/blog/assets/xlscout-case-study/thumbnail.png","src/content/posts/2024-06-25-xlscout-unveils-paraembed-20-a-powerful-embedding-model-tailored-for-patents-and-ip-with-expert-support-from-hugging-face.md","d7e0d29ec3b82592",{"html":25,"metadata":15397},{"headings":15398,"localImagePaths":15399,"remoteImagePaths":15400,"frontmatter":15401,"imagePaths":15403},[],[],[],{"title":15389,"description":25,"summary":15390,"pubDate":15402,"source":2720,"url":15392,"thumbnail":15393},"Tue, 25 Jun 2024 00:00:00 GMT",[],"2024-06-25-xlscout-unveils-paraembed-20-a-powerful-embedding-model-tailored-for-patents-and-ip-with-expert-support-from-hugging-face.md","2024-06-27-finding-gpt-4s-mistakes-with-gpt-4",{"id":15405,"data":15407,"filePath":15412,"digest":15413,"rendered":15414,"legacyId":15422},{"title":15408,"description":15409,"summary":15409,"pubDate":15410,"source":19,"url":15411,"thumbnail":21},"Finding GPT-4’s mistakes with GPT-4","CriticGPT, a model based on GPT-4, writes critiques of ChatGPT responses to help human trainers spot mistakes during RLHF",["Date","2024-06-27T10:00:00.000Z"],"https://openai.com/blog/finding-gpt4s-mistakes-with-gpt-4","src/content/posts/2024-06-27-finding-gpt-4s-mistakes-with-gpt-4.md","1be498581cdff3a5",{"html":25,"metadata":15415},{"headings":15416,"localImagePaths":15417,"remoteImagePaths":15418,"frontmatter":15419,"imagePaths":15421},[],[],[],{"title":15408,"description":15409,"summary":15409,"pubDate":15420,"source":19,"url":15411,"thumbnail":21},"Thu, 27 Jun 2024 10:00:00 GMT",[],"2024-06-27-finding-gpt-4s-mistakes-with-gpt-4.md","2024-06-27-strategic-content-partnership-with-time",{"id":15423,"data":15425,"filePath":15430,"digest":15431,"rendered":15432,"legacyId":15440},{"title":15426,"description":15427,"summary":15427,"pubDate":15428,"source":19,"url":15429,"thumbnail":21},"Strategic Content Partnership with TIME","We’re partnering with TIME and its 101 years of archival content to enhance responses and provide links to stories on Time.com",["Date","2024-06-27T06:00:00.000Z"],"https://openai.com/blog/strategic-content-partnership-with-time","src/content/posts/2024-06-27-strategic-content-partnership-with-time.md","55b63bc256b7208c",{"html":25,"metadata":15433},{"headings":15434,"localImagePaths":15435,"remoteImagePaths":15436,"frontmatter":15437,"imagePaths":15439},[],[],[],{"title":15426,"description":15427,"summary":15427,"pubDate":15438,"source":19,"url":15429,"thumbnail":21},"Thu, 27 Jun 2024 06:00:00 GMT",[],"2024-06-27-strategic-content-partnership-with-time.md","2024-06-27-welcome-gemma-2---googles-new-open-llm",{"id":15441,"data":15443,"filePath":15449,"digest":15450,"rendered":15451,"legacyId":15459},{"title":15444,"description":25,"summary":15445,"pubDate":15446,"source":2720,"url":15447,"thumbnail":15448},"Welcome Gemma 2 - Google's new open LLM","Welcome Gemma 2 - Google’s new open LLM Google released Gemma 2, the latest addition to its family o...",["Date","2024-06-27T00:00:00.000Z"],"https://huggingface.co/blog/gemma2","https://huggingface.co/blog/assets/gemma2/thumbnail.jpg","src/content/posts/2024-06-27-welcome-gemma-2---googles-new-open-llm.md","3dcfd2c0d7677f83",{"html":25,"metadata":15452},{"headings":15453,"localImagePaths":15454,"remoteImagePaths":15455,"frontmatter":15456,"imagePaths":15458},[],[],[],{"title":15444,"description":25,"summary":15445,"pubDate":15457,"source":2720,"url":15447,"thumbnail":15448},"Thu, 27 Jun 2024 00:00:00 GMT",[],"2024-06-27-welcome-gemma-2---googles-new-open-llm.md","2024-07-01-our-transformers-code-agent-beats-the-gaia-benchmark",{"id":15460,"data":15462,"filePath":15468,"digest":15469,"rendered":15470,"legacyId":15478},{"title":15463,"description":25,"summary":15464,"pubDate":15465,"source":2720,"url":15466,"thumbnail":15467},"Our Transformers Code Agent beats the GAIA benchmark!","Our Transformers Code Agent beats the GAIA benchmark! TL;DR After some experiments, we were impresse...",["Date","2024-07-01T00:00:00.000Z"],"https://huggingface.co/blog/beating-gaia","https://huggingface.co/blog/assets/beating-gaia/thumbnail.jpeg","src/content/posts/2024-07-01-our-transformers-code-agent-beats-the-gaia-benchmark.md","55efd2b9f011ee7d",{"html":25,"metadata":15471},{"headings":15472,"localImagePaths":15473,"remoteImagePaths":15474,"frontmatter":15475,"imagePaths":15477},[],[],[],{"title":15463,"description":25,"summary":15464,"pubDate":15476,"source":2720,"url":15466,"thumbnail":15467},"Mon, 01 Jul 2024 00:00:00 GMT",[],"2024-07-01-our-transformers-code-agent-beats-the-gaia-benchmark.md","2024-07-03-accelerating-protein-language-model-protst-on-intel-gaudi-2",{"id":15479,"data":15481,"filePath":15487,"digest":15488,"rendered":15489,"legacyId":15497},{"title":15482,"description":25,"summary":15483,"pubDate":15484,"source":2720,"url":15485,"thumbnail":15486},"Accelerating Protein Language Model ProtST on Intel Gaudi 2","Accelerating Protein Language Model ProtST on Intel Gaudi 2 Introduction Protein Language Models (PL...",["Date","2024-07-03T00:00:00.000Z"],"https://huggingface.co/blog/intel-protein-language-model-protst","https://huggingface.co/blog/assets/intel-protein-language-model-protst/01.jpeg","src/content/posts/2024-07-03-accelerating-protein-language-model-protst-on-intel-gaudi-2.md","f503890d48676373",{"html":25,"metadata":15490},{"headings":15491,"localImagePaths":15492,"remoteImagePaths":15493,"frontmatter":15494,"imagePaths":15496},[],[],[],{"title":15482,"description":25,"summary":15483,"pubDate":15495,"source":2720,"url":15485,"thumbnail":15486},"Wed, 03 Jul 2024 00:00:00 GMT",[],"2024-07-03-accelerating-protein-language-model-protst-on-intel-gaudi-2.md","2024-07-08-announcing-new-dataset-search-features",{"id":15498,"data":15500,"filePath":15506,"digest":15507,"rendered":15508,"legacyId":15516},{"title":15501,"description":25,"summary":15502,"pubDate":15503,"source":2720,"url":15504,"thumbnail":15505},"Announcing New Dataset Search Features","Announcing New Dataset Search Features The AI and ML community has shared more than 180,000 public d...",["Date","2024-07-08T00:00:00.000Z"],"https://huggingface.co/blog/datasets-filters","https://huggingface.co/blog/assets/datasets-filters/thumbnail.png","src/content/posts/2024-07-08-announcing-new-dataset-search-features.md","ce1d4a23a037352a",{"html":25,"metadata":15509},{"headings":15510,"localImagePaths":15511,"remoteImagePaths":15512,"frontmatter":15513,"imagePaths":15515},[],[],[],{"title":15501,"description":25,"summary":15502,"pubDate":15514,"source":2720,"url":15504,"thumbnail":15505},"Mon, 08 Jul 2024 00:00:00 GMT",[],"2024-07-08-announcing-new-dataset-search-features.md","2024-07-09-banque-des-territoires-cdc-group-x-polyconseil-x-hugging-face-enhancing-a-major-french-environmental-program-with-a-sovereign-data-solution",{"id":15517,"data":15519,"filePath":15525,"digest":15526,"rendered":15527,"legacyId":15535},{"title":15520,"description":25,"summary":15521,"pubDate":15522,"source":2720,"url":15523,"thumbnail":15524},"Banque des Territoires (CDC Group) x Polyconseil x Hugging Face: Enhancing a Major French Environmental Program with a Sovereign Data Solution","Banque des Territoires (CDC Group) x Polyconseil x Hugging Face: Enhancing a Major French Environmen...",["Date","2024-07-09T00:00:00.000Z"],"https://huggingface.co/blog/sovereign-data-solution-case-study","https://huggingface.co/blog/assets/78_ml_director_insights/cdc_poly_hf.png","src/content/posts/2024-07-09-banque-des-territoires-cdc-group-x-polyconseil-x-hugging-face-enhancing-a-major-french-environmental-program-with-a-sovereign-data-solution.md","4d2fefa61cd08921",{"html":25,"metadata":15528},{"headings":15529,"localImagePaths":15530,"remoteImagePaths":15531,"frontmatter":15532,"imagePaths":15534},[],[],[],{"title":15520,"description":25,"summary":15521,"pubDate":15533,"source":2720,"url":15523,"thumbnail":15524},"Tue, 09 Jul 2024 00:00:00 GMT",[],"2024-07-09-banque-des-territoires-cdc-group-x-polyconseil-x-hugging-face-enhancing-a-major-french-environmental-program-with-a-sovereign-data-solution.md","2024-07-09-google-cloud-tpus-made-available-to-hugging-face-users",{"id":15536,"data":15538,"filePath":15544,"digest":15545,"rendered":15546,"legacyId":15553},{"title":15539,"description":25,"summary":15540,"pubDate":15541,"source":2720,"url":15542,"thumbnail":15543},"Google Cloud TPUs made available to Hugging Face users","Google Cloud TPUs made available to Hugging Face users We're excited to share some great news! AI bu...",["Date","2024-07-09T00:00:00.000Z"],"https://huggingface.co/blog/tpu-inference-endpoints-spaces","https://huggingface.co/blog/assets/tpu-inference-endpoints-spaces/thumbnail.png","src/content/posts/2024-07-09-google-cloud-tpus-made-available-to-hugging-face-users.md","421092f15dcc8e31",{"html":25,"metadata":15547},{"headings":15548,"localImagePaths":15549,"remoteImagePaths":15550,"frontmatter":15551,"imagePaths":15552},[],[],[],{"title":15539,"description":25,"summary":15540,"pubDate":15533,"source":2720,"url":15542,"thumbnail":15543},[],"2024-07-09-google-cloud-tpus-made-available-to-hugging-face-users.md","2024-07-10-announcing-new-hugging-face-and-kerashub-integration",{"id":15554,"data":15556,"filePath":15562,"digest":15563,"rendered":15564,"legacyId":15572},{"title":15557,"description":25,"summary":15558,"pubDate":15559,"source":2720,"url":15560,"thumbnail":15561},"Announcing New Hugging Face and KerasHub integration","Announcing New Hugging Face and KerasHub integration The Hugging Face Hub is a vast repository, curr...",["Date","2024-07-10T00:00:00.000Z"],"https://huggingface.co/blog/keras-hub-integration","https://huggingface.co/blog/assets/keras-hub-integration/thumbnail.png","src/content/posts/2024-07-10-announcing-new-hugging-face-and-kerashub-integration.md","66cb101a4a417b21",{"html":25,"metadata":15565},{"headings":15566,"localImagePaths":15567,"remoteImagePaths":15568,"frontmatter":15569,"imagePaths":15571},[],[],[],{"title":15557,"description":25,"summary":15558,"pubDate":15570,"source":2720,"url":15560,"thumbnail":15561},"Wed, 10 Jul 2024 00:00:00 GMT",[],"2024-07-10-announcing-new-hugging-face-and-kerashub-integration.md","2024-07-10-experimenting-with-automatic-pii-detection-on-the-hub-using-presidio",{"id":15573,"data":15575,"filePath":15581,"digest":15582,"rendered":15583,"legacyId":15590},{"title":15576,"description":25,"summary":15577,"pubDate":15578,"source":2720,"url":15579,"thumbnail":15580},"Experimenting with Automatic PII Detection on the Hub using Presidio","Experimenting with Automatic PII Detection on the Hub using Presidio At Hugging Face, we've noticed ...",["Date","2024-07-10T00:00:00.000Z"],"https://huggingface.co/blog/presidio-pii-detection","https://huggingface.co/blog/assets/presidio-pii-detection/thumbnail.png","src/content/posts/2024-07-10-experimenting-with-automatic-pii-detection-on-the-hub-using-presidio.md","a882345c3a48ee3c",{"html":25,"metadata":15584},{"headings":15585,"localImagePaths":15586,"remoteImagePaths":15587,"frontmatter":15588,"imagePaths":15589},[],[],[],{"title":15576,"description":25,"summary":15577,"pubDate":15570,"source":2720,"url":15579,"thumbnail":15580},[],"2024-07-10-experimenting-with-automatic-pii-detection-on-the-hub-using-presidio.md","2024-07-10-openai-and-los-alamos-national-laboratory-announce-research-partnership",{"id":15591,"data":15593,"filePath":15598,"digest":15599,"rendered":15600,"legacyId":15608},{"title":15594,"description":15595,"summary":15595,"pubDate":15596,"source":19,"url":15597,"thumbnail":21},"OpenAI and Los Alamos National Laboratory announce research partnership","OpenAI and Los Alamos National Laboratory are working to develop safety evaluations to assess and measure biological capabilities and risks associated with frontier models.",["Date","2024-07-10T06:30:00.000Z"],"https://openai.com/blog/openai-and-los-alamos-national-laboratory-work-together","src/content/posts/2024-07-10-openai-and-los-alamos-national-laboratory-announce-research-partnership.md","c29de380697a09db",{"html":25,"metadata":15601},{"headings":15602,"localImagePaths":15603,"remoteImagePaths":15604,"frontmatter":15605,"imagePaths":15607},[],[],[],{"title":15594,"description":15595,"summary":15595,"pubDate":15606,"source":19,"url":15597,"thumbnail":21},"Wed, 10 Jul 2024 06:30:00 GMT",[],"2024-07-10-openai-and-los-alamos-national-laboratory-announce-research-partnership.md","2024-07-10-preference-optimization-for-vision-language-models",{"id":15609,"data":15611,"filePath":15617,"digest":15618,"rendered":15619,"legacyId":15626},{"title":15612,"description":25,"summary":15613,"pubDate":15614,"source":2720,"url":15615,"thumbnail":15616},"Preference Optimization for Vision Language Models","Preference Optimization for Vision Language Models with TRL Training models to understand and predic...",["Date","2024-07-10T00:00:00.000Z"],"https://huggingface.co/blog/dpo_vlm","https://huggingface.co/blog/assets/dpo_vlm/thumbnail.png","src/content/posts/2024-07-10-preference-optimization-for-vision-language-models.md","7897e05abb3d3faa",{"html":25,"metadata":15620},{"headings":15621,"localImagePaths":15622,"remoteImagePaths":15623,"frontmatter":15624,"imagePaths":15625},[],[],[],{"title":15612,"description":25,"summary":15613,"pubDate":15570,"source":2720,"url":15615,"thumbnail":15616},[],"2024-07-10-preference-optimization-for-vision-language-models.md","2024-07-11-how-numinamath-won-the-1st-aimo-progress-prize",{"id":15627,"data":15629,"filePath":15635,"digest":15636,"rendered":15637,"legacyId":15645},{"title":15630,"description":25,"summary":15631,"pubDate":15632,"source":2720,"url":15633,"thumbnail":15634},"How NuminaMath Won the 1st AIMO Progress Prize","How NuminaMath Won the 1st AIMO Progress Prize This year, Numina and Hugging Face collaborated to co...",["Date","2024-07-11T00:00:00.000Z"],"https://huggingface.co/blog/winning-aimo-progress-prize","https://huggingface.co/blog/assets/winning-aimo-progress-prize/thumbnail.png","src/content/posts/2024-07-11-how-numinamath-won-the-1st-aimo-progress-prize.md","8f9b2da3889b2b4e",{"html":25,"metadata":15638},{"headings":15639,"localImagePaths":15640,"remoteImagePaths":15641,"frontmatter":15642,"imagePaths":15644},[],[],[],{"title":15630,"description":25,"summary":15631,"pubDate":15643,"source":2720,"url":15633,"thumbnail":15634},"Thu, 11 Jul 2024 00:00:00 GMT",[],"2024-07-11-how-numinamath-won-the-1st-aimo-progress-prize.md","2024-07-16-how-we-leveraged-distilabel-to-create-an-argilla-20-chatbot",{"id":15646,"data":15648,"filePath":15654,"digest":15655,"rendered":15656,"legacyId":15664},{"title":15649,"description":25,"summary":15650,"pubDate":15651,"source":2720,"url":15652,"thumbnail":15653},"How we leveraged distilabel to create an Argilla 2.0 Chatbot","How we leveraged distilabel to create an Argilla 2.0 Chatbot TL;DR Discover how to build a Chatbot f...",["Date","2024-07-16T00:00:00.000Z"],"https://huggingface.co/blog/argilla-chatbot","https://huggingface.co/blog/assets/argilla-chatbot/thumbnail.png","src/content/posts/2024-07-16-how-we-leveraged-distilabel-to-create-an-argilla-20-chatbot.md","c8e44e251b337a86",{"html":25,"metadata":15657},{"headings":15658,"localImagePaths":15659,"remoteImagePaths":15660,"frontmatter":15661,"imagePaths":15663},[],[],[],{"title":15649,"description":25,"summary":15650,"pubDate":15662,"source":2720,"url":15652,"thumbnail":15653},"Tue, 16 Jul 2024 00:00:00 GMT",[],"2024-07-16-how-we-leveraged-distilabel-to-create-an-argilla-20-chatbot.md","2024-07-16-smollm---blazingly-fast-and-remarkably-powerful",{"id":15665,"data":15667,"filePath":15673,"digest":15674,"rendered":15675,"legacyId":15682},{"title":15668,"description":25,"summary":15669,"pubDate":15670,"source":2720,"url":15671,"thumbnail":15672},"SmolLM - blazingly fast and remarkably powerful","SmolLM - blazingly fast and remarkably powerful TL;DR This blog post introduces SmolLM, a family of ...",["Date","2024-07-16T00:00:00.000Z"],"https://huggingface.co/blog/smollm","https://huggingface.co/blog/assets/smollm/banner.png","src/content/posts/2024-07-16-smollm---blazingly-fast-and-remarkably-powerful.md","a25b3a69d18fee68",{"html":25,"metadata":15676},{"headings":15677,"localImagePaths":15678,"remoteImagePaths":15679,"frontmatter":15680,"imagePaths":15681},[],[],[],{"title":15668,"description":25,"summary":15669,"pubDate":15662,"source":2720,"url":15671,"thumbnail":15672},[],"2024-07-16-smollm---blazingly-fast-and-remarkably-powerful.md","2024-07-17-prover-verifier-games-improve-legibility-of-language-model-outputs",{"id":15683,"data":15685,"filePath":15690,"digest":15691,"rendered":15692,"legacyId":15700},{"title":15686,"description":15687,"summary":15687,"pubDate":15688,"source":19,"url":15689,"thumbnail":21},"Prover-Verifier Games improve legibility of language model outputs","desc",["Date","2024-07-17T10:00:00.000Z"],"https://openai.com/blog/prover-verifier-games-improve-legibility","src/content/posts/2024-07-17-prover-verifier-games-improve-legibility-of-language-model-outputs.md","642c92c1eec1ebfa",{"html":25,"metadata":15693},{"headings":15694,"localImagePaths":15695,"remoteImagePaths":15696,"frontmatter":15697,"imagePaths":15699},[],[],[],{"title":15686,"description":15687,"summary":15687,"pubDate":15698,"source":19,"url":15689,"thumbnail":21},"Wed, 17 Jul 2024 10:00:00 GMT",[],"2024-07-17-prover-verifier-games-improve-legibility-of-language-model-outputs.md","2024-07-18-docmatix---a-huge-dataset-for-document-visual-question-answering",{"id":15701,"data":15703,"filePath":15709,"digest":15710,"rendered":15711,"legacyId":15719},{"title":15704,"description":25,"summary":15705,"pubDate":15706,"source":2720,"url":15707,"thumbnail":15708},"Docmatix - a huge dataset for Document Visual Question Answering","Docmatix - A huge dataset for Document Visual Question Answering With this blog we are releasing Doc...",["Date","2024-07-18T00:00:00.000Z"],"https://huggingface.co/blog/docmatix","https://huggingface.co/blog/assets/183_docmatix/thumbnail_new.png","src/content/posts/2024-07-18-docmatix---a-huge-dataset-for-document-visual-question-answering.md","589160e67f11aadf",{"html":25,"metadata":15712},{"headings":15713,"localImagePaths":15714,"remoteImagePaths":15715,"frontmatter":15716,"imagePaths":15718},[],[],[],{"title":15704,"description":25,"summary":15705,"pubDate":15717,"source":2720,"url":15707,"thumbnail":15708},"Thu, 18 Jul 2024 00:00:00 GMT",[],"2024-07-18-docmatix---a-huge-dataset-for-document-visual-question-answering.md","2024-07-18-gpt-4o-mini-advancing-cost-efficient-intelligence",{"id":15720,"data":15722,"filePath":15727,"digest":15728,"rendered":15729,"legacyId":15737},{"title":15723,"description":15724,"summary":15724,"pubDate":15725,"source":19,"url":15726,"thumbnail":21},"GPT-4o mini: advancing cost-efficient intelligence","Introducing the most cost-efficient small model in the market",["Date","2024-07-18T10:00:00.000Z"],"https://openai.com/blog/gpt-4o-mini-advancing-cost-efficient-intelligence","src/content/posts/2024-07-18-gpt-4o-mini-advancing-cost-efficient-intelligence.md","b12c68166b0695dd",{"html":25,"metadata":15730},{"headings":15731,"localImagePaths":15732,"remoteImagePaths":15733,"frontmatter":15734,"imagePaths":15736},[],[],[],{"title":15723,"description":15724,"summary":15724,"pubDate":15735,"source":19,"url":15726,"thumbnail":21},"Thu, 18 Jul 2024 10:00:00 GMT",[],"2024-07-18-gpt-4o-mini-advancing-cost-efficient-intelligence.md","2024-07-18-new-compliance-and-administrative-tools-for-chatgpt-enterprise",{"id":15738,"data":15740,"filePath":15745,"digest":15746,"rendered":15747,"legacyId":15754},{"title":15741,"description":15742,"summary":15742,"pubDate":15743,"source":19,"url":15744,"thumbnail":21},"New compliance and administrative tools for ChatGPT Enterprise","Compliance API integrations, SCIM, and GPT controls to support compliance programs, data security, and user access at scale",["Date","2024-07-18T00:00:00.000Z"],"https://openai.com/blog/new-tools-for-chatgpt-enterprise","src/content/posts/2024-07-18-new-compliance-and-administrative-tools-for-chatgpt-enterprise.md","65351644117bc292",{"html":25,"metadata":15748},{"headings":15749,"localImagePaths":15750,"remoteImagePaths":15751,"frontmatter":15752,"imagePaths":15753},[],[],[],{"title":15741,"description":15742,"summary":15742,"pubDate":15717,"source":19,"url":15744,"thumbnail":21},[],"2024-07-18-new-compliance-and-administrative-tools-for-chatgpt-enterprise.md","2024-07-18-tgi-multi-lora-deploy-once-serve-30-models",{"id":15755,"data":15757,"filePath":15763,"digest":15764,"rendered":15765,"legacyId":15772},{"title":15758,"description":25,"summary":15759,"pubDate":15760,"source":2720,"url":15761,"thumbnail":15762},"TGI Multi-LoRA: Deploy Once, Serve 30 Models","TGI Multi-LoRA: Deploy Once, Serve 30 models Are you tired of the complexity and expense of managing...",["Date","2024-07-18T00:00:00.000Z"],"https://huggingface.co/blog/multi-lora-serving","https://huggingface.co/blog/assets/multi-lora-serving/thumbnail.png","src/content/posts/2024-07-18-tgi-multi-lora-deploy-once-serve-30-models.md","17d4c1cace686e2d",{"html":25,"metadata":15766},{"headings":15767,"localImagePaths":15768,"remoteImagePaths":15769,"frontmatter":15770,"imagePaths":15771},[],[],[],{"title":15758,"description":25,"summary":15759,"pubDate":15717,"source":2720,"url":15761,"thumbnail":15762},[],"2024-07-18-tgi-multi-lora-deploy-once-serve-30-models.md","2024-07-19-google-deepmind-at-icml-2024",{"id":15773,"data":15775,"filePath":15781,"digest":15782,"rendered":15783,"legacyId":15791},{"title":15776,"description":15777,"summary":15777,"pubDate":15778,"source":6423,"url":15779,"thumbnail":15780},"Google DeepMind at ICML 2024","Exploring AGI, the challenges of scaling and the future of multimodal generative AI",["Date","2024-07-19T10:00:00.000Z"],"https://deepmind.google/discover/blog/google-deepmind-at-icml-2024/","https://lh3.googleusercontent.com/_o0MU47bgKrTJi6uOWhc3BjWOOENkBczD2x5-tK5aMLBcljJnV-N8tZuSVN42C3d1pSWawY6NsGuoj6vvl0xMk4tpWOeUjXwlgFNZSMyJkFJ02xTauk=w1200-h630-n-nu","src/content/posts/2024-07-19-google-deepmind-at-icml-2024.md","6447a1a89cb0a741",{"html":25,"metadata":15784},{"headings":15785,"localImagePaths":15786,"remoteImagePaths":15787,"frontmatter":15788,"imagePaths":15790},[],[],[],{"title":15776,"description":15777,"summary":15777,"pubDate":15789,"source":6423,"url":15779,"thumbnail":15780},"Fri, 19 Jul 2024 10:00:00 +0000",[],"2024-07-19-google-deepmind-at-icml-2024.md","2024-07-22-wwdc-24-running-mistral-7b-with-core-ml",{"id":15792,"data":15794,"filePath":15800,"digest":15801,"rendered":15802,"legacyId":15810},{"title":15795,"description":25,"summary":15796,"pubDate":15797,"source":2720,"url":15798,"thumbnail":15799},"WWDC 24: Running Mistral 7B with Core ML","WWDC 24: Running Mistral 7B with Core ML WWDC’ 24 is the moment Apple officially unveiled Apple Inte...",["Date","2024-07-22T00:00:00.000Z"],"https://huggingface.co/blog/mistral-coreml","https://huggingface.co/blog/assets/mistral-coreml/thumbnail.png","src/content/posts/2024-07-22-wwdc-24-running-mistral-7b-with-core-ml.md","d94f5f8d281142a6",{"html":25,"metadata":15803},{"headings":15804,"localImagePaths":15805,"remoteImagePaths":15806,"frontmatter":15807,"imagePaths":15809},[],[],[],{"title":15795,"description":25,"summary":15796,"pubDate":15808,"source":2720,"url":15798,"thumbnail":15799},"Mon, 22 Jul 2024 00:00:00 GMT",[],"2024-07-22-wwdc-24-running-mistral-7b-with-core-ml.md","2024-07-23-llama-31---405b-70b-8b-with-multilinguality-and-long-context",{"id":15811,"data":15813,"filePath":15819,"digest":15820,"rendered":15821,"legacyId":15829},{"title":15814,"description":25,"summary":15815,"pubDate":15816,"source":2720,"url":15817,"thumbnail":15818},"Llama 3.1 - 405B, 70B & 8B with multilinguality and long context","Llama 3.1 - 405B, 70B & 8B with multilinguality and long context Llama 3.1 is out! Today we welcome ...",["Date","2024-07-23T00:00:00.000Z"],"https://huggingface.co/blog/llama31","https://huggingface.co/blog/assets/llama31/thumbnail.jpg","src/content/posts/2024-07-23-llama-31---405b-70b-8b-with-multilinguality-and-long-context.md","e12dca3d3b909c41",{"html":25,"metadata":15822},{"headings":15823,"localImagePaths":15824,"remoteImagePaths":15825,"frontmatter":15826,"imagePaths":15828},[],[],[],{"title":15814,"description":25,"summary":15815,"pubDate":15827,"source":2720,"url":15817,"thumbnail":15818},"Tue, 23 Jul 2024 00:00:00 GMT",[],"2024-07-23-llama-31---405b-70b-8b-with-multilinguality-and-long-context.md","2024-07-24-improving-model-safety-behavior-with-rule-based-rewards",{"id":15830,"data":15832,"filePath":15837,"digest":15838,"rendered":15839,"legacyId":15847},{"title":15833,"description":15834,"summary":15834,"pubDate":15835,"source":19,"url":15836,"thumbnail":21},"Improving Model Safety Behavior with Rule-Based Rewards","We've developed and applied a new method leveraging Rule-Based Rewards (RBRs) that aligns models to behave safely without extensive human data collection.",["Date","2024-07-24T09:00:00.000Z"],"https://openai.com/blog/improving-model-safety-behavior-with-rule-based-rewards","src/content/posts/2024-07-24-improving-model-safety-behavior-with-rule-based-rewards.md","ef6ad92475911d85",{"html":25,"metadata":15840},{"headings":15841,"localImagePaths":15842,"remoteImagePaths":15843,"frontmatter":15844,"imagePaths":15846},[],[],[],{"title":15833,"description":15834,"summary":15834,"pubDate":15845,"source":19,"url":15836,"thumbnail":21},"Wed, 24 Jul 2024 09:00:00 GMT",[],"2024-07-24-improving-model-safety-behavior-with-rule-based-rewards.md","2024-07-25-ai-achieves-silver-medal-standard-solving-international-mathematical-olympiad-problems",{"id":15848,"data":15850,"filePath":15856,"digest":15857,"rendered":15858,"legacyId":15866},{"title":15851,"description":15852,"summary":15852,"pubDate":15853,"source":6423,"url":15854,"thumbnail":15855},"AI achieves silver-medal standard solving International Mathematical Olympiad problems","Breakthrough models AlphaProof and AlphaGeometry 2 solve advanced reasoning problems in mathematics",["Date","2024-07-25T15:29:00.000Z"],"https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/","https://lh3.googleusercontent.com/2A21eFt7wdDrmMzzkenrCTuioLWGFdzU5Ao5dPH9yPtAw6QNHxZcDmoQA2_ZriU2gMjX8mzEOtfPbMCRuL5kVzLoz6efLgqT_foBXU3pxKBXTTOXXpc=w1200-h630-n-nu","src/content/posts/2024-07-25-ai-achieves-silver-medal-standard-solving-international-mathematical-olympiad-problems.md","e815bf862c059ba7",{"html":25,"metadata":15859},{"headings":15860,"localImagePaths":15861,"remoteImagePaths":15862,"frontmatter":15863,"imagePaths":15865},[],[],[],{"title":15851,"description":15852,"summary":15852,"pubDate":15864,"source":6423,"url":15854,"thumbnail":15855},"Thu, 25 Jul 2024 15:29:00 +0000",[],"2024-07-25-ai-achieves-silver-medal-standard-solving-international-mathematical-olympiad-problems.md","2024-07-25-lave-zero-shot-vqa-evaluation-on-docmatix-with-llms---do-we-still-need-fine-tuning",{"id":15867,"data":15869,"filePath":15875,"digest":15876,"rendered":15877,"legacyId":15885},{"title":15870,"description":25,"summary":15871,"pubDate":15872,"source":2720,"url":15873,"thumbnail":15874},"LAVE: Zero-shot VQA Evaluation on Docmatix with LLMs - Do We Still Need Fine-Tuning?","LAVE: Zero-shot VQA Evaluation on Docmatix with LLMs - Do We Still Need Fine-Tuning? While developin...",["Date","2024-07-25T00:00:00.000Z"],"https://huggingface.co/blog/zero-shot-vqa-docmatix","https://huggingface.co/blog/assets/184_zero_shot_docmatix/thumb.001.jpeg","src/content/posts/2024-07-25-lave-zero-shot-vqa-evaluation-on-docmatix-with-llms---do-we-still-need-fine-tuning.md","49d65e8fb4e7a9a0",{"html":25,"metadata":15878},{"headings":15879,"localImagePaths":15880,"remoteImagePaths":15881,"frontmatter":15882,"imagePaths":15884},[],[],[],{"title":15870,"description":25,"summary":15871,"pubDate":15883,"source":2720,"url":15873,"thumbnail":15874},"Thu, 25 Jul 2024 00:00:00 GMT",[],"2024-07-25-lave-zero-shot-vqa-evaluation-on-docmatix-with-llms---do-we-still-need-fine-tuning.md","2024-07-25-searchgpt-is-a-prototype-of-new-ai-search-features",{"id":15886,"data":15888,"filePath":15893,"digest":15894,"rendered":15895,"legacyId":15902},{"title":15889,"description":15890,"summary":15890,"pubDate":15891,"source":19,"url":15892,"thumbnail":21},"SearchGPT is a prototype of new AI search features","We’re testing SearchGPT, a temporary prototype of new search features that give you fast and timely answers with clear and relevant sources.",["Date","2024-07-25T00:00:00.000Z"],"https://openai.com/blog/searchgpt-prototype","src/content/posts/2024-07-25-searchgpt-is-a-prototype-of-new-ai-search-features.md","2034f1cd27a01b96",{"html":25,"metadata":15896},{"headings":15897,"localImagePaths":15898,"remoteImagePaths":15899,"frontmatter":15900,"imagePaths":15901},[],[],[],{"title":15889,"description":15890,"summary":15890,"pubDate":15883,"source":19,"url":15892,"thumbnail":21},[],"2024-07-25-searchgpt-is-a-prototype-of-new-ai-search-features.md","2024-07-29-serverless-inference-with-hugging-face-and-nvidia-nims",{"id":15903,"data":15905,"filePath":15910,"digest":15911,"rendered":15912,"legacyId":15920},{"title":15906,"description":25,"summary":15907,"pubDate":15908,"source":2720,"url":15909,"thumbnail":13037},"Serverless Inference with Hugging Face and NVIDIA NIMs","Serverless Inference with Hugging Face and NVIDIA NIM Update: This service is deprecated and no long...",["Date","2024-07-29T00:00:00.000Z"],"https://huggingface.co/blog/inference-dgx-cloud","src/content/posts/2024-07-29-serverless-inference-with-hugging-face-and-nvidia-nims.md","4b889c44550af1b2",{"html":25,"metadata":15913},{"headings":15914,"localImagePaths":15915,"remoteImagePaths":15916,"frontmatter":15917,"imagePaths":15919},[],[],[],{"title":15906,"description":25,"summary":15907,"pubDate":15918,"source":2720,"url":15909,"thumbnail":13037},"Mon, 29 Jul 2024 00:00:00 GMT",[],"2024-07-29-serverless-inference-with-hugging-face-and-nvidia-nims.md","2024-07-30-a-primer-on-the-eu-ai-act-what-it-means-for-ai-providers-and-deployers",{"id":15921,"data":15923,"filePath":15928,"digest":15929,"rendered":15930,"legacyId":15938},{"title":15924,"description":15925,"summary":15925,"pubDate":15926,"source":19,"url":15927,"thumbnail":21},"A Primer on the EU AI Act: What It Means for AI Providers and Deployers","We’re sharing a preliminary overview of the EU AI Act including upcoming deadlines and requirements, with a particular focus on prohibited and high-risk use cases",["Date","2024-07-30T00:00:00.000Z"],"https://openai.com/global-affairs/a-primer-on-the-eu-ai-act","src/content/posts/2024-07-30-a-primer-on-the-eu-ai-act-what-it-means-for-ai-providers-and-deployers.md","44d078e60f0c4fbe",{"html":25,"metadata":15931},{"headings":15932,"localImagePaths":15933,"remoteImagePaths":15934,"frontmatter":15935,"imagePaths":15937},[],[],[],{"title":15924,"description":15925,"summary":15925,"pubDate":15936,"source":19,"url":15927,"thumbnail":21},"Tue, 30 Jul 2024 00:00:00 GMT",[],"2024-07-30-a-primer-on-the-eu-ai-act-what-it-means-for-ai-providers-and-deployers.md","2024-07-30-memory-efficient-diffusion-transformers-with-quanto-and-diffusers",{"id":15939,"data":15941,"filePath":15947,"digest":15948,"rendered":15949,"legacyId":15956},{"title":15942,"description":25,"summary":15943,"pubDate":15944,"source":2720,"url":15945,"thumbnail":15946},"Memory-efficient Diffusion Transformers with Quanto and Diffusers","Memory-efficient Diffusion Transformers with Quanto and Diffusers Over the past few months, we have ...",["Date","2024-07-30T00:00:00.000Z"],"https://huggingface.co/blog/quanto-diffusers","https://huggingface.co/blog/assets/quanto-diffusers/thumbnail.png","src/content/posts/2024-07-30-memory-efficient-diffusion-transformers-with-quanto-and-diffusers.md","f361858302d6a1a2",{"html":25,"metadata":15950},{"headings":15951,"localImagePaths":15952,"remoteImagePaths":15953,"frontmatter":15954,"imagePaths":15955},[],[],[],{"title":15942,"description":25,"summary":15943,"pubDate":15936,"source":2720,"url":15945,"thumbnail":15946},[],"2024-07-30-memory-efficient-diffusion-transformers-with-quanto-and-diffusers.md","2024-07-31-gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models",{"id":15957,"data":15959,"filePath":15965,"digest":15966,"rendered":15967,"legacyId":15975},{"title":15960,"description":15961,"summary":15961,"pubDate":15962,"source":6423,"url":15963,"thumbnail":15964},"Gemma Scope: helping the safety community shed light on the inner workings of language models","Announcing a comprehensive, open suite of sparse autoencoders for language model interpretability.",["Date","2024-07-31T15:59:19.000Z"],"https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/","https://lh3.googleusercontent.com/4amJbS1Q5bh_CoBHPAc4NEn0Q13izqrskMETkJl3h2Jdku08GryCCjW6BM59OKj1-Q7-8ZFCWlgu7tIMzjRBIXImy8wlgTOxYgJ88fQvYJTye07C=w1200-h630-n-nu","src/content/posts/2024-07-31-gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models.md","5ed766db3c05645c",{"html":25,"metadata":15968},{"headings":15969,"localImagePaths":15970,"remoteImagePaths":15971,"frontmatter":15972,"imagePaths":15974},[],[],[],{"title":15960,"description":15961,"summary":15961,"pubDate":15973,"source":6423,"url":15963,"thumbnail":15964},"Wed, 31 Jul 2024 15:59:19 +0000",[],"2024-07-31-gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models.md","2024-07-31-google-releases-gemma-2-2b-shieldgemma-and-gemma-scope",{"id":15976,"data":15978,"filePath":15984,"digest":15985,"rendered":15986,"legacyId":15994},{"title":15979,"description":25,"summary":15980,"pubDate":15981,"source":2720,"url":15982,"thumbnail":15983},"Google releases Gemma 2 2B, ShieldGemma and Gemma Scope","Google releases Gemma 2 2B, ShieldGemma and Gemma Scope One month after the release of Gemma 2, Goog...",["Date","2024-07-31T00:00:00.000Z"],"https://huggingface.co/blog/gemma-july-update","https://huggingface.co/blog/assets/gemma-july-update/thumbnail.jpg","src/content/posts/2024-07-31-google-releases-gemma-2-2b-shieldgemma-and-gemma-scope.md","7913c0bcdb4e6837",{"html":25,"metadata":15987},{"headings":15988,"localImagePaths":15989,"remoteImagePaths":15990,"frontmatter":15991,"imagePaths":15993},[],[],[],{"title":15979,"description":25,"summary":15980,"pubDate":15992,"source":2720,"url":15982,"thumbnail":15983},"Wed, 31 Jul 2024 00:00:00 GMT",[],"2024-07-31-google-releases-gemma-2-2b-shieldgemma-and-gemma-scope.md","2024-08-02-mapping-the-misuse-of-generative-ai",{"id":15995,"data":15997,"filePath":16003,"digest":16004,"rendered":16005,"legacyId":16013},{"title":15998,"description":15999,"summary":15999,"pubDate":16000,"source":6423,"url":16001,"thumbnail":16002},"Mapping the misuse of generative AI","New research analyzes the misuse of multimodal generative AI today, in order to help build safer and more responsible technologies.",["Date","2024-08-02T10:50:58.000Z"],"https://deepmind.google/discover/blog/mapping-the-misuse-of-generative-ai/","https://lh3.googleusercontent.com/IzYg4pdM7_tKoEbQHE4-Em9cvFxbx2Aq4_YOQdLr6VK754c8-bJRW9LWMf1_nUraA5BfNcBjAjpIjcfF1M_qQviR8b7qyRnAiUzapq3LKVbTpoJ8Cw=w1200-h630-n-nu","src/content/posts/2024-08-02-mapping-the-misuse-of-generative-ai.md","bf735d4646980502",{"html":25,"metadata":16006},{"headings":16007,"localImagePaths":16008,"remoteImagePaths":16009,"frontmatter":16010,"imagePaths":16012},[],[],[],{"title":15998,"description":15999,"summary":15999,"pubDate":16011,"source":6423,"url":16001,"thumbnail":16002},"Fri, 02 Aug 2024 10:50:58 +0000",[],"2024-08-02-mapping-the-misuse-of-generative-ai.md","2024-08-06-2024-security-feature-highlights",{"id":16014,"data":16016,"filePath":16022,"digest":16023,"rendered":16024,"legacyId":16032},{"title":16017,"description":25,"summary":16018,"pubDate":16019,"source":2720,"url":16020,"thumbnail":16021},"2024 Security Feature Highlights","2024 Security Feature Highlights Security is a top priority at Hugging Face, and we're committed to ...",["Date","2024-08-06T00:00:00.000Z"],"https://huggingface.co/blog/2024-security-features","https://huggingface.co/blog/assets/2024-security-features/thumbnail.png","src/content/posts/2024-08-06-2024-security-feature-highlights.md","d8af0cb30db8f856",{"html":25,"metadata":16025},{"headings":16026,"localImagePaths":16027,"remoteImagePaths":16028,"frontmatter":16029,"imagePaths":16031},[],[],[],{"title":16017,"description":25,"summary":16018,"pubDate":16030,"source":2720,"url":16020,"thumbnail":16021},"Tue, 06 Aug 2024 00:00:00 GMT",[],"2024-08-06-2024-security-feature-highlights.md","2024-08-06-introducing-structured-outputs-in-the-api",{"id":16033,"data":16035,"filePath":16040,"digest":16041,"rendered":16042,"legacyId":16050},{"title":16036,"description":16037,"summary":16037,"pubDate":16038,"source":19,"url":16039,"thumbnail":21},"Introducing Structured Outputs in the API","We are introducing Structured Outputs in the API—model outputs now reliably adhere to developer-supplied JSON Schemas.",["Date","2024-08-06T10:00:00.000Z"],"https://openai.com/blog/introducing-structured-outputs-in-the-api","src/content/posts/2024-08-06-introducing-structured-outputs-in-the-api.md","7e7a5342957f03f7",{"html":25,"metadata":16043},{"headings":16044,"localImagePaths":16045,"remoteImagePaths":16046,"frontmatter":16047,"imagePaths":16049},[],[],[],{"title":16036,"description":16037,"summary":16037,"pubDate":16048,"source":19,"url":16039,"thumbnail":21},"Tue, 06 Aug 2024 10:00:00 GMT",[],"2024-08-06-introducing-structured-outputs-in-the-api.md","2024-08-06-introducing-textimage-augmentation-for-document-images",{"id":16051,"data":16053,"filePath":16059,"digest":16060,"rendered":16061,"legacyId":16068},{"title":16054,"description":25,"summary":16055,"pubDate":16056,"source":2720,"url":16057,"thumbnail":16058},"Introducing TextImage Augmentation for Document Images","Introducing Multimodal TextImage Augmentation for Document Images In this blog post, we provide a tu...",["Date","2024-08-06T00:00:00.000Z"],"https://huggingface.co/blog/doc_aug_hf_alb","https://huggingface.co/blog/assets/185_albumentations/thumbnail.png","src/content/posts/2024-08-06-introducing-textimage-augmentation-for-document-images.md","c309fce64408c727",{"html":25,"metadata":16062},{"headings":16063,"localImagePaths":16064,"remoteImagePaths":16065,"frontmatter":16066,"imagePaths":16067},[],[],[],{"title":16054,"description":25,"summary":16055,"pubDate":16030,"source":2720,"url":16057,"thumbnail":16058},[],"2024-08-06-introducing-textimage-augmentation-for-document-images.md","2024-08-07-pairing-data-with-apis-to-unlock-customer-value",{"id":16069,"data":16071,"filePath":16076,"digest":16077,"rendered":16078,"legacyId":16086},{"title":16072,"description":16073,"summary":16073,"pubDate":16074,"source":19,"url":16075,"thumbnail":21},"Pairing data with APIs to unlock customer value","Rakuten Pairs Data with AI to Unlock Customer Insights and Value",["Date","2024-08-07T16:00:00.000Z"],"https://openai.com/blog/rakuten","src/content/posts/2024-08-07-pairing-data-with-apis-to-unlock-customer-value.md","f331b5deeabdab88",{"html":25,"metadata":16079},{"headings":16080,"localImagePaths":16081,"remoteImagePaths":16082,"frontmatter":16083,"imagePaths":16085},[],[],[],{"title":16072,"description":16073,"summary":16073,"pubDate":16084,"source":19,"url":16075,"thumbnail":21},"Wed, 07 Aug 2024 16:00:00 GMT",[],"2024-08-07-pairing-data-with-apis-to-unlock-customer-value.md","2024-08-08-enabling-a-data-driven-workforce",{"id":16087,"data":16089,"filePath":16094,"digest":16095,"rendered":16096,"legacyId":16104},{"title":16090,"description":16091,"summary":16091,"pubDate":16092,"source":19,"url":16093,"thumbnail":21},"Enabling a Data-Driven Workforce","In this video, we share practical examples of how employees can use ChatGPT Enterprise to efficiently analyze data and uncover insights.",["Date","2024-08-08T00:00:00.000Z"],"https://openai.com/business/enabling-a-data-driven-workforce-webinar","src/content/posts/2024-08-08-enabling-a-data-driven-workforce.md","fefe8f43fd3886bf",{"html":25,"metadata":16097},{"headings":16098,"localImagePaths":16099,"remoteImagePaths":16100,"frontmatter":16101,"imagePaths":16103},[],[],[],{"title":16090,"description":16091,"summary":16091,"pubDate":16102,"source":19,"url":16093,"thumbnail":21},"Thu, 08 Aug 2024 00:00:00 GMT",[],"2024-08-08-enabling-a-data-driven-workforce.md","2024-08-08-gpt-4o-system-card-external-testers-acknowledgements",{"id":16105,"data":16107,"filePath":16112,"digest":16113,"rendered":16114,"legacyId":16122},{"title":16108,"description":16109,"summary":16109,"pubDate":16110,"source":19,"url":16111,"thumbnail":21},"GPT-4o System Card External Testers Acknowledgements","GPT-4o system card external testers acknowledgements",["Date","2024-08-08T10:00:00.000Z"],"https://openai.com/blog/gpt-4o-system-card/external-testers-acknowledgements","src/content/posts/2024-08-08-gpt-4o-system-card-external-testers-acknowledgements.md","54cf27931f7c4785",{"html":25,"metadata":16115},{"headings":16116,"localImagePaths":16117,"remoteImagePaths":16118,"frontmatter":16119,"imagePaths":16121},[],[],[],{"title":16108,"description":16109,"summary":16109,"pubDate":16120,"source":19,"url":16111,"thumbnail":21},"Thu, 08 Aug 2024 10:00:00 GMT",[],"2024-08-08-gpt-4o-system-card-external-testers-acknowledgements.md","2024-08-08-gpt-4o-system-card",{"id":16123,"data":16125,"filePath":16130,"digest":16131,"rendered":16132,"legacyId":16139},{"title":16126,"description":16127,"summary":16127,"pubDate":16128,"source":19,"url":16129,"thumbnail":21},"GPT-4o System Card","This report outlines the safety work carried out prior to releasing GPT-4o including external red teaming, frontier risk evaluations according to our Preparedness Framework, and an overview of the mitigations we built in to address key risk areas.",["Date","2024-08-08T00:00:00.000Z"],"https://openai.com/blog/gpt-4o-system-card","src/content/posts/2024-08-08-gpt-4o-system-card.md","8b9ad31b29c699e8",{"html":25,"metadata":16133},{"headings":16134,"localImagePaths":16135,"remoteImagePaths":16136,"frontmatter":16137,"imagePaths":16138},[],[],[],{"title":16126,"description":16127,"summary":16127,"pubDate":16102,"source":19,"url":16129,"thumbnail":21},[],"2024-08-08-gpt-4o-system-card.md","2024-08-08-xethub-is-joining-hugging-face",{"id":16140,"data":16142,"filePath":16148,"digest":16149,"rendered":16150,"legacyId":16157},{"title":16143,"description":25,"summary":16144,"pubDate":16145,"source":2720,"url":16146,"thumbnail":16147},"XetHub is joining Hugging Face!","XetHub is joining Hugging Face! We are super excited to officially announce that Hugging Face acquir...",["Date","2024-08-08T00:00:00.000Z"],"https://huggingface.co/blog/xethub-joins-hf","https://huggingface.co/blog/assets/xethub-joins-hf/thumbnail.png","src/content/posts/2024-08-08-xethub-is-joining-hugging-face.md","efa93fb56baf8ab0",{"html":25,"metadata":16151},{"headings":16152,"localImagePaths":16153,"remoteImagePaths":16154,"frontmatter":16155,"imagePaths":16156},[],[],[],{"title":16143,"description":25,"summary":16144,"pubDate":16102,"source":2720,"url":16146,"thumbnail":16147},[],"2024-08-08-xethub-is-joining-hugging-face.md","2024-08-08-zico-kolter-joins-openais-board-of-directors",{"id":16158,"data":16160,"filePath":16165,"digest":16166,"rendered":16167,"legacyId":16175},{"title":16161,"description":16162,"summary":16162,"pubDate":16163,"source":19,"url":16164,"thumbnail":21},"Zico Kolter Joins OpenAI’s Board of Directors","Zico Kolter Joins OpenAI’s Board of Directors We’re strengthening our governance with expertise in AI safety and alignment. Zico will also join the Safety & Security Committee",["Date","2024-08-08T12:00:00.000Z"],"https://openai.com/blog/zico-kolter-joins-openais-board-of-directors","src/content/posts/2024-08-08-zico-kolter-joins-openais-board-of-directors.md","b10cba5b82047498",{"html":25,"metadata":16168},{"headings":16169,"localImagePaths":16170,"remoteImagePaths":16171,"frontmatter":16172,"imagePaths":16174},[],[],[],{"title":16161,"description":16162,"summary":16162,"pubDate":16173,"source":19,"url":16164,"thumbnail":21},"Thu, 08 Aug 2024 12:00:00 GMT",[],"2024-08-08-zico-kolter-joins-openais-board-of-directors.md","2024-08-12-tool-use-unified",{"id":16176,"data":16178,"filePath":16184,"digest":16185,"rendered":16186,"legacyId":16194},{"title":16179,"description":25,"summary":16180,"pubDate":16181,"source":2720,"url":16182,"thumbnail":16183},"Tool Use, Unified","Tool Use, Unified There is now a unified tool use API across several popular families of models. Thi...",["Date","2024-08-12T00:00:00.000Z"],"https://huggingface.co/blog/unified-tool-use","https://huggingface.co/blog/assets/unified-tool-use/thumbnail.png","src/content/posts/2024-08-12-tool-use-unified.md","62d96f396efb0733",{"html":25,"metadata":16187},{"headings":16188,"localImagePaths":16189,"remoteImagePaths":16190,"frontmatter":16191,"imagePaths":16193},[],[],[],{"title":16179,"description":25,"summary":16180,"pubDate":16192,"source":2720,"url":16182,"thumbnail":16183},"Mon, 12 Aug 2024 00:00:00 GMT",[],"2024-08-12-tool-use-unified.md","2024-08-12-welcome-falconmamba-the-first-strong-attention-free-7b-model",{"id":16195,"data":16197,"filePath":16203,"digest":16204,"rendered":16205,"legacyId":16212},{"title":16198,"description":25,"summary":16199,"pubDate":16200,"source":2720,"url":16201,"thumbnail":16202},"Welcome FalconMamba: The first strong attention-free 7B model","Welcome FalconMamba: The first strong attention-free 7B model Falcon Mamba is a new model by Technol...",["Date","2024-08-12T00:00:00.000Z"],"https://huggingface.co/blog/falconmamba","https://huggingface.co/blog/assets/falconmamba/thumbnail.png","src/content/posts/2024-08-12-welcome-falconmamba-the-first-strong-attention-free-7b-model.md","cb85c9a39527853c",{"html":25,"metadata":16206},{"headings":16207,"localImagePaths":16208,"remoteImagePaths":16209,"frontmatter":16210,"imagePaths":16211},[],[],[],{"title":16198,"description":25,"summary":16199,"pubDate":16192,"source":2720,"url":16201,"thumbnail":16202},[],"2024-08-12-welcome-falconmamba-the-first-strong-attention-free-7b-model.md","2024-08-13-introducing-swe-bench-verified",{"id":16213,"data":16215,"filePath":16220,"digest":16221,"rendered":16222,"legacyId":16230},{"title":16216,"description":16217,"summary":16217,"pubDate":16218,"source":19,"url":16219,"thumbnail":21},"Introducing SWE-bench Verified","We’re releasing a human-validated subset of SWE-bench that more reliably evaluates AI models’ ability to solve real-world software issues.",["Date","2024-08-13T10:00:00.000Z"],"https://openai.com/blog/introducing-swe-bench-verified","src/content/posts/2024-08-13-introducing-swe-bench-verified.md","0832080f846b9896",{"html":25,"metadata":16223},{"headings":16224,"localImagePaths":16225,"remoteImagePaths":16226,"frontmatter":16227,"imagePaths":16229},[],[],[],{"title":16216,"description":16217,"summary":16217,"pubDate":16228,"source":19,"url":16219,"thumbnail":21},"Tue, 13 Aug 2024 10:00:00 GMT",[],"2024-08-13-introducing-swe-bench-verified.md","2024-08-13-introduction-to-ggml",{"id":16231,"data":16233,"filePath":16239,"digest":16240,"rendered":16241,"legacyId":16249},{"title":16234,"description":25,"summary":16235,"pubDate":16236,"source":2720,"url":16237,"thumbnail":16238},"Introduction to ggml","Introduction to ggml ggml is a machine learning (ML) library written in C and C++ with a focus on Tr...",["Date","2024-08-13T00:00:00.000Z"],"https://huggingface.co/blog/introduction-to-ggml","https://huggingface.co/blog/assets/introduction-to-ggml/cover.jpg","src/content/posts/2024-08-13-introduction-to-ggml.md","157a378c8c6e10c1",{"html":25,"metadata":16242},{"headings":16243,"localImagePaths":16244,"remoteImagePaths":16245,"frontmatter":16246,"imagePaths":16248},[],[],[],{"title":16234,"description":25,"summary":16235,"pubDate":16247,"source":2720,"url":16237,"thumbnail":16238},"Tue, 13 Aug 2024 00:00:00 GMT",[],"2024-08-13-introduction-to-ggml.md","2024-08-14-a-failed-experiment-infini-attention-and-why-we-should-keep-trying",{"id":16250,"data":16252,"filePath":16258,"digest":16259,"rendered":16260,"legacyId":16268},{"title":16253,"description":25,"summary":16254,"pubDate":16255,"source":2720,"url":16256,"thumbnail":16257},"A failed experiment: Infini-Attention, and why we should keep trying?","A failed experiment: Infini-Attention, and why we should keep trying? TLDR: Infini-attention's perfo...",["Date","2024-08-14T00:00:00.000Z"],"https://huggingface.co/blog/infini-attention","https://huggingface.co/blog/infini-attention/assets/185_infini_attention/infini_attention_thumbnail.png","src/content/posts/2024-08-14-a-failed-experiment-infini-attention-and-why-we-should-keep-trying.md","992eb6e035bef2c7",{"html":25,"metadata":16261},{"headings":16262,"localImagePaths":16263,"remoteImagePaths":16264,"frontmatter":16265,"imagePaths":16267},[],[],[],{"title":16253,"description":25,"summary":16254,"pubDate":16266,"source":2720,"url":16256,"thumbnail":16257},"Wed, 14 Aug 2024 00:00:00 GMT",[],"2024-08-14-a-failed-experiment-infini-attention-and-why-we-should-keep-trying.md","2024-08-14-awakening-sleeping-beauties-at-the-met",{"id":16269,"data":16271,"filePath":16276,"digest":16277,"rendered":16278,"legacyId":16286},{"title":16272,"description":16273,"summary":16273,"pubDate":16274,"source":19,"url":16275,"thumbnail":21},"Awakening Sleeping Beauties at The Met","AI can enrich lives through beauty and creativity, and its artistic potential shines in 'Sleeping Beauties: Reawakening Fashion,' a collaborative exhibit from The Met's Costume Institute.",["Date","2024-08-14T10:00:00.000Z"],"https://openai.com/blog/the-met-museum","src/content/posts/2024-08-14-awakening-sleeping-beauties-at-the-met.md","c786b0e7f19ca7e0",{"html":25,"metadata":16279},{"headings":16280,"localImagePaths":16281,"remoteImagePaths":16282,"frontmatter":16283,"imagePaths":16285},[],[],[],{"title":16272,"description":16273,"summary":16273,"pubDate":16284,"source":19,"url":16275,"thumbnail":21},"Wed, 14 Aug 2024 10:00:00 GMT",[],"2024-08-14-awakening-sleeping-beauties-at-the-met.md","2024-08-15-delivering-contextual-job-matching-for-millions-with-openai",{"id":16287,"data":16289,"filePath":16294,"digest":16295,"rendered":16296,"legacyId":16304},{"title":16290,"description":16291,"summary":16291,"pubDate":16292,"source":19,"url":16293,"thumbnail":21},"Delivering contextual job matching for millions with OpenAI","Indeed, whose mission is to help people get jobs, is the world’s #1 job site. Over 350 million unique visitors come to Indeed every month to connect with more than 3.5 million employers and over 32 million jobs. But what’s more is that every three seconds someone gets hired on Indeed.",["Date","2024-08-15T07:00:00.000Z"],"https://openai.com/blog/indeed","src/content/posts/2024-08-15-delivering-contextual-job-matching-for-millions-with-openai.md","efc0b27345f10b42",{"html":25,"metadata":16297},{"headings":16298,"localImagePaths":16299,"remoteImagePaths":16300,"frontmatter":16301,"imagePaths":16303},[],[],[],{"title":16290,"description":16291,"summary":16291,"pubDate":16302,"source":19,"url":16293,"thumbnail":21},"Thu, 15 Aug 2024 07:00:00 GMT",[],"2024-08-15-delivering-contextual-job-matching-for-millions-with-openai.md","2024-08-16-disrupting-a-covert-iranian-influence-operation",{"id":16305,"data":16307,"filePath":16312,"digest":16313,"rendered":16314,"legacyId":16322},{"title":16308,"description":16309,"summary":16309,"pubDate":16310,"source":19,"url":16311,"thumbnail":21},"Disrupting a covert Iranian influence operation","We banned accounts linked to a covert Iranian influence operation using ChatGPT to generate website and social media content focused on multiple topics, including the U.S. presidential campaign. We have seen no indication that this content reached a meaningful audience.",["Date","2024-08-16T11:00:00.000Z"],"https://openai.com/blog/disrupting-a-covert-iranian-influence-operation","src/content/posts/2024-08-16-disrupting-a-covert-iranian-influence-operation.md","9ae956f8429675d9",{"html":25,"metadata":16315},{"headings":16316,"localImagePaths":16317,"remoteImagePaths":16318,"frontmatter":16319,"imagePaths":16321},[],[],[],{"title":16308,"description":16309,"summary":16309,"pubDate":16320,"source":19,"url":16311,"thumbnail":21},"Fri, 16 Aug 2024 11:00:00 GMT",[],"2024-08-16-disrupting-a-covert-iranian-influence-operation.md","2024-08-19-deploy-meta-llama-31-405b-on-google-cloud-vertex-ai",{"id":16323,"data":16325,"filePath":16331,"digest":16332,"rendered":16333,"legacyId":16341},{"title":16326,"description":25,"summary":16327,"pubDate":16328,"source":2720,"url":16329,"thumbnail":16330},"Deploy Meta Llama 3.1 405B on Google Cloud Vertex AI","Deploy Meta Llama 3.1 405B on Google Cloud Vertex AI Meta Llama 3.1 is the latest open LLM from Meta...",["Date","2024-08-19T00:00:00.000Z"],"https://huggingface.co/blog/llama31-on-vertex-ai","https://huggingface.co/blog/assets/llama31-on-vertex-ai/thumbnail.png","src/content/posts/2024-08-19-deploy-meta-llama-31-405b-on-google-cloud-vertex-ai.md","8f261fa5cb7af362",{"html":25,"metadata":16334},{"headings":16335,"localImagePaths":16336,"remoteImagePaths":16337,"frontmatter":16338,"imagePaths":16340},[],[],[],{"title":16326,"description":25,"summary":16327,"pubDate":16339,"source":2720,"url":16329,"thumbnail":16330},"Mon, 19 Aug 2024 00:00:00 GMT",[],"2024-08-19-deploy-meta-llama-31-405b-on-google-cloud-vertex-ai.md","2024-08-20-fine-tuning-now-available-for-gpt-4o",{"id":16342,"data":16344,"filePath":16349,"digest":16350,"rendered":16351,"legacyId":16359},{"title":16345,"description":16346,"summary":16346,"pubDate":16347,"source":19,"url":16348,"thumbnail":21},"Fine-tuning now available for GPT-4o","Fine-tune custom versions of GPT-4o to increase performance and accuracy for your applications",["Date","2024-08-20T10:00:00.000Z"],"https://openai.com/blog/gpt-4o-fine-tuning","src/content/posts/2024-08-20-fine-tuning-now-available-for-gpt-4o.md","a4522d0e4b82c062",{"html":25,"metadata":16352},{"headings":16353,"localImagePaths":16354,"remoteImagePaths":16355,"frontmatter":16356,"imagePaths":16358},[],[],[],{"title":16345,"description":16346,"summary":16346,"pubDate":16357,"source":19,"url":16348,"thumbnail":21},"Tue, 20 Aug 2024 10:00:00 GMT",[],"2024-08-20-fine-tuning-now-available-for-gpt-4o.md","2024-08-20-openai-partners-with-condé-nast",{"id":16360,"data":16362,"filePath":16367,"digest":16368,"rendered":16369,"legacyId":16377},{"title":16363,"description":16364,"summary":16364,"pubDate":16365,"source":19,"url":16366,"thumbnail":21},"OpenAI partners with Condé Nast","Condé Nast",["Date","2024-08-20T11:00:00.000Z"],"https://openai.com/blog/conde-nast","src/content/posts/2024-08-20-openai-partners-with-condé-nast.md","5459f5a1a79ecd4e",{"html":25,"metadata":16370},{"headings":16371,"localImagePaths":16372,"remoteImagePaths":16373,"frontmatter":16374,"imagePaths":16376},[],[],[],{"title":16363,"description":16364,"summary":16364,"pubDate":16375,"source":19,"url":16366,"thumbnail":21},"Tue, 20 Aug 2024 11:00:00 GMT",[],"2024-08-20-openai-partners-with-condé-nast.md","2024-08-20-putting-ai-to-work-at-upwork",{"id":16378,"data":16380,"filePath":16385,"digest":16386,"rendered":16387,"legacyId":16394},{"title":16381,"description":16382,"summary":16382,"pubDate":16383,"source":19,"url":16384,"thumbnail":21},"Putting AI to work at Upwork","Upwork puts AI to work, uniting team members, operations and product development",["Date","2024-08-20T10:00:00.000Z"],"https://openai.com/blog/upwork","src/content/posts/2024-08-20-putting-ai-to-work-at-upwork.md","6c4eb863d9ceb02c",{"html":25,"metadata":16388},{"headings":16389,"localImagePaths":16390,"remoteImagePaths":16391,"frontmatter":16392,"imagePaths":16393},[],[],[],{"title":16381,"description":16382,"summary":16382,"pubDate":16357,"source":19,"url":16384,"thumbnail":21},[],"2024-08-20-putting-ai-to-work-at-upwork.md","2024-08-21-improving-hugging-face-training-efficiency-through-packing-with-flash-attention",{"id":16395,"data":16397,"filePath":16403,"digest":16404,"rendered":16405,"legacyId":16413},{"title":16398,"description":25,"summary":16399,"pubDate":16400,"source":2720,"url":16401,"thumbnail":16402},"Improving Hugging Face Training Efficiency Through Packing with Flash Attention","Improving Hugging Face Training Efficiency Through Packing with Flash Attention TL;DR Training with ...",["Date","2024-08-21T00:00:00.000Z"],"https://huggingface.co/blog/packing-with-FA2","https://huggingface.co/blog/assets/packing-with-FA2/thumbnail.png","src/content/posts/2024-08-21-improving-hugging-face-training-efficiency-through-packing-with-flash-attention.md","79f72fa0522845c5",{"html":25,"metadata":16406},{"headings":16407,"localImagePaths":16408,"remoteImagePaths":16409,"frontmatter":16410,"imagePaths":16412},[],[],[],{"title":16398,"description":25,"summary":16399,"pubDate":16411,"source":2720,"url":16401,"thumbnail":16402},"Wed, 21 Aug 2024 00:00:00 GMT",[],"2024-08-21-improving-hugging-face-training-efficiency-through-packing-with-flash-attention.md","2024-08-22-ferminet-quantum-physics-and-chemistry-from-first-principles",{"id":16414,"data":16416,"filePath":16422,"digest":16423,"rendered":16424,"legacyId":16432},{"title":16417,"description":16418,"summary":16418,"pubDate":16419,"source":6423,"url":16420,"thumbnail":16421},"FermiNet: Quantum physics and chemistry from first principles","Using deep learning to solve fundamental problems in computational quantum chemistry and explore how matter interacts with light",["Date","2024-08-22T19:00:00.000Z"],"https://deepmind.google/discover/blog/ferminet-quantum-physics-and-chemistry-from-first-principles/","https://lh3.googleusercontent.com/u-LZOO0ynV2UCorbNrUtWS6MJ_sxTfGzObe2YzBt5Grgohx39WcsGiPNOsHwBja8C51lQBclpaovrzUVVQRzj2WpWeM7f7y5eeYt3Dx6l3gxfx9S9g=w1200-h630-n-nu","src/content/posts/2024-08-22-ferminet-quantum-physics-and-chemistry-from-first-principles.md","1e5c2814d183b42a",{"html":25,"metadata":16425},{"headings":16426,"localImagePaths":16427,"remoteImagePaths":16428,"frontmatter":16429,"imagePaths":16431},[],[],[],{"title":16417,"description":16418,"summary":16418,"pubDate":16430,"source":6423,"url":16420,"thumbnail":16421},"Thu, 22 Aug 2024 19:00:00 +0000",[],"2024-08-22-ferminet-quantum-physics-and-chemistry-from-first-principles.md","2024-08-22-the-5-most-under-rated-tools-on-hugging-face",{"id":16433,"data":16435,"filePath":16441,"digest":16442,"rendered":16443,"legacyId":16451},{"title":16436,"description":25,"summary":16437,"pubDate":16438,"source":2720,"url":16439,"thumbnail":16440},"The 5 Most Under-Rated Tools on Hugging Face","The 5 Most Under-Rated Tools on Hugging Face The Hugging Face Hub boasts over 850K public models, wi...",["Date","2024-08-22T00:00:00.000Z"],"https://huggingface.co/blog/unsung-heroes","https://huggingface.co/blog/assets/unsung-heroes/new-thumbnail.png","src/content/posts/2024-08-22-the-5-most-under-rated-tools-on-hugging-face.md","d83dc51fd894b415",{"html":25,"metadata":16444},{"headings":16445,"localImagePaths":16446,"remoteImagePaths":16447,"frontmatter":16448,"imagePaths":16450},[],[],[],{"title":16436,"description":25,"summary":16437,"pubDate":16449,"source":2720,"url":16439,"thumbnail":16440},"Thu, 22 Aug 2024 00:00:00 GMT",[],"2024-08-22-the-5-most-under-rated-tools-on-hugging-face.md","2024-08-26-fine-tuning-gpt-4o-webinar",{"id":16452,"data":16454,"filePath":16458,"digest":16459,"rendered":16460,"legacyId":16468},{"title":16455,"description":16455,"summary":16455,"pubDate":16456,"source":19,"url":16457,"thumbnail":21},"Fine-Tuning GPT-4o Webinar",["Date","2024-08-26T00:00:00.000Z"],"https://openai.com/business/fine-tuning-gpt-4o-webinar","src/content/posts/2024-08-26-fine-tuning-gpt-4o-webinar.md","7dedb4c11312ff4a",{"html":25,"metadata":16461},{"headings":16462,"localImagePaths":16463,"remoteImagePaths":16464,"frontmatter":16465,"imagePaths":16467},[],[],[],{"title":16455,"description":16455,"summary":16455,"pubDate":16466,"source":19,"url":16457,"thumbnail":21},"Mon, 26 Aug 2024 00:00:00 GMT",[],"2024-08-26-fine-tuning-gpt-4o-webinar.md","2024-08-26-personalizing-education-with-chatgpt",{"id":16469,"data":16471,"filePath":16476,"digest":16477,"rendered":16478,"legacyId":16486},{"title":16472,"description":16473,"summary":16473,"pubDate":16474,"source":19,"url":16475,"thumbnail":21},"Personalizing education with ChatGPT","Arizona State University embraces ChatGPT campus-wide to personalize learning, advance research, and prepare students for the future",["Date","2024-08-26T04:00:00.000Z"],"https://openai.com/blog/asu","src/content/posts/2024-08-26-personalizing-education-with-chatgpt.md","f9abc76ef4b7c364",{"html":25,"metadata":16479},{"headings":16480,"localImagePaths":16481,"remoteImagePaths":16482,"frontmatter":16483,"imagePaths":16485},[],[],[],{"title":16472,"description":16473,"summary":16473,"pubDate":16484,"source":19,"url":16475,"thumbnail":21},"Mon, 26 Aug 2024 04:00:00 GMT",[],"2024-08-26-personalizing-education-with-chatgpt.md","2024-08-27-scaling-robotics-datasets-with-video-encoding",{"id":16487,"data":16489,"filePath":16495,"digest":16496,"rendered":16497,"legacyId":16505},{"title":16490,"description":25,"summary":16491,"pubDate":16492,"source":2720,"url":16493,"thumbnail":16494},"Scaling robotics datasets with video encoding","Scaling robotics datasets with video encoding Over the past few years, text and image-based models h...",["Date","2024-08-27T00:00:00.000Z"],"https://huggingface.co/blog/video-encoding","https://huggingface.co/blog/assets/video-encoding/thumbnail.png","src/content/posts/2024-08-27-scaling-robotics-datasets-with-video-encoding.md","76246364bdb8427b",{"html":25,"metadata":16498},{"headings":16499,"localImagePaths":16500,"remoteImagePaths":16501,"frontmatter":16502,"imagePaths":16504},[],[],[],{"title":16490,"description":25,"summary":16491,"pubDate":16503,"source":2720,"url":16493,"thumbnail":16494},"Tue, 27 Aug 2024 00:00:00 GMT",[],"2024-08-27-scaling-robotics-datasets-with-video-encoding.md","2024-09-04-hugging-face-partners-with-trufflehog-to-scan-for-secrets",{"id":16506,"data":16508,"filePath":16514,"digest":16515,"rendered":16516,"legacyId":16524},{"title":16509,"description":25,"summary":16510,"pubDate":16511,"source":2720,"url":16512,"thumbnail":16513},"Hugging Face partners with TruffleHog to Scan for Secrets","Hugging Face partners with TruffleHog to Scan for Secrets We're excited to announce our partnership ...",["Date","2024-09-04T00:00:00.000Z"],"https://huggingface.co/blog/trufflesecurity-partnership","https://huggingface.co/blog/assets/trufflesecurity-partnership/thumbnail.png","src/content/posts/2024-09-04-hugging-face-partners-with-trufflehog-to-scan-for-secrets.md","4a8cf43b4a7076c7",{"html":25,"metadata":16517},{"headings":16518,"localImagePaths":16519,"remoteImagePaths":16520,"frontmatter":16521,"imagePaths":16523},[],[],[],{"title":16509,"description":25,"summary":16510,"pubDate":16522,"source":2720,"url":16512,"thumbnail":16513},"Wed, 04 Sep 2024 00:00:00 GMT",[],"2024-09-04-hugging-face-partners-with-trufflehog-to-scan-for-secrets.md","2024-09-05-alphaproteo-generates-novel-proteins-for-biology-and-health-research",{"id":16525,"data":16527,"filePath":16533,"digest":16534,"rendered":16535,"legacyId":16543},{"title":16528,"description":16529,"summary":16529,"pubDate":16530,"source":6423,"url":16531,"thumbnail":16532},"AlphaProteo generates novel proteins for biology and health research","New AI system designs proteins that successfully bind to target molecules, with potential for advancing drug design, disease understanding and more.",["Date","2024-09-05T15:00:00.000Z"],"https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/","https://lh3.googleusercontent.com/7RKd6r-Wc8JfMau5x9knRq9DrOKGDwS3ye4YxY0jjWGntf74y8WL0lOlktJefxwkJYw33UEf2Ph_BhQ51TIufCxPkmtCPOpakekMpnOUwVI-3R6RzQ=w1200-h630-n-nu","src/content/posts/2024-09-05-alphaproteo-generates-novel-proteins-for-biology-and-health-research.md","57bdf8c7a5f89c4f",{"html":25,"metadata":16536},{"headings":16537,"localImagePaths":16538,"remoteImagePaths":16539,"frontmatter":16540,"imagePaths":16542},[],[],[],{"title":16528,"description":16529,"summary":16529,"pubDate":16541,"source":6423,"url":16531,"thumbnail":16532},"Thu, 05 Sep 2024 15:00:00 +0000",[],"2024-09-05-alphaproteo-generates-novel-proteins-for-biology-and-health-research.md","2024-09-05-using-gpt-4-to-deliver-a-new-customer-service-standard",{"id":16544,"data":16546,"filePath":16551,"digest":16552,"rendered":16553,"legacyId":16561},{"title":16547,"description":16548,"summary":16548,"pubDate":16549,"source":19,"url":16550,"thumbnail":21},"Using GPT-4 to deliver a new customer service standard","Ada uses GPT-4 to deliver a new customer service standard",["Date","2024-09-05T08:00:00.000Z"],"https://openai.com/blog/ada","src/content/posts/2024-09-05-using-gpt-4-to-deliver-a-new-customer-service-standard.md","ab3595b28cd63f1e",{"html":25,"metadata":16554},{"headings":16555,"localImagePaths":16556,"remoteImagePaths":16557,"frontmatter":16558,"imagePaths":16560},[],[],[],{"title":16547,"description":16548,"summary":16548,"pubDate":16559,"source":19,"url":16550,"thumbnail":21},"Thu, 05 Sep 2024 08:00:00 GMT",[],"2024-09-05-using-gpt-4-to-deliver-a-new-customer-service-standard.md","2024-09-10-put-ai-to-work-lessons-from-hundreds-of-successful-deployments",{"id":16562,"data":16564,"filePath":16568,"digest":16569,"rendered":16570,"legacyId":16578},{"title":16565,"description":16565,"summary":16565,"pubDate":16566,"source":19,"url":16567,"thumbnail":21},"Put AI to Work: Lessons from Hundreds of Successful Deployments",["Date","2024-09-10T00:00:00.000Z"],"https://openai.com/business/put-ai-to-work-lessons-from-hundreds-of-successful-deployments","src/content/posts/2024-09-10-put-ai-to-work-lessons-from-hundreds-of-successful-deployments.md","cd943dca72d2d38f",{"html":25,"metadata":16571},{"headings":16572,"localImagePaths":16573,"remoteImagePaths":16574,"frontmatter":16575,"imagePaths":16577},[],[],[],{"title":16565,"description":16565,"summary":16565,"pubDate":16576,"source":19,"url":16567,"thumbnail":21},"Tue, 10 Sep 2024 00:00:00 GMT",[],"2024-09-10-put-ai-to-work-lessons-from-hundreds-of-successful-deployments.md","2024-09-12-answering-quantum-physics-questions-with-openai-o1",{"id":16579,"data":16581,"filePath":16586,"digest":16587,"rendered":16588,"legacyId":16596},{"title":16582,"description":16583,"summary":16583,"pubDate":16584,"source":19,"url":16585,"thumbnail":21},"Answering quantum physics questions with OpenAI o1","Quantum physicist Mario Krenn uses OpenAI o1 to help answer life's biggest questions.",["Date","2024-09-12T00:00:00.000Z"],"https://openai.com/blog/o1-quantum-physics","src/content/posts/2024-09-12-answering-quantum-physics-questions-with-openai-o1.md","f00ba653aa2f742b",{"html":25,"metadata":16589},{"headings":16590,"localImagePaths":16591,"remoteImagePaths":16592,"frontmatter":16593,"imagePaths":16595},[],[],[],{"title":16582,"description":16583,"summary":16583,"pubDate":16594,"source":19,"url":16585,"thumbnail":21},"Thu, 12 Sep 2024 00:00:00 GMT",[],"2024-09-12-answering-quantum-physics-questions-with-openai-o1.md","2024-09-12-coding-with-openai-o1",{"id":16597,"data":16599,"filePath":16604,"digest":16605,"rendered":16606,"legacyId":16613},{"title":16600,"description":16601,"summary":16601,"pubDate":16602,"source":19,"url":16603,"thumbnail":21},"Coding with OpenAI o1","Scott Wu, CEO and Co-Founder of Cognition, explains how OpenAI o1 makes coding decisions in a more human-like way.",["Date","2024-09-12T00:00:00.000Z"],"https://openai.com/blog/o1-coding","src/content/posts/2024-09-12-coding-with-openai-o1.md","44b72abf5de6a3e3",{"html":25,"metadata":16607},{"headings":16608,"localImagePaths":16609,"remoteImagePaths":16610,"frontmatter":16611,"imagePaths":16612},[],[],[],{"title":16600,"description":16601,"summary":16601,"pubDate":16594,"source":19,"url":16603,"thumbnail":21},[],"2024-09-12-coding-with-openai-o1.md","2024-09-12-decoding-genetics-with-openai-o1",{"id":16614,"data":16616,"filePath":16621,"digest":16622,"rendered":16623,"legacyId":16630},{"title":16617,"description":16618,"summary":16618,"pubDate":16619,"source":19,"url":16620,"thumbnail":21},"Decoding genetics with OpenAI o1","Geneticist Catherine Brownstein demonstrates how OpenAI o1 can speed up the process of diagnosing rare medical challenges.",["Date","2024-09-12T00:00:00.000Z"],"https://openai.com/blog/o1-genetics","src/content/posts/2024-09-12-decoding-genetics-with-openai-o1.md","fd629d543d3973cd",{"html":25,"metadata":16624},{"headings":16625,"localImagePaths":16626,"remoteImagePaths":16627,"frontmatter":16628,"imagePaths":16629},[],[],[],{"title":16617,"description":16618,"summary":16618,"pubDate":16594,"source":19,"url":16620,"thumbnail":21},[],"2024-09-12-decoding-genetics-with-openai-o1.md","2024-09-12-economics-and-reasoning-with-openai-o1",{"id":16631,"data":16633,"filePath":16638,"digest":16639,"rendered":16640,"legacyId":16647},{"title":16634,"description":16635,"summary":16635,"pubDate":16636,"source":19,"url":16637,"thumbnail":21},"Economics and reasoning with OpenAI o1","Economist Tyler Cowen explains how OpenAI o1 tackles complex economic questions.",["Date","2024-09-12T00:00:00.000Z"],"https://openai.com/blog/o1-economics","src/content/posts/2024-09-12-economics-and-reasoning-with-openai-o1.md","23a2eded300d4547",{"html":25,"metadata":16641},{"headings":16642,"localImagePaths":16643,"remoteImagePaths":16644,"frontmatter":16645,"imagePaths":16646},[],[],[],{"title":16634,"description":16635,"summary":16635,"pubDate":16594,"source":19,"url":16637,"thumbnail":21},[],"2024-09-12-economics-and-reasoning-with-openai-o1.md","2024-09-12-introducing-openai-o1",{"id":16648,"data":16650,"filePath":16654,"digest":16655,"rendered":16656,"legacyId":16664},{"title":16651,"description":16651,"summary":16651,"pubDate":16652,"source":19,"url":16653,"thumbnail":21},"Introducing OpenAI o1",["Date","2024-09-12T10:03:00.000Z"],"https://openai.com/blog/introducing-openai-o1-preview","src/content/posts/2024-09-12-introducing-openai-o1.md","771e3e7d6b26def6",{"html":25,"metadata":16657},{"headings":16658,"localImagePaths":16659,"remoteImagePaths":16660,"frontmatter":16661,"imagePaths":16663},[],[],[],{"title":16651,"description":16651,"summary":16651,"pubDate":16662,"source":19,"url":16653,"thumbnail":21},"Thu, 12 Sep 2024 10:03:00 GMT",[],"2024-09-12-introducing-openai-o1.md","2024-09-12-learning-to-reason-with-llms",{"id":16665,"data":16667,"filePath":16672,"digest":16673,"rendered":16674,"legacyId":16682},{"title":16668,"description":16669,"summary":16669,"pubDate":16670,"source":19,"url":16671,"thumbnail":21},"Learning to reason with LLMs","We are introducing OpenAI o1, a new large language model trained with reinforcement learning to perform complex reasoning. o1 thinks before it answers—it can produce a long internal chain of thought before responding to the user.",["Date","2024-09-12T10:02:00.000Z"],"https://openai.com/blog/learning-to-reason-with-llms","src/content/posts/2024-09-12-learning-to-reason-with-llms.md","c6fb7bff13c476ad",{"html":25,"metadata":16675},{"headings":16676,"localImagePaths":16677,"remoteImagePaths":16678,"frontmatter":16679,"imagePaths":16681},[],[],[],{"title":16668,"description":16669,"summary":16669,"pubDate":16680,"source":19,"url":16671,"thumbnail":21},"Thu, 12 Sep 2024 10:02:00 GMT",[],"2024-09-12-learning-to-reason-with-llms.md","2024-09-12-openai-o1-contributions",{"id":16683,"data":16685,"filePath":16689,"digest":16690,"rendered":16691,"legacyId":16699},{"title":16686,"description":16686,"summary":16686,"pubDate":16687,"source":19,"url":16688,"thumbnail":21},"OpenAI o1 Contributions",["Date","2024-09-12T10:00:00.000Z"],"https://openai.com/openai-o1-contributions","src/content/posts/2024-09-12-openai-o1-contributions.md","a5a94e416f49130a",{"html":25,"metadata":16692},{"headings":16693,"localImagePaths":16694,"remoteImagePaths":16695,"frontmatter":16696,"imagePaths":16698},[],[],[],{"title":16686,"description":16686,"summary":16686,"pubDate":16697,"source":19,"url":16688,"thumbnail":21},"Thu, 12 Sep 2024 10:00:00 GMT",[],"2024-09-12-openai-o1-contributions.md","2024-09-12-openai-o1-mini",{"id":16700,"data":16702,"filePath":16707,"digest":16708,"rendered":16709,"legacyId":16717},{"title":16703,"description":16704,"summary":16704,"pubDate":16705,"source":19,"url":16706,"thumbnail":21},"OpenAI o1-mini","Advancing cost-efficient reasoning",["Date","2024-09-12T10:01:00.000Z"],"https://openai.com/blog/openai-o1-mini-advancing-cost-efficient-reasoning","src/content/posts/2024-09-12-openai-o1-mini.md","28baadea933bdf49",{"html":25,"metadata":16710},{"headings":16711,"localImagePaths":16712,"remoteImagePaths":16713,"frontmatter":16714,"imagePaths":16716},[],[],[],{"title":16703,"description":16704,"summary":16704,"pubDate":16715,"source":19,"url":16706,"thumbnail":21},"Thu, 12 Sep 2024 10:01:00 GMT",[],"2024-09-12-openai-o1-mini.md","2024-09-12-openai-o1-system-card-external-testers-acknowledgements",{"id":16718,"data":16720,"filePath":16725,"digest":16726,"rendered":16727,"legacyId":16734},{"title":16721,"description":16722,"summary":16722,"pubDate":16723,"source":19,"url":16724,"thumbnail":21},"OpenAI o1 System Card External Testers Acknowledgements","OpenAI o1 system card external testers acknowledgements",["Date","2024-09-12T10:00:00.000Z"],"https://openai.com/blog/openai-o1-system-card/external-testers-acknowledgements","src/content/posts/2024-09-12-openai-o1-system-card-external-testers-acknowledgements.md","e5e095f7d487f4c1",{"html":25,"metadata":16728},{"headings":16729,"localImagePaths":16730,"remoteImagePaths":16731,"frontmatter":16732,"imagePaths":16733},[],[],[],{"title":16721,"description":16722,"summary":16722,"pubDate":16697,"source":19,"url":16724,"thumbnail":21},[],"2024-09-12-openai-o1-system-card-external-testers-acknowledgements.md","2024-09-12-our-latest-advances-in-robot-dexterity",{"id":16735,"data":16737,"filePath":16743,"digest":16744,"rendered":16745,"legacyId":16753},{"title":16738,"description":16739,"summary":16739,"pubDate":16740,"source":6423,"url":16741,"thumbnail":16742},"Our latest advances in robot dexterity","Two new AI systems, ALOHA Unleashed and DemoStart, help robots learn to perform complex tasks that require dexterous movement",["Date","2024-09-12T14:00:00.000Z"],"https://deepmind.google/discover/blog/advances-in-robot-dexterity/","https://lh3.googleusercontent.com/63ROjLq4VNqk3RDA5vl1mYS1i5xvcgU8-augVWQY5OZCtVsm_e4YX8rR4_DLUlQiTmMHT6qx3p9shUtPGUHy_4SA64RDeMghvk0eDKT6Fqh6-P3d4A=w1200-h630-n-nu","src/content/posts/2024-09-12-our-latest-advances-in-robot-dexterity.md","157087c27cdd98a3",{"html":25,"metadata":16746},{"headings":16747,"localImagePaths":16748,"remoteImagePaths":16749,"frontmatter":16750,"imagePaths":16752},[],[],[],{"title":16738,"description":16739,"summary":16739,"pubDate":16751,"source":6423,"url":16741,"thumbnail":16742},"Thu, 12 Sep 2024 14:00:00 +0000",[],"2024-09-12-our-latest-advances-in-robot-dexterity.md","2024-09-13-accelerate-100",{"id":16754,"data":16756,"filePath":16762,"digest":16763,"rendered":16764,"legacyId":16772},{"title":16757,"description":25,"summary":16758,"pubDate":16759,"source":2720,"url":16760,"thumbnail":16761},"Accelerate 1.0.0","Accelerate 1.0.0 What is Accelerate today? 3.5 years ago, Accelerate was a simple framework aimed at...",["Date","2024-09-13T00:00:00.000Z"],"https://huggingface.co/blog/accelerate-v1","https://huggingface.co/blog/assets/186_accelerate_v1/accelerate_v1_thumbnail.png","src/content/posts/2024-09-13-accelerate-100.md","f715550eaf82d48d",{"html":25,"metadata":16765},{"headings":16766,"localImagePaths":16767,"remoteImagePaths":16768,"frontmatter":16769,"imagePaths":16771},[],[],[],{"title":16757,"description":25,"summary":16758,"pubDate":16770,"source":2720,"url":16760,"thumbnail":16761},"Fri, 13 Sep 2024 00:00:00 GMT",[],"2024-09-13-accelerate-100.md","2024-09-16-an-update-on-our-safety-security-practices",{"id":16773,"data":16775,"filePath":16779,"digest":16780,"rendered":16781,"legacyId":16789},{"title":16776,"description":16776,"summary":16776,"pubDate":16777,"source":19,"url":16778,"thumbnail":21},"An update on our safety & security practices",["Date","2024-09-16T13:00:00.000Z"],"https://openai.com/blog/update-on-safety-and-security-practices","src/content/posts/2024-09-16-an-update-on-our-safety-security-practices.md","1632b29ed270a356",{"html":25,"metadata":16782},{"headings":16783,"localImagePaths":16784,"remoteImagePaths":16785,"frontmatter":16786,"imagePaths":16788},[],[],[],{"title":16776,"description":16776,"summary":16776,"pubDate":16787,"source":19,"url":16778,"thumbnail":21},"Mon, 16 Sep 2024 13:00:00 GMT",[],"2024-09-16-an-update-on-our-safety-security-practices.md","2024-09-16-introducing-community-tools-on-huggingchat",{"id":16790,"data":16792,"filePath":16798,"digest":16799,"rendered":16800,"legacyId":16808},{"title":16793,"description":25,"summary":16794,"pubDate":16795,"source":2720,"url":16796,"thumbnail":16797},"Introducing Community Tools on HuggingChat","Introducing Community Tools on HuggingChat Today we’re releasing our latest feature on HuggingChat: ...",["Date","2024-09-16T00:00:00.000Z"],"https://huggingface.co/blog/community-tools","https://huggingface.co/blog/assets/community-tools/thumbnail.png","src/content/posts/2024-09-16-introducing-community-tools-on-huggingchat.md","fcd80ed3cb2d5c0c",{"html":25,"metadata":16801},{"headings":16802,"localImagePaths":16803,"remoteImagePaths":16804,"frontmatter":16805,"imagePaths":16807},[],[],[],{"title":16793,"description":25,"summary":16794,"pubDate":16806,"source":2720,"url":16796,"thumbnail":16797},"Mon, 16 Sep 2024 00:00:00 GMT",[],"2024-09-16-introducing-community-tools-on-huggingchat.md","2024-09-17-introducing-the-sql-console-on-datasets",{"id":16809,"data":16811,"filePath":16817,"digest":16818,"rendered":16819,"legacyId":16827},{"title":16812,"description":25,"summary":16813,"pubDate":16814,"source":2720,"url":16815,"thumbnail":16816},"Introducing the SQL Console on Datasets","Introducing the SQL Console on Datasets Datasets use has been exploding and Hugging Face has become ...",["Date","2024-09-17T00:00:00.000Z"],"https://huggingface.co/blog/sql-console","https://huggingface.co/blog/assets/sql_console/thumbnail.png","src/content/posts/2024-09-17-introducing-the-sql-console-on-datasets.md","d35b2c1b4c9eec6b",{"html":25,"metadata":16820},{"headings":16821,"localImagePaths":16822,"remoteImagePaths":16823,"frontmatter":16824,"imagePaths":16826},[],[],[],{"title":16812,"description":25,"summary":16813,"pubDate":16825,"source":2720,"url":16815,"thumbnail":16816},"Tue, 17 Sep 2024 00:00:00 GMT",[],"2024-09-17-introducing-the-sql-console-on-datasets.md","2024-09-17-using-gpt-4-to-improve-teaching-and-learning-in-brazil",{"id":16828,"data":16830,"filePath":16835,"digest":16836,"rendered":16837,"legacyId":16845},{"title":16831,"description":16832,"summary":16832,"pubDate":16833,"source":19,"url":16834,"thumbnail":21},"Using GPT-4 to improve teaching and learning in Brazil","Improving teaching and learning in Brazil",["Date","2024-09-17T05:00:00.000Z"],"https://openai.com/blog/arco-education","src/content/posts/2024-09-17-using-gpt-4-to-improve-teaching-and-learning-in-brazil.md","50766d2dcbf1a6a2",{"html":25,"metadata":16838},{"headings":16839,"localImagePaths":16840,"remoteImagePaths":16841,"frontmatter":16842,"imagePaths":16844},[],[],[],{"title":16831,"description":16832,"summary":16832,"pubDate":16843,"source":19,"url":16834,"thumbnail":21},"Tue, 17 Sep 2024 05:00:00 GMT",[],"2024-09-17-using-gpt-4-to-improve-teaching-and-learning-in-brazil.md","2024-09-18-empowering-youtube-creators-with-generative-ai",{"id":16846,"data":16848,"filePath":16854,"digest":16855,"rendered":16856,"legacyId":16864},{"title":16849,"description":16850,"summary":16850,"pubDate":16851,"source":6423,"url":16852,"thumbnail":16853},"Empowering YouTube creators with generative AI","New video generation technology in YouTube Shorts will help millions of people realize their creative vision",["Date","2024-09-18T14:30:06.000Z"],"https://deepmind.google/discover/blog/empowering-youtube-creators-with-generative-ai/","https://lh3.googleusercontent.com/Q8qBc1kzbYeksHRjsSuR7HEvezKsw3n1fxYlOqLf2sslqDOqYXJOhxyjznZ4cyq1fwNhpyMTMXW0RRrgHweVg6NaCEPnt3ujcFAIe0bVXK_sHka7cLo=w1200-h630-n-nu","src/content/posts/2024-09-18-empowering-youtube-creators-with-generative-ai.md","5fa2a14c3b20577d",{"html":25,"metadata":16857},{"headings":16858,"localImagePaths":16859,"remoteImagePaths":16860,"frontmatter":16861,"imagePaths":16863},[],[],[],{"title":16849,"description":16850,"summary":16850,"pubDate":16862,"source":6423,"url":16852,"thumbnail":16853},"Wed, 18 Sep 2024 14:30:06 +0000",[],"2024-09-18-empowering-youtube-creators-with-generative-ai.md","2024-09-18-fine-tuning-llms-to-158bit-extreme-quantization-made-easy",{"id":16865,"data":16867,"filePath":16873,"digest":16874,"rendered":16875,"legacyId":16883},{"title":16868,"description":25,"summary":16869,"pubDate":16870,"source":2720,"url":16871,"thumbnail":16872},"Fine-tuning LLMs to 1.58bit: extreme quantization made easy","Fine-tuning LLMs to 1.58bit: extreme quantization made easy As Large Language Models (LLMs) grow in ...",["Date","2024-09-18T00:00:00.000Z"],"https://huggingface.co/blog/1_58_llm_extreme_quantization","https://huggingface.co/blog/assets/1_58_llm_extreme_quantization/thumbnail.png","src/content/posts/2024-09-18-fine-tuning-llms-to-158bit-extreme-quantization-made-easy.md","4312009d23e10c18",{"html":25,"metadata":16876},{"headings":16877,"localImagePaths":16878,"remoteImagePaths":16879,"frontmatter":16880,"imagePaths":16882},[],[],[],{"title":16868,"description":25,"summary":16869,"pubDate":16881,"source":2720,"url":16871,"thumbnail":16872},"Wed, 18 Sep 2024 00:00:00 GMT",[],"2024-09-18-fine-tuning-llms-to-158bit-extreme-quantization-made-easy.md","2024-09-19-genmab-launches-ai-everywhere",{"id":16884,"data":16886,"filePath":16891,"digest":16892,"rendered":16893,"legacyId":16901},{"title":16887,"description":16888,"summary":16888,"pubDate":16889,"source":19,"url":16890,"thumbnail":21},"Genmab launches “AI Everywhere”","Genmab embraces ChatGPT Enterprise, supported by OpenAI’s commitment to security and privacy",["Date","2024-09-19T04:00:00.000Z"],"https://openai.com/blog/genmab","src/content/posts/2024-09-19-genmab-launches-ai-everywhere.md","5c4a3a30ad2fb5bb",{"html":25,"metadata":16894},{"headings":16895,"localImagePaths":16896,"remoteImagePaths":16897,"frontmatter":16898,"imagePaths":16900},[],[],[],{"title":16887,"description":16888,"summary":16888,"pubDate":16899,"source":19,"url":16890,"thumbnail":21},"Thu, 19 Sep 2024 04:00:00 GMT",[],"2024-09-19-genmab-launches-ai-everywhere.md","2024-09-20-optimize-and-deploy-models-with-optimum-intel-and-openvino-genai",{"id":16902,"data":16904,"filePath":16909,"digest":16910,"rendered":16911,"legacyId":16919},{"title":16905,"description":25,"summary":16906,"pubDate":16907,"source":2720,"url":16908,"thumbnail":3514},"Optimize and deploy models with Optimum-Intel and OpenVINO GenAI","Optimize and deploy models with Optimum-Intel and OpenVINO GenAI Deploying Transformers models at th...",["Date","2024-09-20T00:00:00.000Z"],"https://huggingface.co/blog/deploy-with-openvino","src/content/posts/2024-09-20-optimize-and-deploy-models-with-optimum-intel-and-openvino-genai.md","4b51fe9a65266035",{"html":25,"metadata":16912},{"headings":16913,"localImagePaths":16914,"remoteImagePaths":16915,"frontmatter":16916,"imagePaths":16918},[],[],[],{"title":16905,"description":25,"summary":16906,"pubDate":16917,"source":2720,"url":16908,"thumbnail":3514},"Fri, 20 Sep 2024 00:00:00 GMT",[],"2024-09-20-optimize-and-deploy-models-with-optimum-intel-and-openvino-genai.md","2024-09-23-exploring-the-daily-papers-page-on-hugging-face",{"id":16920,"data":16922,"filePath":16928,"digest":16929,"rendered":16930,"legacyId":16938},{"title":16923,"description":25,"summary":16924,"pubDate":16925,"source":2720,"url":16926,"thumbnail":16927},"Exploring the Daily Papers Page on Hugging Face","Exploring the Daily Papers Page on Hugging Face In the fast-paced world of research, staying up-to-d...",["Date","2024-09-23T00:00:00.000Z"],"https://huggingface.co/blog/daily-papers","https://huggingface.co/blog/assets/daily-papers/thumbnail.png","src/content/posts/2024-09-23-exploring-the-daily-papers-page-on-hugging-face.md","e3e539ca4757347a",{"html":25,"metadata":16931},{"headings":16932,"localImagePaths":16933,"remoteImagePaths":16934,"frontmatter":16935,"imagePaths":16937},[],[],[],{"title":16923,"description":25,"summary":16924,"pubDate":16936,"source":2720,"url":16926,"thumbnail":16927},"Mon, 23 Sep 2024 00:00:00 GMT",[],"2024-09-23-exploring-the-daily-papers-page-on-hugging-face.md","2024-09-23-finevideo-behind-the-scenes",{"id":16939,"data":16941,"filePath":16947,"digest":16948,"rendered":16949,"legacyId":16956},{"title":16942,"description":25,"summary":16943,"pubDate":16944,"source":2720,"url":16945,"thumbnail":16946},"FineVideo: behind the scenes","FineVideo: behind the scenes Open video datasets are scarce and therefore slowing down the developme...",["Date","2024-09-23T00:00:00.000Z"],"https://huggingface.co/blog/fine-video","https://huggingface.co/blog/assets/186_fine_video/thumbnail.png","src/content/posts/2024-09-23-finevideo-behind-the-scenes.md","8776c7d613293134",{"html":25,"metadata":16950},{"headings":16951,"localImagePaths":16952,"remoteImagePaths":16953,"frontmatter":16954,"imagePaths":16955},[],[],[],{"title":16942,"description":25,"summary":16943,"pubDate":16936,"source":2720,"url":16945,"thumbnail":16946},[],"2024-09-23-finevideo-behind-the-scenes.md","2024-09-23-introducing-the-openai-academy",{"id":16957,"data":16959,"filePath":16964,"digest":16965,"rendered":16966,"legacyId":16974},{"title":16960,"description":16961,"summary":16961,"pubDate":16962,"source":19,"url":16963,"thumbnail":21},"Introducing the OpenAI Academy","New initiative will fuel innovation by investing in developers and organizations leveraging AI, starting in low- and middle-income countries.",["Date","2024-09-23T03:30:00.000Z"],"https://openai.com/global-affairs/openai-academy","src/content/posts/2024-09-23-introducing-the-openai-academy.md","2a6425f814e14132",{"html":25,"metadata":16967},{"headings":16968,"localImagePaths":16969,"remoteImagePaths":16970,"frontmatter":16971,"imagePaths":16973},[],[],[],{"title":16960,"description":16961,"summary":16961,"pubDate":16972,"source":19,"url":16963,"thumbnail":21},"Mon, 23 Sep 2024 03:30:00 GMT",[],"2024-09-23-introducing-the-openai-academy.md","2024-09-24-introducing-verdi-an-ai-dev-platform-powered-by-gpt-4o",{"id":16975,"data":16977,"filePath":16982,"digest":16983,"rendered":16984,"legacyId":16992},{"title":16978,"description":16979,"summary":16979,"pubDate":16980,"source":19,"url":16981,"thumbnail":21},"Introducing Verdi, an AI dev platform powered by GPT-4o","Mercado Libre introduces Verdi, an AI developer platform powered by GPT-4o",["Date","2024-09-24T07:00:00.000Z"],"https://openai.com/blog/mercado-libre","src/content/posts/2024-09-24-introducing-verdi-an-ai-dev-platform-powered-by-gpt-4o.md","2980266de807f384",{"html":25,"metadata":16985},{"headings":16986,"localImagePaths":16987,"remoteImagePaths":16988,"frontmatter":16989,"imagePaths":16991},[],[],[],{"title":16978,"description":16979,"summary":16979,"pubDate":16990,"source":19,"url":16981,"thumbnail":21},"Tue, 24 Sep 2024 07:00:00 GMT",[],"2024-09-24-introducing-verdi-an-ai-dev-platform-powered-by-gpt-4o.md","2024-09-24-updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more",{"id":16993,"data":16995,"filePath":17001,"digest":17002,"rendered":17003,"legacyId":17011},{"title":16996,"description":16997,"summary":16997,"pubDate":16998,"source":6423,"url":16999,"thumbnail":17000},"Updated production-ready Gemini models, reduced 1.5 Pro pricing, increased rate limits, and more","We’re releasing two updated production-ready Gemini models",["Date","2024-09-24T16:03:03.000Z"],"https://deepmind.google/discover/blog/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/","https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemini-15-Flash-Social_1.2e16d0ba.fill-1200x600.png","src/content/posts/2024-09-24-updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more.md","2496b869c9dd95e1",{"html":25,"metadata":17004},{"headings":17005,"localImagePaths":17006,"remoteImagePaths":17007,"frontmatter":17008,"imagePaths":17010},[],[],[],{"title":16996,"description":16997,"summary":16997,"pubDate":17009,"source":6423,"url":16999,"thumbnail":17000},"Tue, 24 Sep 2024 16:03:03 +0000",[],"2024-09-24-updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more.md","2024-09-25-llama-can-now-see-and-run-on-your-device---welcome-llama-32",{"id":17012,"data":17014,"filePath":17020,"digest":17021,"rendered":17022,"legacyId":17030},{"title":17015,"description":25,"summary":17016,"pubDate":17017,"source":2720,"url":17018,"thumbnail":17019},"Llama can now see and run on your device - welcome Llama 3.2","Llama can now see and run on your device - welcome Llama 3.2 Llama 3.2 is out! Today, we welcome the...",["Date","2024-09-25T00:00:00.000Z"],"https://huggingface.co/blog/llama32","https://huggingface.co/blog/assets/llama32/thumbnail.jpg","src/content/posts/2024-09-25-llama-can-now-see-and-run-on-your-device---welcome-llama-32.md","683bc8d0702bc553",{"html":25,"metadata":17023},{"headings":17024,"localImagePaths":17025,"remoteImagePaths":17026,"frontmatter":17027,"imagePaths":17029},[],[],[],{"title":17015,"description":25,"summary":17016,"pubDate":17028,"source":2720,"url":17018,"thumbnail":17019},"Wed, 25 Sep 2024 00:00:00 GMT",[],"2024-09-25-llama-can-now-see-and-run-on-your-device---welcome-llama-32.md","2024-09-26-how-alphachip-transformed-computer-chip-design",{"id":17031,"data":17033,"filePath":17039,"digest":17040,"rendered":17041,"legacyId":17049},{"title":17034,"description":17035,"summary":17035,"pubDate":17036,"source":6423,"url":17037,"thumbnail":17038},"How AlphaChip transformed computer chip design","Our AI method has accelerated and optimized chip design, and its superhuman chip layouts are used in hardware around the world.",["Date","2024-09-26T14:08:00.000Z"],"https://deepmind.google/discover/blog/how-alphachip-transformed-computer-chip-design/","https://lh3.googleusercontent.com/Y_xdq8eqcQlZXYk-MZ2OWPpppmWG6LAQ8DZ-LZFUh8TV5s2TBb3RK_VkMUe-skRzIop5aP6Ot9xPMWFaWmenz55EwxVFCMszpTg2EzsyOd6ftlllGyE=w1200-h630-n-nu","src/content/posts/2024-09-26-how-alphachip-transformed-computer-chip-design.md","1b3a9baeea780ada",{"html":25,"metadata":17042},{"headings":17043,"localImagePaths":17044,"remoteImagePaths":17045,"frontmatter":17046,"imagePaths":17048},[],[],[],{"title":17034,"description":17035,"summary":17035,"pubDate":17047,"source":6423,"url":17037,"thumbnail":17038},"Thu, 26 Sep 2024 14:08:00 +0000",[],"2024-09-26-how-alphachip-transformed-computer-chip-design.md","2024-09-26-minnesotas-enterprise-translation-office-uses-chatgpt-to-bridge-language-gaps",{"id":17050,"data":17052,"filePath":17056,"digest":17057,"rendered":17058,"legacyId":17066},{"title":17053,"description":17053,"summary":17053,"pubDate":17054,"source":19,"url":17055,"thumbnail":21},"Minnesota’s Enterprise Translation Office uses ChatGPT to bridge language gaps",["Date","2024-09-26T07:00:00.000Z"],"https://openai.com/blog/state-of-minnesota","src/content/posts/2024-09-26-minnesotas-enterprise-translation-office-uses-chatgpt-to-bridge-language-gaps.md","c9ac2c456aa3051b",{"html":25,"metadata":17059},{"headings":17060,"localImagePaths":17061,"remoteImagePaths":17062,"frontmatter":17063,"imagePaths":17065},[],[],[],{"title":17053,"description":17053,"summary":17053,"pubDate":17064,"source":19,"url":17055,"thumbnail":21},"Thu, 26 Sep 2024 07:00:00 GMT",[],"2024-09-26-minnesotas-enterprise-translation-office-uses-chatgpt-to-bridge-language-gaps.md","2024-09-26-openai-and-gedi-partner-for-italian-news-content",{"id":17067,"data":17069,"filePath":17074,"digest":17075,"rendered":17076,"legacyId":17084},{"title":17070,"description":17071,"summary":17071,"pubDate":17072,"source":19,"url":17073,"thumbnail":21},"OpenAI and GEDI partner for Italian news content","OpenAI and GEDI announce strategic partnership to bring Italian-language news content to ChatGPT.",["Date","2024-09-26T04:30:00.000Z"],"https://openai.com/blog/gedi","src/content/posts/2024-09-26-openai-and-gedi-partner-for-italian-news-content.md","91a09458ed889ca5",{"html":25,"metadata":17077},{"headings":17078,"localImagePaths":17079,"remoteImagePaths":17080,"frontmatter":17081,"imagePaths":17083},[],[],[],{"title":17070,"description":17071,"summary":17071,"pubDate":17082,"source":19,"url":17073,"thumbnail":21},"Thu, 26 Sep 2024 04:30:00 GMT",[],"2024-09-26-openai-and-gedi-partner-for-italian-news-content.md","2024-09-26-upgrading-the-moderation-api-with-our-new-multimodal-moderation-model",{"id":17085,"data":17087,"filePath":17092,"digest":17093,"rendered":17094,"legacyId":17102},{"title":17088,"description":17089,"summary":17089,"pubDate":17090,"source":19,"url":17091,"thumbnail":21},"Upgrading the Moderation API with our new multimodal moderation model","We’re introducing a new model built on GPT-4o that is more accurate at detecting harmful text and images, enabling developers to build more robust moderation systems.",["Date","2024-09-26T10:00:00.000Z"],"https://openai.com/blog/upgrading-the-moderation-api-with-our-new-multimodal-moderation-model","src/content/posts/2024-09-26-upgrading-the-moderation-api-with-our-new-multimodal-moderation-model.md","5583b31bd1537b27",{"html":25,"metadata":17095},{"headings":17096,"localImagePaths":17097,"remoteImagePaths":17098,"frontmatter":17099,"imagePaths":17101},[],[],[],{"title":17088,"description":17089,"summary":17089,"pubDate":17100,"source":19,"url":17091,"thumbnail":21},"Thu, 26 Sep 2024 10:00:00 GMT",[],"2024-09-26-upgrading-the-moderation-api-with-our-new-multimodal-moderation-model.md","2024-09-30-converting-vertex-colored-meshes-to-textured-meshes",{"id":17103,"data":17105,"filePath":17111,"digest":17112,"rendered":17113,"legacyId":17121},{"title":17106,"description":25,"summary":17107,"pubDate":17108,"source":2720,"url":17109,"thumbnail":17110},"Converting Vertex-Colored Meshes to Textured Meshes","Converting Vertex-Colored Meshes to Textured Meshes Convert vertex-colored meshes to UV-mapped, text...",["Date","2024-09-30T00:00:00.000Z"],"https://huggingface.co/blog/vertex-colored-to-textured-mesh","https://huggingface.co/blog/assets/vertex-colored-to-textured-mesh/thumbnail.png","src/content/posts/2024-09-30-converting-vertex-colored-meshes-to-textured-meshes.md","c168570dd7487519",{"html":25,"metadata":17114},{"headings":17115,"localImagePaths":17116,"remoteImagePaths":17117,"frontmatter":17118,"imagePaths":17120},[],[],[],{"title":17106,"description":25,"summary":17107,"pubDate":17119,"source":2720,"url":17109,"thumbnail":17110},"Mon, 30 Sep 2024 00:00:00 GMT",[],"2024-09-30-converting-vertex-colored-meshes-to-textured-meshes.md","2024-09-30-put-ai-to-work-automate-and-scale-financial-operations",{"id":17122,"data":17124,"filePath":17128,"digest":17129,"rendered":17130,"legacyId":17137},{"title":17125,"description":17125,"summary":17125,"pubDate":17126,"source":19,"url":17127,"thumbnail":21},"Put AI to work: Automate and Scale Financial Operations",["Date","2024-09-30T00:00:00.000Z"],"https://openai.com/business/put-ai-to-work-automate-and-scale-financial-operations","src/content/posts/2024-09-30-put-ai-to-work-automate-and-scale-financial-operations.md","4e2d093f97a6c2da",{"html":25,"metadata":17131},{"headings":17132,"localImagePaths":17133,"remoteImagePaths":17134,"frontmatter":17135,"imagePaths":17136},[],[],[],{"title":17125,"description":17125,"summary":17125,"pubDate":17119,"source":19,"url":17127,"thumbnail":21},[],"2024-09-30-put-ai-to-work-automate-and-scale-financial-operations.md","2024-10-01-benczechmark---can-your-llm-understand-czech",{"id":17138,"data":17140,"filePath":17146,"digest":17147,"rendered":17148,"legacyId":17156},{"title":17141,"description":25,"summary":17142,"pubDate":17143,"source":2720,"url":17144,"thumbnail":17145},"🇨🇿 BenCzechMark - Can your LLM Understand Czech?","🇨🇿 BenCzechMark - Can your LLM Understand Czech? The 🇨🇿 BenCzechMark is the first and most comprehen...",["Date","2024-10-01T00:00:00.000Z"],"https://huggingface.co/blog/benczechmark","https://huggingface.co/blog/assets/187_benczechmark/thumbnail.png","src/content/posts/2024-10-01-benczechmark---can-your-llm-understand-czech.md","4254218a51c77078",{"html":25,"metadata":17149},{"headings":17150,"localImagePaths":17151,"remoteImagePaths":17152,"frontmatter":17153,"imagePaths":17155},[],[],[],{"title":17141,"description":25,"summary":17142,"pubDate":17154,"source":2720,"url":17144,"thumbnail":17145},"Tue, 01 Oct 2024 00:00:00 GMT",[],"2024-10-01-benczechmark---can-your-llm-understand-czech.md","2024-10-01-creating-agent-and-human-collaboration-with-gpt-4o",{"id":17157,"data":17159,"filePath":17164,"digest":17165,"rendered":17166,"legacyId":17174},{"title":17160,"description":17161,"summary":17161,"pubDate":17162,"source":19,"url":17163,"thumbnail":21},"Creating agent and human collaboration with GPT 4o","Altera uses GPT-4o to build a new area of human collaboration",["Date","2024-10-01T09:59:00.000Z"],"https://openai.com/blog/altera","src/content/posts/2024-10-01-creating-agent-and-human-collaboration-with-gpt-4o.md","dc19e2fce336b0ad",{"html":25,"metadata":17167},{"headings":17168,"localImagePaths":17169,"remoteImagePaths":17170,"frontmatter":17171,"imagePaths":17173},[],[],[],{"title":17160,"description":17161,"summary":17161,"pubDate":17172,"source":19,"url":17163,"thumbnail":21},"Tue, 01 Oct 2024 09:59:00 GMT",[],"2024-10-01-creating-agent-and-human-collaboration-with-gpt-4o.md","2024-10-01-introducing-the-realtime-api",{"id":17175,"data":17177,"filePath":17182,"digest":17183,"rendered":17184,"legacyId":17192},{"title":17178,"description":17179,"summary":17179,"pubDate":17180,"source":19,"url":17181,"thumbnail":21},"Introducing the Realtime API","Developers can now build fast speech-to-speech experiences into their applications",["Date","2024-10-01T10:05:00.000Z"],"https://openai.com/blog/introducing-the-realtime-api","src/content/posts/2024-10-01-introducing-the-realtime-api.md","6b587907d52df6c9",{"html":25,"metadata":17185},{"headings":17186,"localImagePaths":17187,"remoteImagePaths":17188,"frontmatter":17189,"imagePaths":17191},[],[],[],{"title":17178,"description":17179,"summary":17179,"pubDate":17190,"source":19,"url":17181,"thumbnail":21},"Tue, 01 Oct 2024 10:05:00 GMT",[],"2024-10-01-introducing-the-realtime-api.md","2024-10-01-introducing-vision-to-the-fine-tuning-api",{"id":17193,"data":17195,"filePath":17200,"digest":17201,"rendered":17202,"legacyId":17210},{"title":17196,"description":17197,"summary":17197,"pubDate":17198,"source":19,"url":17199,"thumbnail":21},"Introducing vision to the fine-tuning API","Developers can now fine-tune GPT-4o with images and text to improve vision capabilities",["Date","2024-10-01T10:04:00.000Z"],"https://openai.com/blog/introducing-vision-to-the-fine-tuning-api","src/content/posts/2024-10-01-introducing-vision-to-the-fine-tuning-api.md","05a79a4e4ebf2e7a",{"html":25,"metadata":17203},{"headings":17204,"localImagePaths":17205,"remoteImagePaths":17206,"frontmatter":17207,"imagePaths":17209},[],[],[],{"title":17196,"description":17197,"summary":17197,"pubDate":17208,"source":19,"url":17199,"thumbnail":21},"Tue, 01 Oct 2024 10:04:00 GMT",[],"2024-10-01-introducing-vision-to-the-fine-tuning-api.md","2024-10-01-prompt-caching-in-the-api",{"id":17211,"data":17213,"filePath":17218,"digest":17219,"rendered":17220,"legacyId":17228},{"title":17214,"description":17215,"summary":17215,"pubDate":17216,"source":19,"url":17217,"thumbnail":21},"Prompt Caching in the API","Offering automatic discounts on inputs that the model has recently seen",["Date","2024-10-01T10:03:00.000Z"],"https://openai.com/blog/api-prompt-caching","src/content/posts/2024-10-01-prompt-caching-in-the-api.md","789690b694cdf2e7",{"html":25,"metadata":17221},{"headings":17222,"localImagePaths":17223,"remoteImagePaths":17224,"frontmatter":17225,"imagePaths":17227},[],[],[],{"title":17214,"description":17215,"summary":17215,"pubDate":17226,"source":19,"url":17217,"thumbnail":21},"Tue, 01 Oct 2024 10:03:00 GMT",[],"2024-10-01-prompt-caching-in-the-api.md","2024-10-01-model-distillation-in-the-api",{"id":17229,"data":17231,"filePath":17236,"digest":17237,"rendered":17238,"legacyId":17246},{"title":17232,"description":17233,"summary":17233,"pubDate":17234,"source":19,"url":17235,"thumbnail":21},"Model Distillation in the API","Fine-tune a cost-efficient model with the outputs of a large frontier model–all on the OpenAI platform",["Date","2024-10-01T10:02:00.000Z"],"https://openai.com/blog/api-model-distillation","src/content/posts/2024-10-01-model-distillation-in-the-api.md","a1bfc3af7cc900c1",{"html":25,"metadata":17239},{"headings":17240,"localImagePaths":17241,"remoteImagePaths":17242,"frontmatter":17243,"imagePaths":17245},[],[],[],{"title":17232,"description":17233,"summary":17233,"pubDate":17244,"source":19,"url":17235,"thumbnail":21},"Tue, 01 Oct 2024 10:02:00 GMT",[],"2024-10-01-model-distillation-in-the-api.md","2024-10-02-new-funding-to-scale-the-benefits-of-ai",{"id":17247,"data":17249,"filePath":17254,"digest":17255,"rendered":17256,"legacyId":17264},{"title":17250,"description":17251,"summary":17251,"pubDate":17252,"source":19,"url":17253,"thumbnail":21},"New funding to scale the benefits of AI","We are making progress on our mission to ensure that artificial general intelligence benefits all of humanity.",["Date","2024-10-02T10:00:00.000Z"],"https://openai.com/blog/scale-the-benefits-of-ai","src/content/posts/2024-10-02-new-funding-to-scale-the-benefits-of-ai.md","9992eec1d550dc6c",{"html":25,"metadata":17257},{"headings":17258,"localImagePaths":17259,"remoteImagePaths":17260,"frontmatter":17261,"imagePaths":17263},[],[],[],{"title":17250,"description":17251,"summary":17251,"pubDate":17262,"source":19,"url":17253,"thumbnail":21},"Wed, 02 Oct 2024 10:00:00 GMT",[],"2024-10-02-new-funding-to-scale-the-benefits-of-ai.md","2024-10-03-a-short-summary-of-chinese-ai-global-expansion",{"id":17265,"data":17267,"filePath":17273,"digest":17274,"rendered":17275,"legacyId":17283},{"title":17268,"description":25,"summary":17269,"pubDate":17270,"source":2720,"url":17271,"thumbnail":17272},"A Short Summary of Chinese AI Global Expansion","A Short Summary of Chinese AI Global Expansion In the early 15th century, Zheng He (also known as Ch...",["Date","2024-10-03T00:00:00.000Z"],"https://huggingface.co/blog/chinese-ai-expansion","https://huggingface.co/blog/assets/chinese-ai-expansion/thumbnail.png","src/content/posts/2024-10-03-a-short-summary-of-chinese-ai-global-expansion.md","90ea40212d81861c",{"html":25,"metadata":17276},{"headings":17277,"localImagePaths":17278,"remoteImagePaths":17279,"frontmatter":17280,"imagePaths":17282},[],[],[],{"title":17268,"description":25,"summary":17269,"pubDate":17281,"source":2720,"url":17271,"thumbnail":17272},"Thu, 03 Oct 2024 00:00:00 GMT",[],"2024-10-03-a-short-summary-of-chinese-ai-global-expansion.md","2024-10-03-introducing-canvas-a-new-way-to-write-and-code-with-chatgpt",{"id":17284,"data":17286,"filePath":17291,"digest":17292,"rendered":17293,"legacyId":17301},{"title":17287,"description":17288,"summary":17288,"pubDate":17289,"source":19,"url":17290,"thumbnail":21},"Introducing canvas, a new way to write and code with ChatGPT.","Introducing canvas",["Date","2024-10-03T10:00:00.000Z"],"https://openai.com/blog/introducing-canvas","src/content/posts/2024-10-03-introducing-canvas-a-new-way-to-write-and-code-with-chatgpt.md","d52b2b15a6abedaa",{"html":25,"metadata":17294},{"headings":17295,"localImagePaths":17296,"remoteImagePaths":17297,"frontmatter":17298,"imagePaths":17300},[],[],[],{"title":17287,"description":17288,"summary":17288,"pubDate":17299,"source":19,"url":17290,"thumbnail":21},"Thu, 03 Oct 2024 10:00:00 GMT",[],"2024-10-03-introducing-canvas-a-new-way-to-write-and-code-with-chatgpt.md","2024-10-03-new-credit-facility-enhances-financial-flexibility",{"id":17302,"data":17304,"filePath":17309,"digest":17310,"rendered":17311,"legacyId":17319},{"title":17305,"description":17306,"summary":17306,"pubDate":17307,"source":19,"url":17308,"thumbnail":21},"New Credit Facility Enhances Financial Flexibility","In addition to securing $6.6 billion in new funding from leading investors, we have established a new $4 billion credit facility with leading banks, including JPMorgan Chase, Citi, Goldman Sachs, Morgan Stanley, Santander, Wells Fargo, SMBC, UBS, and HSBC.",["Date","2024-10-03T07:00:00.000Z"],"https://openai.com/blog/new-credit-facility-enhances-financial-flexibility","src/content/posts/2024-10-03-new-credit-facility-enhances-financial-flexibility.md","ec1ce919ab857acd",{"html":25,"metadata":17312},{"headings":17313,"localImagePaths":17314,"remoteImagePaths":17315,"frontmatter":17316,"imagePaths":17318},[],[],[],{"title":17305,"description":17306,"summary":17306,"pubDate":17317,"source":19,"url":17308,"thumbnail":21},"Thu, 03 Oct 2024 07:00:00 GMT",[],"2024-10-03-new-credit-facility-enhances-financial-flexibility.md","2024-10-04-introducing-the-open-finllm-leaderboard",{"id":17320,"data":17322,"filePath":17328,"digest":17329,"rendered":17330,"legacyId":17338},{"title":17323,"description":25,"summary":17324,"pubDate":17325,"source":2720,"url":17326,"thumbnail":17327},"Introducing the Open FinLLM Leaderboard","Introducing the Open FinLLM Leaderboard Finding the best LLM models for finance use cases The growin...",["Date","2024-10-04T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-finbench","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_finbench.png","src/content/posts/2024-10-04-introducing-the-open-finllm-leaderboard.md","adbe1f51e8bce2d1",{"html":25,"metadata":17331},{"headings":17332,"localImagePaths":17333,"remoteImagePaths":17334,"frontmatter":17335,"imagePaths":17337},[],[],[],{"title":17323,"description":25,"summary":17324,"pubDate":17336,"source":2720,"url":17326,"thumbnail":17327},"Fri, 04 Oct 2024 00:00:00 GMT",[],"2024-10-04-introducing-the-open-finllm-leaderboard.md","2024-10-05-improving-parquet-dedupe-on-hugging-face-hub",{"id":17339,"data":17341,"filePath":17347,"digest":17348,"rendered":17349,"legacyId":17357},{"title":17342,"description":25,"summary":17343,"pubDate":17344,"source":2720,"url":17345,"thumbnail":17346},"Improving Parquet Dedupe on Hugging Face Hub","Improving Parquet Dedupe on Hugging Face Hub The Xet team at Hugging Face is working on improving th...",["Date","2024-10-05T00:00:00.000Z"],"https://huggingface.co/blog/improve_parquet_dedupe","https://huggingface.co/blog/assets/improve_parquet_dedupe/thumbnail.png","src/content/posts/2024-10-05-improving-parquet-dedupe-on-hugging-face-hub.md","7d10a6c770ebaf69",{"html":25,"metadata":17350},{"headings":17351,"localImagePaths":17352,"remoteImagePaths":17353,"frontmatter":17354,"imagePaths":17356},[],[],[],{"title":17342,"description":25,"summary":17343,"pubDate":17355,"source":2720,"url":17345,"thumbnail":17346},"Sat, 05 Oct 2024 00:00:00 GMT",[],"2024-10-05-improving-parquet-dedupe-on-hugging-face-hub.md","2024-10-08-openai-and-hearst-content-partnership",{"id":17358,"data":17360,"filePath":17365,"digest":17366,"rendered":17367,"legacyId":17375},{"title":17361,"description":17362,"summary":17362,"pubDate":17363,"source":19,"url":17364,"thumbnail":21},"OpenAI and Hearst Content Partnership","Hearst’s iconic brands bring curated lifestyle and local news content to OpenAI’s products.",["Date","2024-10-08T10:00:00.000Z"],"https://openai.com/blog/hearst","src/content/posts/2024-10-08-openai-and-hearst-content-partnership.md","f09328364f91e859",{"html":25,"metadata":17368},{"headings":17369,"localImagePaths":17370,"remoteImagePaths":17371,"frontmatter":17372,"imagePaths":17374},[],[],[],{"title":17361,"description":17362,"summary":17362,"pubDate":17373,"source":19,"url":17364,"thumbnail":21},"Tue, 08 Oct 2024 10:00:00 GMT",[],"2024-10-08-openai-and-hearst-content-partnership.md","2024-10-09-an-update-on-disrupting-deceptive-uses-of-ai",{"id":17376,"data":17378,"filePath":17383,"digest":17384,"rendered":17385,"legacyId":17393},{"title":17379,"description":17380,"summary":17380,"pubDate":17381,"source":19,"url":17382,"thumbnail":21},"An update on disrupting deceptive uses of AI","OpenAI’s mission is to ensure that artificial general intelligence benefits all of humanity. We are dedicated to identifying, preventing, and disrupting attempts to abuse our models for harmful ends.",["Date","2024-10-09T03:30:00.000Z"],"https://openai.com/global-affairs/an-update-on-disrupting-deceptive-uses-of-ai","src/content/posts/2024-10-09-an-update-on-disrupting-deceptive-uses-of-ai.md","e299e10741e7713d",{"html":25,"metadata":17386},{"headings":17387,"localImagePaths":17388,"remoteImagePaths":17389,"frontmatter":17390,"imagePaths":17392},[],[],[],{"title":17379,"description":17380,"summary":17380,"pubDate":17391,"source":19,"url":17382,"thumbnail":21},"Wed, 09 Oct 2024 03:30:00 GMT",[],"2024-10-09-an-update-on-disrupting-deceptive-uses-of-ai.md","2024-10-09-demis-hassabis-john-jumper-awarded-nobel-prize-in-chemistry",{"id":17394,"data":17396,"filePath":17402,"digest":17403,"rendered":17404,"legacyId":17412},{"title":17397,"description":17398,"summary":17398,"pubDate":17399,"source":6423,"url":17400,"thumbnail":17401},"Demis Hassabis & John Jumper awarded Nobel Prize in Chemistry","The award recognizes their work developing AlphaFold, a groundbreaking AI system that predicts the 3D structure of proteins from their amino acid sequences.",["Date","2024-10-09T11:45:00.000Z"],"https://deepmind.google/discover/blog/demis-hassabis-john-jumper-awarded-nobel-prize-in-chemistry/","https://lh3.googleusercontent.com/7ZdZh5xhoD5NnykRBJHACxkxc3VubCdJLGHty2nYdJ36pBLVxRWO3Keu9C2Tum4OHCyGbJ5K5mB8R_oR94JG700qenuZ2rhq2sKjN4IkjIoU9Chv=w1200-h630-n-nu","src/content/posts/2024-10-09-demis-hassabis-john-jumper-awarded-nobel-prize-in-chemistry.md","5d7b4fb7e554944e",{"html":25,"metadata":17405},{"headings":17406,"localImagePaths":17407,"remoteImagePaths":17408,"frontmatter":17409,"imagePaths":17411},[],[],[],{"title":17397,"description":17398,"summary":17398,"pubDate":17410,"source":6423,"url":17400,"thumbnail":17401},"Wed, 09 Oct 2024 11:45:00 +0000",[],"2024-10-09-demis-hassabis-john-jumper-awarded-nobel-prize-in-chemistry.md","2024-10-09-scaling-ai-based-data-processing-with-hugging-face-dask",{"id":17413,"data":17415,"filePath":17421,"digest":17422,"rendered":17423,"legacyId":17431},{"title":17416,"description":25,"summary":17417,"pubDate":17418,"source":2720,"url":17419,"thumbnail":17420},"Scaling AI-based Data Processing with Hugging Face + Dask","Scaling AI-Based Data Processing with Hugging Face + Dask The Hugging Face platform has many dataset...",["Date","2024-10-09T00:00:00.000Z"],"https://huggingface.co/blog/dask-scaling","https://huggingface.co/blog/assets/dask-scaling/thumbnail.png","src/content/posts/2024-10-09-scaling-ai-based-data-processing-with-hugging-face-dask.md","19a3c6c86f44299e",{"html":25,"metadata":17424},{"headings":17425,"localImagePaths":17426,"remoteImagePaths":17427,"frontmatter":17428,"imagePaths":17430},[],[],[],{"title":17416,"description":25,"summary":17417,"pubDate":17429,"source":2720,"url":17419,"thumbnail":17420},"Wed, 09 Oct 2024 00:00:00 GMT",[],"2024-10-09-scaling-ai-based-data-processing-with-hugging-face-dask.md","2024-10-08-faster-assisted-generation-with-dynamic-speculation",{"id":17432,"data":17434,"filePath":17439,"digest":17440,"rendered":17441,"legacyId":17449},{"title":17435,"description":25,"summary":17436,"pubDate":17437,"source":2720,"url":17438,"thumbnail":12264},"Faster Assisted Generation with Dynamic Speculation","Faster Assisted Generation with Dynamic Speculation ⭐ In this blog post, we’ll explore dynamic specu...",["Date","2024-10-08T00:00:00.000Z"],"https://huggingface.co/blog/dynamic_speculation_lookahead","src/content/posts/2024-10-08-faster-assisted-generation-with-dynamic-speculation.md","b8436063f619e481",{"html":25,"metadata":17442},{"headings":17443,"localImagePaths":17444,"remoteImagePaths":17445,"frontmatter":17446,"imagePaths":17448},[],[],[],{"title":17435,"description":25,"summary":17436,"pubDate":17447,"source":2720,"url":17438,"thumbnail":12264},"Tue, 08 Oct 2024 00:00:00 GMT",[],"2024-10-08-faster-assisted-generation-with-dynamic-speculation.md","2024-10-09-welcome-gradio-5",{"id":17450,"data":17452,"filePath":17458,"digest":17459,"rendered":17460,"legacyId":17467},{"title":17453,"description":25,"summary":17454,"pubDate":17455,"source":2720,"url":17456,"thumbnail":17457},"Welcome, Gradio 5","Welcome, Gradio 5 We’ve been hard at work over the past few months, and we are excited to now announ...",["Date","2024-10-09T00:00:00.000Z"],"https://huggingface.co/blog/gradio-5","https://huggingface.co/blog/assets/gradio-5/thumbnail.png","src/content/posts/2024-10-09-welcome-gradio-5.md","96e5fcc5cdde360a",{"html":25,"metadata":17461},{"headings":17462,"localImagePaths":17463,"remoteImagePaths":17464,"frontmatter":17465,"imagePaths":17466},[],[],[],{"title":17453,"description":25,"summary":17454,"pubDate":17429,"source":2720,"url":17456,"thumbnail":17457},[],"2024-10-09-welcome-gradio-5.md","2024-10-10-a-security-review-of-gradio-5",{"id":17468,"data":17470,"filePath":17476,"digest":17477,"rendered":17478,"legacyId":17486},{"title":17471,"description":25,"summary":17472,"pubDate":17473,"source":2720,"url":17474,"thumbnail":17475},"A Security Review of Gradio 5","A Security Review of Gradio 5 We audited Gradio 5 so that your machine learning apps are safe! In th...",["Date","2024-10-10T00:00:00.000Z"],"https://huggingface.co/blog/gradio-5-security","https://huggingface.co/blog/assets/gradio-5-security/thumbnail.png","src/content/posts/2024-10-10-a-security-review-of-gradio-5.md","353fd902ebcc70e9",{"html":25,"metadata":17479},{"headings":17480,"localImagePaths":17481,"remoteImagePaths":17482,"frontmatter":17483,"imagePaths":17485},[],[],[],{"title":17471,"description":25,"summary":17472,"pubDate":17484,"source":2720,"url":17474,"thumbnail":17475},"Thu, 10 Oct 2024 00:00:00 GMT",[],"2024-10-10-a-security-review-of-gradio-5.md","2024-10-10-introducing-the-amd-5th-gen-epyc-cpu",{"id":17487,"data":17489,"filePath":17494,"digest":17495,"rendered":17496,"legacyId":17503},{"title":17490,"description":25,"summary":17491,"pubDate":17492,"source":2720,"url":17493,"thumbnail":11510},"Introducing the AMD 5th Gen EPYC™ CPU","Introducing the AMD 5th Gen EPYC™ CPU AMD has just unveiled its 5th generation of server-grade EPYC ...",["Date","2024-10-10T00:00:00.000Z"],"https://huggingface.co/blog/huggingface-amd-turin","src/content/posts/2024-10-10-introducing-the-amd-5th-gen-epyc-cpu.md","7dcc71febf3083e8",{"html":25,"metadata":17497},{"headings":17498,"localImagePaths":17499,"remoteImagePaths":17500,"frontmatter":17501,"imagePaths":17502},[],[],[],{"title":17490,"description":25,"summary":17491,"pubDate":17484,"source":2720,"url":17493,"thumbnail":11510},[],"2024-10-10-introducing-the-amd-5th-gen-epyc-cpu.md","2024-10-10-mle-bench-evaluating-machine-learning-agents-on-machine-learning-engineering",{"id":17504,"data":17506,"filePath":17511,"digest":17512,"rendered":17513,"legacyId":17521},{"title":17507,"description":17508,"summary":17508,"pubDate":17509,"source":19,"url":17510,"thumbnail":21},"MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering","We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering.",["Date","2024-10-10T10:00:00.000Z"],"https://openai.com/blog/mle-bench","src/content/posts/2024-10-10-mle-bench-evaluating-machine-learning-agents-on-machine-learning-engineering.md","43383422fd955555",{"html":25,"metadata":17514},{"headings":17515,"localImagePaths":17516,"remoteImagePaths":17517,"frontmatter":17518,"imagePaths":17520},[],[],[],{"title":17507,"description":17508,"summary":17508,"pubDate":17519,"source":19,"url":17510,"thumbnail":21},"Thu, 10 Oct 2024 10:00:00 GMT",[],"2024-10-10-mle-bench-evaluating-machine-learning-agents-on-machine-learning-engineering.md","2024-10-04-openais-raising-concerns-policy",{"id":17522,"data":17524,"filePath":17529,"digest":17530,"rendered":17531,"legacyId":17539},{"title":17525,"description":17526,"summary":17526,"pubDate":17527,"source":19,"url":17528,"thumbnail":21},"OpenAI’s Raising Concerns Policy","We’re publishing our Raising Concerns Policy, which protects employees’ rights to make protected disclosures.",["Date","2024-10-04T12:00:00.000Z"],"https://openai.com/blog/openai-raising-concerns-policy","src/content/posts/2024-10-04-openais-raising-concerns-policy.md","e52f71a1544db5ee",{"html":25,"metadata":17532},{"headings":17533,"localImagePaths":17534,"remoteImagePaths":17535,"frontmatter":17536,"imagePaths":17538},[],[],[],{"title":17525,"description":17526,"summary":17526,"pubDate":17537,"source":19,"url":17528,"thumbnail":21},"Fri, 04 Oct 2024 12:00:00 GMT",[],"2024-10-04-openais-raising-concerns-policy.md","2024-10-16-fixing-gradient-accumulation",{"id":17540,"data":17542,"filePath":17548,"digest":17549,"rendered":17550,"legacyId":17558},{"title":17543,"description":25,"summary":17544,"pubDate":17545,"source":2720,"url":17546,"thumbnail":17547},"Fixing Gradient Accumulation","Fixing Gradient Accumulation Our friends at Unsloth shared an issue regarding gradient accumulation ...",["Date","2024-10-16T00:00:00.000Z"],"https://huggingface.co/blog/gradient_accumulation","https://huggingface.co/blog/assets/gradient_accumulation/gradient_accumulation.png","src/content/posts/2024-10-16-fixing-gradient-accumulation.md","4564a0e4a357c600",{"html":25,"metadata":17551},{"headings":17552,"localImagePaths":17553,"remoteImagePaths":17554,"frontmatter":17555,"imagePaths":17557},[],[],[],{"title":17543,"description":25,"summary":17544,"pubDate":17556,"source":2720,"url":17546,"thumbnail":17547},"Wed, 16 Oct 2024 00:00:00 GMT",[],"2024-10-16-fixing-gradient-accumulation.md","2024-10-15-evaluating-fairness-in-chatgpt",{"id":17559,"data":17561,"filePath":17566,"digest":17567,"rendered":17568,"legacyId":17576},{"title":17562,"description":17563,"summary":17563,"pubDate":17564,"source":19,"url":17565,"thumbnail":21},"Evaluating fairness in ChatGPT","We've analyzed how ChatGPT responds to users based on their name, using AI research assistants to protect privacy.",["Date","2024-10-15T10:00:00.000Z"],"https://openai.com/blog/evaluating-fairness-in-chatgpt","src/content/posts/2024-10-15-evaluating-fairness-in-chatgpt.md","0844aac5ba23b19f",{"html":25,"metadata":17569},{"headings":17570,"localImagePaths":17571,"remoteImagePaths":17572,"frontmatter":17573,"imagePaths":17575},[],[],[],{"title":17562,"description":17563,"summary":17563,"pubDate":17574,"source":19,"url":17565,"thumbnail":21},"Tue, 15 Oct 2024 10:00:00 GMT",[],"2024-10-15-evaluating-fairness-in-chatgpt.md","2024-10-17-solving-complex-problems-with-openai-o1-models",{"id":17577,"data":17579,"filePath":17584,"digest":17585,"rendered":17586,"legacyId":17594},{"title":17580,"description":17581,"summary":17581,"pubDate":17582,"source":19,"url":17583,"thumbnail":21},"Solving complex problems with OpenAI o1 models","In this video, we share how the o1 reasoning models can help in domains like coding, strategy, and research.",["Date","2024-10-17T00:00:00.000Z"],"https://openai.com/business/solving-complex-problems-with-openai-o1-models","src/content/posts/2024-10-17-solving-complex-problems-with-openai-o1-models.md","c501fda2e9b7720f",{"html":25,"metadata":17587},{"headings":17588,"localImagePaths":17589,"remoteImagePaths":17590,"frontmatter":17591,"imagePaths":17593},[],[],[],{"title":17580,"description":17581,"summary":17581,"pubDate":17592,"source":19,"url":17583,"thumbnail":21},"Thu, 17 Oct 2024 00:00:00 GMT",[],"2024-10-17-solving-complex-problems-with-openai-o1-models.md","2024-10-21-llama-32-in-keras",{"id":17595,"data":17597,"filePath":17603,"digest":17604,"rendered":17605,"legacyId":17613},{"title":17598,"description":25,"summary":17599,"pubDate":17600,"source":2720,"url":17601,"thumbnail":17602},"Llama 3.2 in Keras","Llama 3.2 in Keras This is going to be the shortest blog post ever. Question: Llama 3.2 landed two w...",["Date","2024-10-21T00:00:00.000Z"],"https://huggingface.co/blog/keras-llama-32","https://huggingface.co/blog/assets/keras_llama_32/thumbnail.jpg","src/content/posts/2024-10-21-llama-32-in-keras.md","a754e2dd8de3dc9a",{"html":25,"metadata":17606},{"headings":17607,"localImagePaths":17608,"remoteImagePaths":17609,"frontmatter":17610,"imagePaths":17612},[],[],[],{"title":17598,"description":25,"summary":17599,"pubDate":17611,"source":2720,"url":17601,"thumbnail":17602},"Mon, 21 Oct 2024 00:00:00 GMT",[],"2024-10-21-llama-32-in-keras.md","2024-10-22-deploying-speech-to-speech-on-hugging-face",{"id":17614,"data":17616,"filePath":17622,"digest":17623,"rendered":17624,"legacyId":17632},{"title":17617,"description":25,"summary":17618,"pubDate":17619,"source":2720,"url":17620,"thumbnail":17621},"Deploying Speech-to-Speech on Hugging Face","Deploying Speech-to-Speech on Hugging Face Introduction Speech-to-Speech (S2S) is an exciting new pr...",["Date","2024-10-22T00:00:00.000Z"],"https://huggingface.co/blog/s2s_endpoint","https://huggingface.co/blog/assets/s2s_endpoint/thumbnail.png","src/content/posts/2024-10-22-deploying-speech-to-speech-on-hugging-face.md","bd057a4ca860d974",{"html":25,"metadata":17625},{"headings":17626,"localImagePaths":17627,"remoteImagePaths":17628,"frontmatter":17629,"imagePaths":17631},[],[],[],{"title":17617,"description":25,"summary":17618,"pubDate":17630,"source":2720,"url":17620,"thumbnail":17621},"Tue, 22 Oct 2024 00:00:00 GMT",[],"2024-10-22-deploying-speech-to-speech-on-hugging-face.md","2024-10-22-diffusers-welcomes-stable-diffusion-35-large",{"id":17633,"data":17635,"filePath":17641,"digest":17642,"rendered":17643,"legacyId":17650},{"title":17636,"description":25,"summary":17637,"pubDate":17638,"source":2720,"url":17639,"thumbnail":17640},"🧨 Diffusers welcomes Stable Diffusion 3.5 Large","🧨 Diffusers welcomes Stable Diffusion 3.5 Large Stable Diffusion 3.5 is the improved variant of its ...",["Date","2024-10-22T00:00:00.000Z"],"https://huggingface.co/blog/sd3-5","https://huggingface.co/blog/assets/sd3-5/thumbnail.png","src/content/posts/2024-10-22-diffusers-welcomes-stable-diffusion-35-large.md","8f7297b4fdd827e0",{"html":25,"metadata":17644},{"headings":17645,"localImagePaths":17646,"remoteImagePaths":17647,"frontmatter":17648,"imagePaths":17649},[],[],[],{"title":17636,"description":25,"summary":17637,"pubDate":17630,"source":2720,"url":17639,"thumbnail":17640},[],"2024-10-22-diffusers-welcomes-stable-diffusion-35-large.md","2024-10-22-dr-ronnie-chatterji-named-openais-first-chief-economist",{"id":17651,"data":17653,"filePath":17657,"digest":17658,"rendered":17659,"legacyId":17667},{"title":17654,"description":17654,"summary":17654,"pubDate":17655,"source":19,"url":17656,"thumbnail":21},"Dr. Ronnie Chatterji named OpenAI’s first Chief Economist",["Date","2024-10-22T10:05:00.000Z"],"https://openai.com/global-affairs/openai-chief-economist-announcement","src/content/posts/2024-10-22-dr-ronnie-chatterji-named-openais-first-chief-economist.md","10dd9578c23db093",{"html":25,"metadata":17660},{"headings":17661,"localImagePaths":17662,"remoteImagePaths":17663,"frontmatter":17664,"imagePaths":17666},[],[],[],{"title":17654,"description":17654,"summary":17654,"pubDate":17665,"source":19,"url":17656,"thumbnail":21},"Tue, 22 Oct 2024 10:05:00 GMT",[],"2024-10-22-dr-ronnie-chatterji-named-openais-first-chief-economist.md","2024-10-22-hugging-face-teams-up-with-protect-ai-enhancing-model-security-for-the-community",{"id":17668,"data":17670,"filePath":17676,"digest":17677,"rendered":17678,"legacyId":17685},{"title":17671,"description":25,"summary":17672,"pubDate":17673,"source":2720,"url":17674,"thumbnail":17675},"Hugging Face Teams Up with Protect AI: Enhancing Model Security for the Community","Hugging Face Teams Up with Protect AI: Enhancing Model Security for the Community We are pleased to ...",["Date","2024-10-22T00:00:00.000Z"],"https://huggingface.co/blog/protectai","https://huggingface.co/blog/assets/protectai/thumbnail.png","src/content/posts/2024-10-22-hugging-face-teams-up-with-protect-ai-enhancing-model-security-for-the-community.md","6587d9a5fedf22d0",{"html":25,"metadata":17679},{"headings":17680,"localImagePaths":17681,"remoteImagePaths":17682,"frontmatter":17683,"imagePaths":17684},[],[],[],{"title":17671,"description":25,"summary":17672,"pubDate":17630,"source":2720,"url":17674,"thumbnail":17675},[],"2024-10-22-hugging-face-teams-up-with-protect-ai-enhancing-model-security-for-the-community.md","2024-10-22-openai-and-the-lenfest-institute-ai-collaborative-and-fellowship-program",{"id":17686,"data":17688,"filePath":17692,"digest":17693,"rendered":17694,"legacyId":17702},{"title":17689,"description":17689,"summary":17689,"pubDate":17690,"source":19,"url":17691,"thumbnail":21},"OpenAI and the Lenfest Institute AI Collaborative and Fellowship program",["Date","2024-10-22T06:05:00.000Z"],"https://openai.com/blog/lenfest-institute","src/content/posts/2024-10-22-openai-and-the-lenfest-institute-ai-collaborative-and-fellowship-program.md","e1b2367293e96f6c",{"html":25,"metadata":17695},{"headings":17696,"localImagePaths":17697,"remoteImagePaths":17698,"frontmatter":17699,"imagePaths":17701},[],[],[],{"title":17689,"description":17689,"summary":17689,"pubDate":17700,"source":19,"url":17691,"thumbnail":21},"Tue, 22 Oct 2024 06:05:00 GMT",[],"2024-10-22-openai-and-the-lenfest-institute-ai-collaborative-and-fellowship-program.md","2024-10-22-openai-appoints-scott-schools-as-chief-compliance-officer",{"id":17703,"data":17705,"filePath":17709,"digest":17710,"rendered":17711,"legacyId":17719},{"title":17706,"description":17706,"summary":17706,"pubDate":17707,"source":19,"url":17708,"thumbnail":21},"OpenAI appoints Scott Schools as Chief Compliance Officer",["Date","2024-10-22T10:30:00.000Z"],"https://openai.com/global-affairs/openai-chief-compliance-officer-announcement","src/content/posts/2024-10-22-openai-appoints-scott-schools-as-chief-compliance-officer.md","d7729a35f7fbf4af",{"html":25,"metadata":17712},{"headings":17713,"localImagePaths":17714,"remoteImagePaths":17715,"frontmatter":17716,"imagePaths":17718},[],[],[],{"title":17706,"description":17706,"summary":17706,"pubDate":17717,"source":19,"url":17708,"thumbnail":21},"Tue, 22 Oct 2024 10:30:00 GMT",[],"2024-10-22-openai-appoints-scott-schools-as-chief-compliance-officer.md","2024-10-22-releasing-outlines-core-010-structured-generation-in-rust-and-python",{"id":17720,"data":17722,"filePath":17728,"digest":17729,"rendered":17730,"legacyId":17737},{"title":17723,"description":25,"summary":17724,"pubDate":17725,"source":2720,"url":17726,"thumbnail":17727},"Releasing Outlines-core 0.1.0: structured generation in Rust and Python","Releasing Outlines-core 0.1.0: structured generation in Rust and Python dottxt and Hugging Face are ...",["Date","2024-10-22T00:00:00.000Z"],"https://huggingface.co/blog/outlines-core","https://huggingface.co/blog/assets/outlines-core/thumbnail.gif","src/content/posts/2024-10-22-releasing-outlines-core-010-structured-generation-in-rust-and-python.md","18b21695d8212cfd",{"html":25,"metadata":17731},{"headings":17732,"localImagePaths":17733,"remoteImagePaths":17734,"frontmatter":17735,"imagePaths":17736},[],[],[],{"title":17723,"description":25,"summary":17724,"pubDate":17630,"source":2720,"url":17726,"thumbnail":17727},[],"2024-10-22-releasing-outlines-core-010-structured-generation-in-rust-and-python.md","2024-10-22-transformersjs-v3-webgpu-support-new-models-tasks-and-more",{"id":17738,"data":17740,"filePath":17746,"digest":17747,"rendered":17748,"legacyId":17755},{"title":17741,"description":25,"summary":17742,"pubDate":17743,"source":2720,"url":17744,"thumbnail":17745},"Transformers.js v3: WebGPU support, new models & tasks, and more…","Transformers.js v3: WebGPU Support, New Models & Tasks, and More… After more than a year of developm...",["Date","2024-10-22T00:00:00.000Z"],"https://huggingface.co/blog/transformersjs-v3","https://huggingface.co/blog/assets/transformersjs-v3/thumbnail.png","src/content/posts/2024-10-22-transformersjs-v3-webgpu-support-new-models-tasks-and-more.md","d4e49e1dd8a5145c",{"html":25,"metadata":17749},{"headings":17750,"localImagePaths":17751,"remoteImagePaths":17752,"frontmatter":17753,"imagePaths":17754},[],[],[],{"title":17741,"description":25,"summary":17742,"pubDate":17630,"source":2720,"url":17744,"thumbnail":17745},[],"2024-10-22-transformersjs-v3-webgpu-support-new-models-tasks-and-more.md","2024-10-23-cinepile-20---making-stronger-datasets-with-adversarial-refinement",{"id":17756,"data":17758,"filePath":17764,"digest":17765,"rendered":17766,"legacyId":17774},{"title":17759,"description":25,"summary":17760,"pubDate":17761,"source":2720,"url":17762,"thumbnail":17763},"CinePile 2.0 - making stronger datasets with adversarial refinement","CinePile 2.0 - making stronger datasets with adversarial refinement In this blog post we share the j...",["Date","2024-10-23T00:00:00.000Z"],"https://huggingface.co/blog/cinepile2","https://huggingface.co/blog/assets/188_cinepile2/thumbnail.png","src/content/posts/2024-10-23-cinepile-20---making-stronger-datasets-with-adversarial-refinement.md","dd4d672c08fc73d4",{"html":25,"metadata":17767},{"headings":17768,"localImagePaths":17769,"remoteImagePaths":17770,"frontmatter":17771,"imagePaths":17773},[],[],[],{"title":17759,"description":25,"summary":17760,"pubDate":17772,"source":2720,"url":17762,"thumbnail":17763},"Wed, 23 Oct 2024 00:00:00 GMT",[],"2024-10-23-cinepile-20---making-stronger-datasets-with-adversarial-refinement.md","2024-10-23-introducing-hugs---scale-your-ai-with-open-models",{"id":17775,"data":17777,"filePath":17783,"digest":17784,"rendered":17785,"legacyId":17792},{"title":17778,"description":25,"summary":17779,"pubDate":17780,"source":2720,"url":17781,"thumbnail":17782},"Introducing HUGS - Scale your AI with Open Models","Introducing HUGS - Scale your AI with Open Models Today, we are thrilled to announce the launch of H...",["Date","2024-10-23T00:00:00.000Z"],"https://huggingface.co/blog/hugs","https://huggingface.co/blog/assets/hugs/thumbnail.jpg","src/content/posts/2024-10-23-introducing-hugs---scale-your-ai-with-open-models.md","a8f0e88cf22cce32",{"html":25,"metadata":17786},{"headings":17787,"localImagePaths":17788,"remoteImagePaths":17789,"frontmatter":17790,"imagePaths":17791},[],[],[],{"title":17778,"description":25,"summary":17779,"pubDate":17772,"source":2720,"url":17781,"thumbnail":17782},[],"2024-10-23-introducing-hugs---scale-your-ai-with-open-models.md","2024-10-23-introducing-synthid-text",{"id":17793,"data":17795,"filePath":17801,"digest":17802,"rendered":17803,"legacyId":17810},{"title":17796,"description":25,"summary":17797,"pubDate":17798,"source":2720,"url":17799,"thumbnail":17800},"Introducing SynthID Text","Introducing SynthID Text Do you find it difficult to tell if text was written by a human or generate...",["Date","2024-10-23T00:00:00.000Z"],"https://huggingface.co/blog/synthid-text","https://huggingface.co/blog/assets/synthid-text/thumbnail.png","src/content/posts/2024-10-23-introducing-synthid-text.md","ce683a55577fb8cd",{"html":25,"metadata":17804},{"headings":17805,"localImagePaths":17806,"remoteImagePaths":17807,"frontmatter":17808,"imagePaths":17809},[],[],[],{"title":17796,"description":25,"summary":17797,"pubDate":17772,"source":2720,"url":17799,"thumbnail":17800},[],"2024-10-23-introducing-synthid-text.md","2024-10-23-new-generative-ai-tools-open-the-doors-of-music-creation",{"id":17811,"data":17813,"filePath":17819,"digest":17820,"rendered":17821,"legacyId":17829},{"title":17814,"description":17815,"summary":17815,"pubDate":17816,"source":6423,"url":17817,"thumbnail":17818},"New generative AI tools open the doors of music creation","Our latest AI music technologies are now available in MusicFX DJ, Music AI Sandbox and YouTube Shorts",["Date","2024-10-23T16:53:00.000Z"],"https://deepmind.google/discover/blog/new-generative-ai-tools-open-the-doors-of-music-creation/","https://lh3.googleusercontent.com/7CWJ9fVeC97FrWgcispxyms9gTL_1PIDMIwBYTQNnU8S56JaxGB2Z4ThqZ-1vBTO-u-UBZg_cYhG8PtZjYP0rPabUbg5x2cCUnNJuiZAZBsE8u7Kvig=w1200-h630-n-nu","src/content/posts/2024-10-23-new-generative-ai-tools-open-the-doors-of-music-creation.md","74e1a6a065d5730b",{"html":25,"metadata":17822},{"headings":17823,"localImagePaths":17824,"remoteImagePaths":17825,"frontmatter":17826,"imagePaths":17828},[],[],[],{"title":17814,"description":17815,"summary":17815,"pubDate":17827,"source":6423,"url":17817,"thumbnail":17818},"Wed, 23 Oct 2024 16:53:00 +0000",[],"2024-10-23-new-generative-ai-tools-open-the-doors-of-music-creation.md","2024-10-23-simplifying-stabilizing-and-scaling-continuous-time-consistency-models",{"id":17830,"data":17832,"filePath":17837,"digest":17838,"rendered":17839,"legacyId":17847},{"title":17833,"description":17834,"summary":17834,"pubDate":17835,"source":19,"url":17836,"thumbnail":21},"Simplifying, stabilizing, and scaling continuous-time consistency models","We’ve simplified, stabilized, and scaled continuous-time consistency models, achieving comparable sample quality to leading diffusion models, while using only two sampling steps.",["Date","2024-10-23T10:00:00.000Z"],"https://openai.com/blog/simplifying-stabilizing-and-scaling-continuous-time-consistency-models","src/content/posts/2024-10-23-simplifying-stabilizing-and-scaling-continuous-time-consistency-models.md","376dc3e0dbb604ec",{"html":25,"metadata":17840},{"headings":17841,"localImagePaths":17842,"remoteImagePaths":17843,"frontmatter":17844,"imagePaths":17846},[],[],[],{"title":17833,"description":17834,"summary":17834,"pubDate":17845,"source":19,"url":17836,"thumbnail":21},"Wed, 23 Oct 2024 10:00:00 GMT",[],"2024-10-23-simplifying-stabilizing-and-scaling-continuous-time-consistency-models.md","2024-10-24-a-deepdive-into-aya-expanse-advancing-the-frontier-of-multilinguality",{"id":17848,"data":17850,"filePath":17856,"digest":17857,"rendered":17858,"legacyId":17866},{"title":17851,"description":25,"summary":17852,"pubDate":17853,"source":2720,"url":17854,"thumbnail":17855},"A Deepdive into Aya Expanse: Advancing the Frontier of Multilinguality","A Deepdive into Aya Expanse: Advancing the Frontier of Multilinguality This is a guest blog post by ...",["Date","2024-10-24T00:00:00.000Z"],"https://huggingface.co/blog/aya-expanse","https://huggingface.co/blog/assets/aya-expanse/thumbnail.jpg","src/content/posts/2024-10-24-a-deepdive-into-aya-expanse-advancing-the-frontier-of-multilinguality.md","acb65daf392e10ee",{"html":25,"metadata":17859},{"headings":17860,"localImagePaths":17861,"remoteImagePaths":17862,"frontmatter":17863,"imagePaths":17865},[],[],[],{"title":17851,"description":25,"summary":17852,"pubDate":17864,"source":2720,"url":17854,"thumbnail":17855},"Thu, 24 Oct 2024 00:00:00 GMT",[],"2024-10-24-a-deepdive-into-aya-expanse-advancing-the-frontier-of-multilinguality.md","2024-10-24-openais-approach-to-ai-and-national-security",{"id":17867,"data":17869,"filePath":17873,"digest":17874,"rendered":17875,"legacyId":17883},{"title":17870,"description":17870,"summary":17870,"pubDate":17871,"source":19,"url":17872,"thumbnail":21},"OpenAI’s approach to AI and national security",["Date","2024-10-24T14:00:00.000Z"],"https://openai.com/global-affairs/openais-approach-to-ai-and-national-security","src/content/posts/2024-10-24-openais-approach-to-ai-and-national-security.md","ca6cc6f0d23c33b7",{"html":25,"metadata":17876},{"headings":17877,"localImagePaths":17878,"remoteImagePaths":17879,"frontmatter":17880,"imagePaths":17882},[],[],[],{"title":17870,"description":17870,"summary":17870,"pubDate":17881,"source":19,"url":17872,"thumbnail":21},"Thu, 24 Oct 2024 14:00:00 GMT",[],"2024-10-24-openais-approach-to-ai-and-national-security.md","2024-10-28-expert-support-case-study-bolstering-a-rag-app-with-llm-as-a-judge",{"id":17884,"data":17886,"filePath":17892,"digest":17893,"rendered":17894,"legacyId":17902},{"title":17887,"description":25,"summary":17888,"pubDate":17889,"source":2720,"url":17890,"thumbnail":17891},"Expert Support case study: Bolstering a RAG app with LLM-as-a-Judge","Expert Support case study: Bolstering a RAG app with LLM-as-a-Judge This is a guest blog post author...",["Date","2024-10-28T00:00:00.000Z"],"https://huggingface.co/blog/digital-green-llm-judge","https://huggingface.co/blog/assets/digital-gren-llm-judge/thumbnail.png","src/content/posts/2024-10-28-expert-support-case-study-bolstering-a-rag-app-with-llm-as-a-judge.md","9b68c2f6cadf2661",{"html":25,"metadata":17895},{"headings":17896,"localImagePaths":17897,"remoteImagePaths":17898,"frontmatter":17899,"imagePaths":17901},[],[],[],{"title":17887,"description":25,"summary":17888,"pubDate":17900,"source":2720,"url":17890,"thumbnail":17891},"Mon, 28 Oct 2024 00:00:00 GMT",[],"2024-10-28-expert-support-case-study-bolstering-a-rag-app-with-llm-as-a-judge.md","2024-10-29-delivering-high-performance-customer-support",{"id":17903,"data":17905,"filePath":17910,"digest":17911,"rendered":17912,"legacyId":17920},{"title":17906,"description":17907,"summary":17907,"pubDate":17908,"source":19,"url":17909,"thumbnail":21},"Delivering high-performance customer support","Decagon and OpenAI deliver high-performance, fully automated customer support at scale",["Date","2024-10-29T10:00:00.000Z"],"https://openai.com/blog/decagon","src/content/posts/2024-10-29-delivering-high-performance-customer-support.md","1a1c17bdb85f6e8d",{"html":25,"metadata":17913},{"headings":17914,"localImagePaths":17915,"remoteImagePaths":17916,"frontmatter":17917,"imagePaths":17919},[],[],[],{"title":17906,"description":17907,"summary":17907,"pubDate":17918,"source":19,"url":17909,"thumbnail":21},"Tue, 29 Oct 2024 10:00:00 GMT",[],"2024-10-29-delivering-high-performance-customer-support.md","2024-10-29-universal-assisted-generation-faster-decoding-with-any-assistant-model",{"id":17921,"data":17923,"filePath":17928,"digest":17929,"rendered":17930,"legacyId":17938},{"title":17924,"description":25,"summary":17925,"pubDate":17926,"source":2720,"url":17927,"thumbnail":12264},"Universal Assisted Generation: Faster Decoding with Any Assistant Model","Universal Assisted Generation: Faster Decoding with Any Assistant Model TL;DR: Many LLMs such as gem...",["Date","2024-10-29T00:00:00.000Z"],"https://huggingface.co/blog/universal_assisted_generation","src/content/posts/2024-10-29-universal-assisted-generation-faster-decoding-with-any-assistant-model.md","7f936b27b26949e0",{"html":25,"metadata":17931},{"headings":17932,"localImagePaths":17933,"remoteImagePaths":17934,"frontmatter":17935,"imagePaths":17937},[],[],[],{"title":17924,"description":25,"summary":17925,"pubDate":17936,"source":2720,"url":17927,"thumbnail":12264},"Tue, 29 Oct 2024 00:00:00 GMT",[],"2024-10-29-universal-assisted-generation-faster-decoding-with-any-assistant-model.md","2024-10-30-introducing-simpleqa",{"id":17939,"data":17941,"filePath":17946,"digest":17947,"rendered":17948,"legacyId":17956},{"title":17942,"description":17943,"summary":17943,"pubDate":17944,"source":19,"url":17945,"thumbnail":21},"Introducing SimpleQA","A factuality benchmark called SimpleQA that measures the ability for language models to answer short, fact-seeking questions.",["Date","2024-10-30T10:00:00.000Z"],"https://openai.com/blog/introducing-simpleqa","src/content/posts/2024-10-30-introducing-simpleqa.md","d280306d33a987ee",{"html":25,"metadata":17949},{"headings":17950,"localImagePaths":17951,"remoteImagePaths":17952,"frontmatter":17953,"imagePaths":17955},[],[],[],{"title":17942,"description":17943,"summary":17943,"pubDate":17954,"source":19,"url":17945,"thumbnail":21},"Wed, 30 Oct 2024 10:00:00 GMT",[],"2024-10-30-introducing-simpleqa.md","2024-10-30-pushing-the-frontiers-of-audio-generation",{"id":17957,"data":17959,"filePath":17965,"digest":17966,"rendered":17967,"legacyId":17975},{"title":17960,"description":17961,"summary":17961,"pubDate":17962,"source":6423,"url":17963,"thumbnail":17964},"Pushing the frontiers of audio generation","Our pioneering speech generation technologies are helping people around the world interact with more natural, conversational and intuitive digital assistants and AI tools.",["Date","2024-10-30T15:00:00.000Z"],"https://deepmind.google/discover/blog/pushing-the-frontiers-of-audio-generation/","https://lh3.googleusercontent.com/wyFc1lo4ByOJsbbSt1NEwBiSi3KpImyqA9ukx-mLxJROIakSxhPwk-kPtlIfFKX9Txm2J_lbpIvnrDhFnegrpN8ihlvYpBTsFNAmOlq0C2rm_gef=w1200-h630-n-nu","src/content/posts/2024-10-30-pushing-the-frontiers-of-audio-generation.md","2c9a811bc059d881",{"html":25,"metadata":17968},{"headings":17969,"localImagePaths":17970,"remoteImagePaths":17971,"frontmatter":17972,"imagePaths":17974},[],[],[],{"title":17960,"description":17961,"summary":17961,"pubDate":17973,"source":6423,"url":17963,"thumbnail":17964},"Wed, 30 Oct 2024 15:00:00 +0000",[],"2024-10-30-pushing-the-frontiers-of-audio-generation.md","2024-10-31-introducing-chatgpt-search",{"id":17976,"data":17978,"filePath":17983,"digest":17984,"rendered":17985,"legacyId":17993},{"title":17979,"description":17980,"summary":17980,"pubDate":17981,"source":19,"url":17982,"thumbnail":21},"Introducing ChatGPT search","Get fast, timely answers with links to relevant web sources",["Date","2024-10-31T10:00:00.000Z"],"https://openai.com/blog/introducing-chatgpt-search","src/content/posts/2024-10-31-introducing-chatgpt-search.md","59f061307003836f",{"html":25,"metadata":17986},{"headings":17987,"localImagePaths":17988,"remoteImagePaths":17989,"frontmatter":17990,"imagePaths":17992},[],[],[],{"title":17979,"description":17980,"summary":17980,"pubDate":17991,"source":19,"url":17982,"thumbnail":21},"Thu, 31 Oct 2024 10:00:00 GMT",[],"2024-10-31-introducing-chatgpt-search.md","2024-10-31-promegas-top-down-adoption-of-chatgpt-accelerates-manufacturing-sales-and-marketing",{"id":17994,"data":17996,"filePath":18001,"digest":18002,"rendered":18003,"legacyId":18011},{"title":17997,"description":17998,"summary":17998,"pubDate":17999,"source":19,"url":18000,"thumbnail":21},"Promega’s top-down adoption of ChatGPT accelerates manufacturing, sales, and marketing","Promega's top-down adoption of ChatGPT accelerates manufacturing, sales, and marketing",["Date","2024-10-31T08:00:00.000Z"],"https://openai.com/blog/promega","src/content/posts/2024-10-31-promegas-top-down-adoption-of-chatgpt-accelerates-manufacturing-sales-and-marketing.md","d83a50ce8ee3d3bf",{"html":25,"metadata":18004},{"headings":18005,"localImagePaths":18006,"remoteImagePaths":18007,"frontmatter":18008,"imagePaths":18010},[],[],[],{"title":17997,"description":17998,"summary":17998,"pubDate":18009,"source":19,"url":18000,"thumbnail":21},"Thu, 31 Oct 2024 08:00:00 GMT",[],"2024-10-31-promegas-top-down-adoption-of-chatgpt-accelerates-manufacturing-sales-and-marketing.md","2024-10-31-put-ai-to-work-for-marketing-teams",{"id":18012,"data":18014,"filePath":18018,"digest":18019,"rendered":18020,"legacyId":18028},{"title":18015,"description":18015,"summary":18015,"pubDate":18016,"source":19,"url":18017,"thumbnail":21},"Put AI to Work for Marketing Teams",["Date","2024-10-31T00:00:00.000Z"],"https://openai.com/business/put-ai-to-work-for-marketing-teams","src/content/posts/2024-10-31-put-ai-to-work-for-marketing-teams.md","aafefae84e9180fd",{"html":25,"metadata":18021},{"headings":18022,"localImagePaths":18023,"remoteImagePaths":18024,"frontmatter":18025,"imagePaths":18027},[],[],[],{"title":18015,"description":18015,"summary":18015,"pubDate":18026,"source":19,"url":18017,"thumbnail":21},"Thu, 31 Oct 2024 00:00:00 GMT",[],"2024-10-31-put-ai-to-work-for-marketing-teams.md","2024-11-04-argilla-24-easily-build-fine-tuning-and-evaluation-datasets-on-the-hub-no-code-required",{"id":18029,"data":18031,"filePath":18037,"digest":18038,"rendered":18039,"legacyId":18047},{"title":18032,"description":25,"summary":18033,"pubDate":18034,"source":2720,"url":18035,"thumbnail":18036},"Argilla 2.4: Easily Build Fine-Tuning and Evaluation datasets on the Hub — No Code Required","Argilla 2.4: Easily Build Fine-Tuning and Evaluation Datasets on the Hub — No Code Required We are i...",["Date","2024-11-04T00:00:00.000Z"],"https://huggingface.co/blog/argilla-ui-hub","https://huggingface.co/blog/assets/argilla-ui-hub/thumbnail.png","src/content/posts/2024-11-04-argilla-24-easily-build-fine-tuning-and-evaluation-datasets-on-the-hub-no-code-required.md","a922fec3ba8b4a89",{"html":25,"metadata":18040},{"headings":18041,"localImagePaths":18042,"remoteImagePaths":18043,"frontmatter":18044,"imagePaths":18046},[],[],[],{"title":18032,"description":25,"summary":18033,"pubDate":18045,"source":2720,"url":18035,"thumbnail":18036},"Mon, 04 Nov 2024 00:00:00 GMT",[],"2024-11-04-argilla-24-easily-build-fine-tuning-and-evaluation-datasets-on-the-hub-no-code-required.md","2024-11-04-openais-comments-to-the-ntia-on-data-center-growth-resilience-and-security",{"id":18048,"data":18050,"filePath":18055,"digest":18056,"rendered":18057,"legacyId":18065},{"title":18051,"description":18052,"summary":18052,"pubDate":18053,"source":19,"url":18054,"thumbnail":21},"OpenAI’s comments to the NTIA on data center growth, resilience, and security","This comment was submitted in response to a request for information from the National Telecommunications and Information Administration (NTIA).",["Date","2024-11-04T12:00:00.000Z"],"https://openai.com/global-affairs/comments-to-the-ntia-on-data-center-growth-resilience-and-security","src/content/posts/2024-11-04-openais-comments-to-the-ntia-on-data-center-growth-resilience-and-security.md","9f294315bab36a97",{"html":25,"metadata":18058},{"headings":18059,"localImagePaths":18060,"remoteImagePaths":18061,"frontmatter":18062,"imagePaths":18064},[],[],[],{"title":18051,"description":18052,"summary":18052,"pubDate":18063,"source":19,"url":18054,"thumbnail":21},"Mon, 04 Nov 2024 12:00:00 GMT",[],"2024-11-04-openais-comments-to-the-ntia-on-data-center-growth-resilience-and-security.md","2024-11-05-hugging-face-pycharm",{"id":18066,"data":18068,"filePath":18074,"digest":18075,"rendered":18076,"legacyId":18084},{"title":18069,"description":25,"summary":18070,"pubDate":18071,"source":2720,"url":18072,"thumbnail":18073},"Hugging Face + PyCharm","Hugging Face + PyCharm It’s a Tuesday morning. As a Transformers maintainer, I’m doing the same thin...",["Date","2024-11-05T00:00:00.000Z"],"https://huggingface.co/blog/pycharm-integration","https://huggingface.co/blog/assets/pycharm-integration/thumbnail.png","src/content/posts/2024-11-05-hugging-face-pycharm.md","5873bbfe62985dc7",{"html":25,"metadata":18077},{"headings":18078,"localImagePaths":18079,"remoteImagePaths":18080,"frontmatter":18081,"imagePaths":18083},[],[],[],{"title":18069,"description":25,"summary":18070,"pubDate":18082,"source":2720,"url":18072,"thumbnail":18073},"Tue, 05 Nov 2024 00:00:00 GMT",[],"2024-11-05-hugging-face-pycharm.md","2024-11-12-share-your-open-ml-datasets-on-hugging-face-hub",{"id":18085,"data":18087,"filePath":18093,"digest":18094,"rendered":18095,"legacyId":18103},{"title":18088,"description":25,"summary":18089,"pubDate":18090,"source":2720,"url":18091,"thumbnail":18092},"Share your open ML datasets on Hugging Face Hub!","Share your open ML datasets on Hugging Face Hub! If you're working on data-intensive research or mac...",["Date","2024-11-12T00:00:00.000Z"],"https://huggingface.co/blog/researcher-dataset-sharing","https://huggingface.co/blog/assets/researcher-dataset-sharing/thumbnail.png","src/content/posts/2024-11-12-share-your-open-ml-datasets-on-hugging-face-hub.md","6586d88aae1bde8c",{"html":25,"metadata":18096},{"headings":18097,"localImagePaths":18098,"remoteImagePaths":18099,"frontmatter":18100,"imagePaths":18102},[],[],[],{"title":18088,"description":25,"summary":18089,"pubDate":18101,"source":2720,"url":18091,"thumbnail":18092},"Tue, 12 Nov 2024 00:00:00 GMT",[],"2024-11-12-share-your-open-ml-datasets-on-hugging-face-hub.md","2024-11-13-a-students-guide-to-writing-with-chatgpt",{"id":18104,"data":18106,"filePath":18110,"digest":18111,"rendered":18112,"legacyId":18120},{"title":18107,"description":18107,"summary":18107,"pubDate":18108,"source":19,"url":18109,"thumbnail":21},"A Student’s Guide to Writing with ChatGPT",["Date","2024-11-13T10:00:00.000Z"],"https://openai.com/chatgpt/use-cases/student-writing-guide","src/content/posts/2024-11-13-a-students-guide-to-writing-with-chatgpt.md","3701173d578375d9",{"html":25,"metadata":18113},{"headings":18114,"localImagePaths":18115,"remoteImagePaths":18116,"frontmatter":18117,"imagePaths":18119},[],[],[],{"title":18107,"description":18107,"summary":18107,"pubDate":18118,"source":19,"url":18109,"thumbnail":21},"Wed, 13 Nov 2024 10:00:00 GMT",[],"2024-11-13-a-students-guide-to-writing-with-chatgpt.md","2024-11-13-data-driven-beauty-and-creativity-with-chatgpt",{"id":18121,"data":18123,"filePath":18128,"digest":18129,"rendered":18130,"legacyId":18138},{"title":18124,"description":18125,"summary":18125,"pubDate":18126,"source":19,"url":18127,"thumbnail":21},"Data-driven beauty and creativity with ChatGPT","Data-driven beauty: How The Estée Lauder Companies unlocks insights with ChatGPT",["Date","2024-11-13T00:00:00.000Z"],"https://openai.com/blog/estee-lauder","src/content/posts/2024-11-13-data-driven-beauty-and-creativity-with-chatgpt.md","5bf9d325c641cee9",{"html":25,"metadata":18131},{"headings":18132,"localImagePaths":18133,"remoteImagePaths":18134,"frontmatter":18135,"imagePaths":18137},[],[],[],{"title":18124,"description":18125,"summary":18125,"pubDate":18136,"source":19,"url":18127,"thumbnail":21},"Wed, 13 Nov 2024 00:00:00 GMT",[],"2024-11-13-data-driven-beauty-and-creativity-with-chatgpt.md","2024-11-15-openai-en-france",{"id":18139,"data":18141,"filePath":18146,"digest":18147,"rendered":18148,"legacyId":18156},{"title":18142,"description":18143,"summary":18143,"pubDate":18144,"source":19,"url":18145,"thumbnail":21},"OpenAI en France","Our first office in continental Europe",["Date","2024-11-15T00:00:00.000Z"],"https://openai.com/blog/openai-en-france","src/content/posts/2024-11-15-openai-en-france.md","9abb26767349fa86",{"html":25,"metadata":18149},{"headings":18150,"localImagePaths":18151,"remoteImagePaths":18152,"frontmatter":18153,"imagePaths":18155},[],[],[],{"title":18142,"description":18143,"summary":18143,"pubDate":18154,"source":19,"url":18145,"thumbnail":21},"Fri, 15 Nov 2024 00:00:00 GMT",[],"2024-11-15-openai-en-france.md","2024-11-18-the-ai-for-science-forum-a-new-era-of-discovery",{"id":18157,"data":18159,"filePath":18165,"digest":18166,"rendered":18167,"legacyId":18175},{"title":18160,"description":18161,"summary":18161,"pubDate":18162,"source":6423,"url":18163,"thumbnail":18164},"The AI for Science Forum: A new era of discovery","The AI Science Forum highlights AI's present and potential role in revolutionizing scientific discovery and solving global challenges, emphasizing collaboration between the scientific community, policymakers, and industry leaders.",["Date","2024-11-18T19:57:00.000Z"],"https://deepmind.google/discover/blog/the-ai-for-science-forum-a-new-era-of-discovery/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AIFS_Collection_SS.max-1440x810.jpg","src/content/posts/2024-11-18-the-ai-for-science-forum-a-new-era-of-discovery.md","e1bf58e940a52f9b",{"html":25,"metadata":18168},{"headings":18169,"localImagePaths":18170,"remoteImagePaths":18171,"frontmatter":18172,"imagePaths":18174},[],[],[],{"title":18160,"description":18161,"summary":18161,"pubDate":18173,"source":6423,"url":18163,"thumbnail":18164},"Mon, 18 Nov 2024 19:57:00 +0000",[],"2024-11-18-the-ai-for-science-forum-a-new-era-of-discovery.md","2024-11-19-judge-arena-benchmarking-llms-as-evaluators",{"id":18176,"data":18178,"filePath":18184,"digest":18185,"rendered":18186,"legacyId":18194},{"title":18179,"description":25,"summary":18180,"pubDate":18181,"source":2720,"url":18182,"thumbnail":18183},"Judge Arena: Benchmarking LLMs as Evaluators","Judge Arena: Benchmarking LLMs as Evaluators LLM-as-a-Judge has emerged as a popular way to grade na...",["Date","2024-11-19T00:00:00.000Z"],"https://huggingface.co/blog/arena-atla","https://huggingface.co/blog/assets/arenas-on-the-hub/thumbnail_atla.png","src/content/posts/2024-11-19-judge-arena-benchmarking-llms-as-evaluators.md","1b92abf34f11d04a",{"html":25,"metadata":18187},{"headings":18188,"localImagePaths":18189,"remoteImagePaths":18190,"frontmatter":18191,"imagePaths":18193},[],[],[],{"title":18179,"description":25,"summary":18180,"pubDate":18192,"source":2720,"url":18182,"thumbnail":18183},"Tue, 19 Nov 2024 00:00:00 GMT",[],"2024-11-19-judge-arena-benchmarking-llms-as-evaluators.md","2024-11-19-rox-goes-all-in-on-openai",{"id":18195,"data":18197,"filePath":18202,"digest":18203,"rendered":18204,"legacyId":18212},{"title":18198,"description":18199,"summary":18199,"pubDate":18200,"source":19,"url":18201,"thumbnail":21},"Rox goes “all in” on OpenAI","By combining commercial experience and deep LLM expertise with OpenAI’s models, Rox makes every seller a top 1% seller.",["Date","2024-11-19T07:00:00.000Z"],"https://openai.com/blog/rox","src/content/posts/2024-11-19-rox-goes-all-in-on-openai.md","52b54640a3371167",{"html":25,"metadata":18205},{"headings":18206,"localImagePaths":18207,"remoteImagePaths":18208,"frontmatter":18209,"imagePaths":18211},[],[],[],{"title":18198,"description":18199,"summary":18199,"pubDate":18210,"source":19,"url":18201,"thumbnail":21},"Tue, 19 Nov 2024 07:00:00 GMT",[],"2024-11-19-rox-goes-all-in-on-openai.md","2024-11-20-alphaqubit-tackles-one-of-quantum-computings-biggest-challenges",{"id":18213,"data":18215,"filePath":18221,"digest":18222,"rendered":18223,"legacyId":18231},{"title":18216,"description":18217,"summary":18217,"pubDate":18218,"source":6423,"url":18219,"thumbnail":18220},"AlphaQubit tackles one of quantum computing’s biggest challenges","Our new AI system accurately identifies errors inside quantum computers, helping to make this new technology more reliable.",["Date","2024-11-20T18:00:00.000Z"],"https://deepmind.google/discover/blog/alphaqubit-tackles-one-of-quantum-computings-biggest-challenges/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Social_Share_Image_-_1920_x_1080.width-1300.png","src/content/posts/2024-11-20-alphaqubit-tackles-one-of-quantum-computings-biggest-challenges.md","d13b85a2d1413d15",{"html":25,"metadata":18224},{"headings":18225,"localImagePaths":18226,"remoteImagePaths":18227,"frontmatter":18228,"imagePaths":18230},[],[],[],{"title":18216,"description":18217,"summary":18217,"pubDate":18229,"source":6423,"url":18219,"thumbnail":18220},"Wed, 20 Nov 2024 18:00:00 +0000",[],"2024-11-20-alphaqubit-tackles-one-of-quantum-computings-biggest-challenges.md","2024-11-20-building-smarter-maps-with-gpt-4o-vision-fine-tuning",{"id":18232,"data":18234,"filePath":18238,"digest":18239,"rendered":18240,"legacyId":18248},{"title":18235,"description":18235,"summary":18235,"pubDate":18236,"source":19,"url":18237,"thumbnail":21},"Building smarter maps with GPT-4o vision fine-tuning",["Date","2024-11-20T17:00:00.000Z"],"https://openai.com/blog/grab","src/content/posts/2024-11-20-building-smarter-maps-with-gpt-4o-vision-fine-tuning.md","05b6588b5f7299b8",{"html":25,"metadata":18241},{"headings":18242,"localImagePaths":18243,"remoteImagePaths":18244,"frontmatter":18245,"imagePaths":18247},[],[],[],{"title":18235,"description":18235,"summary":18235,"pubDate":18246,"source":19,"url":18237,"thumbnail":21},"Wed, 20 Nov 2024 17:00:00 GMT",[],"2024-11-20-building-smarter-maps-with-gpt-4o-vision-fine-tuning.md","2024-11-20-faster-text-generation-with-self-speculative-decoding",{"id":18249,"data":18251,"filePath":18257,"digest":18258,"rendered":18259,"legacyId":18267},{"title":18252,"description":25,"summary":18253,"pubDate":18254,"source":2720,"url":18255,"thumbnail":18256},"Faster Text Generation with Self-Speculative Decoding","Faster Text Generation with Self-Speculative Decoding Self-speculative decoding, proposed in LayerSk...",["Date","2024-11-20T00:00:00.000Z"],"https://huggingface.co/blog/layerskip","https://huggingface.co/blog/assets/layerskip/thumbnail.png","src/content/posts/2024-11-20-faster-text-generation-with-self-speculative-decoding.md","fb83930b452a33cc",{"html":25,"metadata":18260},{"headings":18261,"localImagePaths":18262,"remoteImagePaths":18263,"frontmatter":18264,"imagePaths":18266},[],[],[],{"title":18252,"description":25,"summary":18253,"pubDate":18265,"source":2720,"url":18255,"thumbnail":18256},"Wed, 20 Nov 2024 00:00:00 GMT",[],"2024-11-20-faster-text-generation-with-self-speculative-decoding.md","2024-11-20-from-files-to-chunks-improving-hugging-face-storage-efficiency",{"id":18268,"data":18270,"filePath":18276,"digest":18277,"rendered":18278,"legacyId":18285},{"title":18271,"description":25,"summary":18272,"pubDate":18273,"source":2720,"url":18274,"thumbnail":18275},"From Files to Chunks: Improving Hugging Face Storage Efficiency","From Files to Chunks: Improving HF Storage Efficiency Hugging Face stores over 30 PB of models, data...",["Date","2024-11-20T00:00:00.000Z"],"https://huggingface.co/blog/from-files-to-chunks","https://huggingface.co/blog/assets/from-files-to-chunks/thumbnail.png","src/content/posts/2024-11-20-from-files-to-chunks-improving-hugging-face-storage-efficiency.md","5ea266c75d98ee60",{"html":25,"metadata":18279},{"headings":18280,"localImagePaths":18281,"remoteImagePaths":18282,"frontmatter":18283,"imagePaths":18284},[],[],[],{"title":18271,"description":25,"summary":18272,"pubDate":18265,"source":2720,"url":18274,"thumbnail":18275},[],"2024-11-20-from-files-to-chunks-improving-hugging-face-storage-efficiency.md","2024-11-20-introduction-to-the-open-leaderboard-for-japanese-llms",{"id":18286,"data":18288,"filePath":18294,"digest":18295,"rendered":18296,"legacyId":18303},{"title":18289,"description":25,"summary":18290,"pubDate":18291,"source":2720,"url":18292,"thumbnail":18293},"Introduction to the Open Leaderboard for Japanese LLMs","Introduction to the Open Leaderboard for Japanese LLMs LLMs are now increasingly capable in English,...",["Date","2024-11-20T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-japanese","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_japanese.png","src/content/posts/2024-11-20-introduction-to-the-open-leaderboard-for-japanese-llms.md","34e72dde71b0d57f",{"html":25,"metadata":18297},{"headings":18298,"localImagePaths":18299,"remoteImagePaths":18300,"frontmatter":18301,"imagePaths":18302},[],[],[],{"title":18289,"description":25,"summary":18290,"pubDate":18265,"source":2720,"url":18292,"thumbnail":18293},[],"2024-11-20-introduction-to-the-open-leaderboard-for-japanese-llms.md","2024-11-20-letting-large-models-debate-the-first-multilingual-llm-debate-competition",{"id":18304,"data":18306,"filePath":18311,"digest":18312,"rendered":18313,"legacyId":18320},{"title":18307,"description":25,"summary":18308,"pubDate":18309,"source":2720,"url":18310,"thumbnail":3514},"Letting Large Models Debate: The First Multilingual LLM Debate Competition","Letting Large Models Debate: The First Multilingual LLM Debate Competition Current static evaluation...",["Date","2024-11-20T00:00:00.000Z"],"https://huggingface.co/blog/debate","src/content/posts/2024-11-20-letting-large-models-debate-the-first-multilingual-llm-debate-competition.md","b1781de848b2abd5",{"html":25,"metadata":18314},{"headings":18315,"localImagePaths":18316,"remoteImagePaths":18317,"frontmatter":18318,"imagePaths":18319},[],[],[],{"title":18307,"description":25,"summary":18308,"pubDate":18265,"source":2720,"url":18310,"thumbnail":3514},[],"2024-11-20-letting-large-models-debate-the-first-multilingual-llm-debate-competition.md","2024-11-21-advancing-red-teaming-with-people-and-ai",{"id":18321,"data":18323,"filePath":18327,"digest":18328,"rendered":18329,"legacyId":18337},{"title":18324,"description":18324,"summary":18324,"pubDate":18325,"source":19,"url":18326,"thumbnail":21},"Advancing red teaming with people and AI",["Date","2024-11-21T10:30:00.000Z"],"https://openai.com/blog/advancing-red-teaming-with-people-and-ai","src/content/posts/2024-11-21-advancing-red-teaming-with-people-and-ai.md","8c66d8446c7a0a80",{"html":25,"metadata":18330},{"headings":18331,"localImagePaths":18332,"remoteImagePaths":18333,"frontmatter":18334,"imagePaths":18336},[],[],[],{"title":18324,"description":18324,"summary":18324,"pubDate":18335,"source":19,"url":18326,"thumbnail":21},"Thu, 21 Nov 2024 10:30:00 GMT",[],"2024-11-21-advancing-red-teaming-with-people-and-ai.md","2024-11-21-empowering-a-global-org-with-chatgpt",{"id":18338,"data":18340,"filePath":18344,"digest":18345,"rendered":18346,"legacyId":18354},{"title":18341,"description":18341,"summary":18341,"pubDate":18342,"source":19,"url":18343,"thumbnail":21},"Empowering a global org with ChatGPT",["Date","2024-11-21T05:00:00.000Z"],"https://openai.com/blog/bbva","src/content/posts/2024-11-21-empowering-a-global-org-with-chatgpt.md","1e13272f43451cfb",{"html":25,"metadata":18347},{"headings":18348,"localImagePaths":18349,"remoteImagePaths":18350,"frontmatter":18351,"imagePaths":18353},[],[],[],{"title":18341,"description":18341,"summary":18341,"pubDate":18352,"source":19,"url":18343,"thumbnail":21},"Thu, 21 Nov 2024 05:00:00 GMT",[],"2024-11-21-empowering-a-global-org-with-chatgpt.md","2024-11-25-you-could-have-designed-state-of-the-art-positional-encoding",{"id":18355,"data":18357,"filePath":18363,"digest":18364,"rendered":18365,"legacyId":18373},{"title":18358,"description":25,"summary":18359,"pubDate":18360,"source":2720,"url":18361,"thumbnail":18362},"You could have designed state of the art positional encoding","You could have designed state of the art positional encoding Gall's Law A complex system that works ...",["Date","2024-11-25T00:00:00.000Z"],"https://huggingface.co/blog/designing-positional-encoding","https://huggingface.co/blog/assets/designing-positional-encoding/thumbnail_posenc.png","src/content/posts/2024-11-25-you-could-have-designed-state-of-the-art-positional-encoding.md","6dcb2aa3f8ee9b6a",{"html":25,"metadata":18366},{"headings":18367,"localImagePaths":18368,"remoteImagePaths":18369,"frontmatter":18370,"imagePaths":18372},[],[],[],{"title":18358,"description":25,"summary":18359,"pubDate":18371,"source":2720,"url":18361,"thumbnail":18362},"Mon, 25 Nov 2024 00:00:00 GMT",[],"2024-11-25-you-could-have-designed-state-of-the-art-positional-encoding.md","2024-11-26-rearchitecting-hugging-face-uploads-and-downloads",{"id":18374,"data":18376,"filePath":18382,"digest":18383,"rendered":18384,"legacyId":18392},{"title":18377,"description":25,"summary":18378,"pubDate":18379,"source":2720,"url":18380,"thumbnail":18381},"Rearchitecting Hugging Face Uploads and Downloads","Rearchitecting Hugging Face Uploads and Downloads As part of Hugging Face's Xet team’s work to impro...",["Date","2024-11-26T00:00:00.000Z"],"https://huggingface.co/blog/rearchitecting-uploads-and-downloads","https://huggingface.co/blog/assets/rearchitecting-uploads-and-downloads/thumbnail.png","src/content/posts/2024-11-26-rearchitecting-hugging-face-uploads-and-downloads.md","ca5ed18c35106e6a",{"html":25,"metadata":18385},{"headings":18386,"localImagePaths":18387,"remoteImagePaths":18388,"frontmatter":18389,"imagePaths":18391},[],[],[],{"title":18377,"description":25,"summary":18378,"pubDate":18390,"source":2720,"url":18380,"thumbnail":18381},"Tue, 26 Nov 2024 00:00:00 GMT",[],"2024-11-26-rearchitecting-hugging-face-uploads-and-downloads.md","2024-11-26-smolvlm---small-yet-mighty-vision-language-model",{"id":18393,"data":18395,"filePath":18401,"digest":18402,"rendered":18403,"legacyId":18410},{"title":18396,"description":25,"summary":18397,"pubDate":18398,"source":2720,"url":18399,"thumbnail":18400},"SmolVLM - small yet mighty Vision Language Model","SmolVLM - small yet mighty Vision Language Model TLDR This blog post introduces SmolVLM, a 2B VLM, S...",["Date","2024-11-26T00:00:00.000Z"],"https://huggingface.co/blog/smolvlm","https://huggingface.co/blog/assets/smolvlm/banner.png","src/content/posts/2024-11-26-smolvlm---small-yet-mighty-vision-language-model.md","216e1c5795bd2e04",{"html":25,"metadata":18404},{"headings":18405,"localImagePaths":18406,"remoteImagePaths":18407,"frontmatter":18408,"imagePaths":18409},[],[],[],{"title":18396,"description":25,"summary":18397,"pubDate":18390,"source":2720,"url":18399,"thumbnail":18400},[],"2024-11-26-smolvlm---small-yet-mighty-vision-language-model.md","2024-12-02-open-source-developers-guide-to-the-eu-ai-act",{"id":18411,"data":18413,"filePath":18419,"digest":18420,"rendered":18421,"legacyId":18429},{"title":18414,"description":25,"summary":18415,"pubDate":18416,"source":2720,"url":18417,"thumbnail":18418},"Open Source Developers Guide to the EU AI Act","Open Source Developers Guide to the EU AI Act The EU AI Act, the world’s first comprehensive legisla...",["Date","2024-12-02T00:00:00.000Z"],"https://huggingface.co/blog/eu-ai-act-for-oss-developers","https://huggingface.co/blog/assets/189_eu-ai-act-for-oss-developers/thumbnail.png","src/content/posts/2024-12-02-open-source-developers-guide-to-the-eu-ai-act.md","af8b5bec17954e4c",{"html":25,"metadata":18422},{"headings":18423,"localImagePaths":18424,"remoteImagePaths":18425,"frontmatter":18426,"imagePaths":18428},[],[],[],{"title":18414,"description":25,"summary":18415,"pubDate":18427,"source":2720,"url":18417,"thumbnail":18418},"Mon, 02 Dec 2024 00:00:00 GMT",[],"2024-12-02-open-source-developers-guide-to-the-eu-ai-act.md","2024-12-03-investing-in-performance-fine-tune-small-models-with-llm-insights---a-cfm-case-study",{"id":18430,"data":18432,"filePath":18438,"digest":18439,"rendered":18440,"legacyId":18448},{"title":18433,"description":25,"summary":18434,"pubDate":18435,"source":2720,"url":18436,"thumbnail":18437},"Investing in Performance: Fine-tune small models with LLM insights  - a CFM case study","Investing in Performance: Fine-tune small models with LLM insights - a CFM case study Overview: This...",["Date","2024-12-03T00:00:00.000Z"],"https://huggingface.co/blog/cfm-case-study","https://huggingface.co/blog/assets/cfm-case-study/blogpost_cfm.png","src/content/posts/2024-12-03-investing-in-performance-fine-tune-small-models-with-llm-insights---a-cfm-case-study.md","59cf00713d1b5b89",{"html":25,"metadata":18441},{"headings":18442,"localImagePaths":18443,"remoteImagePaths":18444,"frontmatter":18445,"imagePaths":18447},[],[],[],{"title":18433,"description":25,"summary":18434,"pubDate":18446,"source":2720,"url":18436,"thumbnail":18437},"Tue, 03 Dec 2024 00:00:00 GMT",[],"2024-12-03-investing-in-performance-fine-tune-small-models-with-llm-insights---a-cfm-case-study.md","2024-12-04-gencast-predicts-weather-and-the-risks-of-extreme-conditions-with-state-of-the-art-accuracy",{"id":18449,"data":18451,"filePath":18457,"digest":18458,"rendered":18459,"legacyId":18467},{"title":18452,"description":18453,"summary":18453,"pubDate":18454,"source":6423,"url":18455,"thumbnail":18456},"GenCast predicts weather and the risks of extreme conditions with state-of-the-art accuracy","New AI model advances the prediction of weather uncertainties and risks, delivering faster, more accurate forecasts up to 15 days ahead",["Date","2024-12-04T15:59:00.000Z"],"https://deepmind.google/discover/blog/gencast-predicts-weather-and-the-risks-of-extreme-conditions-with-sota-accuracy/","https://lh3.googleusercontent.com/4u3n6FBe0eE86yXgppDN_yj_AkiCF5FaSToa8f3Mh5bFWzIH01ewGN737emoYKcGXLxQagYFMxi9j-cAZyAzkdFndCDg2ne9E42w4YZD7HyBChaf=w1200-h630-n-nu","src/content/posts/2024-12-04-gencast-predicts-weather-and-the-risks-of-extreme-conditions-with-state-of-the-art-accuracy.md","2bd954a89b818626",{"html":25,"metadata":18460},{"headings":18461,"localImagePaths":18462,"remoteImagePaths":18463,"frontmatter":18464,"imagePaths":18466},[],[],[],{"title":18452,"description":18453,"summary":18453,"pubDate":18465,"source":6423,"url":18455,"thumbnail":18456},"Wed, 04 Dec 2024 15:59:00 +0000",[],"2024-12-04-gencast-predicts-weather-and-the-risks-of-extreme-conditions-with-state-of-the-art-accuracy.md","2024-12-04-genie-2-a-large-scale-foundation-world-model",{"id":18468,"data":18470,"filePath":18476,"digest":18477,"rendered":18478,"legacyId":18486},{"title":18471,"description":18472,"summary":18472,"pubDate":18473,"source":6423,"url":18474,"thumbnail":18475},"Genie 2: A large-scale foundation world model","Generating unlimited diverse training environments for future general agents",["Date","2024-12-04T14:23:00.000Z"],"https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/","https://lh3.googleusercontent.com/wvcJdqh_wddVc-WiMGgcqe7nWp7Ybu0wd-PBDxC_VUQkfxI7HPfQz3fi_HyYTOoRM_XV3Bofp9l1wBZ1CJPZPG6yZMdZxqH8X7_Lb9nhVAquAul1=w1200-h630-n-nu","src/content/posts/2024-12-04-genie-2-a-large-scale-foundation-world-model.md","fa4dbe45dc063441",{"html":25,"metadata":18479},{"headings":18480,"localImagePaths":18481,"remoteImagePaths":18482,"frontmatter":18483,"imagePaths":18485},[],[],[],{"title":18471,"description":18472,"summary":18472,"pubDate":18484,"source":6423,"url":18474,"thumbnail":18475},"Wed, 04 Dec 2024 14:23:00 +0000",[],"2024-12-04-genie-2-a-large-scale-foundation-world-model.md","2024-12-04-openai-and-future-partner-on-specialist-content",{"id":18487,"data":18489,"filePath":18494,"digest":18495,"rendered":18496,"legacyId":18504},{"title":18490,"description":18491,"summary":18491,"pubDate":18492,"source":19,"url":18493,"thumbnail":21},"OpenAI and Future partner on specialist content","OpenAI and Future, the global platform for specialist media, have today announced a strategic partnership to bring content from Future’s 200 plus media brands to OpenAI’s users.",["Date","2024-12-04T23:30:00.000Z"],"https://openai.com/blog/openai-and-future-partner-on-specialist-content","src/content/posts/2024-12-04-openai-and-future-partner-on-specialist-content.md","5e90fffbab827f9d",{"html":25,"metadata":18497},{"headings":18498,"localImagePaths":18499,"remoteImagePaths":18500,"frontmatter":18501,"imagePaths":18503},[],[],[],{"title":18490,"description":18491,"summary":18491,"pubDate":18502,"source":19,"url":18493,"thumbnail":21},"Wed, 04 Dec 2024 23:30:00 GMT",[],"2024-12-04-openai-and-future-partner-on-specialist-content.md","2024-12-04-rethinking-llm-evaluation-with-3c3h-aragen-benchmark-and-leaderboard",{"id":18505,"data":18507,"filePath":18513,"digest":18514,"rendered":18515,"legacyId":18523},{"title":18508,"description":25,"summary":18509,"pubDate":18510,"source":2720,"url":18511,"thumbnail":18512},"Rethinking LLM Evaluation with 3C3H: AraGen Benchmark and Leaderboard","Rethinking LLM Evaluation with 3C3H: AraGen Benchmark and Leaderboard In the rapidly evolving landsc...",["Date","2024-12-04T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-3c3h-aragen","https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_3c3h_aragen.png","src/content/posts/2024-12-04-rethinking-llm-evaluation-with-3c3h-aragen-benchmark-and-leaderboard.md","ae1fb345a56b3877",{"html":25,"metadata":18516},{"headings":18517,"localImagePaths":18518,"remoteImagePaths":18519,"frontmatter":18520,"imagePaths":18522},[],[],[],{"title":18508,"description":25,"summary":18509,"pubDate":18521,"source":2720,"url":18511,"thumbnail":18512},"Wed, 04 Dec 2024 00:00:00 GMT",[],"2024-12-04-rethinking-llm-evaluation-with-3c3h-aragen-benchmark-and-leaderboard.md","2024-12-04-shaping-the-future-of-financial-services",{"id":18524,"data":18526,"filePath":18531,"digest":18532,"rendered":18533,"legacyId":18541},{"title":18527,"description":18528,"summary":18528,"pubDate":18529,"source":19,"url":18530,"thumbnail":21},"Shaping the future of financial services","Morgan Stanley uses AI evals to shape the future of financial services",["Date","2024-12-04T10:00:00.000Z"],"https://openai.com/blog/morgan-stanley","src/content/posts/2024-12-04-shaping-the-future-of-financial-services.md","0c4d86ea71868f2f",{"html":25,"metadata":18534},{"headings":18535,"localImagePaths":18536,"remoteImagePaths":18537,"frontmatter":18538,"imagePaths":18540},[],[],[],{"title":18527,"description":18528,"summary":18528,"pubDate":18539,"source":19,"url":18530,"thumbnail":21},"Wed, 04 Dec 2024 10:00:00 GMT",[],"2024-12-04-shaping-the-future-of-financial-services.md","2024-12-05-google-deepmind-at-neurips-2024",{"id":18542,"data":18544,"filePath":18550,"digest":18551,"rendered":18552,"legacyId":18560},{"title":18545,"description":18546,"summary":18546,"pubDate":18547,"source":6423,"url":18548,"thumbnail":18549},"Google DeepMind at NeurIPS 2024","Advancing adaptive AI agents, empowering 3D scene creation, and innovating LLM training for a smarter, safer future",["Date","2024-12-05T17:45:00.000Z"],"https://deepmind.google/discover/blog/google-deepmind-at-neurips-2024/","https://lh3.googleusercontent.com/cKpWE16vpsZ21VcH-_SdGF8tQEeEMp2phWFajdBq_A7aMVS2axiXQzd7V8mlHdJm-CXVKh1IaY3yeM_lAwu_zxc6SIBdWahdN6nYoaQqUbC8uU0qoY8=w1200-h630-n-nu","src/content/posts/2024-12-05-google-deepmind-at-neurips-2024.md","eb37331a8f562d68",{"html":25,"metadata":18553},{"headings":18554,"localImagePaths":18555,"remoteImagePaths":18556,"frontmatter":18557,"imagePaths":18559},[],[],[],{"title":18545,"description":18546,"summary":18546,"pubDate":18558,"source":6423,"url":18548,"thumbnail":18549},"Thu, 05 Dec 2024 17:45:00 +0000",[],"2024-12-05-google-deepmind-at-neurips-2024.md","2024-12-05-how-good-are-llms-at-fixing-their-mistakes-a-chatbot-arena-experiment-with-keras-and-tpus",{"id":18561,"data":18563,"filePath":18569,"digest":18570,"rendered":18571,"legacyId":18579},{"title":18564,"description":25,"summary":18565,"pubDate":18566,"source":2720,"url":18567,"thumbnail":18568},"How good are LLMs at fixing their mistakes? A chatbot arena experiment with Keras and TPUs","How good are LLMs at fixing their mistakes? A chatbot arena experiment with Keras and TPUs while you...",["Date","2024-12-05T00:00:00.000Z"],"https://huggingface.co/blog/keras-chatbot-arena","https://huggingface.co/blog/assets/keras-chatbot-arena/thumbnail.png","src/content/posts/2024-12-05-how-good-are-llms-at-fixing-their-mistakes-a-chatbot-arena-experiment-with-keras-and-tpus.md","a6ca8cbcbf991fee",{"html":25,"metadata":18572},{"headings":18573,"localImagePaths":18574,"remoteImagePaths":18575,"frontmatter":18576,"imagePaths":18578},[],[],[],{"title":18564,"description":25,"summary":18565,"pubDate":18577,"source":2720,"url":18567,"thumbnail":18568},"Thu, 05 Dec 2024 00:00:00 GMT",[],"2024-12-05-how-good-are-llms-at-fixing-their-mistakes-a-chatbot-arena-experiment-with-keras-and-tpus.md","2024-12-05-introducing-chatgpt-pro",{"id":18580,"data":18582,"filePath":18587,"digest":18588,"rendered":18589,"legacyId":18597},{"title":18583,"description":18584,"summary":18584,"pubDate":18585,"source":19,"url":18586,"thumbnail":21},"Introducing ChatGPT Pro","Broadening usage of frontier AI",["Date","2024-12-05T10:30:00.000Z"],"https://openai.com/blog/introducing-chatgpt-pro","src/content/posts/2024-12-05-introducing-chatgpt-pro.md","81949c4e3eb079d0",{"html":25,"metadata":18590},{"headings":18591,"localImagePaths":18592,"remoteImagePaths":18593,"frontmatter":18594,"imagePaths":18596},[],[],[],{"title":18583,"description":18584,"summary":18584,"pubDate":18595,"source":19,"url":18586,"thumbnail":21},"Thu, 05 Dec 2024 10:30:00 GMT",[],"2024-12-05-introducing-chatgpt-pro.md","2024-12-05-openai-o1-system-card",{"id":18598,"data":18600,"filePath":18605,"digest":18606,"rendered":18607,"legacyId":18615},{"title":18601,"description":18602,"summary":18602,"pubDate":18603,"source":19,"url":18604,"thumbnail":21},"OpenAI o1 System Card","This report outlines the safety work carried out prior to releasing OpenAI o1 and o1-mini, including external red teaming and frontier risk evaluations according to our Preparedness Framework.",["Date","2024-12-05T10:00:00.000Z"],"https://openai.com/blog/openai-o1-system-card","src/content/posts/2024-12-05-openai-o1-system-card.md","6bfc89ad48542252",{"html":25,"metadata":18608},{"headings":18609,"localImagePaths":18610,"remoteImagePaths":18611,"frontmatter":18612,"imagePaths":18614},[],[],[],{"title":18601,"description":18602,"summary":18602,"pubDate":18613,"source":19,"url":18604,"thumbnail":21},"Thu, 05 Dec 2024 10:00:00 GMT",[],"2024-12-05-openai-o1-system-card.md","2024-12-05-welcome-paligemma-2-new-vision-language-models-by-google",{"id":18616,"data":18618,"filePath":18624,"digest":18625,"rendered":18626,"legacyId":18633},{"title":18619,"description":25,"summary":18620,"pubDate":18621,"source":2720,"url":18622,"thumbnail":18623},"Welcome PaliGemma 2 – New vision language models by Google","Welcome PaliGemma 2 – New vision language models by Google We are excited to welcome Google's all-ne...",["Date","2024-12-05T00:00:00.000Z"],"https://huggingface.co/blog/paligemma2","https://huggingface.co/blog/assets/paligemma/Paligemma2.png","src/content/posts/2024-12-05-welcome-paligemma-2-new-vision-language-models-by-google.md","e333556adacb7476",{"html":25,"metadata":18627},{"headings":18628,"localImagePaths":18629,"remoteImagePaths":18630,"frontmatter":18631,"imagePaths":18632},[],[],[],{"title":18619,"description":25,"summary":18620,"pubDate":18577,"source":2720,"url":18622,"thumbnail":18623},[],"2024-12-05-welcome-paligemma-2-new-vision-language-models-by-google.md","2024-12-09-animator-lyndon-barrois-creates-new-worlds-with-sora",{"id":18634,"data":18636,"filePath":18641,"digest":18642,"rendered":18643,"legacyId":18651},{"title":18637,"description":18638,"summary":18638,"pubDate":18639,"source":19,"url":18640,"thumbnail":21},"Animator Lyndon Barrois creates new worlds with Sora","Filmmaker Lyndon Barrois describes how to use Sora as a storytelling tool.",["Date","2024-12-09T00:00:00.000Z"],"https://openai.com/blog/sora-lyndon-barrois","src/content/posts/2024-12-09-animator-lyndon-barrois-creates-new-worlds-with-sora.md","d0f813f62805a3f7",{"html":25,"metadata":18644},{"headings":18645,"localImagePaths":18646,"remoteImagePaths":18647,"frontmatter":18648,"imagePaths":18650},[],[],[],{"title":18637,"description":18638,"summary":18638,"pubDate":18649,"source":19,"url":18640,"thumbnail":21},"Mon, 09 Dec 2024 00:00:00 GMT",[],"2024-12-09-animator-lyndon-barrois-creates-new-worlds-with-sora.md","2024-12-09-hugging-face-models-in-amazon-bedrock",{"id":18652,"data":18654,"filePath":18660,"digest":18661,"rendered":18662,"legacyId":18669},{"title":18655,"description":25,"summary":18656,"pubDate":18657,"source":2720,"url":18658,"thumbnail":18659},"Hugging Face models in Amazon Bedrock","Use Hugging Face models with Amazon Bedrock We are excited to announce that popular open models from...",["Date","2024-12-09T00:00:00.000Z"],"https://huggingface.co/blog/bedrock-marketplace","https://huggingface.co/blog/assets/bedrock-marketplace/thumbnail.png","src/content/posts/2024-12-09-hugging-face-models-in-amazon-bedrock.md","9e7ffaea3c49f9ce",{"html":25,"metadata":18663},{"headings":18664,"localImagePaths":18665,"remoteImagePaths":18666,"frontmatter":18667,"imagePaths":18668},[],[],[],{"title":18655,"description":25,"summary":18656,"pubDate":18649,"source":2720,"url":18658,"thumbnail":18659},[],"2024-12-09-hugging-face-models-in-amazon-bedrock.md","2024-12-09-minne-atairu-sora",{"id":18670,"data":18672,"filePath":18677,"digest":18678,"rendered":18679,"legacyId":18686},{"title":18673,"description":18674,"summary":18674,"pubDate":18675,"source":19,"url":18676,"thumbnail":21},"Minne Atairu & Sora","Interdisciplinary artist Minne Atairu discusses how Sora helps realize her vision.",["Date","2024-12-09T00:00:00.000Z"],"https://openai.com/blog/sora-minne-atairu","src/content/posts/2024-12-09-minne-atairu-sora.md","17d3666f6b9134a3",{"html":25,"metadata":18680},{"headings":18681,"localImagePaths":18682,"remoteImagePaths":18683,"frontmatter":18684,"imagePaths":18685},[],[],[],{"title":18673,"description":18674,"summary":18674,"pubDate":18649,"source":19,"url":18676,"thumbnail":21},[],"2024-12-09-minne-atairu-sora.md","2024-12-09-open-preference-dataset-for-text-to-image-generation-by-the-community",{"id":18687,"data":18689,"filePath":18695,"digest":18696,"rendered":18697,"legacyId":18704},{"title":18690,"description":25,"summary":18691,"pubDate":18692,"source":2720,"url":18693,"thumbnail":18694},"Open Preference Dataset for Text-to-Image Generation by the 🤗 Community","Open Preference Dataset for Text-to-Image Generation by the 🤗 Community The Data is Better Together ...",["Date","2024-12-09T00:00:00.000Z"],"https://huggingface.co/blog/image-preferences","https://huggingface.co/blog/assets/image_preferences/thumbnail.png","src/content/posts/2024-12-09-open-preference-dataset-for-text-to-image-generation-by-the-community.md","a084886d3b8bb498",{"html":25,"metadata":18698},{"headings":18699,"localImagePaths":18700,"remoteImagePaths":18701,"frontmatter":18702,"imagePaths":18703},[],[],[],{"title":18690,"description":25,"summary":18691,"pubDate":18649,"source":2720,"url":18693,"thumbnail":18694},[],"2024-12-09-open-preference-dataset-for-text-to-image-generation-by-the-community.md","2024-12-09-put-ai-to-work-for-your-product-team",{"id":18705,"data":18707,"filePath":18711,"digest":18712,"rendered":18713,"legacyId":18720},{"title":18708,"description":18708,"summary":18708,"pubDate":18709,"source":19,"url":18710,"thumbnail":21},"Put AI to work for your product team",["Date","2024-12-09T00:00:00.000Z"],"https://openai.com/blog/put-ai-to-work-for-your-product-team","src/content/posts/2024-12-09-put-ai-to-work-for-your-product-team.md","9a5d1f7902fa7f67",{"html":25,"metadata":18714},{"headings":18715,"localImagePaths":18716,"remoteImagePaths":18717,"frontmatter":18718,"imagePaths":18719},[],[],[],{"title":18708,"description":18708,"summary":18708,"pubDate":18649,"source":19,"url":18710,"thumbnail":21},[],"2024-12-09-put-ai-to-work-for-your-product-team.md","2024-12-09-sora-system-card",{"id":18721,"data":18723,"filePath":18728,"digest":18729,"rendered":18730,"legacyId":18737},{"title":18724,"description":18725,"summary":18725,"pubDate":18726,"source":19,"url":18727,"thumbnail":21},"Sora System Card","Sora is OpenAI’s video generation model, designed to take text, image, and video inputs and generate a new video as an output. Sora builds on learnings from DALL-E and GPT models, and is designed to give people expanded tools for storytelling and creative expression.",["Date","2024-12-09T00:00:00.000Z"],"https://openai.com/blog/sora-system-card","src/content/posts/2024-12-09-sora-system-card.md","a27ceb3e67789a9b",{"html":25,"metadata":18731},{"headings":18732,"localImagePaths":18733,"remoteImagePaths":18734,"frontmatter":18735,"imagePaths":18736},[],[],[],{"title":18724,"description":18725,"summary":18725,"pubDate":18649,"source":19,"url":18727,"thumbnail":21},[],"2024-12-09-sora-system-card.md","2024-12-09-vallée-duhamel-sora",{"id":18738,"data":18740,"filePath":18745,"digest":18746,"rendered":18747,"legacyId":18754},{"title":18741,"description":18742,"summary":18742,"pubDate":18743,"source":19,"url":18744,"thumbnail":21},"Vallée Duhamel & Sora","Filmmaking duo Vallée Duhamel explains how Sora helps build new worlds.",["Date","2024-12-09T00:00:00.000Z"],"https://openai.com/blog/sora-vallee-duhamel","src/content/posts/2024-12-09-vallée-duhamel-sora.md","f03c85c842bf497d",{"html":25,"metadata":18748},{"headings":18749,"localImagePaths":18750,"remoteImagePaths":18751,"frontmatter":18752,"imagePaths":18753},[],[],[],{"title":18741,"description":18742,"summary":18742,"pubDate":18649,"source":19,"url":18744,"thumbnail":21},[],"2024-12-09-vallée-duhamel-sora.md","2024-12-09-sora-is-here",{"id":18755,"data":18757,"filePath":18762,"digest":18763,"rendered":18764,"legacyId":18772},{"title":18758,"description":18759,"summary":18759,"pubDate":18760,"source":19,"url":18761,"thumbnail":21},"Sora is here","Our video generation model, Sora, is now available to use at sora.com. Users can generate videos up to 1080p resolution, up to 20 sec long, and in widescreen, vertical or square aspect ratios. You can bring your own assets to extend, remix, and blend, or generate entirely new content from text.",["Date","2024-12-09T10:00:00.000Z"],"https://openai.com/blog/sora-is-here","src/content/posts/2024-12-09-sora-is-here.md","fb8741eb157661d5",{"html":25,"metadata":18765},{"headings":18766,"localImagePaths":18767,"remoteImagePaths":18768,"frontmatter":18769,"imagePaths":18771},[],[],[],{"title":18758,"description":18759,"summary":18759,"pubDate":18770,"source":19,"url":18761,"thumbnail":21},"Mon, 09 Dec 2024 10:00:00 GMT",[],"2024-12-09-sora-is-here.md","2024-12-10-lematerial-an-open-source-initiative-to-accelerate-materials-discovery-and-research",{"id":18773,"data":18775,"filePath":18781,"digest":18782,"rendered":18783,"legacyId":18791},{"title":18776,"description":25,"summary":18777,"pubDate":18778,"source":2720,"url":18779,"thumbnail":18780},"LeMaterial: an open source initiative to accelerate materials discovery and research","LeMaterial: an open source initiative to accelerate materials discovery and research Today, we are t...",["Date","2024-12-10T00:00:00.000Z"],"https://huggingface.co/blog/lematerial","https://huggingface.co/blog/assets/lematerial/thumbnail_lematerial.png","src/content/posts/2024-12-10-lematerial-an-open-source-initiative-to-accelerate-materials-discovery-and-research.md","5633e5b7f1f1c6ee",{"html":25,"metadata":18784},{"headings":18785,"localImagePaths":18786,"remoteImagePaths":18787,"frontmatter":18788,"imagePaths":18790},[],[],[],{"title":18776,"description":25,"summary":18777,"pubDate":18789,"source":2720,"url":18779,"thumbnail":18780},"Tue, 10 Dec 2024 00:00:00 GMT",[],"2024-12-10-lematerial-an-open-source-initiative-to-accelerate-materials-discovery-and-research.md","2024-12-11-boosting-the-customer-retail-experience-with-gpt-4o-mini",{"id":18792,"data":18794,"filePath":18799,"digest":18800,"rendered":18801,"legacyId":18809},{"title":18795,"description":18796,"summary":18796,"pubDate":18797,"source":19,"url":18798,"thumbnail":21},"Boosting the customer retail experience with GPT-4o mini","Zalando boosts the customer experience with its Assistant, powered by GPT-4o mini",["Date","2024-12-11T06:00:00.000Z"],"https://openai.com/blog/zalando","src/content/posts/2024-12-11-boosting-the-customer-retail-experience-with-gpt-4o-mini.md","aad437ac476f98cf",{"html":25,"metadata":18802},{"headings":18803,"localImagePaths":18804,"remoteImagePaths":18805,"frontmatter":18806,"imagePaths":18808},[],[],[],{"title":18795,"description":18796,"summary":18796,"pubDate":18807,"source":19,"url":18798,"thumbnail":21},"Wed, 11 Dec 2024 06:00:00 GMT",[],"2024-12-11-boosting-the-customer-retail-experience-with-gpt-4o-mini.md","2024-12-11-introducing-gemini-20-our-new-ai-model-for-the-agentic-era",{"id":18810,"data":18812,"filePath":18818,"digest":18819,"rendered":18820,"legacyId":18828},{"title":18813,"description":18814,"summary":18814,"pubDate":18815,"source":6423,"url":18816,"thumbnail":18817},"Introducing Gemini 2.0: our new AI model for the agentic era","Today, we’re announcing Gemini 2.0, our most capable multimodal AI model yet.",["Date","2024-12-11T15:30:40.000Z"],"https://deepmind.google/discover/blog/introducing-gemini-20-our-new-ai-model-for-the-agentic-era/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_hero_thumbnail.width-1300.png","src/content/posts/2024-12-11-introducing-gemini-20-our-new-ai-model-for-the-agentic-era.md","7c055356d95cdb29",{"html":25,"metadata":18821},{"headings":18822,"localImagePaths":18823,"remoteImagePaths":18824,"frontmatter":18825,"imagePaths":18827},[],[],[],{"title":18813,"description":18814,"summary":18814,"pubDate":18826,"source":6423,"url":18816,"thumbnail":18817},"Wed, 11 Dec 2024 15:30:40 +0000",[],"2024-12-11-introducing-gemini-20-our-new-ai-model-for-the-agentic-era.md","2024-12-13-elon-musk-wanted-an-openai-for-profit",{"id":18829,"data":18831,"filePath":18836,"digest":18837,"rendered":18838,"legacyId":18846},{"title":18832,"description":18833,"summary":18833,"pubDate":18834,"source":19,"url":18835,"thumbnail":21},"Elon Musk wanted an OpenAI for-profit","Elon Musk’s latest legal filing against OpenAI marks his fourth attempt in less than a year to reframe his claims. However, his own words and actions speak for themselves—in 2017, Elon not only wanted, but actually created, a for-profit as OpenAI’s proposed new structure.",["Date","2024-12-13T00:00:00.000Z"],"https://openai.com/blog/elon-musk-wanted-an-openai-for-profit","src/content/posts/2024-12-13-elon-musk-wanted-an-openai-for-profit.md","9d9d4cd940551a5b",{"html":25,"metadata":18839},{"headings":18840,"localImagePaths":18841,"remoteImagePaths":18842,"frontmatter":18843,"imagePaths":18845},[],[],[],{"title":18832,"description":18833,"summary":18833,"pubDate":18844,"source":19,"url":18835,"thumbnail":21},"Fri, 13 Dec 2024 00:00:00 GMT",[],"2024-12-13-elon-musk-wanted-an-openai-for-profit.md","2024-12-16-introducing-the-synthetic-data-generator---build-datasets-with-natural-language",{"id":18847,"data":18849,"filePath":18855,"digest":18856,"rendered":18857,"legacyId":18865},{"title":18850,"description":25,"summary":18851,"pubDate":18852,"source":2720,"url":18853,"thumbnail":18854},"Introducing the Synthetic Data Generator - Build Datasets with Natural Language","Introducing the Synthetic Data Generator - Build Datasets with Natural Language Introducing the Synt...",["Date","2024-12-16T00:00:00.000Z"],"https://huggingface.co/blog/synthetic-data-generator","https://huggingface.co/blog/assets/synthetic-data-generator/_thumbnail.png","src/content/posts/2024-12-16-introducing-the-synthetic-data-generator---build-datasets-with-natural-language.md","bb1068c48b1cc038",{"html":25,"metadata":18858},{"headings":18859,"localImagePaths":18860,"remoteImagePaths":18861,"frontmatter":18862,"imagePaths":18864},[],[],[],{"title":18850,"description":25,"summary":18851,"pubDate":18863,"source":2720,"url":18853,"thumbnail":18854},"Mon, 16 Dec 2024 00:00:00 GMT",[],"2024-12-16-introducing-the-synthetic-data-generator---build-datasets-with-natural-language.md","2024-12-16-state-of-the-art-video-and-image-generation-with-veo-2-and-imagen-3",{"id":18866,"data":18868,"filePath":18874,"digest":18875,"rendered":18876,"legacyId":18884},{"title":18869,"description":18870,"summary":18870,"pubDate":18871,"source":6423,"url":18872,"thumbnail":18873},"State-of-the-art video and image generation with Veo 2 and Imagen 3","We’re rolling out a new, state-of-the-art video model, Veo 2, and updates to Imagen 3. Plus, check out our new experiment, Whisk.",["Date","2024-12-16T17:01:16.000Z"],"https://deepmind.google/discover/blog/state-of-the-art-video-and-image-generation-with-veo-2-and-imagen-3/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-16-24_GenMedia_16x9.width-1300.png","src/content/posts/2024-12-16-state-of-the-art-video-and-image-generation-with-veo-2-and-imagen-3.md","e62a2d93b9c09c3e",{"html":25,"metadata":18877},{"headings":18878,"localImagePaths":18879,"remoteImagePaths":18880,"frontmatter":18881,"imagePaths":18883},[],[],[],{"title":18869,"description":18870,"summary":18870,"pubDate":18882,"source":6423,"url":18872,"thumbnail":18873},"Mon, 16 Dec 2024 17:01:16 +0000",[],"2024-12-16-state-of-the-art-video-and-image-generation-with-veo-2-and-imagen-3.md","2024-12-17-benchmarking-language-model-performance-on-5th-gen-xeon-at-gcp",{"id":18885,"data":18887,"filePath":18892,"digest":18893,"rendered":18894,"legacyId":18902},{"title":18888,"description":25,"summary":18889,"pubDate":18890,"source":2720,"url":18891,"thumbnail":12264},"Benchmarking Language Model Performance on 5th Gen Xeon at GCP","Benchmarking Language Model Performance on 5th Gen Xeon at GCP TL;DR: We benchmark 2 representative ...",["Date","2024-12-17T00:00:00.000Z"],"https://huggingface.co/blog/intel-gcp-c4","src/content/posts/2024-12-17-benchmarking-language-model-performance-on-5th-gen-xeon-at-gcp.md","c5d687c2f1dec8f7",{"html":25,"metadata":18895},{"headings":18896,"localImagePaths":18897,"remoteImagePaths":18898,"frontmatter":18899,"imagePaths":18901},[],[],[],{"title":18888,"description":25,"summary":18889,"pubDate":18900,"source":2720,"url":18891,"thumbnail":12264},"Tue, 17 Dec 2024 00:00:00 GMT",[],"2024-12-17-benchmarking-language-model-performance-on-5th-gen-xeon-at-gcp.md","2024-12-17-facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models",{"id":18903,"data":18905,"filePath":18911,"digest":18912,"rendered":18913,"legacyId":18921},{"title":18906,"description":18907,"summary":18907,"pubDate":18908,"source":6423,"url":18909,"thumbnail":18910},"FACTS Grounding: A new benchmark for evaluating the factuality of large language models","Our comprehensive benchmark and online leaderboard offer a much-needed measure of how accurately LLMs ground their responses in provided source material and avoid hallucinations",["Date","2024-12-17T15:29:00.000Z"],"https://deepmind.google/discover/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/","https://lh3.googleusercontent.com/PNlhxhf4LKLRCezIt7Ap358F91-vbK5dLp56Ak1FejpCZh3YTp6jGqIDJm9c0iAtx8Y73MCTu279c1k2GZkM2qXXaqx315NSOaSiU0y0ATMK2c2Hyw=w1200-h630-n-nu","src/content/posts/2024-12-17-facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models.md","0fd3d816c8daf708",{"html":25,"metadata":18914},{"headings":18915,"localImagePaths":18916,"remoteImagePaths":18917,"frontmatter":18918,"imagePaths":18920},[],[],[],{"title":18906,"description":18907,"summary":18907,"pubDate":18919,"source":6423,"url":18909,"thumbnail":18910},"Tue, 17 Dec 2024 15:29:00 +0000",[],"2024-12-17-facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models.md","2024-12-17-openai-o1-and-new-tools-for-developers",{"id":18922,"data":18924,"filePath":18929,"digest":18930,"rendered":18931,"legacyId":18938},{"title":18925,"description":18926,"summary":18926,"pubDate":18927,"source":19,"url":18928,"thumbnail":21},"OpenAI o1 and new tools for developers","Introducing OpenAI o1, Realtime API improvements, a new fine-tuning method and more for developers.",["Date","2024-12-17T00:00:00.000Z"],"https://openai.com/blog/o1-and-new-tools-for-developers","src/content/posts/2024-12-17-openai-o1-and-new-tools-for-developers.md","3f665442b6edc8b1",{"html":25,"metadata":18932},{"headings":18933,"localImagePaths":18934,"remoteImagePaths":18935,"frontmatter":18936,"imagePaths":18937},[],[],[],{"title":18925,"description":18926,"summary":18926,"pubDate":18900,"source":19,"url":18928,"thumbnail":21},[],"2024-12-17-openai-o1-and-new-tools-for-developers.md","2024-12-17-welcome-the-falcon-3-family-of-open-models",{"id":18939,"data":18941,"filePath":18947,"digest":18948,"rendered":18949,"legacyId":18956},{"title":18942,"description":25,"summary":18943,"pubDate":18944,"source":2720,"url":18945,"thumbnail":18946},"Welcome the Falcon 3 Family of Open Models!","Welcome to the Falcon 3 Family of Open Models! We introduce Falcon3, a family of decoder-only large ...",["Date","2024-12-17T00:00:00.000Z"],"https://huggingface.co/blog/falcon3","https://huggingface.co/blog/assets/falcon3/thumbnail.png","src/content/posts/2024-12-17-welcome-the-falcon-3-family-of-open-models.md","243595b6504b724c",{"html":25,"metadata":18950},{"headings":18951,"localImagePaths":18952,"remoteImagePaths":18953,"frontmatter":18954,"imagePaths":18955},[],[],[],{"title":18942,"description":25,"summary":18943,"pubDate":18900,"source":2720,"url":18945,"thumbnail":18946},[],"2024-12-17-welcome-the-falcon-3-family-of-open-models.md","2024-12-18-bamba-inference-efficient-hybrid-mamba2-model",{"id":18957,"data":18959,"filePath":18965,"digest":18966,"rendered":18967,"legacyId":18975},{"title":18960,"description":25,"summary":18961,"pubDate":18962,"source":2720,"url":18963,"thumbnail":18964},"Bamba: Inference-Efficient Hybrid Mamba2 Model","Bamba: Inference-Efficient Hybrid Mamba2 Model 🐍 TL;DR We introduce Bamba-9B, an inference-efficient...",["Date","2024-12-18T00:00:00.000Z"],"https://huggingface.co/blog/bamba","https://huggingface.co/blog/assets/bamba/bamba_thumbnail.png","src/content/posts/2024-12-18-bamba-inference-efficient-hybrid-mamba2-model.md","50e9b5131cf04239",{"html":25,"metadata":18968},{"headings":18969,"localImagePaths":18970,"remoteImagePaths":18971,"frontmatter":18972,"imagePaths":18974},[],[],[],{"title":18960,"description":25,"summary":18961,"pubDate":18973,"source":2720,"url":18963,"thumbnail":18964},"Wed, 18 Dec 2024 00:00:00 GMT",[],"2024-12-18-bamba-inference-efficient-hybrid-mamba2-model.md","2024-12-19-finally-a-replacement-for-bert-introducing-modernbert",{"id":18976,"data":18978,"filePath":18984,"digest":18985,"rendered":18986,"legacyId":18994},{"title":18979,"description":25,"summary":18980,"pubDate":18981,"source":2720,"url":18982,"thumbnail":18983},"Finally, a Replacement for BERT: Introducing ModernBERT","Finally, a Replacement for BERT TL;DR This blog post introduces ModernBERT, a family of state-of-the...",["Date","2024-12-19T00:00:00.000Z"],"https://huggingface.co/blog/modernbert","https://huggingface.co/blog/assets/modernbert/thumbnail.png","src/content/posts/2024-12-19-finally-a-replacement-for-bert-introducing-modernbert.md","c6e3af4534e6fa17",{"html":25,"metadata":18987},{"headings":18988,"localImagePaths":18989,"remoteImagePaths":18990,"frontmatter":18991,"imagePaths":18993},[],[],[],{"title":18979,"description":25,"summary":18980,"pubDate":18992,"source":2720,"url":18982,"thumbnail":18983},"Thu, 19 Dec 2024 00:00:00 GMT",[],"2024-12-19-finally-a-replacement-for-bert-introducing-modernbert.md","2024-12-20-deliberative-alignment-reasoning-enables-safer-language-models",{"id":18995,"data":18997,"filePath":19002,"digest":19003,"rendered":19004,"legacyId":19012},{"title":18998,"description":18999,"summary":18999,"pubDate":19000,"source":19,"url":19001,"thumbnail":21},"Deliberative alignment: reasoning enables safer language models","Deliberative alignment: reasoning enables safer language models Introducing our new alignment strategy for o1 models, which are directly taught safety specifications and how to reason over them.",["Date","2024-12-20T10:00:00.000Z"],"https://openai.com/blog/deliberative-alignment","src/content/posts/2024-12-20-deliberative-alignment-reasoning-enables-safer-language-models.md","319fd13a57ada7b5",{"html":25,"metadata":19005},{"headings":19006,"localImagePaths":19007,"remoteImagePaths":19008,"frontmatter":19009,"imagePaths":19011},[],[],[],{"title":18998,"description":18999,"summary":18999,"pubDate":19010,"source":19,"url":19001,"thumbnail":21},"Fri, 20 Dec 2024 10:00:00 GMT",[],"2024-12-20-deliberative-alignment-reasoning-enables-safer-language-models.md","2024-12-20-evaluating-audio-reasoning-with-big-bench-audio",{"id":19013,"data":19015,"filePath":19021,"digest":19022,"rendered":19023,"legacyId":19031},{"title":19016,"description":25,"summary":19017,"pubDate":19018,"source":2720,"url":19019,"thumbnail":19020},"Evaluating Audio Reasoning with Big Bench Audio","Evaluating Audio Reasoning with Big Bench Audio The emergence of native Speech to Speech models offe...",["Date","2024-12-20T00:00:00.000Z"],"https://huggingface.co/blog/big-bench-audio-release","https://huggingface.co/blog/assets/big_bench_audio_release/big-bench-audio-thumbnail.png","src/content/posts/2024-12-20-evaluating-audio-reasoning-with-big-bench-audio.md","41f3d2c7aa241a59",{"html":25,"metadata":19024},{"headings":19025,"localImagePaths":19026,"remoteImagePaths":19027,"frontmatter":19028,"imagePaths":19030},[],[],[],{"title":19016,"description":25,"summary":19017,"pubDate":19029,"source":2720,"url":19019,"thumbnail":19020},"Fri, 20 Dec 2024 00:00:00 GMT",[],"2024-12-20-evaluating-audio-reasoning-with-big-bench-audio.md","2024-12-23-controlling-language-model-generation-with-nvidias-logitsprocessorzoo",{"id":19032,"data":19034,"filePath":19040,"digest":19041,"rendered":19042,"legacyId":19050},{"title":19035,"description":25,"summary":19036,"pubDate":19037,"source":2720,"url":19038,"thumbnail":19039},"Controlling Language Model Generation with NVIDIA's LogitsProcessorZoo","Controlling Language Model Generation with NVIDIA's LogitsProcessorZoo Generating text with language...",["Date","2024-12-23T00:00:00.000Z"],"https://huggingface.co/blog/logits-processor-zoo","https://huggingface.co/blog/assets/logits-processor-zoo/thumbnail.png","src/content/posts/2024-12-23-controlling-language-model-generation-with-nvidias-logitsprocessorzoo.md","ad76482a0a4f4e4d",{"html":25,"metadata":19043},{"headings":19044,"localImagePaths":19045,"remoteImagePaths":19046,"frontmatter":19047,"imagePaths":19049},[],[],[],{"title":19035,"description":25,"summary":19036,"pubDate":19048,"source":2720,"url":19038,"thumbnail":19039},"Mon, 23 Dec 2024 00:00:00 GMT",[],"2024-12-23-controlling-language-model-generation-with-nvidias-logitsprocessorzoo.md","2024-12-24-visualize-and-understand-gpu-memory-in-pytorch",{"id":19051,"data":19053,"filePath":19059,"digest":19060,"rendered":19061,"legacyId":19069},{"title":19054,"description":25,"summary":19055,"pubDate":19056,"source":2720,"url":19057,"thumbnail":19058},"Visualize and understand GPU memory in PyTorch","Visualize and understand GPU memory in PyTorch You must be familiar with this message 🤬: RuntimeErro...",["Date","2024-12-24T00:00:00.000Z"],"https://huggingface.co/blog/train_memory","https://huggingface.co/blog/assets/train_memory/thumbnail.png","src/content/posts/2024-12-24-visualize-and-understand-gpu-memory-in-pytorch.md","1de3a95060002710",{"html":25,"metadata":19062},{"headings":19063,"localImagePaths":19064,"remoteImagePaths":19065,"frontmatter":19066,"imagePaths":19068},[],[],[],{"title":19054,"description":25,"summary":19055,"pubDate":19067,"source":2720,"url":19057,"thumbnail":19058},"Tue, 24 Dec 2024 00:00:00 GMT",[],"2024-12-24-visualize-and-understand-gpu-memory-in-pytorch.md","2024-12-27-why-openais-structure-must-evolve-to-advance-our-mission",{"id":19070,"data":19072,"filePath":19077,"digest":19078,"rendered":19079,"legacyId":19087},{"title":19073,"description":19074,"summary":19074,"pubDate":19075,"source":19,"url":19076,"thumbnail":21},"Why OpenAI’s structure must evolve to advance our mission","A stronger non-profit supported by the for-profit’s success.",["Date","2024-12-27T00:00:00.000Z"],"https://openai.com/blog/why-our-structure-must-evolve-to-advance-our-mission","src/content/posts/2024-12-27-why-openais-structure-must-evolve-to-advance-our-mission.md","85a8b4b9a952006e",{"html":25,"metadata":19080},{"headings":19081,"localImagePaths":19082,"remoteImagePaths":19083,"frontmatter":19084,"imagePaths":19086},[],[],[],{"title":19073,"description":19074,"summary":19074,"pubDate":19085,"source":19,"url":19076,"thumbnail":21},"Fri, 27 Dec 2024 00:00:00 GMT",[],"2024-12-27-why-openais-structure-must-evolve-to-advance-our-mission.md","2024-12-31-introducing-smolagents-simple-agents-that-write-actions-in-code",{"id":19088,"data":19090,"filePath":19096,"digest":19097,"rendered":19098,"legacyId":19106},{"title":19091,"description":25,"summary":19092,"pubDate":19093,"source":2720,"url":19094,"thumbnail":19095},"Introducing smolagents: simple agents that write actions in code.","Introducing smolagents, a simple library to build agents Today we are launching smolagents , a very ...",["Date","2024-12-31T00:00:00.000Z"],"https://huggingface.co/blog/smolagents","https://huggingface.co/blog/assets/smolagents/thumbnail.png","src/content/posts/2024-12-31-introducing-smolagents-simple-agents-that-write-actions-in-code.md","0a67ce99b21e9344",{"html":25,"metadata":19099},{"headings":19100,"localImagePaths":19101,"remoteImagePaths":19102,"frontmatter":19103,"imagePaths":19105},[],[],[],{"title":19091,"description":25,"summary":19092,"pubDate":19104,"source":2720,"url":19094,"thumbnail":19095},"Tue, 31 Dec 2024 00:00:00 GMT",[],"2024-12-31-introducing-smolagents-simple-agents-that-write-actions-in-code.md","2025-01-09-co-emissions-and-models-performance-insights-from-the-open-llm-leaderboard",{"id":19107,"data":19109,"filePath":19114,"digest":19115,"rendered":19116,"legacyId":19124},{"title":19110,"description":25,"summary":19111,"pubDate":19112,"source":2720,"url":19113,"thumbnail":9445},"CO₂ Emissions and Models Performance: Insights from the Open LLM Leaderboard","CO₂ Emissions and Models Performance: Insights from the Open LLM Leaderboard Since June 2024, we hav...",["Date","2025-01-09T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-emissions-analysis","src/content/posts/2025-01-09-co₂-emissions-and-models-performance-insights-from-the-open-llm-leaderboard.md","bca3626bd5640444",{"html":25,"metadata":19117},{"headings":19118,"localImagePaths":19119,"remoteImagePaths":19120,"frontmatter":19121,"imagePaths":19123},[],[],[],{"title":19110,"description":25,"summary":19111,"pubDate":19122,"source":2720,"url":19113,"thumbnail":9445},"Thu, 09 Jan 2025 00:00:00 GMT",[],"2025-01-09-co₂-emissions-and-models-performance-insights-from-the-open-llm-leaderboard.md","2025-01-10-visual-document-retrieval-goes-multilingual",{"id":19125,"data":19127,"filePath":19133,"digest":19134,"rendered":19135,"legacyId":19143},{"title":19128,"description":25,"summary":19129,"pubDate":19130,"source":2720,"url":19131,"thumbnail":19132},"Visual Document Retrieval Goes Multilingual","Visual Document Retrieval Goes Multilingual TL;DR: We present vdr-2b-multi-v1 , the best multilingua...",["Date","2025-01-10T00:00:00.000Z"],"https://huggingface.co/blog/vdr-2b-multilingual","https://huggingface.co/blog/assets/vdr-2b-multilingual/thumbnail.png","src/content/posts/2025-01-10-visual-document-retrieval-goes-multilingual.md","b2c4d67f5c00f402",{"html":25,"metadata":19136},{"headings":19137,"localImagePaths":19138,"remoteImagePaths":19139,"frontmatter":19140,"imagePaths":19142},[],[],[],{"title":19128,"description":25,"summary":19129,"pubDate":19141,"source":2720,"url":19131,"thumbnail":19132},"Fri, 10 Jan 2025 00:00:00 GMT",[],"2025-01-10-visual-document-retrieval-goes-multilingual.md","2025-01-13-ai-agents-are-here-what-now",{"id":19144,"data":19146,"filePath":19152,"digest":19153,"rendered":19154,"legacyId":19162},{"title":19147,"description":25,"summary":19148,"pubDate":19149,"source":2720,"url":19150,"thumbnail":19151},"AI Agents Are Here. What Now?","AI Agents Are Here. What Now? Introduction The sudden, rapid advancement of LLM capabilities – such ...",["Date","2025-01-13T00:00:00.000Z"],"https://huggingface.co/blog/ethics-soc-7","https://huggingface.co/blog/assets/190_ethics-soc-7/thumbnail.png","src/content/posts/2025-01-13-ai-agents-are-here-what-now.md","9beb9c9a92880df7",{"html":25,"metadata":19155},{"headings":19156,"localImagePaths":19157,"remoteImagePaths":19158,"frontmatter":19159,"imagePaths":19161},[],[],[],{"title":19147,"description":25,"summary":19148,"pubDate":19160,"source":2720,"url":19150,"thumbnail":19151},"Mon, 13 Jan 2025 00:00:00 GMT",[],"2025-01-13-ai-agents-are-here-what-now.md","2025-01-13-openais-economic-blueprint",{"id":19163,"data":19165,"filePath":19169,"digest":19170,"rendered":19171,"legacyId":19179},{"title":19166,"description":19166,"summary":19166,"pubDate":19167,"source":19,"url":19168,"thumbnail":21},"OpenAI’s Economic Blueprint",["Date","2025-01-13T03:00:00.000Z"],"https://openai.com/global-affairs/openais-economic-blueprint","src/content/posts/2025-01-13-openais-economic-blueprint.md","0b009ee87b735f96",{"html":25,"metadata":19172},{"headings":19173,"localImagePaths":19174,"remoteImagePaths":19175,"frontmatter":19176,"imagePaths":19178},[],[],[],{"title":19166,"description":19166,"summary":19166,"pubDate":19177,"source":19,"url":19168,"thumbnail":21},"Mon, 13 Jan 2025 03:00:00 GMT",[],"2025-01-13-openais-economic-blueprint.md","2025-01-14-adebayo-ogunlesi-joins-openais-board-of-directors",{"id":19180,"data":19182,"filePath":19187,"digest":19188,"rendered":19189,"legacyId":19197},{"title":19183,"description":19184,"summary":19184,"pubDate":19185,"source":19,"url":19186,"thumbnail":21},"Adebayo Ogunlesi joins OpenAI’s Board of Directors","Adebayo Ogunlesi Joins OpenAI’s Board of Directors",["Date","2025-01-14T09:00:00.000Z"],"https://openai.com/blog/adebayo-ogunlesi-joins-openais-board-of-directors","src/content/posts/2025-01-14-adebayo-ogunlesi-joins-openais-board-of-directors.md","718132aa300d4db0",{"html":25,"metadata":19190},{"headings":19191,"localImagePaths":19192,"remoteImagePaths":19193,"frontmatter":19194,"imagePaths":19196},[],[],[],{"title":19183,"description":19184,"summary":19184,"pubDate":19195,"source":19,"url":19186,"thumbnail":21},"Tue, 14 Jan 2025 09:00:00 GMT",[],"2025-01-14-adebayo-ogunlesi-joins-openais-board-of-directors.md","2025-01-15-partnering-with-axios-expands-openais-work-with-the-news-industry",{"id":19198,"data":19200,"filePath":19205,"digest":19206,"rendered":19207,"legacyId":19215},{"title":19201,"description":19202,"summary":19202,"pubDate":19203,"source":19,"url":19204,"thumbnail":21},"Partnering with Axios expands OpenAI’s work with the news industry","Publishers representing hundreds of newsrooms and content brands are using OpenAI partnerships and grant programs to adopt AI tools and strengthen the news ecosystem, while ChatGPT users gain access to information from leading, reliable publications.",["Date","2025-01-15T03:00:00.000Z"],"https://openai.com/blog/partnering-with-axios-expands-openai-work-with-the-news-industry","src/content/posts/2025-01-15-partnering-with-axios-expands-openais-work-with-the-news-industry.md","0835ee3a4bf531b5",{"html":25,"metadata":19208},{"headings":19209,"localImagePaths":19210,"remoteImagePaths":19211,"frontmatter":19212,"imagePaths":19214},[],[],[],{"title":19201,"description":19202,"summary":19202,"pubDate":19213,"source":19,"url":19204,"thumbnail":21},"Wed, 15 Jan 2025 03:00:00 GMT",[],"2025-01-15-partnering-with-axios-expands-openais-work-with-the-news-industry.md","2025-01-15-train-400x-faster-static-embedding-models-with-sentence-transformers",{"id":19216,"data":19218,"filePath":19223,"digest":19224,"rendered":19225,"legacyId":19233},{"title":19219,"description":25,"summary":19220,"pubDate":19221,"source":2720,"url":19222,"thumbnail":14686},"Train 400x faster Static Embedding Models with Sentence Transformers","Train 400x faster Static Embedding Models with Sentence Transformers TL;DR This blog post introduces...",["Date","2025-01-15T00:00:00.000Z"],"https://huggingface.co/blog/static-embeddings","src/content/posts/2025-01-15-train-400x-faster-static-embedding-models-with-sentence-transformers.md","793df2a7d19f48fc",{"html":25,"metadata":19226},{"headings":19227,"localImagePaths":19228,"remoteImagePaths":19229,"frontmatter":19230,"imagePaths":19232},[],[],[],{"title":19219,"description":25,"summary":19220,"pubDate":19231,"source":2720,"url":19222,"thumbnail":14686},"Wed, 15 Jan 2025 00:00:00 GMT",[],"2025-01-15-train-400x-faster-static-embedding-models-with-sentence-transformers.md","2025-01-16-introducing-multi-backends-trt-llm-vllm-support-for-text-generation-inference",{"id":19234,"data":19236,"filePath":19242,"digest":19243,"rendered":19244,"legacyId":19252},{"title":19237,"description":25,"summary":19238,"pubDate":19239,"source":2720,"url":19240,"thumbnail":19241},"Introducing multi-backends (TRT-LLM, vLLM) support for Text Generation Inference","Introducing multi-backends (TRT-LLM, vLLM) support for Text Generation Inference Introduction Since ...",["Date","2025-01-16T00:00:00.000Z"],"https://huggingface.co/blog/tgi-multi-backend","https://huggingface.co/blog/assets/tgi-multi-backend/thumbnail.png","src/content/posts/2025-01-16-introducing-multi-backends-trt-llm-vllm-support-for-text-generation-inference.md","78269eb52ff1f3cd",{"html":25,"metadata":19245},{"headings":19246,"localImagePaths":19247,"remoteImagePaths":19248,"frontmatter":19249,"imagePaths":19251},[],[],[],{"title":19237,"description":25,"summary":19238,"pubDate":19250,"source":2720,"url":19240,"thumbnail":19241},"Thu, 16 Jan 2025 00:00:00 GMT",[],"2025-01-16-introducing-multi-backends-trt-llm-vllm-support-for-text-generation-inference.md","2025-01-16-timm-transformers-use-any-timm-model-with-transformers",{"id":19253,"data":19255,"filePath":19261,"digest":19262,"rendered":19263,"legacyId":19270},{"title":19256,"description":25,"summary":19257,"pubDate":19258,"source":2720,"url":19259,"thumbnail":19260},"Timm ❤️ Transformers: Use any timm model with transformers","Timm ❤️ Transformers: Use any timm model with transformers Get lightning-fast inference, quick quant...",["Date","2025-01-16T00:00:00.000Z"],"https://huggingface.co/blog/timm-transformers","https://huggingface.co/blog/assets/timm-transformers/thumbnail.png","src/content/posts/2025-01-16-timm-transformers-use-any-timm-model-with-transformers.md","41936124dcc079b3",{"html":25,"metadata":19264},{"headings":19265,"localImagePaths":19266,"remoteImagePaths":19267,"frontmatter":19268,"imagePaths":19269},[],[],[],{"title":19256,"description":25,"summary":19257,"pubDate":19250,"source":2720,"url":19259,"thumbnail":19260},[],"2025-01-16-timm-transformers-use-any-timm-model-with-transformers.md","2025-01-17-the-power-of-personalized-ai",{"id":19271,"data":19273,"filePath":19277,"digest":19278,"rendered":19279,"legacyId":19287},{"title":19274,"description":19274,"summary":19274,"pubDate":19275,"source":19,"url":19276,"thumbnail":21},"The power of personalized AI",["Date","2025-01-17T13:00:00.000Z"],"https://openai.com/global-affairs/the-power-of-personalized-ai","src/content/posts/2025-01-17-the-power-of-personalized-ai.md","830ac3997a15c97d",{"html":25,"metadata":19280},{"headings":19281,"localImagePaths":19282,"remoteImagePaths":19283,"frontmatter":19284,"imagePaths":19286},[],[],[],{"title":19274,"description":19274,"summary":19274,"pubDate":19285,"source":19,"url":19276,"thumbnail":21},"Fri, 17 Jan 2025 13:00:00 GMT",[],"2025-01-17-the-power-of-personalized-ai.md","2025-01-21-announcing-the-stargate-project",{"id":19288,"data":19290,"filePath":19294,"digest":19295,"rendered":19296,"legacyId":19304},{"title":19291,"description":19291,"summary":19291,"pubDate":19292,"source":19,"url":19293,"thumbnail":21},"Announcing The Stargate Project",["Date","2025-01-21T13:30:00.000Z"],"https://openai.com/blog/announcing-the-stargate-project","src/content/posts/2025-01-21-announcing-the-stargate-project.md","91139667907d77f8",{"html":25,"metadata":19297},{"headings":19298,"localImagePaths":19299,"remoteImagePaths":19300,"frontmatter":19301,"imagePaths":19303},[],[],[],{"title":19291,"description":19291,"summary":19291,"pubDate":19302,"source":19,"url":19293,"thumbnail":21},"Tue, 21 Jan 2025 13:30:00 GMT",[],"2025-01-21-announcing-the-stargate-project.md","2025-01-21-stargate-infrastructure",{"id":19305,"data":19307,"filePath":19312,"digest":19313,"rendered":19314,"legacyId":19321},{"title":19308,"description":19309,"summary":19309,"pubDate":19310,"source":19,"url":19311,"thumbnail":21},"Stargate Infrastructure","OpenAI, and our strategic partners, are thrilled about our shared vision for the Infrastructure of AGI. We are energized by the challenges we face and are excited by the prospect of partnering with firms across the industrial base to deliver against our ambitious mission. Specifically, we want to connect with firms across the built data center infrastructure landscape, from power and land to construction to equipment, and everything in between.",["Date","2025-01-21T13:30:00.000Z"],"https://openai.com/form/stargate-infrastructure","src/content/posts/2025-01-21-stargate-infrastructure.md","1d5ba65a4c4f7b3b",{"html":25,"metadata":19315},{"headings":19316,"localImagePaths":19317,"remoteImagePaths":19318,"frontmatter":19319,"imagePaths":19320},[],[],[],{"title":19308,"description":19309,"summary":19309,"pubDate":19302,"source":19,"url":19311,"thumbnail":21},[],"2025-01-21-stargate-infrastructure.md","2025-01-22-bertelsmann-powers-creativity-and-productivity-with-openai",{"id":19322,"data":19324,"filePath":19329,"digest":19330,"rendered":19331,"legacyId":19339},{"title":19325,"description":19326,"summary":19326,"pubDate":19327,"source":19,"url":19328,"thumbnail":21},"Bertelsmann powers creativity and productivity with OpenAI","Bertelsmann, the global media, services, and education company headquartered in Germany, will integrate OpenAI’s technology across multiple brands around the world.",["Date","2025-01-22T17:00:00.000Z"],"https://openai.com/blog/bertelsmann-powers-creativity-and-productivity-with-openai","src/content/posts/2025-01-22-bertelsmann-powers-creativity-and-productivity-with-openai.md","26a504427024eb68",{"html":25,"metadata":19332},{"headings":19333,"localImagePaths":19334,"remoteImagePaths":19335,"frontmatter":19336,"imagePaths":19338},[],[],[],{"title":19325,"description":19326,"summary":19326,"pubDate":19337,"source":19,"url":19328,"thumbnail":21},"Wed, 22 Jan 2025 17:00:00 GMT",[],"2025-01-22-bertelsmann-powers-creativity-and-productivity-with-openai.md","2025-01-22-hugging-face-and-friendliai-partner-to-supercharge-model-deployment-on-the-hub",{"id":19340,"data":19342,"filePath":19348,"digest":19349,"rendered":19350,"legacyId":19358},{"title":19343,"description":25,"summary":19344,"pubDate":19345,"source":2720,"url":19346,"thumbnail":19347},"Hugging Face and FriendliAI partner to supercharge model deployment on the Hub","Hugging Face and FriendliAI partner to supercharge model deployment on the Hub FriendliAI’s inferenc...",["Date","2025-01-22T00:00:00.000Z"],"https://huggingface.co/blog/friendliai-partnership","https://huggingface.co/blog/assets/friendliai-partnership/thumbnail.png","src/content/posts/2025-01-22-hugging-face-and-friendliai-partner-to-supercharge-model-deployment-on-the-hub.md","a3217743efab1b86",{"html":25,"metadata":19351},{"headings":19352,"localImagePaths":19353,"remoteImagePaths":19354,"frontmatter":19355,"imagePaths":19357},[],[],[],{"title":19343,"description":25,"summary":19344,"pubDate":19356,"source":2720,"url":19346,"thumbnail":19347},"Wed, 22 Jan 2025 00:00:00 GMT",[],"2025-01-22-hugging-face-and-friendliai-partner-to-supercharge-model-deployment-on-the-hub.md","2025-01-22-trading-inference-time-compute-for-adversarial-robustness",{"id":19359,"data":19361,"filePath":19366,"digest":19367,"rendered":19368,"legacyId":19376},{"title":19362,"description":19363,"summary":19363,"pubDate":19364,"source":19,"url":19365,"thumbnail":21},"Trading inference-time compute for adversarial robustness","Trading Inference-Time Compute for Adversarial Robustness",["Date","2025-01-22T10:00:00.000Z"],"https://openai.com/blog/trading-inference-time-compute-for-adversarial-robustness","src/content/posts/2025-01-22-trading-inference-time-compute-for-adversarial-robustness.md","1069f4b0e5873c35",{"html":25,"metadata":19369},{"headings":19370,"localImagePaths":19371,"remoteImagePaths":19372,"frontmatter":19373,"imagePaths":19375},[],[],[],{"title":19362,"description":19363,"summary":19363,"pubDate":19374,"source":19,"url":19365,"thumbnail":21},"Wed, 22 Jan 2025 10:00:00 GMT",[],"2025-01-22-trading-inference-time-compute-for-adversarial-robustness.md","2025-01-23-computer-using-agent",{"id":19377,"data":19379,"filePath":19384,"digest":19385,"rendered":19386,"legacyId":19394},{"title":19380,"description":19381,"summary":19381,"pubDate":19382,"source":19,"url":19383,"thumbnail":21},"Computer-Using Agent","A universal interface for AI to interact with the digital world.",["Date","2025-01-23T10:00:00.000Z"],"https://openai.com/blog/computer-using-agent","src/content/posts/2025-01-23-computer-using-agent.md","69854aa1ba5ab55d",{"html":25,"metadata":19387},{"headings":19388,"localImagePaths":19389,"remoteImagePaths":19390,"frontmatter":19391,"imagePaths":19393},[],[],[],{"title":19380,"description":19381,"summary":19381,"pubDate":19392,"source":19,"url":19383,"thumbnail":21},"Thu, 23 Jan 2025 10:00:00 GMT",[],"2025-01-23-computer-using-agent.md","2025-01-23-operator-system-card",{"id":19395,"data":19397,"filePath":19402,"digest":19403,"rendered":19404,"legacyId":19411},{"title":19398,"description":19399,"summary":19399,"pubDate":19400,"source":19,"url":19401,"thumbnail":21},"Operator System Card","Drawing from OpenAI’s established safety frameworks, this document highlights our multi-layered approach, including model and product mitigations we’ve implemented to protect against prompt engineering and jailbreaks, protect privacy and security, as well as details our external red teaming efforts, safety evaluations, and ongoing work to further refine these safeguards.",["Date","2025-01-23T10:00:00.000Z"],"https://openai.com/blog/operator-system-card","src/content/posts/2025-01-23-operator-system-card.md","3359de3bc1ed250e",{"html":25,"metadata":19405},{"headings":19406,"localImagePaths":19407,"remoteImagePaths":19408,"frontmatter":19409,"imagePaths":19410},[],[],[],{"title":19398,"description":19399,"summary":19399,"pubDate":19392,"source":19,"url":19401,"thumbnail":21},[],"2025-01-23-operator-system-card.md","2025-01-23-introducing-operator",{"id":19412,"data":19414,"filePath":19419,"digest":19420,"rendered":19421,"legacyId":19428},{"title":19415,"description":19416,"summary":19416,"pubDate":19417,"source":19,"url":19418,"thumbnail":21},"Introducing Operator","A research preview of an agent that can use its own browser to perform tasks for you. Available to Pro users in the U.S.",["Date","2025-01-23T10:00:00.000Z"],"https://openai.com/blog/introducing-operator","src/content/posts/2025-01-23-introducing-operator.md","dbe1bb862a23b796",{"html":25,"metadata":19422},{"headings":19423,"localImagePaths":19424,"remoteImagePaths":19425,"frontmatter":19426,"imagePaths":19427},[],[],[],{"title":19415,"description":19416,"summary":19416,"pubDate":19392,"source":19,"url":19418,"thumbnail":21},[],"2025-01-23-introducing-operator.md","2025-01-23-smolvlm-grows-smaller-introducing-the-250m-500m-models",{"id":19429,"data":19431,"filePath":19437,"digest":19438,"rendered":19439,"legacyId":19447},{"title":19432,"description":25,"summary":19433,"pubDate":19434,"source":2720,"url":19435,"thumbnail":19436},"SmolVLM Grows Smaller – Introducing the 250M & 500M Models!","SmolVLM Grows Smaller – Introducing the 250M & 500M Models! TLDR We’re excited to announce two new a...",["Date","2025-01-23T00:00:00.000Z"],"https://huggingface.co/blog/smolervlm","https://huggingface.co/blog/assets/smolervlm/banner.png","src/content/posts/2025-01-23-smolvlm-grows-smaller-introducing-the-250m-500m-models.md","23a5f93456734ed9",{"html":25,"metadata":19440},{"headings":19441,"localImagePaths":19442,"remoteImagePaths":19443,"frontmatter":19444,"imagePaths":19446},[],[],[],{"title":19432,"description":25,"summary":19433,"pubDate":19445,"source":2720,"url":19435,"thumbnail":19436},"Thu, 23 Jan 2025 00:00:00 GMT",[],"2025-01-23-smolvlm-grows-smaller-introducing-the-250m-500m-models.md","2025-01-24-we-now-support-vlms-in-smolagents",{"id":19448,"data":19450,"filePath":19456,"digest":19457,"rendered":19458,"legacyId":19466},{"title":19451,"description":25,"summary":19452,"pubDate":19453,"source":2720,"url":19454,"thumbnail":19455},"We now support VLMs in smolagents!","We just gave sight to smolagents You hypocrite, first take the log out of your own eye, and then you...",["Date","2025-01-24T00:00:00.000Z"],"https://huggingface.co/blog/smolagents-can-see","https://huggingface.co/blog/assets/smolagents-can-see/thumbnail.png","src/content/posts/2025-01-24-we-now-support-vlms-in-smolagents.md","0d038b21d3ec6cef",{"html":25,"metadata":19459},{"headings":19460,"localImagePaths":19461,"remoteImagePaths":19462,"frontmatter":19463,"imagePaths":19465},[],[],[],{"title":19451,"description":25,"summary":19452,"pubDate":19464,"source":2720,"url":19454,"thumbnail":19455},"Fri, 24 Jan 2025 00:00:00 GMT",[],"2025-01-24-we-now-support-vlms-in-smolagents.md","2025-01-27-state-of-open-video-generation-models-in-diffusers",{"id":19467,"data":19469,"filePath":19475,"digest":19476,"rendered":19477,"legacyId":19485},{"title":19470,"description":25,"summary":19471,"pubDate":19472,"source":2720,"url":19473,"thumbnail":19474},"State of open video generation models in Diffusers","State of open video generation models in Diffusers OpenAI’s Sora demo marked a striking advance in A...",["Date","2025-01-27T00:00:00.000Z"],"https://huggingface.co/blog/video_gen","https://huggingface.co/blog/assets/video_gen/thumbnail.png","src/content/posts/2025-01-27-state-of-open-video-generation-models-in-diffusers.md","88efc8aeec81b840",{"html":25,"metadata":19478},{"headings":19479,"localImagePaths":19480,"remoteImagePaths":19481,"frontmatter":19482,"imagePaths":19484},[],[],[],{"title":19470,"description":25,"summary":19471,"pubDate":19483,"source":2720,"url":19473,"thumbnail":19474},"Mon, 27 Jan 2025 00:00:00 GMT",[],"2025-01-27-state-of-open-video-generation-models-in-diffusers.md","2025-01-28-introducing-chatgpt-gov",{"id":19486,"data":19488,"filePath":19493,"digest":19494,"rendered":19495,"legacyId":19503},{"title":19489,"description":19490,"summary":19490,"pubDate":19491,"source":19,"url":19492,"thumbnail":21},"Introducing ChatGPT Gov","ChatGPT Gov is designed to streamline government agencies’ access to OpenAI’s frontier models.",["Date","2025-01-28T06:00:00.000Z"],"https://openai.com/global-affairs/introducing-chatgpt-gov","src/content/posts/2025-01-28-introducing-chatgpt-gov.md","5f0e8f6fa7cb0876",{"html":25,"metadata":19496},{"headings":19497,"localImagePaths":19498,"remoteImagePaths":19499,"frontmatter":19500,"imagePaths":19502},[],[],[],{"title":19489,"description":19490,"summary":19490,"pubDate":19501,"source":19,"url":19492,"thumbnail":21},"Tue, 28 Jan 2025 06:00:00 GMT",[],"2025-01-28-introducing-chatgpt-gov.md","2025-01-28-open-r1-a-fully-open-reproduction-of-deepseek-r1",{"id":19504,"data":19506,"filePath":19512,"digest":19513,"rendered":19514,"legacyId":19522},{"title":19507,"description":25,"summary":19508,"pubDate":19509,"source":2720,"url":19510,"thumbnail":19511},"Open-R1: a fully open reproduction of DeepSeek-R1","Open-R1: a fully open reproduction of DeepSeek-R1 What is DeepSeek-R1? If you’ve ever struggled with...",["Date","2025-01-28T00:00:00.000Z"],"https://huggingface.co/blog/open-r1","https://huggingface.co/blog/assets/open-r1/thumbnails.png","src/content/posts/2025-01-28-open-r1-a-fully-open-reproduction-of-deepseek-r1.md","e179d995cd47de4e",{"html":25,"metadata":19515},{"headings":19516,"localImagePaths":19517,"remoteImagePaths":19518,"frontmatter":19519,"imagePaths":19521},[],[],[],{"title":19507,"description":25,"summary":19508,"pubDate":19520,"source":2720,"url":19510,"thumbnail":19511},"Tue, 28 Jan 2025 00:00:00 GMT",[],"2025-01-28-open-r1-a-fully-open-reproduction-of-deepseek-r1.md","2025-01-28-welcome-to-inference-providers-on-the-hub",{"id":19523,"data":19525,"filePath":19531,"digest":19532,"rendered":19533,"legacyId":19540},{"title":19526,"description":25,"summary":19527,"pubDate":19528,"source":2720,"url":19529,"thumbnail":19530},"Welcome to Inference Providers on the Hub 🔥","Welcome to Inference Providers on the Hub 🔥 Today, we are launching the integration of four awesome ...",["Date","2025-01-28T00:00:00.000Z"],"https://huggingface.co/blog/inference-providers","https://huggingface.co/blog/assets/inference-providers/thumbnail.png","src/content/posts/2025-01-28-welcome-to-inference-providers-on-the-hub.md","71eb41977e4c1876",{"html":25,"metadata":19534},{"headings":19535,"localImagePaths":19536,"remoteImagePaths":19537,"frontmatter":19538,"imagePaths":19539},[],[],[],{"title":19526,"description":25,"summary":19527,"pubDate":19520,"source":2720,"url":19529,"thumbnail":19530},[],"2025-01-28-welcome-to-inference-providers-on-the-hub.md","2025-01-30-how-to-deploy-and-fine-tune-deepseek-models-on-aws",{"id":19541,"data":19543,"filePath":19549,"digest":19550,"rendered":19551,"legacyId":19559},{"title":19544,"description":25,"summary":19545,"pubDate":19546,"source":2720,"url":19547,"thumbnail":19548},"How to deploy and fine-tune DeepSeek models on AWS","How to deploy and fine-tune DeepSeek models on AWS A running document to showcase how to deploy and ...",["Date","2025-01-30T00:00:00.000Z"],"https://huggingface.co/blog/deepseek-r1-aws","https://huggingface.co/blog/assets/deepseek-r1-aws/thumbnail.png","src/content/posts/2025-01-30-how-to-deploy-and-fine-tune-deepseek-models-on-aws.md","a355e8d4f930f1fa",{"html":25,"metadata":19552},{"headings":19553,"localImagePaths":19554,"remoteImagePaths":19555,"frontmatter":19556,"imagePaths":19558},[],[],[],{"title":19544,"description":25,"summary":19545,"pubDate":19557,"source":2720,"url":19547,"thumbnail":19548},"Thu, 30 Jan 2025 00:00:00 GMT",[],"2025-01-30-how-to-deploy-and-fine-tune-deepseek-models-on-aws.md","2025-01-30-strengthening-americas-ai-leadership-with-the-us-national-laboratories",{"id":19560,"data":19562,"filePath":19567,"digest":19568,"rendered":19569,"legacyId":19577},{"title":19563,"description":19564,"summary":19564,"pubDate":19565,"source":19,"url":19566,"thumbnail":21},"Strengthening America’s AI leadership with the U.S. National Laboratories","OpenAI’s latest line of reasoning models will be used by nation’s leading scientists to drive scientific breakthroughs.",["Date","2025-01-30T10:00:00.000Z"],"https://openai.com/blog/strengthening-americas-ai-leadership-with-the-us-national-laboratories","src/content/posts/2025-01-30-strengthening-americas-ai-leadership-with-the-us-national-laboratories.md","9329c093b59b3381",{"html":25,"metadata":19570},{"headings":19571,"localImagePaths":19572,"remoteImagePaths":19573,"frontmatter":19574,"imagePaths":19576},[],[],[],{"title":19563,"description":19564,"summary":19564,"pubDate":19575,"source":19,"url":19566,"thumbnail":21},"Thu, 30 Jan 2025 10:00:00 GMT",[],"2025-01-30-strengthening-americas-ai-leadership-with-the-us-national-laboratories.md","2025-01-31-openai-o3-mini-system-card",{"id":19578,"data":19580,"filePath":19585,"digest":19586,"rendered":19587,"legacyId":19595},{"title":19581,"description":19582,"summary":19582,"pubDate":19583,"source":19,"url":19584,"thumbnail":21},"OpenAI o3-mini System Card","This report outlines the safety work carried out for the OpenAI o3-mini model, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",["Date","2025-01-31T11:00:00.000Z"],"https://openai.com/blog/o3-mini-system-card","src/content/posts/2025-01-31-openai-o3-mini-system-card.md","f7b34c85df3f4f54",{"html":25,"metadata":19588},{"headings":19589,"localImagePaths":19590,"remoteImagePaths":19591,"frontmatter":19592,"imagePaths":19594},[],[],[],{"title":19581,"description":19582,"summary":19582,"pubDate":19593,"source":19,"url":19584,"thumbnail":21},"Fri, 31 Jan 2025 11:00:00 GMT",[],"2025-01-31-openai-o3-mini-system-card.md","2025-01-31-openai-o3-mini",{"id":19596,"data":19598,"filePath":19603,"digest":19604,"rendered":19605,"legacyId":19612},{"title":19599,"description":19600,"summary":19600,"pubDate":19601,"source":19,"url":19602,"thumbnail":21},"OpenAI o3-mini","Pushing the frontier of cost-effective reasoning.",["Date","2025-01-31T11:00:00.000Z"],"https://openai.com/blog/openai-o3-mini","src/content/posts/2025-01-31-openai-o3-mini.md","28d7a7e58d834cb6",{"html":25,"metadata":19606},{"headings":19607,"localImagePaths":19608,"remoteImagePaths":19609,"frontmatter":19610,"imagePaths":19611},[],[],[],{"title":19599,"description":19600,"summary":19600,"pubDate":19593,"source":19,"url":19602,"thumbnail":21},[],"2025-01-31-openai-o3-mini.md","2025-01-31-the-ai-tools-for-art-newsletter---issue-1",{"id":19613,"data":19615,"filePath":19621,"digest":19622,"rendered":19623,"legacyId":19631},{"title":19616,"description":25,"summary":19617,"pubDate":19618,"source":2720,"url":19619,"thumbnail":19620},"The AI tools for Art Newsletter - Issue 1","The AI tools for Art Newsletter First issue 🎉 The AI space is moving so fast it’s hard to believe th...",["Date","2025-01-31T00:00:00.000Z"],"https://huggingface.co/blog/ai-art-newsletter-jan-25","https://huggingface.co/blog/assets/ai_art_newsletter_1/thumbnail.png","src/content/posts/2025-01-31-the-ai-tools-for-art-newsletter---issue-1.md","aefec1c9c25660be",{"html":25,"metadata":19624},{"headings":19625,"localImagePaths":19626,"remoteImagePaths":19627,"frontmatter":19628,"imagePaths":19630},[],[],[],{"title":19616,"description":25,"summary":19617,"pubDate":19629,"source":2720,"url":19619,"thumbnail":19620},"Fri, 31 Jan 2025 00:00:00 GMT",[],"2025-01-31-the-ai-tools-for-art-newsletter---issue-1.md","2025-02-02-introducing-deep-research",{"id":19632,"data":19634,"filePath":19639,"digest":19640,"rendered":19641,"legacyId":19649},{"title":19635,"description":19636,"summary":19636,"pubDate":19637,"source":19,"url":19638,"thumbnail":21},"Introducing deep research","An agent that uses reasoning to synthesize large amounts of online information and complete multi-step research tasks for you. Available to Pro users today, Plus and Team next.",["Date","2025-02-02T16:00:00.000Z"],"https://openai.com/blog/introducing-deep-research","src/content/posts/2025-02-02-introducing-deep-research.md","5d3b505bfcd1cce2",{"html":25,"metadata":19642},{"headings":19643,"localImagePaths":19644,"remoteImagePaths":19645,"frontmatter":19646,"imagePaths":19648},[],[],[],{"title":19635,"description":19636,"summary":19636,"pubDate":19647,"source":19,"url":19638,"thumbnail":21},"Sun, 02 Feb 2025 16:00:00 GMT",[],"2025-02-02-introducing-deep-research.md","2025-02-02-understanding-complex-trends-with-deep-research",{"id":19650,"data":19652,"filePath":19657,"digest":19658,"rendered":19659,"legacyId":19666},{"title":19653,"description":19654,"summary":19654,"pubDate":19655,"source":19,"url":19656,"thumbnail":21},"Understanding complex trends with deep research","How OpenAI deep research helps Bain & Company understand complex industry trends.",["Date","2025-02-02T16:00:00.000Z"],"https://openai.com/blog/deep-research","src/content/posts/2025-02-02-understanding-complex-trends-with-deep-research.md","38209f65dd0ea2c2",{"html":25,"metadata":19660},{"headings":19661,"localImagePaths":19662,"remoteImagePaths":19663,"frontmatter":19664,"imagePaths":19665},[],[],[],{"title":19653,"description":19654,"summary":19654,"pubDate":19647,"source":19,"url":19656,"thumbnail":21},[],"2025-02-02-understanding-complex-trends-with-deep-research.md","2025-02-04-building-a-custom-math-tutor-powered-by-chatgpt",{"id":19667,"data":19669,"filePath":19674,"digest":19675,"rendered":19676,"legacyId":19684},{"title":19670,"description":19671,"summary":19671,"pubDate":19672,"source":19,"url":19673,"thumbnail":21},"Building a custom math tutor powered by ChatGPT","ChatGPT and personal tutoring",["Date","2025-02-04T00:00:00.000Z"],"https://openai.com/blog/my-dog-the-math-tutor","src/content/posts/2025-02-04-building-a-custom-math-tutor-powered-by-chatgpt.md","c70e2f0c9b66cbbc",{"html":25,"metadata":19677},{"headings":19678,"localImagePaths":19679,"remoteImagePaths":19680,"frontmatter":19681,"imagePaths":19683},[],[],[],{"title":19670,"description":19671,"summary":19671,"pubDate":19682,"source":19,"url":19673,"thumbnail":21},"Tue, 04 Feb 2025 00:00:00 GMT",[],"2025-02-04-building-a-custom-math-tutor-powered-by-chatgpt.md","2025-02-04-catching-halibut-with-chatgpt",{"id":19685,"data":19687,"filePath":19692,"digest":19693,"rendered":19694,"legacyId":19701},{"title":19688,"description":19689,"summary":19689,"pubDate":19690,"source":19,"url":19691,"thumbnail":21},"Catching halibut with ChatGPT","Using ChatGPT to catch halibut",["Date","2025-02-04T00:00:00.000Z"],"https://openai.com/blog/fishing-for-first-timers","src/content/posts/2025-02-04-catching-halibut-with-chatgpt.md","01958dca7b8c6602",{"html":25,"metadata":19695},{"headings":19696,"localImagePaths":19697,"remoteImagePaths":19698,"frontmatter":19699,"imagePaths":19700},[],[],[],{"title":19688,"description":19689,"summary":19689,"pubDate":19682,"source":19,"url":19691,"thumbnail":21},[],"2025-02-04-catching-halibut-with-chatgpt.md","2025-02-04-creating-nail-art-with-chatgpt",{"id":19702,"data":19704,"filePath":19709,"digest":19710,"rendered":19711,"legacyId":19718},{"title":19705,"description":19706,"summary":19706,"pubDate":19707,"source":19,"url":19708,"thumbnail":21},"Creating nail art with ChatGPT","Using ChatGPT to find inspiration for nail art",["Date","2025-02-04T00:00:00.000Z"],"https://openai.com/blog/ten-tiny-canvases","src/content/posts/2025-02-04-creating-nail-art-with-chatgpt.md","ac580ec51cbb3cc0",{"html":25,"metadata":19712},{"headings":19713,"localImagePaths":19714,"remoteImagePaths":19715,"frontmatter":19716,"imagePaths":19717},[],[],[],{"title":19705,"description":19706,"summary":19706,"pubDate":19682,"source":19,"url":19708,"thumbnail":21},[],"2025-02-04-creating-nail-art-with-chatgpt.md","2025-02-04-open-source-deepresearch-freeing-our-search-agents",{"id":19719,"data":19721,"filePath":19727,"digest":19728,"rendered":19729,"legacyId":19736},{"title":19722,"description":25,"summary":19723,"pubDate":19724,"source":2720,"url":19725,"thumbnail":19726},"Open-source DeepResearch – Freeing our search agents","Open-source DeepResearch – Freeing our search agents TLDR Yesterday, OpenAI released Deep Research, ...",["Date","2025-02-04T00:00:00.000Z"],"https://huggingface.co/blog/open-deep-research","https://huggingface.co/blog/assets/open-deep-research/thumbnail.png","src/content/posts/2025-02-04-open-source-deepresearch-freeing-our-search-agents.md","35e57893678980f7",{"html":25,"metadata":19730},{"headings":19731,"localImagePaths":19732,"remoteImagePaths":19733,"frontmatter":19734,"imagePaths":19735},[],[],[],{"title":19722,"description":25,"summary":19723,"pubDate":19682,"source":2720,"url":19725,"thumbnail":19726},[],"2025-02-04-open-source-deepresearch-freeing-our-search-agents.md","2025-02-04-openai-and-the-csu-system-bring-ai-to-500000-students-faculty",{"id":19737,"data":19739,"filePath":19744,"digest":19745,"rendered":19746,"legacyId":19754},{"title":19740,"description":19741,"summary":19741,"pubDate":19742,"source":19,"url":19743,"thumbnail":21},"OpenAI and the CSU system bring AI to 500,000 students & faculty","The largest deployment of ChatGPT to date will expand the use of AI in education and help the United States build an AI-ready workforce.",["Date","2025-02-04T11:30:00.000Z"],"https://openai.com/blog/openai-and-the-csu-system","src/content/posts/2025-02-04-openai-and-the-csu-system-bring-ai-to-500000-students-faculty.md","cc8762ef4ad6e06b",{"html":25,"metadata":19747},{"headings":19748,"localImagePaths":19749,"remoteImagePaths":19750,"frontmatter":19751,"imagePaths":19753},[],[],[],{"title":19740,"description":19741,"summary":19741,"pubDate":19752,"source":19,"url":19743,"thumbnail":21},"Tue, 04 Feb 2025 11:30:00 GMT",[],"2025-02-04-openai-and-the-csu-system-bring-ai-to-500000-students-faculty.md","2025-02-04-dabstep-data-agent-benchmark-for-multi-step-reasoning",{"id":19755,"data":19757,"filePath":19763,"digest":19764,"rendered":19765,"legacyId":19772},{"title":19758,"description":25,"summary":19759,"pubDate":19760,"source":2720,"url":19761,"thumbnail":19762},"DABStep: Data Agent Benchmark for Multi-step Reasoning","DABStep: Data Agent Benchmark for Multi-step Reasoning Language models are becoming increasingly cap...",["Date","2025-02-04T00:00:00.000Z"],"https://huggingface.co/blog/dabstep","https://huggingface.co/blog/assets/dabstep/thumbnail.png","src/content/posts/2025-02-04-dabstep-data-agent-benchmark-for-multi-step-reasoning.md","94f704e666e1b679",{"html":25,"metadata":19766},{"headings":19767,"localImagePaths":19768,"remoteImagePaths":19769,"frontmatter":19770,"imagePaths":19771},[],[],[],{"title":19758,"description":25,"summary":19759,"pubDate":19682,"source":2720,"url":19761,"thumbnail":19762},[],"2025-02-04-dabstep-data-agent-benchmark-for-multi-step-reasoning.md","2025-02-04-updating-the-frontier-safety-framework",{"id":19773,"data":19775,"filePath":19781,"digest":19782,"rendered":19783,"legacyId":19791},{"title":19776,"description":19777,"summary":19777,"pubDate":19778,"source":6423,"url":19779,"thumbnail":19780},"Updating the Frontier Safety Framework","Our next iteration of the FSF sets out stronger security protocols on the path to AGI",["Date","2025-02-04T16:41:00.000Z"],"https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/","https://lh3.googleusercontent.com/0fu18H8X3miSAuwcVJ7Zulis_LZAL7F4bIFU7FYFA2dGx3Rm3HHlm5N202B0dtKBuS7iI5SD1QgpFPuU-O3TPzb7iG1Ns-loZzinRB3M3X3W-MAgIQ=w1200-h630-n-nu","src/content/posts/2025-02-04-updating-the-frontier-safety-framework.md","481303d85e28bd88",{"html":25,"metadata":19784},{"headings":19785,"localImagePaths":19786,"remoteImagePaths":19787,"frontmatter":19788,"imagePaths":19790},[],[],[],{"title":19776,"description":19777,"summary":19777,"pubDate":19789,"source":6423,"url":19779,"thumbnail":19780},"Tue, 04 Feb 2025 16:41:00 +0000",[],"2025-02-04-updating-the-frontier-safety-framework.md","2025-02-04-π0-and-π0-fast-vision-language-action-models-for-general-robot-control",{"id":19792,"data":19794,"filePath":19800,"digest":19801,"rendered":19802,"legacyId":19809},{"title":19795,"description":25,"summary":19796,"pubDate":19797,"source":2720,"url":19798,"thumbnail":19799},"π0 and π0-FAST: Vision-Language-Action Models for General Robot Control","π0 and π0-FAST: Vision-Language-Action Models for General Robot Control We have ported the first rob...",["Date","2025-02-04T00:00:00.000Z"],"https://huggingface.co/blog/pi0","https://huggingface.co/blog/assets/192_pi0/new_thumbnail_pi0.001.png","src/content/posts/2025-02-04-π0-and-π0-fast-vision-language-action-models-for-general-robot-control.md","8a3b21ba0df1ce33",{"html":25,"metadata":19803},{"headings":19804,"localImagePaths":19805,"remoteImagePaths":19806,"frontmatter":19807,"imagePaths":19808},[],[],[],{"title":19795,"description":25,"summary":19796,"pubDate":19682,"source":2720,"url":19798,"thumbnail":19799},[],"2025-02-04-π0-and-π0-fast-vision-language-action-models-for-general-robot-control.md","2025-02-05-gemini-20-is-now-available-to-everyone",{"id":19810,"data":19812,"filePath":19818,"digest":19819,"rendered":19820,"legacyId":19828},{"title":19813,"description":19814,"summary":19814,"pubDate":19815,"source":6423,"url":19816,"thumbnail":19817},"Gemini 2.0 is now available to everyone","We’re announcing new updates to Gemini 2.0 Flash, plus introducing Gemini 2.0 Flash-Lite and Gemini 2.0 Pro Experimental.",["Date","2025-02-05T16:00:00.000Z"],"https://deepmind.google/discover/blog/gemini-2-0-is-now-available-to-everyone/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini_28.01.25_keyword_social.width-1300.png","src/content/posts/2025-02-05-gemini-20-is-now-available-to-everyone.md","2312361b288b9b43",{"html":25,"metadata":19821},{"headings":19822,"localImagePaths":19823,"remoteImagePaths":19824,"frontmatter":19825,"imagePaths":19827},[],[],[],{"title":19813,"description":19814,"summary":19814,"pubDate":19826,"source":6423,"url":19816,"thumbnail":19817},"Wed, 05 Feb 2025 16:00:00 +0000",[],"2025-02-05-gemini-20-is-now-available-to-everyone.md","2025-02-05-introducing-data-residency-in-europe",{"id":19829,"data":19831,"filePath":19836,"digest":19837,"rendered":19838,"legacyId":19846},{"title":19832,"description":19833,"summary":19833,"pubDate":19834,"source":19,"url":19835,"thumbnail":21},"Introducing data residency in Europe","Data residency builds on OpenAI’s enterprise-grade data privacy, security, and compliance programs supporting customers worldwide.",["Date","2025-02-05T22:00:00.000Z"],"https://openai.com/blog/introducing-data-residency-in-europe","src/content/posts/2025-02-05-introducing-data-residency-in-europe.md","836c477156cd4eec",{"html":25,"metadata":19839},{"headings":19840,"localImagePaths":19841,"remoteImagePaths":19842,"frontmatter":19843,"imagePaths":19845},[],[],[],{"title":19832,"description":19833,"summary":19833,"pubDate":19844,"source":19,"url":19835,"thumbnail":21},"Wed, 05 Feb 2025 22:00:00 GMT",[],"2025-02-05-introducing-data-residency-in-europe.md","2025-02-07-openai-at-the-paris-ai-action-summit",{"id":19847,"data":19849,"filePath":19854,"digest":19855,"rendered":19856,"legacyId":19864},{"title":19850,"description":19851,"summary":19851,"pubDate":19852,"source":19,"url":19853,"thumbnail":21},"OpenAI at the Paris AI Action Summit","OpenAI looks forward to engaging with global leaders on AI’s role in shaping innovation and economic prosperity.",["Date","2025-02-07T17:00:00.000Z"],"https://openai.com/global-affairs/openai-at-the-paris-ai-action-summit","src/content/posts/2025-02-07-openai-at-the-paris-ai-action-summit.md","a112cbc0740fc5e6",{"html":25,"metadata":19857},{"headings":19858,"localImagePaths":19859,"remoteImagePaths":19860,"frontmatter":19861,"imagePaths":19863},[],[],[],{"title":19850,"description":19851,"summary":19851,"pubDate":19862,"source":19,"url":19853,"thumbnail":21},"Fri, 07 Feb 2025 17:00:00 GMT",[],"2025-02-07-openai-at-the-paris-ai-action-summit.md","2025-02-09-introducing-the-intelligence-age",{"id":19865,"data":19867,"filePath":19872,"digest":19873,"rendered":19874,"legacyId":19882},{"title":19868,"description":19869,"summary":19869,"pubDate":19870,"source":19,"url":19871,"thumbnail":21},"Introducing the Intelligence Age","We aired our first-ever television ad during the Super Bowl to pique people’s curiosity and help us all realize how AI can open up new possibilities for us, create more fulfillment in our lives, and make us more productive, just as all the tools that came before AI did for those who came before us.",["Date","2025-02-09T22:00:00.000Z"],"https://openai.com/global-affairs/introducing-the-intelligence-age","src/content/posts/2025-02-09-introducing-the-intelligence-age.md","b539b12516895335",{"html":25,"metadata":19875},{"headings":19876,"localImagePaths":19877,"remoteImagePaths":19878,"frontmatter":19879,"imagePaths":19881},[],[],[],{"title":19868,"description":19869,"summary":19869,"pubDate":19880,"source":19,"url":19871,"thumbnail":21},"Sun, 09 Feb 2025 22:00:00 GMT",[],"2025-02-09-introducing-the-intelligence-age.md","2025-02-10-openai-partners-with-schibsted-media-group",{"id":19883,"data":19885,"filePath":19890,"digest":19891,"rendered":19892,"legacyId":19900},{"title":19886,"description":19887,"summary":19887,"pubDate":19888,"source":19,"url":19889,"thumbnail":21},"OpenAI partners with Schibsted Media Group","OpenAI and Schibsted Media Group announce content partnership to bring Guardian news and archive content to  ChatGPT.",["Date","2025-02-10T06:00:00.000Z"],"https://openai.com/blog/openai-partners-with-schibsted-media-group","src/content/posts/2025-02-10-openai-partners-with-schibsted-media-group.md","cd338c9258200327",{"html":25,"metadata":19893},{"headings":19894,"localImagePaths":19895,"remoteImagePaths":19896,"frontmatter":19897,"imagePaths":19899},[],[],[],{"title":19886,"description":19887,"summary":19887,"pubDate":19898,"source":19,"url":19889,"thumbnail":21},"Mon, 10 Feb 2025 06:00:00 GMT",[],"2025-02-10-openai-partners-with-schibsted-media-group.md","2025-02-10-the-open-arabic-llm-leaderboard-2",{"id":19901,"data":19903,"filePath":19908,"digest":19909,"rendered":19910,"legacyId":19918},{"title":19904,"description":25,"summary":19905,"pubDate":19906,"source":2720,"url":19907,"thumbnail":14303},"The Open Arabic LLM Leaderboard 2","The Open Arabic LLM Leaderboard 2 Current status of Arabic LLMs leaderboards The growing availabilit...",["Date","2025-02-10T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-arabic-v2","src/content/posts/2025-02-10-the-open-arabic-llm-leaderboard-2.md","0d822eb504290037",{"html":25,"metadata":19911},{"headings":19912,"localImagePaths":19913,"remoteImagePaths":19914,"frontmatter":19915,"imagePaths":19917},[],[],[],{"title":19904,"description":25,"summary":19905,"pubDate":19916,"source":2720,"url":19907,"thumbnail":14303},"Mon, 10 Feb 2025 00:00:00 GMT",[],"2025-02-10-the-open-arabic-llm-leaderboard-2.md","2025-02-12-build-awesome-datasets-for-video-generation",{"id":19919,"data":19921,"filePath":19927,"digest":19928,"rendered":19929,"legacyId":19937},{"title":19922,"description":25,"summary":19923,"pubDate":19924,"source":2720,"url":19925,"thumbnail":19926},"Build awesome datasets for video generation","Build awesome datasets for video generation Tooling for image generation datasets is well establishe...",["Date","2025-02-12T00:00:00.000Z"],"https://huggingface.co/blog/vid_ds_scripts","https://huggingface.co/blog/assets/vid_ds_scripts/thumbnail.png","src/content/posts/2025-02-12-build-awesome-datasets-for-video-generation.md","59f9cd56e70c5677",{"html":25,"metadata":19930},{"headings":19931,"localImagePaths":19932,"remoteImagePaths":19933,"frontmatter":19934,"imagePaths":19936},[],[],[],{"title":19922,"description":25,"summary":19923,"pubDate":19935,"source":2720,"url":19925,"thumbnail":19926},"Wed, 12 Feb 2025 00:00:00 GMT",[],"2025-02-12-build-awesome-datasets-for-video-generation.md","2025-02-12-from-chunks-to-blocks-accelerating-uploads-and-downloads-on-the-hub",{"id":19938,"data":19940,"filePath":19946,"digest":19947,"rendered":19948,"legacyId":19955},{"title":19941,"description":25,"summary":19942,"pubDate":19943,"source":2720,"url":19944,"thumbnail":19945},"From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub","From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub Content-defined chunking (CDC) ...",["Date","2025-02-12T00:00:00.000Z"],"https://huggingface.co/blog/from-chunks-to-blocks","https://huggingface.co/blog/assets/from-chunks-to-blocks/thumbnail.png","src/content/posts/2025-02-12-from-chunks-to-blocks-accelerating-uploads-and-downloads-on-the-hub.md","9624628c42750bb1",{"html":25,"metadata":19949},{"headings":19950,"localImagePaths":19951,"remoteImagePaths":19952,"frontmatter":19953,"imagePaths":19954},[],[],[],{"title":19941,"description":25,"summary":19942,"pubDate":19935,"source":2720,"url":19944,"thumbnail":19945},[],"2025-02-12-from-chunks-to-blocks-accelerating-uploads-and-downloads-on-the-hub.md","2025-02-12-sharing-the-latest-model-spec",{"id":19956,"data":19958,"filePath":19963,"digest":19964,"rendered":19965,"legacyId":19973},{"title":19959,"description":19960,"summary":19960,"pubDate":19961,"source":19,"url":19962,"thumbnail":21},"Sharing the latest Model Spec","We’ve made updates to the Model Spec based on external feedback and our continued research in shaping desired model behavior.",["Date","2025-02-12T13:00:00.000Z"],"https://openai.com/blog/sharing-the-latest-model-spec","src/content/posts/2025-02-12-sharing-the-latest-model-spec.md","9a57f0b8605f91e0",{"html":25,"metadata":19966},{"headings":19967,"localImagePaths":19968,"remoteImagePaths":19969,"frontmatter":19970,"imagePaths":19972},[],[],[],{"title":19959,"description":19960,"summary":19960,"pubDate":19971,"source":19,"url":19962,"thumbnail":21},"Wed, 12 Feb 2025 13:00:00 GMT",[],"2025-02-12-sharing-the-latest-model-spec.md","2025-02-13-1-billion-classifications",{"id":19974,"data":19976,"filePath":19982,"digest":19983,"rendered":19984,"legacyId":19992},{"title":19977,"description":25,"summary":19978,"pubDate":19979,"source":2720,"url":19980,"thumbnail":19981},"1 Billion Classifications","1 Billion Classifications You’ve optimized your model. Your pipeline is running smoothly. But now, y...",["Date","2025-02-13T00:00:00.000Z"],"https://huggingface.co/blog/billion-classifications","https://huggingface.co/blog/assets/billion-classifications/billion-classifications-thumbnail.png","src/content/posts/2025-02-13-1-billion-classifications.md","59371a708408cc09",{"html":25,"metadata":19985},{"headings":19986,"localImagePaths":19987,"remoteImagePaths":19988,"frontmatter":19989,"imagePaths":19991},[],[],[],{"title":19977,"description":25,"summary":19978,"pubDate":19990,"source":2720,"url":19980,"thumbnail":19981},"Thu, 13 Feb 2025 00:00:00 GMT",[],"2025-02-13-1-billion-classifications.md","2025-02-13-using-openai-o1-for-financial-analysis",{"id":19993,"data":19995,"filePath":20000,"digest":20001,"rendered":20002,"legacyId":20010},{"title":19996,"description":19997,"summary":19997,"pubDate":19998,"source":19,"url":19999,"thumbnail":21},"Using OpenAI o1 for financial analysis","Rogo scales AI-driven financial research with OpenAI o1",["Date","2025-02-13T07:00:00.000Z"],"https://openai.com/blog/rogo","src/content/posts/2025-02-13-using-openai-o1-for-financial-analysis.md","f92ca7408c14cab2",{"html":25,"metadata":20003},{"headings":20004,"localImagePaths":20005,"remoteImagePaths":20006,"frontmatter":20007,"imagePaths":20009},[],[],[],{"title":19996,"description":19997,"summary":19997,"pubDate":20008,"source":19,"url":19999,"thumbnail":21},"Thu, 13 Feb 2025 07:00:00 GMT",[],"2025-02-13-using-openai-o1-for-financial-analysis.md","2025-02-13-fanatics-betting-and-gaming-uses-ai-to-focus-on-the-big-picture",{"id":20011,"data":20013,"filePath":20018,"digest":20019,"rendered":20020,"legacyId":20028},{"title":20014,"description":20015,"summary":20015,"pubDate":20016,"source":19,"url":20017,"thumbnail":21},"Fanatics Betting and Gaming uses AI to focus on the big picture","A conversation with Andrea Ellis, Chief Financial Officer of Fanatics Betting and Gaming.",["Date","2025-02-13T10:01:00.000Z"],"https://openai.com/blog/fanatics-betting-gaming-andrea-ellis","src/content/posts/2025-02-13-fanatics-betting-and-gaming-uses-ai-to-focus-on-the-big-picture.md","3b0cd88ba20026bc",{"html":25,"metadata":20021},{"headings":20022,"localImagePaths":20023,"remoteImagePaths":20024,"frontmatter":20025,"imagePaths":20027},[],[],[],{"title":20014,"description":20015,"summary":20015,"pubDate":20026,"source":19,"url":20017,"thumbnail":21},"Thu, 13 Feb 2025 10:01:00 GMT",[],"2025-02-13-fanatics-betting-and-gaming-uses-ai-to-focus-on-the-big-picture.md","2025-02-13-wayfair-is-shaping-the-future-of-retail-with-ai",{"id":20029,"data":20031,"filePath":20036,"digest":20037,"rendered":20038,"legacyId":20046},{"title":20032,"description":20033,"summary":20033,"pubDate":20034,"source":19,"url":20035,"thumbnail":21},"Wayfair is shaping the future of retail with AI","A conversation with Fiona Tan, Chief Technology Officer of Wayfair.",["Date","2025-02-13T10:00:00.000Z"],"https://openai.com/blog/wayfair-fiona-tan","src/content/posts/2025-02-13-wayfair-is-shaping-the-future-of-retail-with-ai.md","bee2cd1ee6fea6c5",{"html":25,"metadata":20039},{"headings":20040,"localImagePaths":20041,"remoteImagePaths":20042,"frontmatter":20043,"imagePaths":20045},[],[],[],{"title":20032,"description":20033,"summary":20033,"pubDate":20044,"source":19,"url":20035,"thumbnail":21},"Thu, 13 Feb 2025 10:00:00 GMT",[],"2025-02-13-wayfair-is-shaping-the-future-of-retail-with-ai.md","2025-02-14-fixing-open-llm-leaderboard-with-math-verify",{"id":20047,"data":20049,"filePath":20055,"digest":20056,"rendered":20057,"legacyId":20065},{"title":20050,"description":25,"summary":20051,"pubDate":20052,"source":2720,"url":20053,"thumbnail":20054},"Fixing Open LLM Leaderboard with Math-Verify","Fixing Open LLM Leaderboard with Math-Verify 3 weeks ago, we showed how hard it is to correctly eval...",["Date","2025-02-14T00:00:00.000Z"],"https://huggingface.co/blog/math_verify_leaderboard","https://huggingface.co/blog/assets/math_verify_leaderboard/thumbnail.png","src/content/posts/2025-02-14-fixing-open-llm-leaderboard-with-math-verify.md","d2ea5c8e303d234c",{"html":25,"metadata":20058},{"headings":20059,"localImagePaths":20060,"remoteImagePaths":20061,"frontmatter":20062,"imagePaths":20064},[],[],[],{"title":20050,"description":25,"summary":20051,"pubDate":20063,"source":2720,"url":20053,"thumbnail":20054},"Fri, 14 Feb 2025 00:00:00 GMT",[],"2025-02-14-fixing-open-llm-leaderboard-with-math-verify.md","2025-02-14-openai-and-guardian-media-group-launch-content-partnership",{"id":20066,"data":20068,"filePath":20073,"digest":20074,"rendered":20075,"legacyId":20083},{"title":20069,"description":20070,"summary":20070,"pubDate":20071,"source":19,"url":20072,"thumbnail":21},"OpenAI and Guardian Media Group launch content partnership","OpenAI and Guardian Media Group announce content partnership to bring Guardian news content to ChatGPT.",["Date","2025-02-14T07:00:00.000Z"],"https://openai.com/blog/openai-and-guardian-media-group-launch-content-partnership","src/content/posts/2025-02-14-openai-and-guardian-media-group-launch-content-partnership.md","3b71e9bcf1bc6542",{"html":25,"metadata":20076},{"headings":20077,"localImagePaths":20078,"remoteImagePaths":20079,"frontmatter":20080,"imagePaths":20082},[],[],[],{"title":20069,"description":20070,"summary":20070,"pubDate":20081,"source":19,"url":20072,"thumbnail":21},"Fri, 14 Feb 2025 07:00:00 GMT",[],"2025-02-14-openai-and-guardian-media-group-launch-content-partnership.md","2025-02-14-welcome-fireworksai-on-the-hub",{"id":20084,"data":20086,"filePath":20092,"digest":20093,"rendered":20094,"legacyId":20101},{"title":20087,"description":25,"summary":20088,"pubDate":20089,"source":2720,"url":20090,"thumbnail":20091},"Welcome Fireworks.ai on the Hub 🎆","Welcome Fireworks.ai on the Hub 🎆 Following our recent announcement on Inference Providers on the Hu...",["Date","2025-02-14T00:00:00.000Z"],"https://huggingface.co/blog/fireworks-ai","https://huggingface.co/blog/assets/inference-providers/welcome-fireworks.jpg","src/content/posts/2025-02-14-welcome-fireworksai-on-the-hub.md","df8050190e71d0ca",{"html":25,"metadata":20095},{"headings":20096,"localImagePaths":20097,"remoteImagePaths":20098,"frontmatter":20099,"imagePaths":20100},[],[],[],{"title":20087,"description":25,"summary":20088,"pubDate":20063,"source":2720,"url":20090,"thumbnail":20091},[],"2025-02-14-welcome-fireworksai-on-the-hub.md","2025-02-18-introducing-the-swe-lancer-benchmark",{"id":20102,"data":20104,"filePath":20109,"digest":20110,"rendered":20111,"legacyId":20119},{"title":20105,"description":20106,"summary":20106,"pubDate":20107,"source":19,"url":20108,"thumbnail":21},"Introducing the SWE-Lancer benchmark","Can frontier LLMs earn $1 million from real-world freelance software engineering?",["Date","2025-02-18T10:00:00.000Z"],"https://openai.com/blog/swe-lancer","src/content/posts/2025-02-18-introducing-the-swe-lancer-benchmark.md","d47d093dccd533ee",{"html":25,"metadata":20112},{"headings":20113,"localImagePaths":20114,"remoteImagePaths":20115,"frontmatter":20116,"imagePaths":20118},[],[],[],{"title":20105,"description":20106,"summary":20106,"pubDate":20117,"source":19,"url":20108,"thumbnail":21},"Tue, 18 Feb 2025 10:00:00 GMT",[],"2025-02-18-introducing-the-swe-lancer-benchmark.md","2025-02-18-introducing-three-new-serverless-inference-providers-hyperbolic-nebius-ai-studio-and-novita",{"id":20120,"data":20122,"filePath":20128,"digest":20129,"rendered":20130,"legacyId":20138},{"title":20123,"description":25,"summary":20124,"pubDate":20125,"source":2720,"url":20126,"thumbnail":20127},"Introducing Three New Serverless Inference Providers: Hyperbolic, Nebius AI Studio, and Novita 🔥","Introducing Three New Serverless Inference Providers: Hyperbolic, Nebius AI Studio, and Novita 🔥 We’...",["Date","2025-02-18T00:00:00.000Z"],"https://huggingface.co/blog/inference-providers-nebius-novita-hyperbolic","https://huggingface.co/blog/assets/inference-providers/second-batch-thumbnail.webp","src/content/posts/2025-02-18-introducing-three-new-serverless-inference-providers-hyperbolic-nebius-ai-studio-and-novita.md","1c52c159f70a18d0",{"html":25,"metadata":20131},{"headings":20132,"localImagePaths":20133,"remoteImagePaths":20134,"frontmatter":20135,"imagePaths":20137},[],[],[],{"title":20123,"description":25,"summary":20124,"pubDate":20136,"source":2720,"url":20126,"thumbnail":20127},"Tue, 18 Feb 2025 00:00:00 GMT",[],"2025-02-18-introducing-three-new-serverless-inference-providers-hyperbolic-nebius-ai-studio-and-novita.md","2025-02-19-paligemma-2-mix---new-instruction-vision-language-models-by-google",{"id":20139,"data":20141,"filePath":20147,"digest":20148,"rendered":20149,"legacyId":20157},{"title":20142,"description":25,"summary":20143,"pubDate":20144,"source":2720,"url":20145,"thumbnail":20146},"PaliGemma 2 Mix - New Instruction Vision Language Models by Google","PaliGemma 2 Mix - New Instruction Vision Language Models by Google TL;DR Last December, Google relea...",["Date","2025-02-19T00:00:00.000Z"],"https://huggingface.co/blog/paligemma2mix","https://huggingface.co/blog/assets/paligemma2/thumbnail.png","src/content/posts/2025-02-19-paligemma-2-mix---new-instruction-vision-language-models-by-google.md","732cd97c7adb6dd2",{"html":25,"metadata":20150},{"headings":20151,"localImagePaths":20152,"remoteImagePaths":20153,"frontmatter":20154,"imagePaths":20156},[],[],[],{"title":20142,"description":25,"summary":20143,"pubDate":20155,"source":2720,"url":20145,"thumbnail":20146},"Wed, 19 Feb 2025 00:00:00 GMT",[],"2025-02-19-paligemma-2-mix---new-instruction-vision-language-models-by-google.md","2025-02-20-college-students-and-chatgpt-adoption-in-the-us",{"id":20158,"data":20160,"filePath":20165,"digest":20166,"rendered":20167,"legacyId":20175},{"title":20161,"description":20162,"summary":20162,"pubDate":20163,"source":19,"url":20164,"thumbnail":21},"College students and ChatGPT adoption in the US","A look into state-by-state adoption and how gaps might impact workforce readiness.",["Date","2025-02-20T06:00:00.000Z"],"https://openai.com/global-affairs/college-students-and-chatgpt","src/content/posts/2025-02-20-college-students-and-chatgpt-adoption-in-the-us.md","51b3b96406bb280a",{"html":25,"metadata":20168},{"headings":20169,"localImagePaths":20170,"remoteImagePaths":20171,"frontmatter":20172,"imagePaths":20174},[],[],[],{"title":20161,"description":20162,"summary":20162,"pubDate":20173,"source":19,"url":20164,"thumbnail":21},"Thu, 20 Feb 2025 06:00:00 GMT",[],"2025-02-20-college-students-and-chatgpt-adoption-in-the-us.md","2025-02-20-smolvlm2-bringing-video-understanding-to-every-device",{"id":20176,"data":20178,"filePath":20184,"digest":20185,"rendered":20186,"legacyId":20194},{"title":20179,"description":25,"summary":20180,"pubDate":20181,"source":2720,"url":20182,"thumbnail":20183},"SmolVLM2: Bringing Video Understanding to Every Device","SmolVLM2: Bringing Video Understanding to Every Device TL;DR: SmolVLM can now watch 📺 with even bett...",["Date","2025-02-20T00:00:00.000Z"],"https://huggingface.co/blog/smolvlm2","https://huggingface.co/blog/assets/smolvlm2/banner.png","src/content/posts/2025-02-20-smolvlm2-bringing-video-understanding-to-every-device.md","a65ac300813f8dec",{"html":25,"metadata":20187},{"headings":20188,"localImagePaths":20189,"remoteImagePaths":20190,"frontmatter":20191,"imagePaths":20193},[],[],[],{"title":20179,"description":25,"summary":20180,"pubDate":20192,"source":2720,"url":20182,"thumbnail":20183},"Thu, 20 Feb 2025 00:00:00 GMT",[],"2025-02-20-smolvlm2-bringing-video-understanding-to-every-device.md","2025-02-21-disrupting-malicious-uses-of-ai",{"id":20195,"data":20197,"filePath":20202,"digest":20203,"rendered":20204,"legacyId":20212},{"title":20198,"description":20199,"summary":20199,"pubDate":20200,"source":19,"url":20201,"thumbnail":21},"Disrupting malicious uses of AI","Ensuring AI benefits humanity by advancing democratic AI, preventing misuse, and protecting against authoritarian threats.",["Date","2025-02-21T06:30:00.000Z"],"https://openai.com/global-affairs/disrupting-malicious-uses-of-ai","src/content/posts/2025-02-21-disrupting-malicious-uses-of-ai.md","bfc482eddb1ec074",{"html":25,"metadata":20205},{"headings":20206,"localImagePaths":20207,"remoteImagePaths":20208,"frontmatter":20209,"imagePaths":20211},[],[],[],{"title":20198,"description":20199,"summary":20199,"pubDate":20210,"source":19,"url":20201,"thumbnail":21},"Fri, 21 Feb 2025 06:30:00 GMT",[],"2025-02-21-disrupting-malicious-uses-of-ai.md","2025-02-20-uber-enables-outstanding-on-demand-experiences-with-ai",{"id":20213,"data":20215,"filePath":20220,"digest":20221,"rendered":20222,"legacyId":20230},{"title":20216,"description":20217,"summary":20217,"pubDate":20218,"source":19,"url":20219,"thumbnail":21},"Uber enables outstanding on-demand experiences with AI","A conversation with Jai Malkani, Head of AI and Product, Customer Obsession at Uber.",["Date","2025-02-20T10:00:00.000Z"],"https://openai.com/blog/uber-enables-outstanding-experiences","src/content/posts/2025-02-20-uber-enables-outstanding-on-demand-experiences-with-ai.md","51a08cf3c46d38f8",{"html":25,"metadata":20223},{"headings":20224,"localImagePaths":20225,"remoteImagePaths":20226,"frontmatter":20227,"imagePaths":20229},[],[],[],{"title":20216,"description":20217,"summary":20217,"pubDate":20228,"source":19,"url":20219,"thumbnail":21},"Thu, 20 Feb 2025 10:00:00 GMT",[],"2025-02-20-uber-enables-outstanding-on-demand-experiences-with-ai.md","2025-02-21-siglip-2-a-better-multilingual-vision-language-encoder",{"id":20231,"data":20233,"filePath":20239,"digest":20240,"rendered":20241,"legacyId":20249},{"title":20234,"description":25,"summary":20235,"pubDate":20236,"source":2720,"url":20237,"thumbnail":20238},"SigLIP 2: A better multilingual vision language encoder","SigLIP 2: A better multilingual vision language encoder TL;DR Today Google releases a new and better...",["Date","2025-02-21T00:00:00.000Z"],"https://huggingface.co/blog/siglip2","https://huggingface.co/blog/assets/siglip2/thumbnail.png","src/content/posts/2025-02-21-siglip-2-a-better-multilingual-vision-language-encoder.md","85d092bbaa4d4814",{"html":25,"metadata":20242},{"headings":20243,"localImagePaths":20244,"remoteImagePaths":20245,"frontmatter":20246,"imagePaths":20248},[],[],[],{"title":20234,"description":25,"summary":20235,"pubDate":20247,"source":2720,"url":20237,"thumbnail":20238},"Fri, 21 Feb 2025 00:00:00 GMT",[],"2025-02-21-siglip-2-a-better-multilingual-vision-language-encoder.md","2025-02-24-remote-vaes-for-decoding-with-hf-endpoints",{"id":20250,"data":20252,"filePath":20258,"digest":20259,"rendered":20260,"legacyId":20268},{"title":20253,"description":25,"summary":20254,"pubDate":20255,"source":2720,"url":20256,"thumbnail":20257},"Remote VAEs for decoding with HF endpoints 🤗","Remote VAEs for decoding with Inference Endpoints 🤗 When operating with latent-space diffusion model...",["Date","2025-02-24T00:00:00.000Z"],"https://huggingface.co/blog/remote_vae","https://huggingface.co/blog/assets/remote_vae/thumbnail.png","src/content/posts/2025-02-24-remote-vaes-for-decoding-with-hf-endpoints.md","a4a5ac74244d414a",{"html":25,"metadata":20261},{"headings":20262,"localImagePaths":20263,"remoteImagePaths":20264,"frontmatter":20265,"imagePaths":20267},[],[],[],{"title":20253,"description":25,"summary":20254,"pubDate":20266,"source":2720,"url":20256,"thumbnail":20257},"Mon, 24 Feb 2025 00:00:00 GMT",[],"2025-02-24-remote-vaes-for-decoding-with-hf-endpoints.md","2025-02-25-deep-research-system-card",{"id":20269,"data":20271,"filePath":20276,"digest":20277,"rendered":20278,"legacyId":20286},{"title":20272,"description":20273,"summary":20273,"pubDate":20274,"source":19,"url":20275,"thumbnail":21},"Deep research System Card","This report outlines the safety work carried out prior to releasing deep research including external red teaming, frontier risk evaluations according to our Preparedness Framework, and an overview of the mitigations we built in to address key risk areas.",["Date","2025-02-25T10:00:00.000Z"],"https://openai.com/blog/deep-research-system-card","src/content/posts/2025-02-25-deep-research-system-card.md","df35cdb19ce0f61f",{"html":25,"metadata":20279},{"headings":20280,"localImagePaths":20281,"remoteImagePaths":20282,"frontmatter":20283,"imagePaths":20285},[],[],[],{"title":20272,"description":20273,"summary":20273,"pubDate":20284,"source":19,"url":20275,"thumbnail":21},"Tue, 25 Feb 2025 10:00:00 GMT",[],"2025-02-25-deep-research-system-card.md","2025-02-25-estonia-and-openai-to-bring-chatgpt-to-schools-nationwide",{"id":20287,"data":20289,"filePath":20294,"digest":20295,"rendered":20296,"legacyId":20304},{"title":20290,"description":20291,"summary":20291,"pubDate":20292,"source":19,"url":20293,"thumbnail":21},"Estonia and OpenAI to bring ChatGPT to schools nationwide","Estonia and OpenAI to bring ChatGPT to schools nationwide. OpenAI will work with the Estonian Government to provide students and teachers in the secondary school system with access to ChatGPT Edu.",["Date","2025-02-25T04:15:00.000Z"],"https://openai.com/blog/estonia-schools-and-chatgpt","src/content/posts/2025-02-25-estonia-and-openai-to-bring-chatgpt-to-schools-nationwide.md","d5449e3956a27bbf",{"html":25,"metadata":20297},{"headings":20298,"localImagePaths":20299,"remoteImagePaths":20300,"frontmatter":20301,"imagePaths":20303},[],[],[],{"title":20290,"description":20291,"summary":20291,"pubDate":20302,"source":19,"url":20293,"thumbnail":21},"Tue, 25 Feb 2025 04:15:00 GMT",[],"2025-02-25-estonia-and-openai-to-bring-chatgpt-to-schools-nationwide.md","2025-02-25-fastrtc-the-real-time-communication-library-for-python",{"id":20305,"data":20307,"filePath":20313,"digest":20314,"rendered":20315,"legacyId":20323},{"title":20308,"description":25,"summary":20309,"pubDate":20310,"source":2720,"url":20311,"thumbnail":20312},"FastRTC: The Real-Time Communication Library for Python","FastRTC: The Real-Time Communication Library for Python In the last few months, many new real-time s...",["Date","2025-02-25T00:00:00.000Z"],"https://huggingface.co/blog/fastrtc","https://huggingface.co/blog/assets/fastrtc/fastrtc_logo.jpg","src/content/posts/2025-02-25-fastrtc-the-real-time-communication-library-for-python.md","51de85f0959f9cf5",{"html":25,"metadata":20316},{"headings":20317,"localImagePaths":20318,"remoteImagePaths":20319,"frontmatter":20320,"imagePaths":20322},[],[],[],{"title":20308,"description":25,"summary":20309,"pubDate":20321,"source":2720,"url":20311,"thumbnail":20312},"Tue, 25 Feb 2025 00:00:00 GMT",[],"2025-02-25-fastrtc-the-real-time-communication-library-for-python.md","2025-02-25-start-building-with-gemini-20-flash-and-flash-lite",{"id":20324,"data":20326,"filePath":20332,"digest":20333,"rendered":20334,"legacyId":20342},{"title":20327,"description":20328,"summary":20328,"pubDate":20329,"source":6423,"url":20330,"thumbnail":20331},"Start building with Gemini 2.0 Flash and Flash-Lite","Gemini 2.0 Flash-Lite is now generally available in the Gemini API for production use in Google AI Studio and for enterprise customers on Vertex AI",["Date","2025-02-25T18:02:12.000Z"],"https://deepmind.google/discover/blog/start-building-with-gemini-20-flash-and-flash-lite/","https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Flash_Family_meta.2e16d0ba.fill-1200x600.png","src/content/posts/2025-02-25-start-building-with-gemini-20-flash-and-flash-lite.md","3cee4289b6f9b100",{"html":25,"metadata":20335},{"headings":20336,"localImagePaths":20337,"remoteImagePaths":20338,"frontmatter":20339,"imagePaths":20341},[],[],[],{"title":20327,"description":20328,"summary":20328,"pubDate":20340,"source":6423,"url":20330,"thumbnail":20331},"Tue, 25 Feb 2025 18:02:12 +0000",[],"2025-02-25-start-building-with-gemini-20-flash-and-flash-lite.md","2025-02-27-building-an-autonomous-financial-analyst-with-o1-and-o3-mini",{"id":20343,"data":20345,"filePath":20350,"digest":20351,"rendered":20352,"legacyId":20360},{"title":20346,"description":20347,"summary":20347,"pubDate":20348,"source":19,"url":20349,"thumbnail":21},"Building an autonomous financial analyst with o1 and o3-mini","Endex builds the future of financial analysis, powered by OpenAI’s reasoning models.",["Date","2025-02-27T09:30:00.000Z"],"https://openai.com/blog/endex","src/content/posts/2025-02-27-building-an-autonomous-financial-analyst-with-o1-and-o3-mini.md","b8c650f00cf24362",{"html":25,"metadata":20353},{"headings":20354,"localImagePaths":20355,"remoteImagePaths":20356,"frontmatter":20357,"imagePaths":20359},[],[],[],{"title":20346,"description":20347,"summary":20347,"pubDate":20358,"source":19,"url":20349,"thumbnail":21},"Thu, 27 Feb 2025 09:30:00 GMT",[],"2025-02-27-building-an-autonomous-financial-analyst-with-o1-and-o3-mini.md","2025-02-27-huggingface-iisc-partner-to-supercharge-model-building-on-indias-diverse-languages",{"id":20361,"data":20363,"filePath":20369,"digest":20370,"rendered":20371,"legacyId":20379},{"title":20364,"description":25,"summary":20365,"pubDate":20366,"source":2720,"url":20367,"thumbnail":20368},"HuggingFace, IISc partner to supercharge model building on India's diverse languages","HuggingFace, IISc partner to supercharge model building on India's diverse languages The Indian Inst...",["Date","2025-02-27T00:00:00.000Z"],"https://huggingface.co/blog/iisc-huggingface-collab","https://huggingface.co/blog/assets/iisc-huggingface-collab/thumbnail.png","src/content/posts/2025-02-27-huggingface-iisc-partner-to-supercharge-model-building-on-indias-diverse-languages.md","950e760339ffd292",{"html":25,"metadata":20372},{"headings":20373,"localImagePaths":20374,"remoteImagePaths":20375,"frontmatter":20376,"imagePaths":20378},[],[],[],{"title":20364,"description":25,"summary":20365,"pubDate":20377,"source":2720,"url":20367,"thumbnail":20368},"Thu, 27 Feb 2025 00:00:00 GMT",[],"2025-02-27-huggingface-iisc-partner-to-supercharge-model-building-on-indias-diverse-languages.md","2025-02-27-openai-gpt-45-system-card",{"id":20380,"data":20382,"filePath":20387,"digest":20388,"rendered":20389,"legacyId":20397},{"title":20383,"description":20384,"summary":20384,"pubDate":20385,"source":19,"url":20386,"thumbnail":21},"OpenAI GPT-4.5 System Card","We’re releasing a research preview of OpenAI GPT‑4.5, our largest and most knowledgeable model yet.",["Date","2025-02-27T12:00:00.000Z"],"https://openai.com/blog/gpt-4-5-system-card","src/content/posts/2025-02-27-openai-gpt-45-system-card.md","784f8209436b0cac",{"html":25,"metadata":20390},{"headings":20391,"localImagePaths":20392,"remoteImagePaths":20393,"frontmatter":20394,"imagePaths":20396},[],[],[],{"title":20383,"description":20384,"summary":20384,"pubDate":20395,"source":19,"url":20386,"thumbnail":21},"Thu, 27 Feb 2025 12:00:00 GMT",[],"2025-02-27-openai-gpt-45-system-card.md","2025-02-27-supporting-sellers-with-enhanced-product-listings",{"id":20398,"data":20400,"filePath":20405,"digest":20406,"rendered":20407,"legacyId":20415},{"title":20401,"description":20402,"summary":20402,"pubDate":20403,"source":19,"url":20404,"thumbnail":21},"Supporting sellers with enhanced product listings","Mercari leverages GPT-4o mini and GPT-4 to streamline selling, enhance product listings, and boost sales, transforming the online marketplace with features like AI Listing Support and Mercari AI Assistant.",["Date","2025-02-27T14:00:00.000Z"],"https://openai.com/blog/mercari","src/content/posts/2025-02-27-supporting-sellers-with-enhanced-product-listings.md","ecfdd64006f6ca90",{"html":25,"metadata":20408},{"headings":20409,"localImagePaths":20410,"remoteImagePaths":20411,"frontmatter":20412,"imagePaths":20414},[],[],[],{"title":20401,"description":20402,"summary":20402,"pubDate":20413,"source":19,"url":20404,"thumbnail":21},"Thu, 27 Feb 2025 14:00:00 GMT",[],"2025-02-27-supporting-sellers-with-enhanced-product-listings.md","2025-02-27-introducing-gpt-45",{"id":20416,"data":20418,"filePath":20423,"digest":20424,"rendered":20425,"legacyId":20433},{"title":20419,"description":20420,"summary":20420,"pubDate":20421,"source":19,"url":20422,"thumbnail":21},"Introducing GPT-4.5","We’re releasing a research preview of GPT‑4.5—our largest and best model for chat yet. GPT‑4.5 is a step forward in scaling up pre-training and post-training.",["Date","2025-02-27T10:00:00.000Z"],"https://openai.com/blog/introducing-gpt-4-5","src/content/posts/2025-02-27-introducing-gpt-45.md","2043f5f5ef25de66",{"html":25,"metadata":20426},{"headings":20427,"localImagePaths":20428,"remoteImagePaths":20429,"frontmatter":20430,"imagePaths":20432},[],[],[],{"title":20419,"description":20420,"summary":20420,"pubDate":20431,"source":19,"url":20422,"thumbnail":21},"Thu, 27 Feb 2025 10:00:00 GMT",[],"2025-02-27-introducing-gpt-45.md","2025-02-28-1000-scientist-ai-jam-session",{"id":20434,"data":20436,"filePath":20441,"digest":20442,"rendered":20443,"legacyId":20451},{"title":20437,"description":20438,"summary":20438,"pubDate":20439,"source":19,"url":20440,"thumbnail":21},"1,000 Scientist AI Jam Session","OpenAI and nine national labs bring together leading scientists for first-of-its kind event.",["Date","2025-02-28T08:00:00.000Z"],"https://openai.com/global-affairs/1000-scientist-ai-jam-session","src/content/posts/2025-02-28-1000-scientist-ai-jam-session.md","0b29dcf62e1c2b90",{"html":25,"metadata":20444},{"headings":20445,"localImagePaths":20446,"remoteImagePaths":20447,"frontmatter":20448,"imagePaths":20450},[],[],[],{"title":20437,"description":20438,"summary":20438,"pubDate":20449,"source":19,"url":20440,"thumbnail":21},"Fri, 28 Feb 2025 08:00:00 GMT",[],"2025-02-28-1000-scientist-ai-jam-session.md","2025-02-28-trace-evaluate-your-agent-with-arize-phoenix",{"id":20452,"data":20454,"filePath":20460,"digest":20461,"rendered":20462,"legacyId":20470},{"title":20455,"description":25,"summary":20456,"pubDate":20457,"source":2720,"url":20458,"thumbnail":20459},"Trace & Evaluate your Agent with Arize Phoenix","Trace & Evaluate your Agent with Arize Phoenix So, you’ve built your agent. It takes in inputs and t...",["Date","2025-02-28T00:00:00.000Z"],"https://huggingface.co/blog/smolagents-phoenix","https://huggingface.co/blog/assets/smolagents-phoenix/thumbnail.jpg","src/content/posts/2025-02-28-trace-evaluate-your-agent-with-arize-phoenix.md","6aa776656fe84da7",{"html":25,"metadata":20463},{"headings":20464,"localImagePaths":20465,"remoteImagePaths":20466,"frontmatter":20467,"imagePaths":20469},[],[],[],{"title":20455,"description":25,"summary":20456,"pubDate":20468,"source":2720,"url":20458,"thumbnail":20459},"Fri, 28 Feb 2025 00:00:00 GMT",[],"2025-02-28-trace-evaluate-your-agent-with-arize-phoenix.md","2025-03-04-hugging-face-and-jfrog-partner-to-make-ai-security-more-transparent",{"id":20471,"data":20473,"filePath":20479,"digest":20480,"rendered":20481,"legacyId":20489},{"title":20474,"description":25,"summary":20475,"pubDate":20476,"source":2720,"url":20477,"thumbnail":20478},"Hugging Face and JFrog partner to make AI Security more transparent","Hugging Face and JFrog partner to make AI Security more transparent We are pleased to announce our p...",["Date","2025-03-04T00:00:00.000Z"],"https://huggingface.co/blog/jfrog","https://huggingface.co/blog/assets/jfrog/thumbnail.png","src/content/posts/2025-03-04-hugging-face-and-jfrog-partner-to-make-ai-security-more-transparent.md","7d1ebaaceb401636",{"html":25,"metadata":20482},{"headings":20483,"localImagePaths":20484,"remoteImagePaths":20485,"frontmatter":20486,"imagePaths":20488},[],[],[],{"title":20474,"description":25,"summary":20475,"pubDate":20487,"source":2720,"url":20477,"thumbnail":20478},"Tue, 04 Mar 2025 00:00:00 GMT",[],"2025-03-04-hugging-face-and-jfrog-partner-to-make-ai-security-more-transparent.md","2025-03-04-a-deepdive-into-aya-vision-advancing-the-frontier-of-multilingual-multimodality",{"id":20490,"data":20492,"filePath":20498,"digest":20499,"rendered":20500,"legacyId":20507},{"title":20493,"description":25,"summary":20494,"pubDate":20495,"source":2720,"url":20496,"thumbnail":20497},"A Deepdive into Aya Vision: Advancing the Frontier of Multilingual Multimodality","A Deepdive into Aya Vision: Advancing the Frontier of Multilingual Multimodality With the release of...",["Date","2025-03-04T00:00:00.000Z"],"https://huggingface.co/blog/aya-vision","https://huggingface.co/blog/assets/aya-vision/thumbnail.png","src/content/posts/2025-03-04-a-deepdive-into-aya-vision-advancing-the-frontier-of-multilingual-multimodality.md","5c32eb55d4bf5084",{"html":25,"metadata":20501},{"headings":20502,"localImagePaths":20503,"remoteImagePaths":20504,"frontmatter":20505,"imagePaths":20506},[],[],[],{"title":20493,"description":25,"summary":20494,"pubDate":20487,"source":2720,"url":20496,"thumbnail":20497},[],"2025-03-04-a-deepdive-into-aya-vision-advancing-the-frontier-of-multilingual-multimodality.md","2025-03-04-introducing-nextgenai",{"id":20508,"data":20510,"filePath":20515,"digest":20516,"rendered":20517,"legacyId":20525},{"title":20511,"description":20512,"summary":20512,"pubDate":20513,"source":19,"url":20514,"thumbnail":21},"Introducing NextGenAI","OpenAI commits $50M in funding and tools to leading institutions.",["Date","2025-03-04T06:00:00.000Z"],"https://openai.com/blog/introducing-nextgenai","src/content/posts/2025-03-04-introducing-nextgenai.md","8df2367188ce722b",{"html":25,"metadata":20518},{"headings":20519,"localImagePaths":20520,"remoteImagePaths":20521,"frontmatter":20522,"imagePaths":20524},[],[],[],{"title":20511,"description":20512,"summary":20512,"pubDate":20523,"source":19,"url":20514,"thumbnail":21},"Tue, 04 Mar 2025 06:00:00 GMT",[],"2025-03-04-introducing-nextgenai.md","2025-03-04-launchdarklys-approach-to-ai-powered-product-management",{"id":20526,"data":20528,"filePath":20533,"digest":20534,"rendered":20535,"legacyId":20543},{"title":20529,"description":20530,"summary":20530,"pubDate":20531,"source":19,"url":20532,"thumbnail":21},"LaunchDarkly's approach to AI-powered product management","A conversation with Claire Vo, Chief Product Officer of LaunchDarkly, about the changing role of product managers, her anti-to-do list, and building AI-native teams.",["Date","2025-03-04T10:00:00.000Z"],"https://openai.com/blog/launchdarkly-claire-vo","src/content/posts/2025-03-04-launchdarklys-approach-to-ai-powered-product-management.md","8ca6b0c2ac2f8720",{"html":25,"metadata":20536},{"headings":20537,"localImagePaths":20538,"remoteImagePaths":20539,"frontmatter":20540,"imagePaths":20542},[],[],[],{"title":20529,"description":20530,"summary":20530,"pubDate":20541,"source":19,"url":20532,"thumbnail":21},"Tue, 04 Mar 2025 10:00:00 GMT",[],"2025-03-04-launchdarklys-approach-to-ai-powered-product-management.md","2025-03-06-accelerating-engineering-cycles-20-with-openai",{"id":20544,"data":20546,"filePath":20551,"digest":20552,"rendered":20553,"legacyId":20561},{"title":20547,"description":20548,"summary":20548,"pubDate":20549,"source":19,"url":20550,"thumbnail":21},"Accelerating engineering cycles 20% with OpenAI","Accelerating engineering cycles 20% with OpenAI.",["Date","2025-03-06T09:00:00.000Z"],"https://openai.com/blog/factory","src/content/posts/2025-03-06-accelerating-engineering-cycles-20-with-openai.md","54f03e35b0c59811",{"html":25,"metadata":20554},{"headings":20555,"localImagePaths":20556,"remoteImagePaths":20557,"frontmatter":20558,"imagePaths":20560},[],[],[],{"title":20547,"description":20548,"summary":20548,"pubDate":20559,"source":19,"url":20550,"thumbnail":21},"Thu, 06 Mar 2025 09:00:00 GMT",[],"2025-03-06-accelerating-engineering-cycles-20-with-openai.md","2025-03-07-llm-inference-on-edge-a-fun-and-easy-guide-to-run-llms-via-react-native-on-your-phone",{"id":20562,"data":20564,"filePath":20570,"digest":20571,"rendered":20572,"legacyId":20580},{"title":20565,"description":25,"summary":20566,"pubDate":20567,"source":2720,"url":20568,"thumbnail":20569},"LLM Inference on Edge: A Fun and Easy Guide to run LLMs via React Native on your Phone!","LLM Inference on Edge: A Fun and Easy Guide to run LLMs via React Native on your Phone! As LLMs cont...",["Date","2025-03-07T00:00:00.000Z"],"https://huggingface.co/blog/llm-inference-on-edge","https://huggingface.co/blog/assets/llm_inference_on_edge/thumbnail.png","src/content/posts/2025-03-07-llm-inference-on-edge-a-fun-and-easy-guide-to-run-llms-via-react-native-on-your-phone.md","3ecaec8e3653d3d0",{"html":25,"metadata":20573},{"headings":20574,"localImagePaths":20575,"remoteImagePaths":20576,"frontmatter":20577,"imagePaths":20579},[],[],[],{"title":20565,"description":25,"summary":20566,"pubDate":20578,"source":2720,"url":20568,"thumbnail":20569},"Fri, 07 Mar 2025 00:00:00 GMT",[],"2025-03-07-llm-inference-on-edge-a-fun-and-easy-guide-to-run-llms-via-react-native-on-your-phone.md","2025-03-07-nubank-elevates-customer-experiences-with-openai",{"id":20581,"data":20583,"filePath":20587,"digest":20588,"rendered":20589,"legacyId":20597},{"title":20584,"description":20584,"summary":20584,"pubDate":20585,"source":19,"url":20586,"thumbnail":21},"Nubank elevates customer experiences with OpenAI",["Date","2025-03-07T08:00:00.000Z"],"https://openai.com/blog/nubank","src/content/posts/2025-03-07-nubank-elevates-customer-experiences-with-openai.md","12c55af6d70686e9",{"html":25,"metadata":20590},{"headings":20591,"localImagePaths":20592,"remoteImagePaths":20593,"frontmatter":20594,"imagePaths":20596},[],[],[],{"title":20584,"description":20584,"summary":20584,"pubDate":20595,"source":19,"url":20586,"thumbnail":21},"Fri, 07 Mar 2025 08:00:00 GMT",[],"2025-03-07-nubank-elevates-customer-experiences-with-openai.md","2025-03-11-new-tools-for-building-agents",{"id":20598,"data":20600,"filePath":20605,"digest":20606,"rendered":20607,"legacyId":20615},{"title":20601,"description":20602,"summary":20602,"pubDate":20603,"source":19,"url":20604,"thumbnail":21},"New tools for building agents","We’re evolving our platform to help developers and enterprises build useful and reliable agents.",["Date","2025-03-11T10:00:00.000Z"],"https://openai.com/blog/new-tools-for-building-agents","src/content/posts/2025-03-11-new-tools-for-building-agents.md","bb4cdac3b1c30146",{"html":25,"metadata":20608},{"headings":20609,"localImagePaths":20610,"remoteImagePaths":20611,"frontmatter":20612,"imagePaths":20614},[],[],[],{"title":20601,"description":20602,"summary":20602,"pubDate":20613,"source":19,"url":20604,"thumbnail":21},"Tue, 11 Mar 2025 10:00:00 GMT",[],"2025-03-11-new-tools-for-building-agents.md","2025-03-12-driving-growth-and-wow-moments-with-openai",{"id":20616,"data":20618,"filePath":20623,"digest":20624,"rendered":20625,"legacyId":20633},{"title":20619,"description":20620,"summary":20620,"pubDate":20621,"source":19,"url":20622,"thumbnail":21},"Driving growth and ‘WOW’ moments with OpenAI","LY Corporation: Driving growth and ‘WOW’ moments with OpenAI",["Date","2025-03-12T18:00:00.000Z"],"https://openai.com/blog/ly-corporation","src/content/posts/2025-03-12-driving-growth-and-wow-moments-with-openai.md","93fa09835082c972",{"html":25,"metadata":20626},{"headings":20627,"localImagePaths":20628,"remoteImagePaths":20629,"frontmatter":20630,"imagePaths":20632},[],[],[],{"title":20619,"description":20620,"summary":20620,"pubDate":20631,"source":19,"url":20622,"thumbnail":21},"Wed, 12 Mar 2025 18:00:00 GMT",[],"2025-03-12-driving-growth-and-wow-moments-with-openai.md","2025-03-12-experiment-with-gemini-20-flash-native-image-generation",{"id":20634,"data":20636,"filePath":20642,"digest":20643,"rendered":20644,"legacyId":20652},{"title":20637,"description":20638,"summary":20638,"pubDate":20639,"source":6423,"url":20640,"thumbnail":20641},"Experiment with Gemini 2.0 Flash native image generation","Native image output is available in Gemini 2.0 Flash for developers to experiment with in Google AI Studio and the Gemini API.",["Date","2025-03-12T14:58:00.000Z"],"https://deepmind.google/discover/blog/experiment-with-gemini-20-flash-native-image-generation/","https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini-image-generation_1.2e16d0ba.fill-1200x600.png","src/content/posts/2025-03-12-experiment-with-gemini-20-flash-native-image-generation.md","3ea66202aa2308cb",{"html":25,"metadata":20645},{"headings":20646,"localImagePaths":20647,"remoteImagePaths":20648,"frontmatter":20649,"imagePaths":20651},[],[],[],{"title":20637,"description":20638,"summary":20638,"pubDate":20650,"source":6423,"url":20640,"thumbnail":20641},"Wed, 12 Mar 2025 14:58:00 +0000",[],"2025-03-12-experiment-with-gemini-20-flash-native-image-generation.md","2025-03-11-lerobot-goes-to-driving-school-worlds-largest-open-source-self-driving-dataset",{"id":20653,"data":20655,"filePath":20661,"digest":20662,"rendered":20663,"legacyId":20671},{"title":20656,"description":25,"summary":20657,"pubDate":20658,"source":2720,"url":20659,"thumbnail":20660},"LeRobot goes to driving school: World’s largest open-source self-driving dataset","LeRobot goes to driving school TL;DR of L2D, the world's largest self-driving dataset! - 90+ TeraByt...",["Date","2025-03-11T00:00:00.000Z"],"https://huggingface.co/blog/lerobot-goes-to-driving-school","https://huggingface.co/blog/assets/193_l2d/lerobot-driver.gif","src/content/posts/2025-03-11-lerobot-goes-to-driving-school-worlds-largest-open-source-self-driving-dataset.md","e485203ad9710f5b",{"html":25,"metadata":20664},{"headings":20665,"localImagePaths":20666,"remoteImagePaths":20667,"frontmatter":20668,"imagePaths":20670},[],[],[],{"title":20656,"description":25,"summary":20657,"pubDate":20669,"source":2720,"url":20659,"thumbnail":20660},"Tue, 11 Mar 2025 00:00:00 GMT",[],"2025-03-11-lerobot-goes-to-driving-school-worlds-largest-open-source-self-driving-dataset.md","2025-03-12-gemini-robotics-brings-ai-into-the-physical-world",{"id":20672,"data":20674,"filePath":20680,"digest":20681,"rendered":20682,"legacyId":20690},{"title":20675,"description":20676,"summary":20676,"pubDate":20677,"source":6423,"url":20678,"thumbnail":20679},"Gemini Robotics brings AI into the physical world","Introducing Gemini Robotics and Gemini Robotics-ER, AI models designed for robots to understand, act and react to the physical world.",["Date","2025-03-12T15:00:00.000Z"],"https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/","https://lh3.googleusercontent.com/J74rVi68EPPNMBLxhxI76Bli7QggLtYRYfp5Pk2HVPtSt2NIIk2VmLktQbwDZeIlZiW3AHwlpLNcswHuz_ecR-oj4kI-mtF53yYsGJKfvPugAw5ulQ=w1200-h630-n-nu","src/content/posts/2025-03-12-gemini-robotics-brings-ai-into-the-physical-world.md","046dcd7968802843",{"html":25,"metadata":20683},{"headings":20684,"localImagePaths":20685,"remoteImagePaths":20686,"frontmatter":20687,"imagePaths":20689},[],[],[],{"title":20675,"description":20676,"summary":20676,"pubDate":20688,"source":6423,"url":20678,"thumbnail":20679},"Wed, 12 Mar 2025 15:00:00 +0000",[],"2025-03-12-gemini-robotics-brings-ai-into-the-physical-world.md","2025-03-12-introducing-gemma-3",{"id":20691,"data":20693,"filePath":20699,"digest":20700,"rendered":20701,"legacyId":20709},{"title":20694,"description":20695,"summary":20695,"pubDate":20696,"source":6423,"url":20697,"thumbnail":20698},"Introducing Gemma 3","The most capable model you can run on a single GPU or TPU.",["Date","2025-03-12T08:00:00.000Z"],"https://deepmind.google/discover/blog/introducing-gemma-3/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemma3_KeywordBlog_RD3_V01b_SocialShare.width-1300.png","src/content/posts/2025-03-12-introducing-gemma-3.md","333481f41a942c8a",{"html":25,"metadata":20702},{"headings":20703,"localImagePaths":20704,"remoteImagePaths":20705,"frontmatter":20706,"imagePaths":20708},[],[],[],{"title":20694,"description":20695,"summary":20695,"pubDate":20707,"source":6423,"url":20697,"thumbnail":20698},"Wed, 12 Mar 2025 08:00:00 +0000",[],"2025-03-12-introducing-gemma-3.md","2025-03-10-detecting-misbehavior-in-frontier-reasoning-models",{"id":20710,"data":20712,"filePath":20717,"digest":20718,"rendered":20719,"legacyId":20727},{"title":20713,"description":20714,"summary":20714,"pubDate":20715,"source":19,"url":20716,"thumbnail":21},"Detecting misbehavior in frontier reasoning models","Frontier reasoning models exploit loopholes when given the chance. We show we can detect exploits using an LLM to monitor their chains-of-thought. Penalizing their “bad thoughts” doesn’t stop the majority of misbehavior—it makes them hide their intent.",["Date","2025-03-10T10:00:00.000Z"],"https://openai.com/blog/chain-of-thought-monitoring","src/content/posts/2025-03-10-detecting-misbehavior-in-frontier-reasoning-models.md","52aabc74b9b3333b",{"html":25,"metadata":20720},{"headings":20721,"localImagePaths":20722,"remoteImagePaths":20723,"frontmatter":20724,"imagePaths":20726},[],[],[],{"title":20713,"description":20714,"summary":20714,"pubDate":20725,"source":19,"url":20716,"thumbnail":21},"Mon, 10 Mar 2025 10:00:00 GMT",[],"2025-03-10-detecting-misbehavior-in-frontier-reasoning-models.md","2025-03-12-welcome-gemma-3-googles-all-new-multimodal-multilingual-long-context-open-llm",{"id":20728,"data":20730,"filePath":20736,"digest":20737,"rendered":20738,"legacyId":20746},{"title":20731,"description":25,"summary":20732,"pubDate":20733,"source":2720,"url":20734,"thumbnail":20735},"Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM","Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM TL;DR Today Google...",["Date","2025-03-12T00:00:00.000Z"],"https://huggingface.co/blog/gemma3","https://huggingface.co/blog/assets/gemma3/thumbnail.png","src/content/posts/2025-03-12-welcome-gemma-3-googles-all-new-multimodal-multilingual-long-context-open-llm.md","ab2f3e463d230770",{"html":25,"metadata":20739},{"headings":20740,"localImagePaths":20741,"remoteImagePaths":20742,"frontmatter":20743,"imagePaths":20745},[],[],[],{"title":20731,"description":25,"summary":20732,"pubDate":20744,"source":2720,"url":20734,"thumbnail":20735},"Wed, 12 Mar 2025 00:00:00 GMT",[],"2025-03-12-welcome-gemma-3-googles-all-new-multimodal-multilingual-long-context-open-llm.md","2025-03-13-openais-proposals-for-the-us-ai-action-plan",{"id":20747,"data":20749,"filePath":20754,"digest":20755,"rendered":20756,"legacyId":20764},{"title":20750,"description":20751,"summary":20751,"pubDate":20752,"source":19,"url":20753,"thumbnail":21},"OpenAI’s proposals for the U.S. AI Action Plan","Recommendations build on OpenAI’s Economic Blueprint to strengthen America’s AI leadership.",["Date","2025-03-13T03:00:00.000Z"],"https://openai.com/global-affairs/openai-proposals-for-the-us-ai-action-plan","src/content/posts/2025-03-13-openais-proposals-for-the-us-ai-action-plan.md","f5533c7df06c0909",{"html":25,"metadata":20757},{"headings":20758,"localImagePaths":20759,"remoteImagePaths":20760,"frontmatter":20761,"imagePaths":20763},[],[],[],{"title":20750,"description":20751,"summary":20751,"pubDate":20762,"source":19,"url":20753,"thumbnail":21},"Thu, 13 Mar 2025 03:00:00 GMT",[],"2025-03-13-openais-proposals-for-the-us-ai-action-plan.md","2025-03-14-the-court-rejects-elons-latest-attempt-to-slow-openai-down",{"id":20765,"data":20767,"filePath":20772,"digest":20773,"rendered":20774,"legacyId":20782},{"title":20768,"description":20769,"summary":20769,"pubDate":20770,"source":19,"url":20771,"thumbnail":21},"The court rejects Elon’s latest attempt to slow OpenAI down","We welcome the court’s March 4, 2025, decision rejecting Elon Musk’s latest attempt to slow down OpenAI for his personal benefit.",["Date","2025-03-14T09:00:00.000Z"],"https://openai.com/blog/court-rejects-elon","src/content/posts/2025-03-14-the-court-rejects-elons-latest-attempt-to-slow-openai-down.md","d09634bf3598e7a8",{"html":25,"metadata":20775},{"headings":20776,"localImagePaths":20777,"remoteImagePaths":20778,"frontmatter":20779,"imagePaths":20781},[],[],[],{"title":20768,"description":20769,"summary":20769,"pubDate":20780,"source":19,"url":20771,"thumbnail":21},"Fri, 14 Mar 2025 09:00:00 GMT",[],"2025-03-14-the-court-rejects-elons-latest-attempt-to-slow-openai-down.md","2025-03-14-言語処理学会第31回年次大会nlp2025-発表報告",{"id":20783,"data":20785,"filePath":20792,"digest":20793,"rendered":20794,"legacyId":20802},{"title":20786,"description":20787,"summary":20787,"pubDate":20788,"source":20789,"url":20790,"thumbnail":20791},"言語処理学会第31回年次大会(NLP2025) 発表報告","\u003Cp>1. はじめに こんにちは。AIチームの栗原です。2025年3月10日(月)〜3月14日(金)に出島メッセ長崎にて行われた言語処理学会第31回年次大会で、弊社からポスター発表で3件、口頭発表で1件の発表を行いました。 昨 [&#8230;]\u003C/p> \u003Cp>投稿 \u003Ca href='https://www.ai-shift.co.jp/techblog/5586' rel='nofollow'>言語処理学会第31回年次大会(NLP2025) 発表報告\u003C/a> は \u003Ca href='https://www.ai-shift.co.jp' rel='nofollow'>株式会社AI Shift\u003C/a> に最初に表示されました。\u003C/p>",["Date","2025-03-14T10:50:19.000Z"],"AI Shift","https://www.ai-shift.co.jp/techblog/5586","https://www.ai-shift.co.jp/wp-content/uploads/2025/03/IMG_1253-1-scaled.jpg","src/content/posts/2025-03-14-言語処理学会第31回年次大会nlp2025-発表報告.md","aa117bfe29e99024",{"html":25,"metadata":20795},{"headings":20796,"localImagePaths":20797,"remoteImagePaths":20798,"frontmatter":20799,"imagePaths":20801},[],[],[],{"title":20786,"description":20787,"summary":20787,"pubDate":20800,"source":20789,"url":20790,"thumbnail":20791},"Fri, 14 Mar 2025 10:50:19 +0000",[],"2025-03-14-言語処理学会第31回年次大会nlp2025-発表報告.md","2025-03-17-llmで挑むtitanic生存予測-few-shot-leaningで表形式データはどこま解ける",{"id":20803,"data":20805,"filePath":20811,"digest":20812,"rendered":20813,"legacyId":20821},{"title":20806,"description":20807,"summary":20807,"pubDate":20808,"source":20789,"url":20809,"thumbnail":20810},"LLMで挑むTitanic生存予測: Few-Shot Leaningで表形式データはどこま解ける？","\u003Cp>こんにちは、AIチームの戸田です。 KaggleのTitanicデータセットは、機械学習の入門として定番のデータセットです。 多くの機械学習手法が試されてきたこのデータセットに対し、今回は少し異なるアプローチを試みたいと [&#8230;]\u003C/p> \u003Cp>投稿 \u003Ca href='https://www.ai-shift.co.jp/techblog/5560' rel='nofollow'>LLMで挑むTitanic生存予測: Few-Shot Leaningで表形式データはどこま解ける？\u003C/a> は \u003Ca href='https://www.ai-shift.co.jp' rel='nofollow'>株式会社AI Shift\u003C/a> に最初に表示されました。\u003C/p>",["Date","2025-03-17T21:16:00.000Z"],"https://www.ai-shift.co.jp/techblog/5560","https://www.ai-shift.co.jp/wp-content/uploads/2025/03/f81fd2e4c52864042852c112ce927ae2.png","src/content/posts/2025-03-17-llmで挑むtitanic生存予測-few-shot-leaningで表形式データはどこま解ける.md","7945615fbd6768cf",{"html":25,"metadata":20814},{"headings":20815,"localImagePaths":20816,"remoteImagePaths":20817,"frontmatter":20818,"imagePaths":20820},[],[],[],{"title":20806,"description":20807,"summary":20807,"pubDate":20819,"source":20789,"url":20809,"thumbnail":20810},"Mon, 17 Mar 2025 21:16:00 +0000",[],"2025-03-17-llmで挑むtitanic生存予測-few-shot-leaningで表形式データはどこま解ける.md","2025-03-18-eliseai-improves-housing-and-healthcare-efficiency-with-ai",{"id":20822,"data":20824,"filePath":20829,"digest":20830,"rendered":20831,"legacyId":20839},{"title":20825,"description":20826,"summary":20826,"pubDate":20827,"source":19,"url":20828,"thumbnail":21},"EliseAI improves housing and healthcare efficiency with AI","A conversation with Minna Song, CEO & Co-founder of EliseAI.",["Date","2025-03-18T10:00:00.000Z"],"https://openai.com/blog/eliseai-minna-song","src/content/posts/2025-03-18-eliseai-improves-housing-and-healthcare-efficiency-with-ai.md","e9fa64cb623ed342",{"html":25,"metadata":20832},{"headings":20833,"localImagePaths":20834,"remoteImagePaths":20835,"frontmatter":20836,"imagePaths":20838},[],[],[],{"title":20825,"description":20826,"summary":20826,"pubDate":20837,"source":19,"url":20828,"thumbnail":21},"Tue, 18 Mar 2025 10:00:00 GMT",[],"2025-03-18-eliseai-improves-housing-and-healthcare-efficiency-with-ai.md","2025-03-18-new-in-chatgpt-for-business-march-2025",{"id":20840,"data":20842,"filePath":20847,"digest":20848,"rendered":20849,"legacyId":20857},{"title":20843,"description":20844,"summary":20844,"pubDate":20845,"source":19,"url":20846,"thumbnail":21},"New in ChatGPT for Business: March 2025","Join us as we share our latest releases and how ChatGPT is becoming more interactive, customized to the way your teams work, and agentic.",["Date","2025-03-18T00:00:00.000Z"],"https://openai.com/business/new-in-chatgpt-for-work-march-updates-2025","src/content/posts/2025-03-18-new-in-chatgpt-for-business-march-2025.md","b6f67886ae10b14b",{"html":25,"metadata":20850},{"headings":20851,"localImagePaths":20852,"remoteImagePaths":20853,"frontmatter":20854,"imagePaths":20856},[],[],[],{"title":20843,"description":20844,"summary":20844,"pubDate":20855,"source":19,"url":20846,"thumbnail":21},"Tue, 18 Mar 2025 00:00:00 GMT",[],"2025-03-18-new-in-chatgpt-for-business-march-2025.md","2025-03-18-nvidias-gtc-2025-announcement-for-physical-ai-developers-new-open-models-and-datasets",{"id":20858,"data":20860,"filePath":20866,"digest":20867,"rendered":20868,"legacyId":20875},{"title":20861,"description":25,"summary":20862,"pubDate":20863,"source":2720,"url":20864,"thumbnail":20865},"NVIDIA's GTC 2025 Announcement for Physical AI Developers: New Open Models and Datasets","NVIDIA's GTC 2025 Announcement for Physical AI Developers: New Open Models and Datasets At its annua...",["Date","2025-03-18T00:00:00.000Z"],"https://huggingface.co/blog/nvidia-physical-ai","https://huggingface.co/blog/assets/nvidia-physical-ai/thumbnail.png","src/content/posts/2025-03-18-nvidias-gtc-2025-announcement-for-physical-ai-developers-new-open-models-and-datasets.md","fbdc8f65f3b4b67e",{"html":25,"metadata":20869},{"headings":20870,"localImagePaths":20871,"remoteImagePaths":20872,"frontmatter":20873,"imagePaths":20874},[],[],[],{"title":20861,"description":25,"summary":20862,"pubDate":20855,"source":2720,"url":20864,"thumbnail":20865},[],"2025-03-18-nvidias-gtc-2025-announcement-for-physical-ai-developers-new-open-models-and-datasets.md","2025-03-18-xet-is-on-the-hub",{"id":20876,"data":20878,"filePath":20884,"digest":20885,"rendered":20886,"legacyId":20893},{"title":20879,"description":25,"summary":20880,"pubDate":20881,"source":2720,"url":20882,"thumbnail":20883},"Xet is on the Hub","Xet is on the Hub Click here to read about joining the Xet waitlist (or head over to join immediatel...",["Date","2025-03-18T00:00:00.000Z"],"https://huggingface.co/blog/xet-on-the-hub","https://huggingface.co/blog/assets/xet-on-the-hub/thumbnail.png","src/content/posts/2025-03-18-xet-is-on-the-hub.md","2e65c6fd9997f390",{"html":25,"metadata":20887},{"headings":20888,"localImagePaths":20889,"remoteImagePaths":20890,"frontmatter":20891,"imagePaths":20892},[],[],[],{"title":20879,"description":25,"summary":20880,"pubDate":20855,"source":2720,"url":20882,"thumbnail":20883},[],"2025-03-18-xet-is-on-the-hub.md","2025-03-19-ai-policy-response-to-the-white-house-ai-action-plan-rfi",{"id":20894,"data":20896,"filePath":20901,"digest":20902,"rendered":20903,"legacyId":20911},{"title":20897,"description":25,"summary":20898,"pubDate":20899,"source":2720,"url":20900,"thumbnail":9354},"AI Policy: 🤗 Response to the White House AI Action Plan RFI","AI Policy @🤗: Response to the White House AI Action Plan RFI On March 14, we submitted Hugging Face'...",["Date","2025-03-19T00:00:00.000Z"],"https://huggingface.co/blog/ai-action-wh-2025","src/content/posts/2025-03-19-ai-policy-response-to-the-white-house-ai-action-plan-rfi.md","05467c710a3c2fbb",{"html":25,"metadata":20904},{"headings":20905,"localImagePaths":20906,"remoteImagePaths":20907,"frontmatter":20908,"imagePaths":20910},[],[],[],{"title":20897,"description":25,"summary":20898,"pubDate":20909,"source":2720,"url":20900,"thumbnail":9354},"Wed, 19 Mar 2025 00:00:00 GMT",[],"2025-03-19-ai-policy-response-to-the-white-house-ai-action-plan-rfi.md","2025-03-20-introducing-next-generation-audio-models-in-the-api",{"id":20912,"data":20914,"filePath":20919,"digest":20920,"rendered":20921,"legacyId":20929},{"title":20915,"description":20916,"summary":20916,"pubDate":20917,"source":19,"url":20918,"thumbnail":21},"Introducing next-generation audio models in the API","For the first time, developers can also instruct the text-to-speech model to speak in a specific way—for example, “talk like a sympathetic customer service agent”—unlocking a new level of customization for voice agents.",["Date","2025-03-20T11:00:00.000Z"],"https://openai.com/blog/introducing-our-next-generation-audio-models","src/content/posts/2025-03-20-introducing-next-generation-audio-models-in-the-api.md","fe586bacc119586a",{"html":25,"metadata":20922},{"headings":20923,"localImagePaths":20924,"remoteImagePaths":20925,"frontmatter":20926,"imagePaths":20928},[],[],[],{"title":20915,"description":20916,"summary":20916,"pubDate":20927,"source":19,"url":20918,"thumbnail":21},"Thu, 20 Mar 2025 11:00:00 GMT",[],"2025-03-20-introducing-next-generation-audio-models-in-the-api.md","2025-03-20-open-r1-how-to-use-olympiccoder-locally-for-coding",{"id":20930,"data":20932,"filePath":20938,"digest":20939,"rendered":20940,"legacyId":20948},{"title":20933,"description":25,"summary":20934,"pubDate":20935,"source":2720,"url":20936,"thumbnail":20937},"Open R1: How to use OlympicCoder locally for coding?","Open R1: How to use OlympicCoder locally for coding Everyone’s been using Claude and OpenAI as codin...",["Date","2025-03-20T00:00:00.000Z"],"https://huggingface.co/blog/olympic-coder-lmstudio","https://huggingface.co/blog/assets/olympic-coder-lmstudio/banner.png","src/content/posts/2025-03-20-open-r1-how-to-use-olympiccoder-locally-for-coding.md","a594b6be9b51431c",{"html":25,"metadata":20941},{"headings":20942,"localImagePaths":20943,"remoteImagePaths":20944,"frontmatter":20945,"imagePaths":20947},[],[],[],{"title":20933,"description":25,"summary":20934,"pubDate":20946,"source":2720,"url":20936,"thumbnail":20937},"Thu, 20 Mar 2025 00:00:00 GMT",[],"2025-03-20-open-r1-how-to-use-olympiccoder-locally-for-coding.md","2025-03-20-personalizing-travel-at-scale-with-openai",{"id":20949,"data":20951,"filePath":20956,"digest":20957,"rendered":20958,"legacyId":20966},{"title":20952,"description":20953,"summary":20953,"pubDate":20954,"source":19,"url":20955,"thumbnail":21},"Personalizing travel at scale with OpenAI","By integrating its data systems with OpenAI’s LLMs, Booking.com delivers smarter search, faster support, and intent-driven travel experiences.",["Date","2025-03-20T23:00:00.000Z"],"https://openai.com/blog/booking-com","src/content/posts/2025-03-20-personalizing-travel-at-scale-with-openai.md","731c17f1a81246d7",{"html":25,"metadata":20959},{"headings":20960,"localImagePaths":20961,"remoteImagePaths":20962,"frontmatter":20963,"imagePaths":20965},[],[],[],{"title":20952,"description":20953,"summary":20953,"pubDate":20964,"source":19,"url":20955,"thumbnail":21},"Thu, 20 Mar 2025 23:00:00 GMT",[],"2025-03-20-personalizing-travel-at-scale-with-openai.md","2025-03-21-early-methods-for-studying-affective-use-and-emotional-well-being-on-chatgpt",{"id":20967,"data":20969,"filePath":20974,"digest":20975,"rendered":20976,"legacyId":20984},{"title":20970,"description":20971,"summary":20971,"pubDate":20972,"source":19,"url":20973,"thumbnail":21},"Early methods for studying affective use and emotional well-being on ChatGPT","An OpenAI and MIT Media Lab Research collaboration.",["Date","2025-03-21T10:00:00.000Z"],"https://openai.com/blog/affective-use-study","src/content/posts/2025-03-21-early-methods-for-studying-affective-use-and-emotional-well-being-on-chatgpt.md","7545a8d2a73c33bb",{"html":25,"metadata":20977},{"headings":20978,"localImagePaths":20979,"remoteImagePaths":20980,"frontmatter":20981,"imagePaths":20983},[],[],[],{"title":20970,"description":20971,"summary":20971,"pubDate":20982,"source":19,"url":20973,"thumbnail":21},"Fri, 21 Mar 2025 10:00:00 GMT",[],"2025-03-21-early-methods-for-studying-affective-use-and-emotional-well-being-on-chatgpt.md","2025-03-21-the-new-and-fresh-analytics-in-inference-endpoints",{"id":20985,"data":20987,"filePath":20993,"digest":20994,"rendered":20995,"legacyId":21003},{"title":20988,"description":25,"summary":20989,"pubDate":20990,"source":2720,"url":20991,"thumbnail":20992},"The New and Fresh analytics in Inference Endpoints","Analytics is important Analytics and metrics are the cornerstone of understanding what's happening w...",["Date","2025-03-21T00:00:00.000Z"],"https://huggingface.co/blog/endpoint-analytics","https://huggingface.co/blog/assets/endpoint-analytics/thumbnail.png","src/content/posts/2025-03-21-the-new-and-fresh-analytics-in-inference-endpoints.md","f2eeafa037044e3c",{"html":25,"metadata":20996},{"headings":20997,"localImagePaths":20998,"remoteImagePaths":20999,"frontmatter":21000,"imagePaths":21002},[],[],[],{"title":20988,"description":25,"summary":20989,"pubDate":21001,"source":2720,"url":20991,"thumbnail":20992},"Fri, 21 Mar 2025 00:00:00 GMT",[],"2025-03-21-the-new-and-fresh-analytics-in-inference-endpoints.md","2025-03-24-introducing-gradios-new-dataframe",{"id":21004,"data":21006,"filePath":21012,"digest":21013,"rendered":21014,"legacyId":21022},{"title":21007,"description":25,"summary":21008,"pubDate":21009,"source":2720,"url":21010,"thumbnail":21011},"Introducing Gradio's new Dataframe!","Introducing Gradio's new Dataframe! Gradio’s gr.Dataframe component is one of our most popular compo...",["Date","2025-03-24T00:00:00.000Z"],"https://huggingface.co/blog/gradio-dataframe-upgrade","https://huggingface.co/blog/assets/gradio-dataframe-upgrade/thumbnail.png","src/content/posts/2025-03-24-introducing-gradios-new-dataframe.md","56271d269d0d5979",{"html":25,"metadata":21015},{"headings":21016,"localImagePaths":21017,"remoteImagePaths":21018,"frontmatter":21019,"imagePaths":21021},[],[],[],{"title":21007,"description":25,"summary":21008,"pubDate":21020,"source":2720,"url":21010,"thumbnail":21011},"Mon, 24 Mar 2025 00:00:00 GMT",[],"2025-03-24-introducing-gradios-new-dataframe.md","2025-03-24-leadership-updates",{"id":21023,"data":21025,"filePath":21030,"digest":21031,"rendered":21032,"legacyId":21040},{"title":21026,"description":21027,"summary":21027,"pubDate":21028,"source":19,"url":21029,"thumbnail":21},"Leadership updates","OpenAI has grown a lot. We remain focused on the same core—pursuing frontier AI research that accelerates human progress–but we now also deliver products used by hundreds of millions of people.",["Date","2025-03-24T10:00:00.000Z"],"https://openai.com/blog/leadership-updates-march-2025","src/content/posts/2025-03-24-leadership-updates.md","7c5c27cb3dae913b",{"html":25,"metadata":21033},{"headings":21034,"localImagePaths":21035,"remoteImagePaths":21036,"frontmatter":21037,"imagePaths":21039},[],[],[],{"title":21026,"description":21027,"summary":21027,"pubDate":21038,"source":19,"url":21029,"thumbnail":21},"Mon, 24 Mar 2025 10:00:00 GMT",[],"2025-03-24-leadership-updates.md","2025-03-25-addendum-to-gpt-4o-system-card-4o-image-generation",{"id":21041,"data":21043,"filePath":21048,"digest":21049,"rendered":21050,"legacyId":21058},{"title":21044,"description":21045,"summary":21045,"pubDate":21046,"source":19,"url":21047,"thumbnail":21},"Addendum to GPT-4o System Card: 4o image generation","4o image generation is a new, significantly more capable image generation approach than our earlier DALL·E 3 series of models. It can create photorealistic output. It can take images as inputs and transform them.",["Date","2025-03-25T11:00:00.000Z"],"https://openai.com/blog/gpt-4o-image-generation-system-card-addendum","src/content/posts/2025-03-25-addendum-to-gpt-4o-system-card-4o-image-generation.md","7571fad8e449dd5a",{"html":25,"metadata":21051},{"headings":21052,"localImagePaths":21053,"remoteImagePaths":21054,"frontmatter":21055,"imagePaths":21057},[],[],[],{"title":21044,"description":21045,"summary":21045,"pubDate":21056,"source":19,"url":21047,"thumbnail":21},"Tue, 25 Mar 2025 11:00:00 GMT",[],"2025-03-25-addendum-to-gpt-4o-system-card-4o-image-generation.md","2025-03-25-arize-phoenix-で実現する-llm-アプリケーションのトレース",{"id":21059,"data":21061,"filePath":21067,"digest":21068,"rendered":21069,"legacyId":21077},{"title":21062,"description":21063,"summary":21063,"pubDate":21064,"source":20789,"url":21065,"thumbnail":21066},"Arize Phoenix で実現する LLM アプリケーションのトレース","\u003Cp>こんにちは、AI チームの長澤(@sp_1999N)です。 今回は Arize AI 社が開発・提供する LLM アプリケーション向けの監視ツール Phoenix の紹介および簡単なデモ構築を行いたいと思います。 デモと [&#8230;]\u003C/p> \u003Cp>投稿 \u003Ca href='https://www.ai-shift.co.jp/techblog/5608' rel='nofollow'>Arize Phoenix で実現する LLM アプリケーションのトレース\u003C/a> は \u003Ca href='https://www.ai-shift.co.jp' rel='nofollow'>株式会社AI Shift\u003C/a> に最初に表示されました。\u003C/p>",["Date","2025-03-25T10:22:46.000Z"],"https://www.ai-shift.co.jp/techblog/5608","https://www.ai-shift.co.jp/wp-content/uploads/2025/03/image-4.png","src/content/posts/2025-03-25-arize-phoenix-で実現する-llm-アプリケーションのトレース.md","da64d94bb6cd3dc7",{"html":25,"metadata":21070},{"headings":21071,"localImagePaths":21072,"remoteImagePaths":21073,"frontmatter":21074,"imagePaths":21076},[],[],[],{"title":21062,"description":21063,"summary":21063,"pubDate":21075,"source":20789,"url":21065,"thumbnail":21066},"Tue, 25 Mar 2025 10:22:46 +0000",[],"2025-03-25-arize-phoenix-で実現する-llm-アプリケーションのトレース.md","2025-03-25-automating-90-of-finance-and-legal-work-with-agents",{"id":21078,"data":21080,"filePath":21085,"digest":21086,"rendered":21087,"legacyId":21095},{"title":21081,"description":21082,"summary":21082,"pubDate":21083,"source":19,"url":21084,"thumbnail":21},"Automating 90% of finance and legal work with agents","Hebbia’s deep research automates 90% of finance and legal work, powered by OpenAI",["Date","2025-03-25T10:00:00.000Z"],"https://openai.com/blog/hebbia","src/content/posts/2025-03-25-automating-90-of-finance-and-legal-work-with-agents.md","96f5146d8b474768",{"html":25,"metadata":21088},{"headings":21089,"localImagePaths":21090,"remoteImagePaths":21091,"frontmatter":21092,"imagePaths":21094},[],[],[],{"title":21081,"description":21082,"summary":21082,"pubDate":21093,"source":19,"url":21084,"thumbnail":21},"Tue, 25 Mar 2025 10:00:00 GMT",[],"2025-03-25-automating-90-of-finance-and-legal-work-with-agents.md","2025-03-25-gemini-25-our-most-intelligent-ai-model",{"id":21096,"data":21098,"filePath":21104,"digest":21105,"rendered":21106,"legacyId":21114},{"title":21099,"description":21100,"summary":21100,"pubDate":21101,"source":6423,"url":21102,"thumbnail":21103},"Gemini 2.5: Our most intelligent AI model","Gemini 2.5 is our most intelligent AI model, now with thinking built in.",["Date","2025-03-25T17:00:36.000Z"],"https://deepmind.google/discover/blog/gemini-2-5-our-most-intelligent-ai-model/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_keyword_social_share_text.width-1300.png","src/content/posts/2025-03-25-gemini-25-our-most-intelligent-ai-model.md","08215afca68abfc4",{"html":25,"metadata":21107},{"headings":21108,"localImagePaths":21109,"remoteImagePaths":21110,"frontmatter":21111,"imagePaths":21113},[],[],[],{"title":21099,"description":21100,"summary":21100,"pubDate":21112,"source":6423,"url":21102,"thumbnail":21103},"Tue, 25 Mar 2025 17:00:36 +0000",[],"2025-03-25-gemini-25-our-most-intelligent-ai-model.md","2025-03-25-introducing-4o-image-generation",{"id":21115,"data":21117,"filePath":21122,"digest":21123,"rendered":21124,"legacyId":21132},{"title":21118,"description":21119,"summary":21119,"pubDate":21120,"source":19,"url":21121,"thumbnail":21},"Introducing 4o Image Generation","At OpenAI, we have long believed image generation should be a primary capability of our language models. That’s why we’ve built our most advanced image generator yet into GPT‑4o. The result—image generation that is not only beautiful, but useful.",["Date","2025-03-25T11:05:00.000Z"],"https://openai.com/blog/introducing-4o-image-generation","src/content/posts/2025-03-25-introducing-4o-image-generation.md","83c8f85b4c3970e5",{"html":25,"metadata":21125},{"headings":21126,"localImagePaths":21127,"remoteImagePaths":21128,"frontmatter":21129,"imagePaths":21131},[],[],[],{"title":21118,"description":21119,"summary":21119,"pubDate":21130,"source":19,"url":21121,"thumbnail":21},"Tue, 25 Mar 2025 11:05:00 GMT",[],"2025-03-25-introducing-4o-image-generation.md","2025-03-25-scaling-the-openai-academy",{"id":21133,"data":21135,"filePath":21140,"digest":21141,"rendered":21142,"legacyId":21150},{"title":21136,"description":21137,"summary":21137,"pubDate":21138,"source":19,"url":21139,"thumbnail":21},"Scaling the OpenAI Academy","Online resource hub will support AI literacy and help people from all backgrounds access tools, best practices, and peer insights to use AI.",["Date","2025-03-25T07:00:00.000Z"],"https://openai.com/global-affairs/scaling-the-openai-academy","src/content/posts/2025-03-25-scaling-the-openai-academy.md","005825146379ef49",{"html":25,"metadata":21143},{"headings":21144,"localImagePaths":21145,"remoteImagePaths":21146,"frontmatter":21147,"imagePaths":21149},[],[],[],{"title":21136,"description":21137,"summary":21137,"pubDate":21148,"source":19,"url":21139,"thumbnail":21},"Tue, 25 Mar 2025 07:00:00 GMT",[],"2025-03-25-scaling-the-openai-academy.md","2025-03-26-security-on-the-path-to-agi",{"id":21151,"data":21153,"filePath":21158,"digest":21159,"rendered":21160,"legacyId":21168},{"title":21154,"description":21155,"summary":21155,"pubDate":21156,"source":19,"url":21157,"thumbnail":21},"Security on the path to AGI","At OpenAI, we proactively adapt, including by building comprehensive security measures directly into our infrastructure and models.",["Date","2025-03-26T10:00:00.000Z"],"https://openai.com/blog/security-on-the-path-to-agi","src/content/posts/2025-03-26-security-on-the-path-to-agi.md","2630f6b734d94144",{"html":25,"metadata":21161},{"headings":21162,"localImagePaths":21163,"remoteImagePaths":21164,"frontmatter":21165,"imagePaths":21167},[],[],[],{"title":21154,"description":21155,"summary":21155,"pubDate":21166,"source":19,"url":21157,"thumbnail":21},"Wed, 26 Mar 2025 10:00:00 GMT",[],"2025-03-26-security-on-the-path-to-agi.md","2025-03-26-training-and-finetuning-reranker-models-with-sentence-transformers-v4",{"id":21169,"data":21171,"filePath":21176,"digest":21177,"rendered":21178,"legacyId":21186},{"title":21172,"description":25,"summary":21173,"pubDate":21174,"source":2720,"url":21175,"thumbnail":14686},"Training and Finetuning Reranker Models with Sentence Transformers v4","Training and Finetuning Reranker Models with Sentence Transformers v4 Sentence Transformers is a Pyt...",["Date","2025-03-26T00:00:00.000Z"],"https://huggingface.co/blog/train-reranker","src/content/posts/2025-03-26-training-and-finetuning-reranker-models-with-sentence-transformers-v4.md","b22ec40b50104c49",{"html":25,"metadata":21179},{"headings":21180,"localImagePaths":21181,"remoteImagePaths":21182,"frontmatter":21183,"imagePaths":21185},[],[],[],{"title":21172,"description":25,"summary":21173,"pubDate":21184,"source":2720,"url":21175,"thumbnail":14686},"Wed, 26 Mar 2025 00:00:00 GMT",[],"2025-03-26-training-and-finetuning-reranker-models-with-sentence-transformers-v4.md","2025-03-27-moving-from-intent-based-bots-to-proactive-ai-agents",{"id":21187,"data":21189,"filePath":21194,"digest":21195,"rendered":21196,"legacyId":21204},{"title":21190,"description":21191,"summary":21191,"pubDate":21192,"source":19,"url":21193,"thumbnail":21},"Moving from intent-based bots to proactive AI agents","Moving from intent-based bots to proactive AI agents.",["Date","2025-03-27T09:00:00.000Z"],"https://openai.com/blog/zendesk","src/content/posts/2025-03-27-moving-from-intent-based-bots-to-proactive-ai-agents.md","b25df71b7379d9d0",{"html":25,"metadata":21197},{"headings":21198,"localImagePaths":21199,"remoteImagePaths":21200,"frontmatter":21201,"imagePaths":21203},[],[],[],{"title":21190,"description":21191,"summary":21191,"pubDate":21202,"source":19,"url":21193,"thumbnail":21},"Thu, 27 Mar 2025 09:00:00 GMT",[],"2025-03-27-moving-from-intent-based-bots-to-proactive-ai-agents.md","2025-03-28-accelerating-llm-inference-with-tgi-on-intel-gaudi",{"id":21205,"data":21207,"filePath":21213,"digest":21214,"rendered":21215,"legacyId":21223},{"title":21208,"description":25,"summary":21209,"pubDate":21210,"source":2720,"url":21211,"thumbnail":21212},"Accelerating LLM Inference with TGI on Intel Gaudi","🚀 Accelerating LLM Inference with TGI on Intel Gaudi We're excited to announce the native integratio...",["Date","2025-03-28T00:00:00.000Z"],"https://huggingface.co/blog/intel-gaudi-backend-for-tgi","https://huggingface.co/blog/assets/intel-gaudi-backend-for-tgi/tgi-gaudi-thumbnail.png","src/content/posts/2025-03-28-accelerating-llm-inference-with-tgi-on-intel-gaudi.md","163db5dea3f2cc6d",{"html":25,"metadata":21216},{"headings":21217,"localImagePaths":21218,"remoteImagePaths":21219,"frontmatter":21220,"imagePaths":21222},[],[],[],{"title":21208,"description":25,"summary":21209,"pubDate":21221,"source":2720,"url":21211,"thumbnail":21212},"Fri, 28 Mar 2025 00:00:00 GMT",[],"2025-03-28-accelerating-llm-inference-with-tgi-on-intel-gaudi.md","2025-03-31-how-hugging-face-scaled-secrets-management-for-ai-infrastructure",{"id":21224,"data":21226,"filePath":21232,"digest":21233,"rendered":21234,"legacyId":21242},{"title":21227,"description":25,"summary":21228,"pubDate":21229,"source":2720,"url":21230,"thumbnail":21231},"How Hugging Face Scaled Secrets Management for AI Infrastructure","How Hugging Face Scaled Secrets Management for AI Infrastructure Hugging Face has become synonymous ...",["Date","2025-03-31T00:00:00.000Z"],"https://huggingface.co/blog/scaling-secrets-management","https://huggingface.co/blog/assets/infisical/thumbnail.png","src/content/posts/2025-03-31-how-hugging-face-scaled-secrets-management-for-ai-infrastructure.md","fc6fa2dc476c52cb",{"html":25,"metadata":21235},{"headings":21236,"localImagePaths":21237,"remoteImagePaths":21238,"frontmatter":21239,"imagePaths":21241},[],[],[],{"title":21227,"description":25,"summary":21228,"pubDate":21240,"source":2720,"url":21230,"thumbnail":21231},"Mon, 31 Mar 2025 00:00:00 GMT",[],"2025-03-31-how-hugging-face-scaled-secrets-management-for-ai-infrastructure.md","2025-03-31-new-funding-to-build-towards-agi",{"id":21243,"data":21245,"filePath":21250,"digest":21251,"rendered":21252,"legacyId":21260},{"title":21246,"description":21247,"summary":21247,"pubDate":21248,"source":19,"url":21249,"thumbnail":21},"New funding to build towards AGI","Today we’re announcing new funding—$40B at a $300B post-money valuation, which enables us to push the frontiers of AI research even further, scale our compute infrastructure, and deliver increasingly powerful tools for the 500 million people who use ChatGPT every week.",["Date","2025-03-31T15:00:00.000Z"],"https://openai.com/blog/march-funding-updates","src/content/posts/2025-03-31-new-funding-to-build-towards-agi.md","d9e79d7d5f6752d3",{"html":25,"metadata":21253},{"headings":21254,"localImagePaths":21255,"remoteImagePaths":21256,"frontmatter":21257,"imagePaths":21259},[],[],[],{"title":21246,"description":21247,"summary":21247,"pubDate":21258,"source":19,"url":21249,"thumbnail":21},"Mon, 31 Mar 2025 15:00:00 GMT",[],"2025-03-31-new-funding-to-build-towards-agi.md","2025-04-02-evaluating-potential-cybersecurity-threats-of-advanced-ai",{"id":21261,"data":21263,"filePath":21269,"digest":21270,"rendered":21271,"legacyId":21279},{"title":21264,"description":21265,"summary":21265,"pubDate":21266,"source":6423,"url":21267,"thumbnail":21268},"Evaluating potential cybersecurity threats of advanced AI","Our framework enables cybersecurity experts to identify which defenses are necessary—and how to prioritize them",["Date","2025-04-02T13:30:00.000Z"],"https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/","https://lh3.googleusercontent.com/qVftghWK2fcPAfl80FKEGIuxUxYuwlN2guNdIpH5A1nF4KYf5jufujNE7j3zv5uJ3CGPEJ47ec4UaUa1vl8H3rpuEX8jIkdQlXgCEYeGhAAEj3p06IY=w1200-h630-n-nu","src/content/posts/2025-04-02-evaluating-potential-cybersecurity-threats-of-advanced-ai.md","6527c4906d22f681",{"html":25,"metadata":21272},{"headings":21273,"localImagePaths":21274,"remoteImagePaths":21275,"frontmatter":21276,"imagePaths":21278},[],[],[],{"title":21264,"description":21265,"summary":21265,"pubDate":21277,"source":6423,"url":21267,"thumbnail":21268},"Wed, 02 Apr 2025 13:30:00 +0000",[],"2025-04-02-evaluating-potential-cybersecurity-threats-of-advanced-ai.md","2025-04-02-new-commission-to-provide-insight-as-openai-builds-the-worlds-best-equipped-nonprofit",{"id":21280,"data":21282,"filePath":21287,"digest":21288,"rendered":21289,"legacyId":21297},{"title":21283,"description":21284,"summary":21284,"pubDate":21285,"source":19,"url":21286,"thumbnail":21},"New commission to provide insight as OpenAI builds the world’s best-equipped nonprofit","Already a nonprofit, and already using AI to help people solve hard problems, OpenAI aims to build the best-equipped nonprofit the world has ever seen—combining potentially historic financial resources with something even more powerful: technology that can scale human ingenuity itself.",["Date","2025-04-02T12:00:00.000Z"],"https://openai.com/blog/nonprofit-commission-guidance","src/content/posts/2025-04-02-new-commission-to-provide-insight-as-openai-builds-the-worlds-best-equipped-nonprofit.md","e87419f803a4d030",{"html":25,"metadata":21290},{"headings":21291,"localImagePaths":21292,"remoteImagePaths":21293,"frontmatter":21294,"imagePaths":21296},[],[],[],{"title":21283,"description":21284,"summary":21284,"pubDate":21295,"source":19,"url":21286,"thumbnail":21},"Wed, 02 Apr 2025 12:00:00 GMT",[],"2025-04-02-new-commission-to-provide-insight-as-openai-builds-the-worlds-best-equipped-nonprofit.md","2025-04-02-our-response-to-the-uks-copyright-consultation",{"id":21298,"data":21300,"filePath":21305,"digest":21306,"rendered":21307,"legacyId":21315},{"title":21301,"description":21302,"summary":21302,"pubDate":21303,"source":19,"url":21304,"thumbnail":21},"Our response to the UK’s copyright consultation","Recommendations for pro-innovation policies that can help make the UK the AI capital of Europe.",["Date","2025-04-02T07:00:00.000Z"],"https://openai.com/global-affairs/response-to-uk-copyright-consultation","src/content/posts/2025-04-02-our-response-to-the-uks-copyright-consultation.md","8eb1c0252a8bb4fa",{"html":25,"metadata":21308},{"headings":21309,"localImagePaths":21310,"remoteImagePaths":21311,"frontmatter":21312,"imagePaths":21314},[],[],[],{"title":21301,"description":21302,"summary":21302,"pubDate":21313,"source":19,"url":21304,"thumbnail":21},"Wed, 02 Apr 2025 07:00:00 GMT",[],"2025-04-02-our-response-to-the-uks-copyright-consultation.md","2025-04-02-paperbench-evaluating-ais-ability-to-replicate-ai-research",{"id":21316,"data":21318,"filePath":21323,"digest":21324,"rendered":21325,"legacyId":21333},{"title":21319,"description":21320,"summary":21320,"pubDate":21321,"source":19,"url":21322,"thumbnail":21},"PaperBench: Evaluating AI’s Ability to Replicate AI Research","We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research.",["Date","2025-04-02T10:15:00.000Z"],"https://openai.com/blog/paperbench","src/content/posts/2025-04-02-paperbench-evaluating-ais-ability-to-replicate-ai-research.md","e6b6481f66c3b7d8",{"html":25,"metadata":21326},{"headings":21327,"localImagePaths":21328,"remoteImagePaths":21329,"frontmatter":21330,"imagePaths":21332},[],[],[],{"title":21319,"description":21320,"summary":21320,"pubDate":21331,"source":19,"url":21322,"thumbnail":21},"Wed, 02 Apr 2025 10:15:00 GMT",[],"2025-04-02-paperbench-evaluating-ais-ability-to-replicate-ai-research.md","2025-04-02-researchers-teach-llms-to-solve-complex-planning-challenges",{"id":21334,"data":21336,"filePath":21343,"digest":21344,"rendered":21345,"legacyId":21353},{"title":21337,"description":21338,"summary":21338,"pubDate":21339,"source":21340,"url":21341,"thumbnail":21342},"Researchers teach LLMs to solve complex planning challenges","This new framework leverages a model’s reasoning abilities to create a “smart assistant” that finds the optimal solution to multistep problems.",["Date","2025-04-02T04:00:00.000Z"],"MIT","https://news.mit.edu/2025/researchers-teach-llms-to-solve-complex-planning-challenges-0402","https://news.mit.edu/sites/default/files/images/202504/MIT-Formalized-Programming-01-press.jpg","src/content/posts/2025-04-02-researchers-teach-llms-to-solve-complex-planning-challenges.md","c62e5e7a3cbcae95",{"html":25,"metadata":21346},{"headings":21347,"localImagePaths":21348,"remoteImagePaths":21349,"frontmatter":21350,"imagePaths":21352},[],[],[],{"title":21337,"description":21338,"summary":21338,"pubDate":21351,"source":21340,"url":21341,"thumbnail":21342},"Wed, 02 Apr 2025 00:00:00 -0400",[],"2025-04-02-researchers-teach-llms-to-solve-complex-planning-challenges.md","2025-04-02-taking-a-responsible-path-to-agi",{"id":21354,"data":21356,"filePath":21362,"digest":21363,"rendered":21364,"legacyId":21372},{"title":21357,"description":21358,"summary":21358,"pubDate":21359,"source":6423,"url":21360,"thumbnail":21361},"Taking a responsible path to AGI","We’re exploring the frontiers of AGI, prioritizing technical safety, proactive risk assessment, and collaboration with the AI community.",["Date","2025-04-02T13:31:00.000Z"],"https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/","https://lh3.googleusercontent.com/0sOE0EshCImNhSW7FRZvw-v_eyJJt_WUEh9evgRbhB4tl0o7qY2VAJdAloF5q3Q6CKTCiXdEvv1kUfsyZz8h6rR7Rl9jUhH02ADOyl7A7w-0QDWWr1Y=w1200-h630-n-nu","src/content/posts/2025-04-02-taking-a-responsible-path-to-agi.md","aa2ecf85238e291b",{"html":25,"metadata":21365},{"headings":21366,"localImagePaths":21367,"remoteImagePaths":21368,"frontmatter":21369,"imagePaths":21371},[],[],[],{"title":21357,"description":21358,"summary":21358,"pubDate":21370,"source":6423,"url":21360,"thumbnail":21361},"Wed, 02 Apr 2025 13:31:00 +0000",[],"2025-04-02-taking-a-responsible-path-to-agi.md","2025-04-03-bringing-intelligence-to-every-workflow",{"id":21373,"data":21375,"filePath":21380,"digest":21381,"rendered":21382,"legacyId":21390},{"title":21376,"description":21377,"summary":21377,"pubDate":21378,"source":19,"url":21379,"thumbnail":21},"Bringing intelligence to every workflow","Notion is a connected workspace where teams write, plan, and organize everything from meeting notes to product roadmaps. Today, it’s also a deeply AI-powered platform, used by millions to summarize content, generate writing, and ask questions in natural language across their entire workspace.",["Date","2025-04-03T10:00:00.000Z"],"https://openai.com/blog/notion","src/content/posts/2025-04-03-bringing-intelligence-to-every-workflow.md","5ffc553452db0e17",{"html":25,"metadata":21383},{"headings":21384,"localImagePaths":21385,"remoteImagePaths":21386,"frontmatter":21387,"imagePaths":21389},[],[],[],{"title":21376,"description":21377,"summary":21377,"pubDate":21388,"source":19,"url":21379,"thumbnail":21},"Thu, 03 Apr 2025 10:00:00 GMT",[],"2025-04-03-bringing-intelligence-to-every-workflow.md","2025-04-03-the-nlp-course-is-becoming-the-llm-course",{"id":21391,"data":21393,"filePath":21399,"digest":21400,"rendered":21401,"legacyId":21409},{"title":21394,"description":25,"summary":21395,"pubDate":21396,"source":2720,"url":21397,"thumbnail":21398},"The NLP Course is becoming the LLM Course!","The NLP Course is becoming the LLM Course! Education has always been at the heart of Hugging Face’s ...",["Date","2025-04-03T00:00:00.000Z"],"https://huggingface.co/blog/llm-course","https://huggingface.co/blog/assets/llm-course/llm-course-rename-thumbnail.png","src/content/posts/2025-04-03-the-nlp-course-is-becoming-the-llm-course.md","5ca4cae6c0f63e28",{"html":25,"metadata":21402},{"headings":21403,"localImagePaths":21404,"remoteImagePaths":21405,"frontmatter":21406,"imagePaths":21408},[],[],[],{"title":21394,"description":25,"summary":21395,"pubDate":21407,"source":2720,"url":21397,"thumbnail":21398},"Thu, 03 Apr 2025 00:00:00 GMT",[],"2025-04-03-the-nlp-course-is-becoming-the-llm-course.md","2025-04-03-vana-is-letting-users-own-a-piece-of-the-ai-models-trained-on-their-data",{"id":21410,"data":21412,"filePath":21418,"digest":21419,"rendered":21420,"legacyId":21428},{"title":21413,"description":21414,"summary":21414,"pubDate":21415,"source":21340,"url":21416,"thumbnail":21417},"Vana is letting users own a piece of the AI models trained on their data","More than 1 million people are contributing their data to Vana’s decentralized network, which started as an MIT class project.",["Date","2025-04-03T04:00:00.000Z"],"https://news.mit.edu/2025/vana-lets-users-own-piece-ai-models-trained-on-their-data-0403","https://news.mit.edu/sites/default/files/images/202504/MIT-Vana-01.jpg","src/content/posts/2025-04-03-vana-is-letting-users-own-a-piece-of-the-ai-models-trained-on-their-data.md","0907080551b01dee",{"html":25,"metadata":21421},{"headings":21422,"localImagePaths":21423,"remoteImagePaths":21424,"frontmatter":21425,"imagePaths":21427},[],[],[],{"title":21413,"description":21414,"summary":21414,"pubDate":21426,"source":21340,"url":21416,"thumbnail":21417},"Thu, 03 Apr 2025 00:00:00 -0400",[],"2025-04-03-vana-is-letting-users-own-a-piece-of-the-ai-models-trained-on-their-data.md","2025-04-04-journey-to-1-million-gradio-users",{"id":21429,"data":21431,"filePath":21437,"digest":21438,"rendered":21439,"legacyId":21447},{"title":21432,"description":25,"summary":21433,"pubDate":21434,"source":2720,"url":21435,"thumbnail":21436},"Journey to 1 Million Gradio Users!","Journey to 1 Million Gradio Users! 5 years ago, we launched Gradio as a simple Python library to let...",["Date","2025-04-04T00:00:00.000Z"],"https://huggingface.co/blog/gradio-1m","https://huggingface.co/blog/assets/gradio-1m/thumbnail.png","src/content/posts/2025-04-04-journey-to-1-million-gradio-users.md","881e36f12e0dd1d4",{"html":25,"metadata":21440},{"headings":21441,"localImagePaths":21442,"remoteImagePaths":21443,"frontmatter":21444,"imagePaths":21446},[],[],[],{"title":21432,"description":25,"summary":21433,"pubDate":21445,"source":2720,"url":21435,"thumbnail":21436},"Fri, 04 Apr 2025 00:00:00 GMT",[],"2025-04-04-journey-to-1-million-gradio-users.md","2025-04-04-new-method-assesses-and-improves-the-reliability-of-radiologists-diagnostic-reports",{"id":21448,"data":21450,"filePath":21456,"digest":21457,"rendered":21458,"legacyId":21466},{"title":21451,"description":21452,"summary":21452,"pubDate":21453,"source":21340,"url":21454,"thumbnail":21455},"New method assesses and improves the reliability of radiologists’ diagnostic reports","The framework helps clinicians choose phrases that more accurately reflect the likelihood that certain conditions are present in X-rays.",["Date","2025-04-04T04:00:00.000Z"],"https://news.mit.edu/2025/new-method-assesses-and-improves-reliability-radiologists-diagnostic-reports-0404","https://news.mit.edu/sites/default/files/images/202504/MIT-Calibrating-Certainty-01-press_2.jpg","src/content/posts/2025-04-04-new-method-assesses-and-improves-the-reliability-of-radiologists-diagnostic-reports.md","66ce1c55ac91a894",{"html":25,"metadata":21459},{"headings":21460,"localImagePaths":21461,"remoteImagePaths":21462,"frontmatter":21463,"imagePaths":21465},[],[],[],{"title":21451,"description":21452,"summary":21452,"pubDate":21464,"source":21340,"url":21454,"thumbnail":21455},"Fri, 04 Apr 2025 00:00:00 -0400",[],"2025-04-04-new-method-assesses-and-improves-the-reliability-of-radiologists-diagnostic-reports.md","2025-04-05-welcome-llama-4-maverick-scout-on-hugging-face",{"id":21467,"data":21469,"filePath":21475,"digest":21476,"rendered":21477,"legacyId":21485},{"title":21470,"description":25,"summary":21471,"pubDate":21472,"source":2720,"url":21473,"thumbnail":21474},"Welcome Llama 4 Maverick & Scout on Hugging Face!","Welcome Llama 4 Maverick & Scout on Hugging Face We are incredibly excited to welcome the next gener...",["Date","2025-04-05T00:00:00.000Z"],"https://huggingface.co/blog/llama4-release","https://huggingface.co/blog/assets/llama_4.png","src/content/posts/2025-04-05-welcome-llama-4-maverick-scout-on-hugging-face.md","d39a460944ab4f65",{"html":25,"metadata":21478},{"headings":21479,"localImagePaths":21480,"remoteImagePaths":21481,"frontmatter":21482,"imagePaths":21484},[],[],[],{"title":21470,"description":25,"summary":21471,"pubDate":21483,"source":2720,"url":21473,"thumbnail":21474},"Sat, 05 Apr 2025 00:00:00 GMT",[],"2025-04-05-welcome-llama-4-maverick-scout-on-hugging-face.md","2025-04-07-canva-enables-creativity-with-ai",{"id":21486,"data":21488,"filePath":21493,"digest":21494,"rendered":21495,"legacyId":21503},{"title":21489,"description":21490,"summary":21490,"pubDate":21491,"source":19,"url":21492,"thumbnail":21},"Canva enables creativity with AI","A conversation with Cameron Adams, Chief Product Officer and Co-founder of Canva.",["Date","2025-04-07T00:00:00.000Z"],"https://openai.com/blog/canva-cam-adams","src/content/posts/2025-04-07-canva-enables-creativity-with-ai.md","a4e86809f10576eb",{"html":25,"metadata":21496},{"headings":21497,"localImagePaths":21498,"remoteImagePaths":21499,"frontmatter":21500,"imagePaths":21502},[],[],[],{"title":21489,"description":21490,"summary":21490,"pubDate":21501,"source":19,"url":21492,"thumbnail":21},"Mon, 07 Apr 2025 00:00:00 GMT",[],"2025-04-07-canva-enables-creativity-with-ai.md","2025-04-07-openais-eu-economic-blueprint",{"id":21504,"data":21506,"filePath":21511,"digest":21512,"rendered":21513,"legacyId":21520},{"title":21507,"description":21508,"summary":21508,"pubDate":21509,"source":19,"url":21510,"thumbnail":21},"OpenAI’s EU Economic Blueprint","Today, OpenAI is sharing the EU Economic Blueprint—a set of proposals to help Europe seize the promise of artificial intelligence, drive sustainable economic growth across the region, and ensure that AI is developed and deployed by Europe, in Europe, for Europe.",["Date","2025-04-07T00:00:00.000Z"],"https://openai.com/global-affairs/openais-eu-economic-blueprint","src/content/posts/2025-04-07-openais-eu-economic-blueprint.md","653f9bfcb389b413",{"html":25,"metadata":21514},{"headings":21515,"localImagePaths":21516,"remoteImagePaths":21517,"frontmatter":21518,"imagePaths":21519},[],[],[],{"title":21507,"description":21508,"summary":21508,"pubDate":21501,"source":19,"url":21510,"thumbnail":21},[],"2025-04-07-openais-eu-economic-blueprint.md","2025-04-08-arabic-leaderboards-introducing-arabic-instruction-following-updating-aragen-and-more",{"id":21521,"data":21523,"filePath":21528,"digest":21529,"rendered":21530,"legacyId":21538},{"title":21524,"description":25,"summary":21525,"pubDate":21526,"source":2720,"url":21527,"thumbnail":18512},"Arabic Leaderboards: Introducing Arabic Instruction Following, Updating AraGen, and More","Arabic Leaderboards: Introducing Arabic Instruction Following, Updating AraGen, and More At Inceptio...",["Date","2025-04-08T00:00:00.000Z"],"https://huggingface.co/blog/leaderboard-3c3h-aragen-ifeval","src/content/posts/2025-04-08-arabic-leaderboards-introducing-arabic-instruction-following-updating-aragen-and-more.md","106644ac28af19f1",{"html":25,"metadata":21531},{"headings":21532,"localImagePaths":21533,"remoteImagePaths":21534,"frontmatter":21535,"imagePaths":21537},[],[],[],{"title":21524,"description":25,"summary":21525,"pubDate":21536,"source":2720,"url":21527,"thumbnail":18512},"Tue, 08 Apr 2025 00:00:00 GMT",[],"2025-04-08-arabic-leaderboards-introducing-arabic-instruction-following-updating-aragen-and-more.md","2025-04-09-could-llms-help-design-our-next-medicines-and-materials",{"id":21539,"data":21541,"filePath":21547,"digest":21548,"rendered":21549,"legacyId":21557},{"title":21542,"description":21543,"summary":21543,"pubDate":21544,"source":21340,"url":21545,"thumbnail":21546},"Could LLMs help design our next medicines and materials?","A new method lets users ask, in plain language, for a new molecule with certain properties, and receive a detailed description of how to synthesize it.",["Date","2025-04-09T04:00:00.000Z"],"https://news.mit.edu/2025/could-llms-help-design-our-next-medicines-and-materials-0409","https://news.mit.edu/sites/default/files/images/202504/MIT-inverse-molecule-01-press.jpg","src/content/posts/2025-04-09-could-llms-help-design-our-next-medicines-and-materials.md","57045024f0e6bd6e",{"html":25,"metadata":21550},{"headings":21551,"localImagePaths":21552,"remoteImagePaths":21553,"frontmatter":21554,"imagePaths":21556},[],[],[],{"title":21542,"description":21543,"summary":21543,"pubDate":21555,"source":21340,"url":21545,"thumbnail":21546},"Wed, 09 Apr 2025 00:00:00 -0400",[],"2025-04-09-could-llms-help-design-our-next-medicines-and-materials.md","2025-04-09-hugging-face-and-cloudflare-partner-to-make-real-time-speech-and-video-seamless-with-fastrtc",{"id":21558,"data":21560,"filePath":21566,"digest":21567,"rendered":21568,"legacyId":21576},{"title":21561,"description":25,"summary":21562,"pubDate":21563,"source":2720,"url":21564,"thumbnail":21565},"Hugging Face and Cloudflare Partner to Make Real-Time Speech and Video Seamless with FastRTC","Hugging Face and Cloudflare Partner to Make Real-Time Speech and Video Seamless with FastRTC We're e...",["Date","2025-04-09T00:00:00.000Z"],"https://huggingface.co/blog/fastrtc-cloudflare","https://huggingface.co/blog/assets/fastrtc-cloudflare/fastrtc_cloudflare.png","src/content/posts/2025-04-09-hugging-face-and-cloudflare-partner-to-make-real-time-speech-and-video-seamless-with-fastrtc.md","e882d04d99a56425",{"html":25,"metadata":21569},{"headings":21570,"localImagePaths":21571,"remoteImagePaths":21572,"frontmatter":21573,"imagePaths":21575},[],[],[],{"title":21561,"description":25,"summary":21562,"pubDate":21574,"source":2720,"url":21564,"thumbnail":21565},"Wed, 09 Apr 2025 00:00:00 GMT",[],"2025-04-09-hugging-face-and-cloudflare-partner-to-make-real-time-speech-and-video-seamless-with-fastrtc.md","2025-04-09-openai-pioneers-program",{"id":21577,"data":21579,"filePath":21584,"digest":21585,"rendered":21586,"legacyId":21594},{"title":21580,"description":21581,"summary":21581,"pubDate":21582,"source":19,"url":21583,"thumbnail":21},"OpenAI Pioneers Program","Advancing model performance and real world evaluation in applied domains.",["Date","2025-04-09T10:00:00.000Z"],"https://openai.com/blog/openai-pioneers-program","src/content/posts/2025-04-09-openai-pioneers-program.md","faacda1a8940454f",{"html":25,"metadata":21587},{"headings":21588,"localImagePaths":21589,"remoteImagePaths":21590,"frontmatter":21591,"imagePaths":21593},[],[],[],{"title":21580,"description":21581,"summary":21581,"pubDate":21592,"source":19,"url":21583,"thumbnail":21},"Wed, 09 Apr 2025 10:00:00 GMT",[],"2025-04-09-openai-pioneers-program.md","2025-04-10-browsecomp-a-benchmark-for-browsing-agents",{"id":21595,"data":21597,"filePath":21602,"digest":21603,"rendered":21604,"legacyId":21612},{"title":21598,"description":21599,"summary":21599,"pubDate":21600,"source":19,"url":21601,"thumbnail":21},"BrowseComp: a benchmark for browsing agents","BrowseComp: a benchmark for browsing agents.",["Date","2025-04-10T10:00:00.000Z"],"https://openai.com/blog/browsecomp","src/content/posts/2025-04-10-browsecomp-a-benchmark-for-browsing-agents.md","9f69c3c1e0a1f70e",{"html":25,"metadata":21605},{"headings":21606,"localImagePaths":21607,"remoteImagePaths":21608,"frontmatter":21609,"imagePaths":21611},[],[],[],{"title":21598,"description":21599,"summary":21599,"pubDate":21610,"source":19,"url":21601,"thumbnail":21},"Thu, 10 Apr 2025 10:00:00 GMT",[],"2025-04-10-browsecomp-a-benchmark-for-browsing-agents.md","2025-04-11-new-method-efficiently-safeguards-sensitive-ai-training-data",{"id":21613,"data":21615,"filePath":21621,"digest":21622,"rendered":21623,"legacyId":21631},{"title":21616,"description":21617,"summary":21617,"pubDate":21618,"source":21340,"url":21619,"thumbnail":21620},"New method efficiently safeguards sensitive AI training data","The approach maintains an AI model’s accuracy while ensuring attackers can’t extract secret information.",["Date","2025-04-11T04:00:00.000Z"],"https://news.mit.edu/2025/new-method-efficiently-safeguards-sensitive-ai-training-data-0411","https://news.mit.edu/sites/default/files/images/202504/MIT-Private-Algorithm-01-press.jpg","src/content/posts/2025-04-11-new-method-efficiently-safeguards-sensitive-ai-training-data.md","5031fbe2ee26f8d3",{"html":25,"metadata":21624},{"headings":21625,"localImagePaths":21626,"remoteImagePaths":21627,"frontmatter":21628,"imagePaths":21630},[],[],[],{"title":21616,"description":21617,"summary":21617,"pubDate":21629,"source":21340,"url":21619,"thumbnail":21620},"Fri, 11 Apr 2025 00:00:00 -0400",[],"2025-04-11-new-method-efficiently-safeguards-sensitive-ai-training-data.md","2025-04-14-4m-models-scanned-protect-ai-hugging-face-6-months-in",{"id":21632,"data":21634,"filePath":21640,"digest":21641,"rendered":21642,"legacyId":21650},{"title":21635,"description":25,"summary":21636,"pubDate":21637,"source":2720,"url":21638,"thumbnail":21639},"4M Models Scanned: Protect AI + Hugging Face 6 Months In","4M Models Scanned: Protect AI + Hugging Face 6 Months In Hugging Face and Protect AI partnered in Oc...",["Date","2025-04-14T00:00:00.000Z"],"https://huggingface.co/blog/pai-6-month","https://huggingface.co/blog/assets/pai-6-month/thumbnail.png","src/content/posts/2025-04-14-4m-models-scanned-protect-ai-hugging-face-6-months-in.md","c27c283c2600ca3c",{"html":25,"metadata":21643},{"headings":21644,"localImagePaths":21645,"remoteImagePaths":21646,"frontmatter":21647,"imagePaths":21649},[],[],[],{"title":21635,"description":25,"summary":21636,"pubDate":21648,"source":2720,"url":21638,"thumbnail":21639},"Mon, 14 Apr 2025 00:00:00 GMT",[],"2025-04-14-4m-models-scanned-protect-ai-hugging-face-6-months-in.md","2025-04-14-dolphingemma-how-google-ai-is-helping-decode-dolphin-communication",{"id":21651,"data":21653,"filePath":21659,"digest":21660,"rendered":21661,"legacyId":21669},{"title":21654,"description":21655,"summary":21655,"pubDate":21656,"source":6423,"url":21657,"thumbnail":21658},"DolphinGemma: How Google AI is helping decode dolphin communication","DolphinGemma, a large language model developed by Google, is helping scientists study how dolphins communicate — and hopefully find out what they're saying, too.",["Date","2025-04-14T17:00:00.000Z"],"https://deepmind.google/discover/blog/dolphingemma-how-google-ai-is-helping-decode-dolphin-communication/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_SocialExplainers_16x9_DolphinGem.width-1300.png","src/content/posts/2025-04-14-dolphingemma-how-google-ai-is-helping-decode-dolphin-communication.md","e0eea5614ee67048",{"html":25,"metadata":21662},{"headings":21663,"localImagePaths":21664,"remoteImagePaths":21665,"frontmatter":21666,"imagePaths":21668},[],[],[],{"title":21654,"description":21655,"summary":21655,"pubDate":21667,"source":6423,"url":21657,"thumbnail":21658},"Mon, 14 Apr 2025 17:00:00 +0000",[],"2025-04-14-dolphingemma-how-google-ai-is-helping-decode-dolphin-communication.md","2025-04-14-hugging-face-to-sell-open-source-robots-thanks-to-pollen-robotics-acquisition",{"id":21670,"data":21672,"filePath":21678,"digest":21679,"rendered":21680,"legacyId":21687},{"title":21673,"description":25,"summary":21674,"pubDate":21675,"source":2720,"url":21676,"thumbnail":21677},"Hugging Face to sell open-source robots thanks to Pollen Robotics acquisition 🤖","Hugging Face to sell open-source robots thanks to Pollen Robotics acquisition 🤖 Simon Alibert and Ré...",["Date","2025-04-14T00:00:00.000Z"],"https://huggingface.co/blog/hugging-face-pollen-robotics-acquisition","https://huggingface.co/blog/assets/hugging-face-pollen-robotics-acquisition/hf-pollen.png","src/content/posts/2025-04-14-hugging-face-to-sell-open-source-robots-thanks-to-pollen-robotics-acquisition.md","73bea2ac9e1c2c07",{"html":25,"metadata":21681},{"headings":21682,"localImagePaths":21683,"remoteImagePaths":21684,"frontmatter":21685,"imagePaths":21686},[],[],[],{"title":21673,"description":25,"summary":21674,"pubDate":21648,"source":2720,"url":21676,"thumbnail":21677},[],"2025-04-14-hugging-face-to-sell-open-source-robots-thanks-to-pollen-robotics-acquisition.md","2025-04-14-introducing-gpt-41-in-the-api",{"id":21688,"data":21690,"filePath":21695,"digest":21696,"rendered":21697,"legacyId":21705},{"title":21691,"description":21692,"summary":21692,"pubDate":21693,"source":19,"url":21694,"thumbnail":21},"Introducing GPT-4.1 in the API","Introducing GPT-4.1 in the API—a new family of models with across-the-board improvements, including major gains in coding, instruction following, and long-context understanding. We’re also releasing our first nano model. Available to developers worldwide starting today.",["Date","2025-04-14T10:00:00.000Z"],"https://openai.com/blog/gpt-4-1","src/content/posts/2025-04-14-introducing-gpt-41-in-the-api.md","8de16b3b44ac1081",{"html":25,"metadata":21698},{"headings":21699,"localImagePaths":21700,"remoteImagePaths":21701,"frontmatter":21702,"imagePaths":21704},[],[],[],{"title":21691,"description":21692,"summary":21692,"pubDate":21703,"source":19,"url":21694,"thumbnail":21},"Mon, 14 Apr 2025 10:00:00 GMT",[],"2025-04-14-introducing-gpt-41-in-the-api.md","2025-04-15-generate-videos-in-gemini-and-whisk-with-veo-2",{"id":21706,"data":21708,"filePath":21714,"digest":21715,"rendered":21716,"legacyId":21724},{"title":21709,"description":21710,"summary":21710,"pubDate":21711,"source":6423,"url":21712,"thumbnail":21713},"Generate videos in Gemini and Whisk with Veo 2","Transform text-based prompts into high-resolution eight-second videos in Gemini Advanced and use Whisk Animate to turn images into eight-second animated clips.",["Date","2025-04-15T17:00:00.000Z"],"https://deepmind.google/discover/blog/generate-videos-in-gemini-and-whisk-with-veo-2/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/GenerateVideos_Static1_1920x1080.width-1300.png","src/content/posts/2025-04-15-generate-videos-in-gemini-and-whisk-with-veo-2.md","25f592f1e524eb29",{"html":25,"metadata":21717},{"headings":21718,"localImagePaths":21719,"remoteImagePaths":21720,"frontmatter":21721,"imagePaths":21723},[],[],[],{"title":21709,"description":21710,"summary":21710,"pubDate":21722,"source":6423,"url":21712,"thumbnail":21713},"Tue, 15 Apr 2025 17:00:00 +0000",[],"2025-04-15-generate-videos-in-gemini-and-whisk-with-veo-2.md","2025-04-15-icassp2025-発表報告-hyderabad-india",{"id":21725,"data":21727,"filePath":21733,"digest":21734,"rendered":21735,"legacyId":21743},{"title":21728,"description":21729,"summary":21729,"pubDate":21730,"source":20789,"url":21731,"thumbnail":21732},"ICASSP2025 発表報告 @Hyderabad, India","\u003Cp>はじめに こんにちは、AIチームの大竹です。 2025年4月6日(日)〜4月11日(金)にインド・ハイデラバード、Hyderabad International Convention Centreにて開催された、音響・音 [&#8230;]\u003C/p> \u003Cp>投稿 \u003Ca href='https://www.ai-shift.co.jp/techblog/5728' rel='nofollow'>ICASSP2025 発表報告 @Hyderabad, India\u003C/a> は \u003Ca href='https://www.ai-shift.co.jp' rel='nofollow'>株式会社AI Shift\u003C/a> に最初に表示されました。\u003C/p>",["Date","2025-04-15T01:50:49.000Z"],"https://www.ai-shift.co.jp/techblog/5728","https://www.ai-shift.co.jp/wp-content/uploads/2025/04/IMG_20250406_091623-1.jpg","src/content/posts/2025-04-15-icassp2025-発表報告-hyderabad-india.md","8f1ec35d72ec45f0",{"html":25,"metadata":21736},{"headings":21737,"localImagePaths":21738,"remoteImagePaths":21739,"frontmatter":21740,"imagePaths":21742},[],[],[],{"title":21728,"description":21729,"summary":21729,"pubDate":21741,"source":20789,"url":21731,"thumbnail":21732},"Tue, 15 Apr 2025 01:50:49 +0000",[],"2025-04-15-icassp2025-発表報告-hyderabad-india.md","2025-04-15-openai-announces-nonprofit-commission-advisors",{"id":21744,"data":21746,"filePath":21751,"digest":21752,"rendered":21753,"legacyId":21761},{"title":21747,"description":21748,"summary":21748,"pubDate":21749,"source":19,"url":21750,"thumbnail":21},"OpenAI announces nonprofit commission advisors","OpenAI is appointing four new advisors to help inform OpenAI’s philanthropic efforts.",["Date","2025-04-15T13:00:00.000Z"],"https://openai.com/blog/nonprofit-commission-advisors","src/content/posts/2025-04-15-openai-announces-nonprofit-commission-advisors.md","1d5e854a56d9d69a",{"html":25,"metadata":21754},{"headings":21755,"localImagePaths":21756,"remoteImagePaths":21757,"frontmatter":21758,"imagePaths":21760},[],[],[],{"title":21747,"description":21748,"summary":21748,"pubDate":21759,"source":19,"url":21750,"thumbnail":21},"Tue, 15 Apr 2025 13:00:00 GMT",[],"2025-04-15-openai-announces-nonprofit-commission-advisors.md","2025-04-15-our-updated-preparedness-framework",{"id":21762,"data":21764,"filePath":21769,"digest":21770,"rendered":21771,"legacyId":21779},{"title":21765,"description":21766,"summary":21766,"pubDate":21767,"source":19,"url":21768,"thumbnail":21},"Our updated Preparedness Framework","Sharing our updated framework for measuring and protecting against severe harm from frontier AI capabilities.",["Date","2025-04-15T00:00:00.000Z"],"https://openai.com/blog/updating-our-preparedness-framework","src/content/posts/2025-04-15-our-updated-preparedness-framework.md","75295df9e17f0209",{"html":25,"metadata":21772},{"headings":21773,"localImagePaths":21774,"remoteImagePaths":21775,"frontmatter":21776,"imagePaths":21778},[],[],[],{"title":21765,"description":21766,"summary":21766,"pubDate":21777,"source":19,"url":21768,"thumbnail":21},"Tue, 15 Apr 2025 00:00:00 GMT",[],"2025-04-15-our-updated-preparedness-framework.md","2025-04-16-17-reasons-why-gradio-isnt-just-another-ui-library",{"id":21780,"data":21782,"filePath":21788,"digest":21789,"rendered":21790,"legacyId":21798},{"title":21783,"description":25,"summary":21784,"pubDate":21785,"source":2720,"url":21786,"thumbnail":21787},"17 Reasons Why Gradio Isn't Just Another UI Library","17 Reasons Why Gradio Isn't Just Another UI Library Introduction 'Oh, Gradio? That's a Python librar...",["Date","2025-04-16T00:00:00.000Z"],"https://huggingface.co/blog/why-gradio-stands-out","https://huggingface.co/blog/assets/why-gradio-stands-out/thumbnail.png","src/content/posts/2025-04-16-17-reasons-why-gradio-isnt-just-another-ui-library.md","e819a336a720180e",{"html":25,"metadata":21791},{"headings":21792,"localImagePaths":21793,"remoteImagePaths":21794,"frontmatter":21795,"imagePaths":21797},[],[],[],{"title":21783,"description":25,"summary":21784,"pubDate":21796,"source":2720,"url":21786,"thumbnail":21787},"Wed, 16 Apr 2025 00:00:00 GMT",[],"2025-04-16-17-reasons-why-gradio-isnt-just-another-ui-library.md","2025-04-16-a-faster-way-to-solve-complex-planning-problems",{"id":21799,"data":21801,"filePath":21807,"digest":21808,"rendered":21809,"legacyId":21817},{"title":21802,"description":21803,"summary":21803,"pubDate":21804,"source":21340,"url":21805,"thumbnail":21806},"A faster way to solve complex planning problems","By eliminating redundant computations, a new data-driven method can streamline processes like scheduling trains, routing delivery drivers, or assigning airline crews.",["Date","2025-04-16T04:00:00.000Z"],"https://news.mit.edu/2025/faster-way-solve-complex-planning-problems-0416","https://news.mit.edu/sites/default/files/images/202504/MIT_Long-Horizon-01.jpg","src/content/posts/2025-04-16-a-faster-way-to-solve-complex-planning-problems.md","333402b546411b15",{"html":25,"metadata":21810},{"headings":21811,"localImagePaths":21812,"remoteImagePaths":21813,"frontmatter":21814,"imagePaths":21816},[],[],[],{"title":21802,"description":21803,"summary":21803,"pubDate":21815,"source":21340,"url":21805,"thumbnail":21806},"Wed, 16 Apr 2025 00:00:00 -0400",[],"2025-04-16-a-faster-way-to-solve-complex-planning-problems.md","2025-04-16-cohere-on-hugging-face-inference-providers",{"id":21818,"data":21820,"filePath":21826,"digest":21827,"rendered":21828,"legacyId":21835},{"title":21821,"description":25,"summary":21822,"pubDate":21823,"source":2720,"url":21824,"thumbnail":21825},"Cohere on Hugging Face Inference Providers 🔥","Cohere on Hugging Face Inference Providers 🔥 We're thrilled to share that Cohere is now a supported ...",["Date","2025-04-16T00:00:00.000Z"],"https://huggingface.co/blog/inference-providers-cohere","https://huggingface.co/blog/assets/inference-providers-cohere/thumbnail.png","src/content/posts/2025-04-16-cohere-on-hugging-face-inference-providers.md","67784f7e1524d37b",{"html":25,"metadata":21829},{"headings":21830,"localImagePaths":21831,"remoteImagePaths":21832,"frontmatter":21833,"imagePaths":21834},[],[],[],{"title":21821,"description":25,"summary":21822,"pubDate":21796,"source":2720,"url":21824,"thumbnail":21825},[],"2025-04-16-cohere-on-hugging-face-inference-providers.md","2025-04-16-fastrtcを使って爆速でvoicebotを構築する",{"id":21836,"data":21838,"filePath":21844,"digest":21845,"rendered":21846,"legacyId":21854},{"title":21839,"description":21840,"summary":21840,"pubDate":21841,"source":20789,"url":21842,"thumbnail":21843},"FastRTCを使って爆速でVoicebotを構築する","\u003Cp>こんにちは、 AIチームの戸田です 今回はPythonでリアルタイムなAIアプリケーションを作る際に役立つライブラリ、FastRTCを使って簡単なVoicebotを構築してみたいと思います。 FastRTC FastRT [&#8230;]\u003C/p> \u003Cp>投稿 \u003Ca href='https://www.ai-shift.co.jp/techblog/5680' rel='nofollow'>FastRTCを使って爆速でVoicebotを構築する\u003C/a> は \u003Ca href='https://www.ai-shift.co.jp' rel='nofollow'>株式会社AI Shift\u003C/a> に最初に表示されました。\u003C/p>",["Date","2025-04-16T00:41:50.000Z"],"https://www.ai-shift.co.jp/techblog/5680","https://www.ai-shift.co.jp/wp-content/uploads/2025/04/f81fd2e4c52864042852c112ce927ae2.png","src/content/posts/2025-04-16-fastrtcを使って爆速でvoicebotを構築する.md","80bb311fd8ff5682",{"html":25,"metadata":21847},{"headings":21848,"localImagePaths":21849,"remoteImagePaths":21850,"frontmatter":21851,"imagePaths":21853},[],[],[],{"title":21839,"description":21840,"summary":21840,"pubDate":21852,"source":20789,"url":21842,"thumbnail":21843},"Wed, 16 Apr 2025 00:41:50 +0000",[],"2025-04-16-fastrtcを使って爆速でvoicebotを構築する.md","2025-04-16-introducing-helmet",{"id":21855,"data":21857,"filePath":21863,"digest":21864,"rendered":21865,"legacyId":21872},{"title":21858,"description":25,"summary":21859,"pubDate":21860,"source":2720,"url":21861,"thumbnail":21862},"Introducing HELMET","Introducing HELMET: Holistically Evaluating Long-context Language Models Contact: hyen@cs.princeton....",["Date","2025-04-16T00:00:00.000Z"],"https://huggingface.co/blog/helmet","https://huggingface.co/blog/assets/helmet/thumbnail.png","src/content/posts/2025-04-16-introducing-helmet.md","13f2e33bfe104192",{"html":25,"metadata":21866},{"headings":21867,"localImagePaths":21868,"remoteImagePaths":21869,"frontmatter":21870,"imagePaths":21871},[],[],[],{"title":21858,"description":25,"summary":21859,"pubDate":21796,"source":2720,"url":21861,"thumbnail":21862},[],"2025-04-16-introducing-helmet.md","2025-04-16-introducing-openai-o3-and-o4-mini",{"id":21873,"data":21875,"filePath":21880,"digest":21881,"rendered":21882,"legacyId":21890},{"title":21876,"description":21877,"summary":21877,"pubDate":21878,"source":19,"url":21879,"thumbnail":21},"Introducing OpenAI o3 and o4-mini","Our smartest and most capable models to date with full tool access",["Date","2025-04-16T10:00:00.000Z"],"https://openai.com/blog/introducing-o3-and-o4-mini","src/content/posts/2025-04-16-introducing-openai-o3-and-o4-mini.md","6b2bceb8415dbf30",{"html":25,"metadata":21883},{"headings":21884,"localImagePaths":21885,"remoteImagePaths":21886,"frontmatter":21887,"imagePaths":21889},[],[],[],{"title":21876,"description":21877,"summary":21877,"pubDate":21888,"source":19,"url":21879,"thumbnail":21},"Wed, 16 Apr 2025 10:00:00 GMT",[],"2025-04-16-introducing-openai-o3-and-o4-mini.md","2025-04-16-openai-o3-and-o4-mini-system-card",{"id":21891,"data":21893,"filePath":21898,"digest":21899,"rendered":21900,"legacyId":21907},{"title":21894,"description":21895,"summary":21895,"pubDate":21896,"source":19,"url":21897,"thumbnail":21},"OpenAI o3 and o4-mini System Card","OpenAI o3 and OpenAI o4-mini combine state-of-the-art reasoning with full tool capabilities—web browsing, Python, image and file analysis, image generation, canvas, automations, file search, and memory.",["Date","2025-04-16T10:00:00.000Z"],"https://openai.com/blog/o3-o4-mini-system-card","src/content/posts/2025-04-16-openai-o3-and-o4-mini-system-card.md","b348aabb943a6b66",{"html":25,"metadata":21901},{"headings":21902,"localImagePaths":21903,"remoteImagePaths":21904,"frontmatter":21905,"imagePaths":21906},[],[],[],{"title":21894,"description":21895,"summary":21895,"pubDate":21888,"source":19,"url":21897,"thumbnail":21},[],"2025-04-16-openai-o3-and-o4-mini-system-card.md","2025-04-16-thinking-with-images",{"id":21908,"data":21910,"filePath":21915,"digest":21916,"rendered":21917,"legacyId":21924},{"title":21911,"description":21912,"summary":21912,"pubDate":21913,"source":19,"url":21914,"thumbnail":21},"Thinking with images","OpenAI o3 and o4-mini represent a significant breakthrough in visual perception by reasoning with images in their chain of thought.",["Date","2025-04-16T10:00:00.000Z"],"https://openai.com/blog/thinking-with-images","src/content/posts/2025-04-16-thinking-with-images.md","063779f5e83d7c4e",{"html":25,"metadata":21918},{"headings":21919,"localImagePaths":21920,"remoteImagePaths":21921,"frontmatter":21922,"imagePaths":21923},[],[],[],{"title":21911,"description":21912,"summary":21912,"pubDate":21888,"source":19,"url":21914,"thumbnail":21},[],"2025-04-16-thinking-with-images.md","2025-04-17-introducing-gemini-25-flash",{"id":21925,"data":21927,"filePath":21933,"digest":21934,"rendered":21935,"legacyId":21943},{"title":21928,"description":21929,"summary":21929,"pubDate":21930,"source":6423,"url":21931,"thumbnail":21932},"Introducing Gemini 2.5 Flash","Gemini 2.5 Flash is our first fully hybrid reasoning model, giving developers the ability to turn thinking on or off.",["Date","2025-04-17T19:02:00.000Z"],"https://deepmind.google/discover/blog/introducing-gemini-2-5-flash/","https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemini-2-5-Flash-ai.dev.2e16d0ba.fill-1200x600.png","src/content/posts/2025-04-17-introducing-gemini-25-flash.md","3a79b5743720ee5b",{"html":25,"metadata":21936},{"headings":21937,"localImagePaths":21938,"remoteImagePaths":21939,"frontmatter":21940,"imagePaths":21942},[],[],[],{"title":21928,"description":21929,"summary":21929,"pubDate":21941,"source":6423,"url":21931,"thumbnail":21932},"Thu, 17 Apr 2025 19:02:00 +0000",[],"2025-04-17-introducing-gemini-25-flash.md","2025-04-17-langgraph-codeactをe2bの安全な仮想環境で動かす",{"id":21944,"data":21946,"filePath":21952,"digest":21953,"rendered":21954,"legacyId":21962},{"title":21947,"description":21948,"summary":21948,"pubDate":21949,"source":20789,"url":21950,"thumbnail":21951},"LangGraph CodeActをE2Bの安全な仮想環境で動かす","\u003Cp>こんにちは、 AIチームの戸田です 今回は先日LangChainから発表されたLangGraph CodeActをE2Bの仮想環境で動かしてみようと思います。CodeActは最近注目を集めているAI AgentのTool [&#8230;]\u003C/p> \u003Cp>投稿 \u003Ca href='https://www.ai-shift.co.jp/techblog/5652' rel='nofollow'>LangGraph CodeActをE2Bの安全な仮想環境で動かす\u003C/a> は \u003Ca href='https://www.ai-shift.co.jp' rel='nofollow'>株式会社AI Shift\u003C/a> に最初に表示されました。\u003C/p>",["Date","2025-04-17T01:11:36.000Z"],"https://www.ai-shift.co.jp/techblog/5652","https://www.ai-shift.co.jp/wp-content/uploads/2025/03/f81fd2e4c52864042852c112ce927ae2-1.png","src/content/posts/2025-04-17-langgraph-codeactをe2bの安全な仮想環境で動かす.md","52602faba3255aed",{"html":25,"metadata":21955},{"headings":21956,"localImagePaths":21957,"remoteImagePaths":21958,"frontmatter":21959,"imagePaths":21961},[],[],[],{"title":21947,"description":21948,"summary":21948,"pubDate":21960,"source":20789,"url":21950,"thumbnail":21951},"Thu, 17 Apr 2025 01:11:36 +0000",[],"2025-04-17-langgraph-codeactをe2bの安全な仮想環境で動かす.md","2025-04-18-making-ai-generated-code-more-accurate-in-any-language",{"id":21963,"data":21965,"filePath":21971,"digest":21972,"rendered":21973,"legacyId":21981},{"title":21966,"description":21967,"summary":21967,"pubDate":21968,"source":21340,"url":21969,"thumbnail":21970},"Making AI-generated code more accurate in any language","A new technique automatically guides an LLM toward outputs that adhere to the rules of whatever programming language or other format is being used.",["Date","2025-04-18T04:00:00.000Z"],"https://news.mit.edu/2025/making-ai-generated-code-more-accurate-0418","https://news.mit.edu/sites/default/files/images/202504/MIT-Probalistic-Control-compressed.gif","src/content/posts/2025-04-18-making-ai-generated-code-more-accurate-in-any-language.md","7a8b6c6f7175e74f",{"html":25,"metadata":21974},{"headings":21975,"localImagePaths":21976,"remoteImagePaths":21977,"frontmatter":21978,"imagePaths":21980},[],[],[],{"title":21966,"description":21967,"summary":21967,"pubDate":21979,"source":21340,"url":21969,"thumbnail":21970},"Fri, 18 Apr 2025 00:00:00 -0400",[],"2025-04-18-making-ai-generated-code-more-accurate-in-any-language.md","2025-04-18-mits-mcgovern-institute-is-shaping-brain-science-and-improving-human-lives-on-a-global-scale",{"id":21982,"data":21984,"filePath":21990,"digest":21991,"rendered":21992,"legacyId":22000},{"title":21985,"description":21986,"summary":21986,"pubDate":21987,"source":21340,"url":21988,"thumbnail":21989},"MIT’s McGovern Institute is shaping brain science and improving human lives on a global scale","A quarter century after its founding, the McGovern Institute reflects on its discoveries in the areas of neuroscience, neurotechnology, artificial intelligence, brain-body connections, and therapeutics.",["Date","2025-04-18T14:40:00.000Z"],"https://news.mit.edu/2025/mit-mcgovern-institute-shaping-brain-science-improving-human-lives-0418","https://news.mit.edu/sites/default/files/images/202504/mit-mcgovern-madonna-fmri-600x900.jpg","src/content/posts/2025-04-18-mits-mcgovern-institute-is-shaping-brain-science-and-improving-human-lives-on-a-global-scale.md","471e2bc692c0a55e",{"html":25,"metadata":21993},{"headings":21994,"localImagePaths":21995,"remoteImagePaths":21996,"frontmatter":21997,"imagePaths":21999},[],[],[],{"title":21985,"description":21986,"summary":21986,"pubDate":21998,"source":21340,"url":21988,"thumbnail":21989},"Fri, 18 Apr 2025 10:40:00 -0400",[],"2025-04-18-mits-mcgovern-institute-is-shaping-brain-science-and-improving-human-lives-on-a-global-scale.md","2025-04-22-3d-modeling-you-can-feel",{"id":22001,"data":22003,"filePath":22009,"digest":22010,"rendered":22011,"legacyId":22019},{"title":22004,"description":22005,"summary":22005,"pubDate":22006,"source":21340,"url":22007,"thumbnail":22008},"3D modeling you can feel","TactStyle, a system developed by CSAIL researchers, uses image prompts to replicate both the visual appearance and tactile properties of 3D models.",["Date","2025-04-22T19:00:00.000Z"],"https://news.mit.edu/2025/3d-modeling-you-can-feel-0422","https://news.mit.edu/sites/default/files/images/202504/MIT-TactStyle-00.jpg","src/content/posts/2025-04-22-3d-modeling-you-can-feel.md","b37e28c8299e9ac3",{"html":25,"metadata":22012},{"headings":22013,"localImagePaths":22014,"remoteImagePaths":22015,"frontmatter":22016,"imagePaths":22018},[],[],[],{"title":22004,"description":22005,"summary":22005,"pubDate":22017,"source":21340,"url":22007,"thumbnail":22008},"Tue, 22 Apr 2025 15:00:00 -0400",[],"2025-04-22-3d-modeling-you-can-feel.md","2025-04-22-norma-kamali-is-transforming-the-future-of-fashion-with-ai",{"id":22020,"data":22022,"filePath":22028,"digest":22029,"rendered":22030,"legacyId":22038},{"title":22023,"description":22024,"summary":22024,"pubDate":22025,"source":21340,"url":22026,"thumbnail":22027},"Norma Kamali is transforming the future of fashion with AI","The renowned designer embraces generative AI to preserve and propel her legacy.",["Date","2025-04-22T18:00:00.000Z"],"https://news.mit.edu/2025/norma-kamali-transforming-future-fashion-ai-0422","https://news.mit.edu/sites/default/files/images/202504/Norma-Kamali.jpg","src/content/posts/2025-04-22-norma-kamali-is-transforming-the-future-of-fashion-with-ai.md","ce8ad8eb17a43844",{"html":25,"metadata":22031},{"headings":22032,"localImagePaths":22033,"remoteImagePaths":22034,"frontmatter":22035,"imagePaths":22037},[],[],[],{"title":22023,"description":22024,"summary":22024,"pubDate":22036,"source":21340,"url":22026,"thumbnail":22027},"Tue, 22 Apr 2025 14:00:00 -0400",[],"2025-04-22-norma-kamali-is-transforming-the-future-of-fashion-with-ai.md","2025-04-22-speak-is-personalizing-language-learning-with-ai",{"id":22039,"data":22041,"filePath":22046,"digest":22047,"rendered":22048,"legacyId":22056},{"title":22042,"description":22043,"summary":22043,"pubDate":22044,"source":19,"url":22045,"thumbnail":21},"Speak is personalizing language learning with AI","A conversation with Connor Zwick, CEO & Co-founder of Speak.",["Date","2025-04-22T10:00:00.000Z"],"https://openai.com/blog/speak-connor-zwick","src/content/posts/2025-04-22-speak-is-personalizing-language-learning-with-ai.md","28333f5ee436ca97",{"html":25,"metadata":22049},{"headings":22050,"localImagePaths":22051,"remoteImagePaths":22052,"frontmatter":22053,"imagePaths":22055},[],[],[],{"title":22042,"description":22043,"summary":22043,"pubDate":22054,"source":19,"url":22045,"thumbnail":21},"Tue, 22 Apr 2025 10:00:00 GMT",[],"2025-04-22-speak-is-personalizing-language-learning-with-ai.md","2025-04-22-the-washington-post-partners-with-openai-on-search-content",{"id":22057,"data":22059,"filePath":22064,"digest":22065,"rendered":22066,"legacyId":22074},{"title":22060,"description":22061,"summary":22061,"pubDate":22062,"source":19,"url":22063,"thumbnail":21},"The Washington Post partners with OpenAI on search content","The Washington Post is partnering with with OpenAI to integrate news into ChatGPT, providing users with summaries, quotes, and direct links to original reporting.",["Date","2025-04-22T06:00:00.000Z"],"https://openai.com/global-affairs/the-washington-post-partners-with-openai","src/content/posts/2025-04-22-the-washington-post-partners-with-openai-on-search-content.md","29fc34b647222c61",{"html":25,"metadata":22067},{"headings":22068,"localImagePaths":22069,"remoteImagePaths":22070,"frontmatter":22071,"imagePaths":22073},[],[],[],{"title":22060,"description":22061,"summary":22061,"pubDate":22072,"source":19,"url":22063,"thumbnail":21},"Tue, 22 Apr 2025 06:00:00 GMT",[],"2025-04-22-the-washington-post-partners-with-openai-on-search-content.md","2025-04-23-introducing-our-latest-image-generation-model-in-the-api",{"id":22075,"data":22077,"filePath":22082,"digest":22083,"rendered":22084,"legacyId":22092},{"title":22078,"description":22079,"summary":22079,"pubDate":22080,"source":19,"url":22081,"thumbnail":21},"Introducing our latest image generation model in the API","Our latest image generation model is now available in the API via ‘gpt-image-1’—enabling developers and businesses to build professional-grade, customizable visuals directly into their own tools and platforms.",["Date","2025-04-23T10:00:00.000Z"],"https://openai.com/blog/image-generation-api","src/content/posts/2025-04-23-introducing-our-latest-image-generation-model-in-the-api.md","655442b932f56490",{"html":25,"metadata":22085},{"headings":22086,"localImagePaths":22087,"remoteImagePaths":22088,"frontmatter":22089,"imagePaths":22091},[],[],[],{"title":22078,"description":22079,"summary":22079,"pubDate":22090,"source":19,"url":22081,"thumbnail":21},"Wed, 23 Apr 2025 10:00:00 GMT",[],"2025-04-23-introducing-our-latest-image-generation-model-in-the-api.md","2025-04-23-periodic-table-of-machine-learning-could-fuel-ai-discovery",{"id":22093,"data":22095,"filePath":22101,"digest":22102,"rendered":22103,"legacyId":22111},{"title":22096,"description":22097,"summary":22097,"pubDate":22098,"source":21340,"url":22099,"thumbnail":22100},"“Periodic table of machine learning” could fuel AI discovery","Researchers have created a unifying framework that can help scientists combine existing ideas to improve AI models or create new ones.",["Date","2025-04-23T04:00:00.000Z"],"https://news.mit.edu/2025/machine-learning-periodic-table-could-fuel-ai-discovery-0423","https://news.mit.edu/sites/default/files/images/202504/MIT_Periodic-Algorithm-01-PRESS.jpg","src/content/posts/2025-04-23-periodic-table-of-machine-learning-could-fuel-ai-discovery.md","06ac7aeec879be59",{"html":25,"metadata":22104},{"headings":22105,"localImagePaths":22106,"remoteImagePaths":22107,"frontmatter":22108,"imagePaths":22110},[],[],[],{"title":22096,"description":22097,"summary":22097,"pubDate":22109,"source":21340,"url":22099,"thumbnail":22100},"Wed, 23 Apr 2025 00:00:00 -0400",[],"2025-04-23-periodic-table-of-machine-learning-could-fuel-ai-discovery.md","2025-04-23-new-model-predicts-a-chemical-reactions-point-of-no-return",{"id":22112,"data":22114,"filePath":22120,"digest":22121,"rendered":22122,"legacyId":22130},{"title":22115,"description":22116,"summary":22116,"pubDate":22117,"source":21340,"url":22118,"thumbnail":22119},"New model predicts a chemical reaction’s point of no return","Chemists could use this quick computational method to design more efficient reactions that yield useful compounds, from fuels to pharmaceuticals.",["Date","2025-04-23T15:00:00.000Z"],"https://news.mit.edu/2025/new-model-predicts-chemical-reactions-no-return-point-0423","https://news.mit.edu/sites/default/files/images/202504/BetterPredict-01-press.jpg","src/content/posts/2025-04-23-new-model-predicts-a-chemical-reactions-point-of-no-return.md","ab8ba07169811254",{"html":25,"metadata":22123},{"headings":22124,"localImagePaths":22125,"remoteImagePaths":22126,"frontmatter":22127,"imagePaths":22129},[],[],[],{"title":22115,"description":22116,"summary":22116,"pubDate":22128,"source":21340,"url":22118,"thumbnail":22119},"Wed, 23 Apr 2025 11:00:00 -0400",[],"2025-04-23-new-model-predicts-a-chemical-reactions-point-of-no-return.md","2025-04-24-designing-a-new-way-to-optimize-complex-coordinated-systems",{"id":22131,"data":22133,"filePath":22139,"digest":22140,"rendered":22141,"legacyId":22149},{"title":22134,"description":22135,"summary":22135,"pubDate":22136,"source":21340,"url":22137,"thumbnail":22138},"Designing a new way to optimize complex coordinated systems","Using diagrams to represent interactions in multipart systems can provide a faster way to design software improvements.",["Date","2025-04-24T19:00:00.000Z"],"https://news.mit.edu/2025/designing-new-way-optimize-complex-coordinated-systems-0424","https://news.mit.edu/sites/default/files/images/202504/deep-learning-diagram.jpg","src/content/posts/2025-04-24-designing-a-new-way-to-optimize-complex-coordinated-systems.md","bab819f2a4823a25",{"html":25,"metadata":22142},{"headings":22143,"localImagePaths":22144,"remoteImagePaths":22145,"frontmatter":22146,"imagePaths":22148},[],[],[],{"title":22134,"description":22135,"summary":22135,"pubDate":22147,"source":21340,"url":22137,"thumbnail":22138},"Thu, 24 Apr 2025 15:00:00 -0400",[],"2025-04-24-designing-a-new-way-to-optimize-complex-coordinated-systems.md","2025-04-24-music-ai-sandbox-now-with-new-features-and-broader-access",{"id":22150,"data":22152,"filePath":22158,"digest":22159,"rendered":22160,"legacyId":22168},{"title":22153,"description":22154,"summary":22154,"pubDate":22155,"source":6423,"url":22156,"thumbnail":22157},"Music AI Sandbox, now with new features and broader access","Helping music professionals explore the potential of generative AI",["Date","2025-04-24T15:01:00.000Z"],"https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/","https://lh3.googleusercontent.com/t_n_87B373tBNlvzgBy7RuJXb5hoPdLtBBgWjzfJnVuauI0JFwiYAyGM_LMl-yeJ3zNWO782VBE8m6ByaxDJoIvIbWoQ_DQPMdxszprk5Tbh2xQx5Q=w1200-h630-n-nu","src/content/posts/2025-04-24-music-ai-sandbox-now-with-new-features-and-broader-access.md","967ec46e1f3c209f",{"html":25,"metadata":22161},{"headings":22162,"localImagePaths":22163,"remoteImagePaths":22164,"frontmatter":22165,"imagePaths":22167},[],[],[],{"title":22153,"description":22154,"summary":22154,"pubDate":22166,"source":6423,"url":22156,"thumbnail":22157},"Thu, 24 Apr 2025 15:01:00 +0000",[],"2025-04-24-music-ai-sandbox-now-with-new-features-and-broader-access.md","2025-04-24-new-in-chatgpt-for-business-april-2025",{"id":22169,"data":22171,"filePath":22176,"digest":22177,"rendered":22178,"legacyId":22186},{"title":22172,"description":22173,"summary":22173,"pubDate":22174,"source":19,"url":22175,"thumbnail":21},"New in ChatGPT for Business: April 2025","Watch hands-on demos of the lastest in ChatGPT for Business: o3, image generation, enhanced memory, and internal knowledge.",["Date","2025-04-24T00:00:00.000Z"],"https://openai.com/business/new-in-chatgpt-for-business-april-updates-2025","src/content/posts/2025-04-24-new-in-chatgpt-for-business-april-2025.md","28b497995f8c43fe",{"html":25,"metadata":22179},{"headings":22180,"localImagePaths":22181,"remoteImagePaths":22182,"frontmatter":22183,"imagePaths":22185},[],[],[],{"title":22172,"description":22173,"summary":22173,"pubDate":22184,"source":19,"url":22175,"thumbnail":21},"Thu, 24 Apr 2025 00:00:00 GMT",[],"2025-04-24-new-in-chatgpt-for-business-april-2025.md","2025-04-25-artificial-intelligence-enhances-air-mobility-planning",{"id":22187,"data":22189,"filePath":22195,"digest":22196,"rendered":22197,"legacyId":22205},{"title":22190,"description":22191,"summary":22191,"pubDate":22192,"source":21340,"url":22193,"thumbnail":22194},"Artificial intelligence enhances air mobility planning","Lincoln Laboratory is transitioning tools to the 618th Air Operations Center to streamline global transport logistics.",["Date","2025-04-25T16:00:00.000Z"],"https://news.mit.edu/2025/artificial-intelligence-enhances-air-mobility-planning-0425","https://news.mit.edu/sites/default/files/images/202504/mit-lincoln-lab-us-air-mobility-00.jpg","src/content/posts/2025-04-25-artificial-intelligence-enhances-air-mobility-planning.md","8449141c526ccfb6",{"html":25,"metadata":22198},{"headings":22199,"localImagePaths":22200,"remoteImagePaths":22201,"frontmatter":22202,"imagePaths":22204},[],[],[],{"title":22190,"description":22191,"summary":22191,"pubDate":22203,"source":21340,"url":22193,"thumbnail":22194},"Fri, 25 Apr 2025 12:00:00 -0400",[],"2025-04-25-artificial-intelligence-enhances-air-mobility-planning.md","2025-04-25-novel-method-detects-microbial-contamination-in-cell-cultures",{"id":22206,"data":22208,"filePath":22214,"digest":22215,"rendered":22216,"legacyId":22224},{"title":22209,"description":22210,"summary":22210,"pubDate":22211,"source":21340,"url":22212,"thumbnail":22213},"Novel method detects microbial contamination in cell cultures","Ultraviolet light “fingerprints” on cell cultures and machine learning can provide a definitive yes/no contamination assessment within 30 minutes.",["Date","2025-04-26T02:00:00.000Z"],"https://news.mit.edu/2025/novel-method-detects-microbial-contamination-smart-0425","https://news.mit.edu/sites/default/files/images/202504/SMART-CAMP-Senior-Research-Engineer.jpg","src/content/posts/2025-04-25-novel-method-detects-microbial-contamination-in-cell-cultures.md","00d7a92e19cf9973",{"html":25,"metadata":22217},{"headings":22218,"localImagePaths":22219,"remoteImagePaths":22220,"frontmatter":22221,"imagePaths":22223},[],[],[],{"title":22209,"description":22210,"summary":22210,"pubDate":22222,"source":21340,"url":22212,"thumbnail":22213},"Fri, 25 Apr 2025 22:00:00 -0400",[],"2025-04-25-novel-method-detects-microbial-contamination-in-cell-cultures.md","2025-04-25-tiny-agents-a-mcp-powered-agent-in-50-lines-of-code",{"id":22225,"data":22227,"filePath":22233,"digest":22234,"rendered":22235,"legacyId":22243},{"title":22228,"description":25,"summary":22229,"pubDate":22230,"source":2720,"url":22231,"thumbnail":22232},"Tiny Agents: a MCP-powered agent in 50 lines of code","Tiny Agents: an MCP-powered agent in 50 lines of code New! (May 23, '25) If you prefer Python, check...",["Date","2025-04-25T00:00:00.000Z"],"https://huggingface.co/blog/tiny-agents","https://huggingface.co/blog/assets/tiny-agents/thumbnail.jpg","src/content/posts/2025-04-25-tiny-agents-a-mcp-powered-agent-in-50-lines-of-code.md","71be4eb22470f6b8",{"html":25,"metadata":22236},{"headings":22237,"localImagePaths":22238,"remoteImagePaths":22239,"frontmatter":22240,"imagePaths":22242},[],[],[],{"title":22228,"description":25,"summary":22229,"pubDate":22241,"source":2720,"url":22231,"thumbnail":22232},"Fri, 25 Apr 2025 00:00:00 GMT",[],"2025-04-25-tiny-agents-a-mcp-powered-agent-in-50-lines-of-code.md","2025-04-28-merging-design-and-computer-science-in-creative-ways",{"id":22244,"data":22246,"filePath":22252,"digest":22253,"rendered":22254,"legacyId":22262},{"title":22247,"description":22248,"summary":22248,"pubDate":22249,"source":21340,"url":22250,"thumbnail":22251},"Merging design and computer science in creative ways","MAD Fellow Alexander Htet Kyaw connects humans, machines, and the physical world using AI and augmented reality.",["Date","2025-04-28T20:55:00.000Z"],"https://news.mit.edu/2025/alexander-htet-kyaw-merging-design-computer-science-in-creative-ways-0428","https://news.mit.edu/sites/default/files/images/202504/mit-mad-Alexander-htet-kyaw_0.jpg","src/content/posts/2025-04-28-merging-design-and-computer-science-in-creative-ways.md","6929ea775d0b90e5",{"html":25,"metadata":22255},{"headings":22256,"localImagePaths":22257,"remoteImagePaths":22258,"frontmatter":22259,"imagePaths":22261},[],[],[],{"title":22247,"description":22248,"summary":22248,"pubDate":22260,"source":21340,"url":22250,"thumbnail":22251},"Mon, 28 Apr 2025 16:55:00 -0400",[],"2025-04-28-merging-design-and-computer-science-in-creative-ways.md","2025-04-29-introducing-autoround-intels-advanced-quantization-for-llms-and-vlms",{"id":22263,"data":22265,"filePath":22271,"digest":22272,"rendered":22273,"legacyId":22281},{"title":22266,"description":25,"summary":22267,"pubDate":22268,"source":2720,"url":22269,"thumbnail":22270},"Introducing AutoRound: Intel’s Advanced Quantization for LLMs and VLMs","What is AutoRound? As large language models (LLMs) and vision-language models (VLMs) continue to gro...",["Date","2025-04-29T00:00:00.000Z"],"https://huggingface.co/blog/autoround","https://huggingface.co/blog/assets/autoround/thumbnail.png","src/content/posts/2025-04-29-introducing-autoround-intels-advanced-quantization-for-llms-and-vlms.md","1fbda5d0692ccd35",{"html":25,"metadata":22274},{"headings":22275,"localImagePaths":22276,"remoteImagePaths":22277,"frontmatter":22278,"imagePaths":22280},[],[],[],{"title":22266,"description":25,"summary":22267,"pubDate":22279,"source":2720,"url":22269,"thumbnail":22270},"Tue, 29 Apr 2025 00:00:00 GMT",[],"2025-04-29-introducing-autoround-intels-advanced-quantization-for-llms-and-vlms.md","2025-04-29-sycophancy-in-gpt-4o-what-happened-and-what-were-doing-about-it",{"id":22282,"data":22284,"filePath":22289,"digest":22290,"rendered":22291,"legacyId":22299},{"title":22285,"description":22286,"summary":22286,"pubDate":22287,"source":19,"url":22288,"thumbnail":21},"Sycophancy in GPT-4o: what happened and what we’re doing about it","We have rolled back last week’s GPT‑4o update in ChatGPT so people are now using an earlier version with more balanced behavior. The update we removed was overly flattering or agreeable—often described as sycophantic.",["Date","2025-04-29T18:00:00.000Z"],"https://openai.com/blog/sycophancy-in-gpt-4o","src/content/posts/2025-04-29-sycophancy-in-gpt-4o-what-happened-and-what-were-doing-about-it.md","149dbc980250537c",{"html":25,"metadata":22292},{"headings":22293,"localImagePaths":22294,"remoteImagePaths":22295,"frontmatter":22296,"imagePaths":22298},[],[],[],{"title":22285,"description":22286,"summary":22286,"pubDate":22297,"source":19,"url":22288,"thumbnail":21},"Tue, 29 Apr 2025 18:00:00 GMT",[],"2025-04-29-sycophancy-in-gpt-4o-what-happened-and-what-were-doing-about-it.md","2025-04-29-welcoming-llama-guard-4-on-hugging-face-hub",{"id":22300,"data":22302,"filePath":22308,"digest":22309,"rendered":22310,"legacyId":22317},{"title":22303,"description":25,"summary":22304,"pubDate":22305,"source":2720,"url":22306,"thumbnail":22307},"Welcoming Llama Guard 4 on Hugging Face Hub","Welcoming Llama Guard 4 on Hugging Face Hub TL;DR: Today, Meta releases Llama Guard 4, a 12B dense (...",["Date","2025-04-29T00:00:00.000Z"],"https://huggingface.co/blog/llama-guard-4","https://huggingface.co/blog/assets/llama-guard-4/thumbnail.png","src/content/posts/2025-04-29-welcoming-llama-guard-4-on-hugging-face-hub.md","a807cd8b54a865cd",{"html":25,"metadata":22311},{"headings":22312,"localImagePaths":22313,"remoteImagePaths":22314,"frontmatter":22315,"imagePaths":22316},[],[],[],{"title":22303,"description":25,"summary":22304,"pubDate":22279,"source":2720,"url":22306,"thumbnail":22307},[],"2025-04-29-welcoming-llama-guard-4-on-hugging-face-hub.md","2025-04-30-how-to-build-an-mcp-server-with-gradio",{"id":22318,"data":22320,"filePath":22326,"digest":22327,"rendered":22328,"legacyId":22336},{"title":22321,"description":25,"summary":22322,"pubDate":22323,"source":2720,"url":22324,"thumbnail":22325},"How to Build an MCP Server with Gradio","How to Build an MCP Server in 5 Lines of Python Gradio is a Python library used by more than 1 milli...",["Date","2025-04-30T00:00:00.000Z"],"https://huggingface.co/blog/gradio-mcp","https://huggingface.co/blog/assets/gradio-mcp/thumbnail.png","src/content/posts/2025-04-30-how-to-build-an-mcp-server-with-gradio.md","51b90ca95e0b36f7",{"html":25,"metadata":22329},{"headings":22330,"localImagePaths":22331,"remoteImagePaths":22332,"frontmatter":22333,"imagePaths":22335},[],[],[],{"title":22321,"description":25,"summary":22322,"pubDate":22334,"source":2720,"url":22324,"thumbnail":22325},"Wed, 30 Apr 2025 00:00:00 GMT",[],"2025-04-30-how-to-build-an-mcp-server-with-gradio.md","2025-04-30-the-4-things-qwen-3s-chat-template-teaches-us",{"id":22337,"data":22339,"filePath":22345,"digest":22346,"rendered":22347,"legacyId":22354},{"title":22340,"description":25,"summary":22341,"pubDate":22342,"source":2720,"url":22343,"thumbnail":22344},"The 4 Things Qwen-3's Chat Template Teaches Us","The 4 Things Qwen-3’s Chat Template Teaches Us What a boring Jinja snippet tells us about the new Qw...",["Date","2025-04-30T00:00:00.000Z"],"https://huggingface.co/blog/qwen-3-chat-template-deep-dive","https://huggingface.co/blog/assets/qwen-3-chat-template-deep-dive/thumbnail.png","src/content/posts/2025-04-30-the-4-things-qwen-3s-chat-template-teaches-us.md","5333a9da0cad484f",{"html":25,"metadata":22348},{"headings":22349,"localImagePaths":22350,"remoteImagePaths":22351,"frontmatter":22352,"imagePaths":22353},[],[],[],{"title":22340,"description":25,"summary":22341,"pubDate":22334,"source":2720,"url":22343,"thumbnail":22344},[],"2025-04-30-the-4-things-qwen-3s-chat-template-teaches-us.md","2025-04-30-the-mit-portugal-program-enters-phase-4",{"id":22355,"data":22357,"filePath":22363,"digest":22364,"rendered":22365,"legacyId":22373},{"title":22358,"description":22359,"summary":22359,"pubDate":22360,"source":21340,"url":22361,"thumbnail":22362},"The MIT-Portugal Program enters Phase 4","New phase will support continued exploration of ideas and solutions in fields ranging from AI to nanotech to climate — with emphasis on educational exchanges and entrepreneurship.",["Date","2025-04-30T20:20:00.000Z"],"https://news.mit.edu/2025/mit-portugal-program-enters-phase-4-0430","https://news.mit.edu/sites/default/files/images/202504/mit-portugal-2024-Conference.jpg","src/content/posts/2025-04-30-the-mit-portugal-program-enters-phase-4.md","ccd932034841858e",{"html":25,"metadata":22366},{"headings":22367,"localImagePaths":22368,"remoteImagePaths":22369,"frontmatter":22370,"imagePaths":22372},[],[],[],{"title":22358,"description":22359,"summary":22359,"pubDate":22371,"source":21340,"url":22361,"thumbnail":22362},"Wed, 30 Apr 2025 16:20:00 -0400",[],"2025-04-30-the-mit-portugal-program-enters-phase-4.md","2025-05-01-inception-labsの拡散言語モデルを試してみた",{"id":22374,"data":22376,"filePath":22382,"digest":22383,"rendered":22384,"legacyId":22392},{"title":22377,"description":22378,"summary":22378,"pubDate":22379,"source":20789,"url":22380,"thumbnail":22381},"Inception Labsの拡散言語モデルを試してみた","\u003Cp>こんにちは、 AIチームの戸田です。 本記事ではInception LabsのMercury APIのベータ版が使えるようになったので、簡単に試してみました。 ドキュメントはこちらで確認できます。 拡散言語モデル 現在の [&#8230;]\u003C/p> \u003Cp>投稿 \u003Ca href='https://www.ai-shift.co.jp/techblog/5738' rel='nofollow'>Inception Labsの拡散言語モデルを試してみた\u003C/a> は \u003Ca href='https://www.ai-shift.co.jp' rel='nofollow'>株式会社AI Shift\u003C/a> に最初に表示されました。\u003C/p>",["Date","2025-05-01T03:02:11.000Z"],"https://www.ai-shift.co.jp/techblog/5738","https://www.ai-shift.co.jp/wp-content/uploads/2025/04/f81fd2e4c52864042852c112ce927ae2-1.png","src/content/posts/2025-05-01-inception-labsの拡散言語モデルを試してみた.md","75dc10c7fa2afa01",{"html":25,"metadata":22385},{"headings":22386,"localImagePaths":22387,"remoteImagePaths":22388,"frontmatter":22389,"imagePaths":22391},[],[],[],{"title":22377,"description":22378,"summary":22378,"pubDate":22390,"source":20789,"url":22380,"thumbnail":22381},"Thu, 01 May 2025 03:02:11 +0000",[],"2025-05-01-inception-labsの拡散言語モデルを試してみた.md","2025-05-01-making-ai-models-more-trustworthy-for-high-stakes-settings",{"id":22393,"data":22395,"filePath":22401,"digest":22402,"rendered":22403,"legacyId":22411},{"title":22396,"description":22397,"summary":22397,"pubDate":22398,"source":21340,"url":22399,"thumbnail":22400},"Making AI models more trustworthy for high-stakes settings","A new method helps convey uncertainty more precisely, which could give researchers and medical clinicians better information to make decisions.",["Date","2025-05-01T04:00:00.000Z"],"https://news.mit.edu/2025/making-ai-models-more-trustworthy-high-stakes-settings-0501","https://news.mit.edu/sites/default/files/images/202504/MIT_Conformal-Prediction-01.jpg","src/content/posts/2025-05-01-making-ai-models-more-trustworthy-for-high-stakes-settings.md","57d2c68fb6b9c908",{"html":25,"metadata":22404},{"headings":22405,"localImagePaths":22406,"remoteImagePaths":22407,"frontmatter":22408,"imagePaths":22410},[],[],[],{"title":22396,"description":22397,"summary":22397,"pubDate":22409,"source":21340,"url":22399,"thumbnail":22400},"Thu, 01 May 2025 00:00:00 -0400",[],"2025-05-01-making-ai-models-more-trustworthy-for-high-stakes-settings.md","2025-05-02-expanding-on-what-we-missed-with-sycophancy",{"id":22412,"data":22414,"filePath":22419,"digest":22420,"rendered":22421,"legacyId":22429},{"title":22415,"description":22416,"summary":22416,"pubDate":22417,"source":19,"url":22418,"thumbnail":21},"Expanding on what we missed with sycophancy","A deeper dive on our findings, what went wrong, and future changes we’re making.",["Date","2025-05-02T08:00:00.000Z"],"https://openai.com/blog/expanding-on-sycophancy","src/content/posts/2025-05-02-expanding-on-what-we-missed-with-sycophancy.md","b32883b31121177e",{"html":25,"metadata":22422},{"headings":22423,"localImagePaths":22424,"remoteImagePaths":22425,"frontmatter":22426,"imagePaths":22428},[],[],[],{"title":22415,"description":22416,"summary":22416,"pubDate":22427,"source":19,"url":22418,"thumbnail":21},"Fri, 02 May 2025 08:00:00 GMT",[],"2025-05-02-expanding-on-what-we-missed-with-sycophancy.md","2025-05-02-novel-ai-model-inspired-by-neural-dynamics-from-the-brain",{"id":22430,"data":22432,"filePath":22438,"digest":22439,"rendered":22440,"legacyId":22448},{"title":22433,"description":22434,"summary":22434,"pubDate":22435,"source":21340,"url":22436,"thumbnail":22437},"Novel AI model inspired by neural dynamics from the brain","New type of “state-space model” leverages principles of harmonic oscillators.",["Date","2025-05-02T19:30:00.000Z"],"https://news.mit.edu/2025/novel-ai-model-inspired-neural-dynamics-from-brain-0502","https://news.mit.edu/sites/default/files/images/202504/MIT-LinOSS.jpg","src/content/posts/2025-05-02-novel-ai-model-inspired-by-neural-dynamics-from-the-brain.md","9dbda81247fee75e",{"html":25,"metadata":22441},{"headings":22442,"localImagePaths":22443,"remoteImagePaths":22444,"frontmatter":22445,"imagePaths":22447},[],[],[],{"title":22433,"description":22434,"summary":22434,"pubDate":22446,"source":21340,"url":22436,"thumbnail":22437},"Fri, 02 May 2025 15:30:00 -0400",[],"2025-05-02-novel-ai-model-inspired-by-neural-dynamics-from-the-brain.md","2025-05-05-evolving-openais-structure",{"id":22449,"data":22451,"filePath":22456,"digest":22457,"rendered":22458,"legacyId":22466},{"title":22452,"description":22453,"summary":22453,"pubDate":22454,"source":19,"url":22455,"thumbnail":21},"Evolving OpenAI’s structure","An update from the OpenAI board on transitioning its for-profit entity to a Public Benefit Corporation, reinforcing its mission-driven structure under nonprofit oversight while enabling greater impact and long-term alignment with the public good.",["Date","2025-05-05T11:00:00.000Z"],"https://openai.com/blog/evolving-our-structure","src/content/posts/2025-05-05-evolving-openais-structure.md","1c0751eb22602f6a",{"html":25,"metadata":22459},{"headings":22460,"localImagePaths":22461,"remoteImagePaths":22462,"frontmatter":22463,"imagePaths":22465},[],[],[],{"title":22452,"description":22453,"summary":22453,"pubDate":22464,"source":19,"url":22455,"thumbnail":21},"Mon, 05 May 2025 11:00:00 GMT",[],"2025-05-05-evolving-openais-structure.md","2025-05-05-new-tool-evaluates-progress-in-reinforcement-learning",{"id":22467,"data":22469,"filePath":22475,"digest":22476,"rendered":22477,"legacyId":22485},{"title":22470,"description":22471,"summary":22471,"pubDate":22472,"source":21340,"url":22473,"thumbnail":22474},"New tool evaluates progress in reinforcement learning","“IntersectionZoo,” a benchmarking tool, uses a real-world traffic problem to test progress in deep reinforcement learning algorithms.",["Date","2025-05-05T20:00:00.000Z"],"https://news.mit.edu/2025/new-tool-evaluate-progress-reinforcement-learning-0505","https://news.mit.edu/sites/default/files/images/202504/Intersection-Zoo.jpg","src/content/posts/2025-05-05-new-tool-evaluates-progress-in-reinforcement-learning.md","a5d818600e9c1b2d",{"html":25,"metadata":22478},{"headings":22479,"localImagePaths":22480,"remoteImagePaths":22481,"frontmatter":22482,"imagePaths":22484},[],[],[],{"title":22470,"description":22471,"summary":22471,"pubDate":22483,"source":21340,"url":22473,"thumbnail":22474},"Mon, 05 May 2025 16:00:00 -0400",[],"2025-05-05-new-tool-evaluates-progress-in-reinforcement-learning.md","2025-05-05-lowes-leverages-ai-to-power-home-improvement-retail",{"id":22486,"data":22488,"filePath":22493,"digest":22494,"rendered":22495,"legacyId":22503},{"title":22489,"description":22490,"summary":22490,"pubDate":22491,"source":19,"url":22492,"thumbnail":21},"Lowe’s leverages AI to power home improvement retail","A conversation with Chandhu Nair, Senior Vice President of Data, AI, and Innovation.",["Date","2025-05-05T05:00:00.000Z"],"https://openai.com/blog/lowes-chandhu-nair","src/content/posts/2025-05-05-lowes-leverages-ai-to-power-home-improvement-retail.md","3d9b4d82a2400f79",{"html":25,"metadata":22496},{"headings":22497,"localImagePaths":22498,"remoteImagePaths":22499,"frontmatter":22500,"imagePaths":22502},[],[],[],{"title":22489,"description":22490,"summary":22490,"pubDate":22501,"source":19,"url":22492,"thumbnail":21},"Mon, 05 May 2025 05:00:00 GMT",[],"2025-05-05-lowes-leverages-ai-to-power-home-improvement-retail.md","2025-05-05-qa-a-roadmap-for-revolutionizing-health-care-through-data-driven-innovation",{"id":22504,"data":22506,"filePath":22512,"digest":22513,"rendered":22514,"legacyId":22522},{"title":22507,"description":22508,"summary":22508,"pubDate":22509,"source":21340,"url":22510,"thumbnail":22511},"Q&A: A roadmap for revolutionizing health care through data-driven innovation","A new book coauthored by MIT’s Dimitris Bertsimas explores how analytics is driving decisions and outcomes in health care.",["Date","2025-05-05T20:15:00.000Z"],"https://news.mit.edu/2025/qa-roadmap-revolutionizing-health-care-through-data-driven-innovation-0505","https://news.mit.edu/sites/default/files/images/202505/Analytics-Edge-in-Healthcare-Dimitris-Bertsimas-00.png","src/content/posts/2025-05-05-qa-a-roadmap-for-revolutionizing-health-care-through-data-driven-innovation.md","9be0cf5ac4eab7bb",{"html":25,"metadata":22515},{"headings":22516,"localImagePaths":22517,"remoteImagePaths":22518,"frontmatter":22519,"imagePaths":22521},[],[],[],{"title":22507,"description":22508,"summary":22508,"pubDate":22520,"source":21340,"url":22510,"thumbnail":22511},"Mon, 05 May 2025 16:15:00 -0400",[],"2025-05-05-qa-a-roadmap-for-revolutionizing-health-care-through-data-driven-innovation.md","2025-05-06-ai-helps-john-deere-transform-agriculture",{"id":22523,"data":22525,"filePath":22530,"digest":22531,"rendered":22532,"legacyId":22540},{"title":22526,"description":22527,"summary":22527,"pubDate":22528,"source":19,"url":22529,"thumbnail":21},"AI helps John Deere transform agriculture","John Deere’s Justin Rose talks about transforming agriculture with AI and shares how the company is scaling innovation to help farmers work smarter, more efficiently, and sustainably.",["Date","2025-05-06T00:00:00.000Z"],"https://openai.com/blog/john-deere-justin-rose","src/content/posts/2025-05-06-ai-helps-john-deere-transform-agriculture.md","771dea9c8f0d1aab",{"html":25,"metadata":22533},{"headings":22534,"localImagePaths":22535,"remoteImagePaths":22536,"frontmatter":22537,"imagePaths":22539},[],[],[],{"title":22526,"description":22527,"summary":22527,"pubDate":22538,"source":19,"url":22529,"thumbnail":21},"Tue, 06 May 2025 00:00:00 GMT",[],"2025-05-06-ai-helps-john-deere-transform-agriculture.md","2025-05-06-build-rich-interactive-web-apps-with-an-updated-gemini-25-pro",{"id":22541,"data":22543,"filePath":22549,"digest":22550,"rendered":22551,"legacyId":22559},{"title":22544,"description":22545,"summary":22545,"pubDate":22546,"source":6423,"url":22547,"thumbnail":22548},"Build rich, interactive web apps with an updated Gemini 2.5 Pro","Our updated version of Gemini 2.5 Pro Preview has improved capabilities for coding.",["Date","2025-05-06T15:00:00.000Z"],"https://deepmind.google/discover/blog/build-rich-interactive-web-apps-with-an-updated-gemini-25-pro/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini25prohero.width-1300.png","src/content/posts/2025-05-06-build-rich-interactive-web-apps-with-an-updated-gemini-25-pro.md","9567facd5774f7a8",{"html":25,"metadata":22552},{"headings":22553,"localImagePaths":22554,"remoteImagePaths":22555,"frontmatter":22556,"imagePaths":22558},[],[],[],{"title":22544,"description":22545,"summary":22545,"pubDate":22557,"source":6423,"url":22547,"thumbnail":22548},"Tue, 06 May 2025 15:00:00 +0000",[],"2025-05-06-build-rich-interactive-web-apps-with-an-updated-gemini-25-pro.md","2025-05-06-gemini-25-pro-preview-even-better-coding-performance",{"id":22560,"data":22562,"filePath":22568,"digest":22569,"rendered":22570,"legacyId":22578},{"title":22563,"description":22564,"summary":22564,"pubDate":22565,"source":6423,"url":22566,"thumbnail":22567},"Gemini 2.5 Pro Preview: even better coding performance","We’ve seen developers doing amazing things with Gemini 2.5 Pro, so we decided to release an updated version a couple of weeks early to get into developers hands sooner.",["Date","2025-05-06T15:06:55.000Z"],"https://deepmind.google/discover/blog/gemini-25-pro-preview-even-better-coding-performance/","https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini_2-5_pro_claybrook__dev_her.2e16d0ba.fill-1200x600.jpg","src/content/posts/2025-05-06-gemini-25-pro-preview-even-better-coding-performance.md","7f49b522ceb3ce78",{"html":25,"metadata":22571},{"headings":22572,"localImagePaths":22573,"remoteImagePaths":22574,"frontmatter":22575,"imagePaths":22577},[],[],[],{"title":22563,"description":22564,"summary":22564,"pubDate":22576,"source":6423,"url":22566,"thumbnail":22567},"Tue, 06 May 2025 15:06:55 +0000",[],"2025-05-06-gemini-25-pro-preview-even-better-coding-performance.md","2025-05-06-hybrid-ai-model-crafts-smooth-high-quality-videos-in-seconds",{"id":22579,"data":22581,"filePath":22587,"digest":22588,"rendered":22589,"legacyId":22597},{"title":22582,"description":22583,"summary":22583,"pubDate":22584,"source":21340,"url":22585,"thumbnail":22586},"Hybrid AI model crafts smooth, high-quality videos in seconds","The CausVid generative AI tool uses a diffusion model to teach an autoregressive (frame-by-frame) system to rapidly produce stable, high-resolution videos.",["Date","2025-05-06T16:15:00.000Z"],"https://news.mit.edu/2025/causevid-hybrid-ai-model-crafts-smooth-high-quality-videos-in-seconds-0506","https://news.mit.edu/sites/default/files/images/202504/MIT-CausVid.jpg","src/content/posts/2025-05-06-hybrid-ai-model-crafts-smooth-high-quality-videos-in-seconds.md","b700940f268d1e50",{"html":25,"metadata":22590},{"headings":22591,"localImagePaths":22592,"remoteImagePaths":22593,"frontmatter":22594,"imagePaths":22596},[],[],[],{"title":22582,"description":22583,"summary":22583,"pubDate":22595,"source":21340,"url":22585,"thumbnail":22586},"Tue, 06 May 2025 12:15:00 -0400",[],"2025-05-06-hybrid-ai-model-crafts-smooth-high-quality-videos-in-seconds.md","2025-05-06-introducing-ai-stories-daily-benefits-shine-a-light-on-bigger-opportunities",{"id":22598,"data":22600,"filePath":22605,"digest":22606,"rendered":22607,"legacyId":22615},{"title":22601,"description":22602,"summary":22602,"pubDate":22603,"source":19,"url":22604,"thumbnail":21},"Introducing AI stories: daily benefits shine a light on bigger opportunities","Sam Altman has written that we are entering the Intelligence Age, a time when AI will help people become dramatically more capable. The biggest problems of today—across science, medicine, education, national defense—will no longer seem intractable, but will in fact be solvable. New horizons of possibility and prosperity will open up.",["Date","2025-05-06T10:30:00.000Z"],"https://openai.com/global-affairs/ai-stories-daily-benefits-bigger-opportunities","src/content/posts/2025-05-06-introducing-ai-stories-daily-benefits-shine-a-light-on-bigger-opportunities.md","7a93e879674980fc",{"html":25,"metadata":22608},{"headings":22609,"localImagePaths":22610,"remoteImagePaths":22611,"frontmatter":22612,"imagePaths":22614},[],[],[],{"title":22601,"description":22602,"summary":22602,"pubDate":22613,"source":19,"url":22604,"thumbnail":21},"Tue, 06 May 2025 10:30:00 GMT",[],"2025-05-06-introducing-ai-stories-daily-benefits-shine-a-light-on-bigger-opportunities.md","2025-05-07-introducing-data-residency-in-asia",{"id":22616,"data":22618,"filePath":22622,"digest":22623,"rendered":22624,"legacyId":22632},{"title":22619,"description":19833,"summary":19833,"pubDate":22620,"source":19,"url":22621,"thumbnail":21},"Introducing data residency in Asia",["Date","2025-05-07T18:00:00.000Z"],"https://openai.com/blog/introducing-data-residency-in-asia","src/content/posts/2025-05-07-introducing-data-residency-in-asia.md","a64f4fd112f44f03",{"html":25,"metadata":22625},{"headings":22626,"localImagePaths":22627,"remoteImagePaths":22628,"frontmatter":22629,"imagePaths":22631},[],[],[],{"title":22619,"description":19833,"summary":19833,"pubDate":22630,"source":19,"url":22621,"thumbnail":21},"Wed, 07 May 2025 18:00:00 GMT",[],"2025-05-07-introducing-data-residency-in-asia.md","2025-05-07-introducing-openai-for-countries",{"id":22633,"data":22635,"filePath":22640,"digest":22641,"rendered":22642,"legacyId":22650},{"title":22636,"description":22637,"summary":22637,"pubDate":22638,"source":19,"url":22639,"thumbnail":21},"Introducing OpenAI for Countries","A new initiative to support countries around the world that want to build on democratic AI rails.",["Date","2025-05-07T03:00:00.000Z"],"https://openai.com/global-affairs/openai-for-countries","src/content/posts/2025-05-07-introducing-openai-for-countries.md","9f0bf89ec8f3c39b",{"html":25,"metadata":22643},{"headings":22644,"localImagePaths":22645,"remoteImagePaths":22646,"frontmatter":22647,"imagePaths":22649},[],[],[],{"title":22636,"description":22637,"summary":22637,"pubDate":22648,"source":19,"url":22639,"thumbnail":21},"Wed, 07 May 2025 03:00:00 GMT",[],"2025-05-07-introducing-openai-for-countries.md","2025-05-07-lowes-puts-project-expertise-into-every-hand",{"id":22651,"data":22653,"filePath":22658,"digest":22659,"rendered":22660,"legacyId":22668},{"title":22654,"description":22655,"summary":22655,"pubDate":22656,"source":19,"url":22657,"thumbnail":21},"Lowe’s puts project expertise into every hand","Lowe’s partnered with OpenAI to build Mylow and Mylow Companion, AI-powered tools that bring expert help to both customers and store associates—making complex home improvement projects easier to plan, navigate, and complete.",["Date","2025-05-07T07:00:00.000Z"],"https://openai.com/blog/lowes","src/content/posts/2025-05-07-lowes-puts-project-expertise-into-every-hand.md","79c1a3bf3749c56b",{"html":25,"metadata":22661},{"headings":22662,"localImagePaths":22663,"remoteImagePaths":22664,"frontmatter":22665,"imagePaths":22667},[],[],[],{"title":22654,"description":22655,"summary":22655,"pubDate":22666,"source":19,"url":22657,"thumbnail":21},"Wed, 07 May 2025 07:00:00 GMT",[],"2025-05-07-lowes-puts-project-expertise-into-every-hand.md","2025-05-07-openai-expands-leadership-with-fidji-simo",{"id":22669,"data":22671,"filePath":22676,"digest":22677,"rendered":22678,"legacyId":22686},{"title":22672,"description":22673,"summary":22673,"pubDate":22674,"source":19,"url":22675,"thumbnail":21},"OpenAI Expands Leadership with Fidji Simo","Read the message Sam shared with the company earlier today.",["Date","2025-05-07T21:00:00.000Z"],"https://openai.com/blog/leadership-expansion-with-fidji-simo","src/content/posts/2025-05-07-openai-expands-leadership-with-fidji-simo.md","38258d268e7bb99f",{"html":25,"metadata":22679},{"headings":22680,"localImagePaths":22681,"remoteImagePaths":22682,"frontmatter":22683,"imagePaths":22685},[],[],[],{"title":22672,"description":22673,"summary":22673,"pubDate":22684,"source":19,"url":22675,"thumbnail":21},"Wed, 07 May 2025 21:00:00 GMT",[],"2025-05-07-openai-expands-leadership-with-fidji-simo.md","2025-05-07-openais-response-to-the-department-of-energy-on-ai-infrastructure",{"id":22687,"data":22689,"filePath":22694,"digest":22695,"rendered":22696,"legacyId":22704},{"title":22690,"description":22691,"summary":22691,"pubDate":22692,"source":19,"url":22693,"thumbnail":21},"OpenAI’s response to the Department of Energy on AI infrastructure","Why infrastructure is destiny and how the US can seize it.",["Date","2025-05-07T18:30:00.000Z"],"https://openai.com/global-affairs/response-to-department-of-energy","src/content/posts/2025-05-07-openais-response-to-the-department-of-energy-on-ai-infrastructure.md","470f9a20ed6ec78d",{"html":25,"metadata":22697},{"headings":22698,"localImagePaths":22699,"remoteImagePaths":22700,"frontmatter":22701,"imagePaths":22703},[],[],[],{"title":22690,"description":22691,"summary":22691,"pubDate":22702,"source":19,"url":22693,"thumbnail":21},"Wed, 07 May 2025 18:30:00 GMT",[],"2025-05-07-openais-response-to-the-department-of-energy-on-ai-infrastructure.md","2025-05-07-the-san-antonio-spurs-use-chatgpt-to-scale-impact-on-and-off-the-court",{"id":22705,"data":22707,"filePath":22712,"digest":22713,"rendered":22714,"legacyId":22722},{"title":22708,"description":22709,"summary":22709,"pubDate":22710,"source":19,"url":22711,"thumbnail":21},"The San Antonio Spurs use ChatGPT to scale impact on and off the court","Discover how the San Antonio Spurs are using custom GPTs to enhance fan engagement, streamline operations, and drive innovation across teams.",["Date","2025-05-07T09:00:00.000Z"],"https://openai.com/blog/san-antonio-spurs","src/content/posts/2025-05-07-the-san-antonio-spurs-use-chatgpt-to-scale-impact-on-and-off-the-court.md","3d53c33a670422ad",{"html":25,"metadata":22715},{"headings":22716,"localImagePaths":22717,"remoteImagePaths":22718,"frontmatter":22719,"imagePaths":22721},[],[],[],{"title":22708,"description":22709,"summary":22709,"pubDate":22720,"source":19,"url":22711,"thumbnail":21},"Wed, 07 May 2025 09:00:00 GMT",[],"2025-05-07-the-san-antonio-spurs-use-chatgpt-to-scale-impact-on-and-off-the-court.md","2025-05-11-lerobot-community-datasets-the-imagenet-of-robotics-when-and-how",{"id":22723,"data":22725,"filePath":22731,"digest":22732,"rendered":22733,"legacyId":22741},{"title":22726,"description":25,"summary":22727,"pubDate":22728,"source":2720,"url":22729,"thumbnail":22730},"LeRobot Community Datasets: The “ImageNet” of Robotics — When and How?","LeRobot Community Datasets: The “ImageNet” of Robotics — When and How? 🧭 TL;DR — Why This Blogpost? ...",["Date","2025-05-11T00:00:00.000Z"],"https://huggingface.co/blog/lerobot-datasets","https://huggingface.co/blog/assets/195_lerobot_datasets/1.png","src/content/posts/2025-05-11-lerobot-community-datasets-the-imagenet-of-robotics-when-and-how.md","0bb26937a18bbb16",{"html":25,"metadata":22734},{"headings":22735,"localImagePaths":22736,"remoteImagePaths":22737,"frontmatter":22738,"imagePaths":22740},[],[],[],{"title":22726,"description":25,"summary":22727,"pubDate":22739,"source":2720,"url":22729,"thumbnail":22730},"Sun, 11 May 2025 00:00:00 GMT",[],"2025-05-11-lerobot-community-datasets-the-imagenet-of-robotics-when-and-how.md","2025-05-12-introducing-healthbench",{"id":22742,"data":22744,"filePath":22749,"digest":22750,"rendered":22751,"legacyId":22759},{"title":22745,"description":22746,"summary":22746,"pubDate":22747,"source":19,"url":22748,"thumbnail":21},"Introducing HealthBench","HealthBench is a new evaluation benchmark for AI in healthcare which evaluates models in realistic scenarios. Built with input from 250+ physicians, it aims to provide a shared standard for model performance and safety in health.",["Date","2025-05-12T10:30:00.000Z"],"https://openai.com/blog/healthbench","src/content/posts/2025-05-12-introducing-healthbench.md","6a4fbdd1d60cb6d2",{"html":25,"metadata":22752},{"headings":22753,"localImagePaths":22754,"remoteImagePaths":22755,"frontmatter":22756,"imagePaths":22758},[],[],[],{"title":22745,"description":22746,"summary":22746,"pubDate":22757,"source":19,"url":22748,"thumbnail":21},"Mon, 12 May 2025 10:30:00 GMT",[],"2025-05-12-introducing-healthbench.md","2025-05-12-vision-language-models-better-faster-stronger",{"id":22760,"data":22762,"filePath":22768,"digest":22769,"rendered":22770,"legacyId":22778},{"title":22763,"description":25,"summary":22764,"pubDate":22765,"source":2720,"url":22766,"thumbnail":22767},"Vision Language Models (Better, Faster, Stronger)","Vision Language Models (Better, Faster, Stronger) Motivation Vision Language Models (VLMs) are the t...",["Date","2025-05-12T00:00:00.000Z"],"https://huggingface.co/blog/vlms-2025","https://huggingface.co/blog/assets/vlms2/vlms2.png","src/content/posts/2025-05-12-vision-language-models-better-faster-stronger.md","5024685f7655298d",{"html":25,"metadata":22771},{"headings":22772,"localImagePaths":22773,"remoteImagePaths":22774,"frontmatter":22775,"imagePaths":22777},[],[],[],{"title":22763,"description":25,"summary":22764,"pubDate":22776,"source":2720,"url":22766,"thumbnail":22767},"Mon, 12 May 2025 00:00:00 GMT",[],"2025-05-12-vision-language-models-better-faster-stronger.md","2025-05-13-blazingly-fast-whisper-transcriptions-with-inference-endpoints",{"id":22779,"data":22781,"filePath":22787,"digest":22788,"rendered":22789,"legacyId":22797},{"title":22782,"description":25,"summary":22783,"pubDate":22784,"source":2720,"url":22785,"thumbnail":22786},"Blazingly fast whisper transcriptions with Inference Endpoints","Blazingly fast whisper transcriptions with Inference Endpoints Today we are happy to introduce a new...",["Date","2025-05-13T00:00:00.000Z"],"https://huggingface.co/blog/fast-whisper-endpoints","https://huggingface.co/blog/assets/fast-whisper-endpoints/thumbnail.png","src/content/posts/2025-05-13-blazingly-fast-whisper-transcriptions-with-inference-endpoints.md","6e677cf75b06407c",{"html":25,"metadata":22790},{"headings":22791,"localImagePaths":22792,"remoteImagePaths":22793,"frontmatter":22794,"imagePaths":22796},[],[],[],{"title":22782,"description":25,"summary":22783,"pubDate":22795,"source":2720,"url":22785,"thumbnail":22786},"Tue, 13 May 2025 00:00:00 GMT",[],"2025-05-13-blazingly-fast-whisper-transcriptions-with-inference-endpoints.md","2025-05-13-mit-department-of-economics-to-launch-james-m-and-cathleen-d-stone-center-on-inequality-and-shaping-the-future-of-work",{"id":22798,"data":22800,"filePath":22806,"digest":22807,"rendered":22808,"legacyId":22816},{"title":22801,"description":22802,"summary":22802,"pubDate":22803,"source":21340,"url":22804,"thumbnail":22805},"MIT Department of Economics to launch James M. and Cathleen D. Stone Center on Inequality and Shaping the Future of Work","With support from the Stone Foundation, the center will advance cutting-edge research and inform policy.",["Date","2025-05-13T20:35:00.000Z"],"https://news.mit.edu/2025/mit-economics-department-launches-james-cathleen-stone-center-inequality-shaping-future-work-0513","https://news.mit.edu/sites/default/files/images/202505/mit-campus.jpg","src/content/posts/2025-05-13-mit-department-of-economics-to-launch-james-m-and-cathleen-d-stone-center-on-inequality-and-shaping-the-future-of-work.md","aed1fa96b12f1a3b",{"html":25,"metadata":22809},{"headings":22810,"localImagePaths":22811,"remoteImagePaths":22812,"frontmatter":22813,"imagePaths":22815},[],[],[],{"title":22801,"description":22802,"summary":22802,"pubDate":22814,"source":21340,"url":22804,"thumbnail":22805},"Tue, 13 May 2025 16:35:00 -0400",[],"2025-05-13-mit-department-of-economics-to-launch-james-m-and-cathleen-d-stone-center-on-inequality-and-shaping-the-future-of-work.md","2025-05-14-ai-powers-expedias-marketing-evolution",{"id":22817,"data":22819,"filePath":22824,"digest":22825,"rendered":22826,"legacyId":22834},{"title":22820,"description":22821,"summary":22821,"pubDate":22822,"source":19,"url":22823,"thumbnail":21},"AI powers Expedia’s marketing evolution","A conversation with Jochen Koedijk, Chief Marketing Officer of Expedia Group.",["Date","2025-05-14T10:00:00.000Z"],"https://openai.com/blog/expedia-jochen-koedijk","src/content/posts/2025-05-14-ai-powers-expedias-marketing-evolution.md","64d1f7f0dd2ffe95",{"html":25,"metadata":22827},{"headings":22828,"localImagePaths":22829,"remoteImagePaths":22830,"frontmatter":22831,"imagePaths":22833},[],[],[],{"title":22820,"description":22821,"summary":22821,"pubDate":22832,"source":19,"url":22823,"thumbnail":21},"Wed, 14 May 2025 10:00:00 GMT",[],"2025-05-14-ai-powers-expedias-marketing-evolution.md","2025-05-14-alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms",{"id":22835,"data":22837,"filePath":22843,"digest":22844,"rendered":22845,"legacyId":22853},{"title":22838,"description":22839,"summary":22839,"pubDate":22840,"source":6423,"url":22841,"thumbnail":22842},"AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms","New AI agent evolves algorithms for math and practical applications in computing by combining the creativity of large language models with automated evaluators",["Date","2025-05-14T14:59:00.000Z"],"https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/","https://lh3.googleusercontent.com/tG6-MqdlvhQ-z7ENzGxR-kpGPPdPHbJ8UZtbTP66Rxi0UftTFU1yAvaBCVuigYuKvESMeEFf4jqNBVENFcZXEUnj8SSqj8zsop8UHAl0eD9A-hUCvQ=w1200-h630-n-nu","src/content/posts/2025-05-14-alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms.md","470571697aebc8d2",{"html":25,"metadata":22846},{"headings":22847,"localImagePaths":22848,"remoteImagePaths":22849,"frontmatter":22850,"imagePaths":22852},[],[],[],{"title":22838,"description":22839,"summary":22839,"pubDate":22851,"source":6423,"url":22841,"thumbnail":22842},"Wed, 14 May 2025 14:59:00 +0000",[],"2025-05-14-alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms.md","2025-05-14-study-shows-vision-language-models-cant-handle-queries-with-negation-words",{"id":22854,"data":22856,"filePath":22862,"digest":22863,"rendered":22864,"legacyId":22872},{"title":22857,"description":22858,"summary":22858,"pubDate":22859,"source":21340,"url":22860,"thumbnail":22861},"Study shows vision-language models can’t handle queries with negation words","Words like “no” and “not” can cause this popular class of AI models to fail unexpectedly in high-stakes settings, such as medical diagnosis.",["Date","2025-05-14T04:00:00.000Z"],"https://news.mit.edu/2025/study-shows-vision-language-models-cant-handle-negation-words-queries-0514","https://news.mit.edu/sites/default/files/images/202505/MIT-LMNegation-01-press.jpg","src/content/posts/2025-05-14-study-shows-vision-language-models-cant-handle-queries-with-negation-words.md","d704683a5e7b797c",{"html":25,"metadata":22865},{"headings":22866,"localImagePaths":22867,"remoteImagePaths":22868,"frontmatter":22869,"imagePaths":22871},[],[],[],{"title":22857,"description":22858,"summary":22858,"pubDate":22870,"source":21340,"url":22860,"thumbnail":22861},"Wed, 14 May 2025 00:00:00 -0400",[],"2025-05-14-study-shows-vision-language-models-cant-handle-queries-with-negation-words.md","2025-05-14-improving-hugging-face-model-access-for-kaggle-users",{"id":22873,"data":22875,"filePath":22881,"digest":22882,"rendered":22883,"legacyId":22891},{"title":22876,"description":25,"summary":22877,"pubDate":22878,"source":2720,"url":22879,"thumbnail":22880},"Improving Hugging Face Model Access for Kaggle Users","Improving Hugging Face Model Access for Kaggle Users Kaggle and Hugging Face users are part of one A...",["Date","2025-05-14T00:00:00.000Z"],"https://huggingface.co/blog/kaggle-integration","https://huggingface.co/blog/assets/kaggle-integration/thumbnail.png","src/content/posts/2025-05-14-improving-hugging-face-model-access-for-kaggle-users.md","01ac106740e878a1",{"html":25,"metadata":22884},{"headings":22885,"localImagePaths":22886,"remoteImagePaths":22887,"frontmatter":22888,"imagePaths":22890},[],[],[],{"title":22876,"description":25,"summary":22877,"pubDate":22889,"source":2720,"url":22879,"thumbnail":22880},"Wed, 14 May 2025 00:00:00 GMT",[],"2025-05-14-improving-hugging-face-model-access-for-kaggle-users.md","2025-05-15-the-transformers-library-standardizing-model-definitions",{"id":22892,"data":22894,"filePath":22900,"digest":22901,"rendered":22902,"legacyId":22910},{"title":22895,"description":25,"summary":22896,"pubDate":22897,"source":2720,"url":22898,"thumbnail":22899},"The Transformers Library: standardizing model definitions","The Transformers Library: standardizing model definitions TLDR: Going forward, we're aiming for Tran...",["Date","2025-05-15T00:00:00.000Z"],"https://huggingface.co/blog/transformers-model-definition","https://huggingface.co/blog/assets/transformers-model-definition/transformers-thumbnail.png","src/content/posts/2025-05-15-the-transformers-library-standardizing-model-definitions.md","ed20259410384f28",{"html":25,"metadata":22903},{"headings":22904,"localImagePaths":22905,"remoteImagePaths":22906,"frontmatter":22907,"imagePaths":22909},[],[],[],{"title":22895,"description":25,"summary":22896,"pubDate":22908,"source":2720,"url":22898,"thumbnail":22899},"Thu, 15 May 2025 00:00:00 GMT",[],"2025-05-15-the-transformers-library-standardizing-model-definitions.md","2025-05-15-with-ai-researchers-predict-the-location-of-virtually-any-protein-within-a-human-cell",{"id":22911,"data":22913,"filePath":22919,"digest":22920,"rendered":22921,"legacyId":22929},{"title":22914,"description":22915,"summary":22915,"pubDate":22916,"source":21340,"url":22917,"thumbnail":22918},"With AI, researchers predict the location of virtually any protein within a human cell","Trained with a joint understanding of protein and cell behavior, the model could help with diagnosing disease and developing new drugs.",["Date","2025-05-15T14:30:00.000Z"],"https://news.mit.edu/2025/researchers-predict-protein-location-within-human-cell-using-ai-0515","https://news.mit.edu/sites/default/files/images/202505/MIT-ProteinLocalization-01-press.jpg","src/content/posts/2025-05-15-with-ai-researchers-predict-the-location-of-virtually-any-protein-within-a-human-cell.md","a1297de4037fc8b0",{"html":25,"metadata":22922},{"headings":22923,"localImagePaths":22924,"remoteImagePaths":22925,"frontmatter":22926,"imagePaths":22928},[],[],[],{"title":22914,"description":22915,"summary":22915,"pubDate":22927,"source":21340,"url":22917,"thumbnail":22918},"Thu, 15 May 2025 10:30:00 -0400",[],"2025-05-15-with-ai-researchers-predict-the-location-of-virtually-any-protein-within-a-human-cell.md","2025-05-16-addendum-to-o3-and-o4-mini-system-card-codex",{"id":22930,"data":22932,"filePath":22937,"digest":22938,"rendered":22939,"legacyId":22947},{"title":22933,"description":22934,"summary":22934,"pubDate":22935,"source":19,"url":22936,"thumbnail":21},"Addendum to o3 and o4-mini system card: Codex","Codex is a cloud-based coding agent. Codex is powered by codex-1, a version of OpenAI o3 optimized for software engineering. codex-1 was trained using reinforcement learning on real-world coding tasks in a variety of environments to generate code that closely mirrors human style and PR preferences, adheres precisely to instructions, and iteratively runs tests until passing results are achieved.",["Date","2025-05-16T08:00:00.000Z"],"https://openai.com/blog/o3-o4-mini-codex-system-card-addendum","src/content/posts/2025-05-16-addendum-to-o3-and-o4-mini-system-card-codex.md","41d1f1c29a71162e",{"html":25,"metadata":22940},{"headings":22941,"localImagePaths":22942,"remoteImagePaths":22943,"frontmatter":22944,"imagePaths":22946},[],[],[],{"title":22933,"description":22934,"summary":22934,"pubDate":22945,"source":19,"url":22936,"thumbnail":21},"Fri, 16 May 2025 08:00:00 GMT",[],"2025-05-16-addendum-to-o3-and-o4-mini-system-card-codex.md","2025-05-16-introducing-codex",{"id":22948,"data":22950,"filePath":22955,"digest":22956,"rendered":22957,"legacyId":22964},{"title":22951,"description":22952,"summary":22952,"pubDate":22953,"source":19,"url":22954,"thumbnail":21},"Introducing Codex","Introducing Codex: a cloud-based software engineering agent that can work on many tasks in parallel, powered by codex-1. With Codex, developers can simultaneously deploy multiple agents to independently handle coding tasks such as writing features, answering questions about your codebase, fixing bugs, and proposing pull requests for review.",["Date","2025-05-16T08:00:00.000Z"],"https://openai.com/blog/introducing-codex","src/content/posts/2025-05-16-introducing-codex.md","aca793344979d58e",{"html":25,"metadata":22958},{"headings":22959,"localImagePaths":22960,"remoteImagePaths":22961,"frontmatter":22962,"imagePaths":22963},[],[],[],{"title":22951,"description":22952,"summary":22952,"pubDate":22945,"source":19,"url":22954,"thumbnail":21},[],"2025-05-16-introducing-codex.md","2025-05-16-llmの推論における-aha-moment-について調べてみた",{"id":22965,"data":22967,"filePath":22973,"digest":22974,"rendered":22975,"legacyId":22983},{"title":22968,"description":22969,"summary":22969,"pubDate":22970,"source":20789,"url":22971,"thumbnail":22972},"LLMの推論における “aha moment” について調べてみた","\u003Cp>こんにちは AIチームの戸田です 先日、LLMの 'aha moment' に関して興味を持ち、関連論文やWeb上の記事を読んでみたところ、賛否両論の様々な見解があり興味深かったので、今回はその内容を共有したいと思います [&#8230;]\u003C/p> \u003Cp>投稿 \u003Ca href='https://www.ai-shift.co.jp/techblog/5766' rel='nofollow'>LLMの推論における &#8220;aha moment&#8221; について調べてみた\u003C/a> は \u003Ca href='https://www.ai-shift.co.jp' rel='nofollow'>株式会社AI Shift\u003C/a> に最初に表示されました。\u003C/p>",["Date","2025-05-16T04:41:37.000Z"],"https://www.ai-shift.co.jp/techblog/5766","https://www.ai-shift.co.jp/wp-content/uploads/2025/05/d099d886ed65ef765625779e628d2c5f.png","src/content/posts/2025-05-16-llmの推論における-aha-moment-について調べてみた.md","b63c59f6a88d3d49",{"html":25,"metadata":22976},{"headings":22977,"localImagePaths":22978,"remoteImagePaths":22979,"frontmatter":22980,"imagePaths":22982},[],[],[],{"title":22968,"description":22969,"summary":22969,"pubDate":22981,"source":20789,"url":22971,"thumbnail":22972},"Fri, 16 May 2025 04:41:37 +0000",[],"2025-05-16-llmの推論における-aha-moment-について調べてみた.md","2025-05-19-microsoft-and-hugging-face-expand-collaboration",{"id":22984,"data":22986,"filePath":22992,"digest":22993,"rendered":22994,"legacyId":23002},{"title":22987,"description":25,"summary":22988,"pubDate":22989,"source":2720,"url":22990,"thumbnail":22991},"Microsoft and Hugging Face expand collaboration","Microsoft and Hugging Face expand collaboration to make open models easy to use on Azure Today at th...",["Date","2025-05-19T00:00:00.000Z"],"https://huggingface.co/blog/azure-ai-foundry","https://huggingface.co/blog/assets/azure-ai-foundry/satya-hf-build-compressed.png","src/content/posts/2025-05-19-microsoft-and-hugging-face-expand-collaboration.md","07a1f16c35358b95",{"html":25,"metadata":22995},{"headings":22996,"localImagePaths":22997,"remoteImagePaths":22998,"frontmatter":22999,"imagePaths":23001},[],[],[],{"title":22987,"description":25,"summary":22988,"pubDate":23000,"source":2720,"url":22990,"thumbnail":22991},"Mon, 19 May 2025 00:00:00 GMT",[],"2025-05-19-microsoft-and-hugging-face-expand-collaboration.md","2025-05-19-the-sweet-taste-of-a-new-idea",{"id":23003,"data":23005,"filePath":23011,"digest":23012,"rendered":23013,"legacyId":23021},{"title":23006,"description":23007,"summary":23007,"pubDate":23008,"source":21340,"url":23009,"thumbnail":23010},"The sweet taste of a new idea","Sendhil Mullainathan brings a lifetime of unique perspectives to research in behavioral economics and machine learning.",["Date","2025-05-19T20:30:00.000Z"],"https://news.mit.edu/2025/sweet-taste-new-idea-sendhil-mullainathan-0519","https://news.mit.edu/sites/default/files/images/202505/mit-sendhil-Mullainathan.jpg","src/content/posts/2025-05-19-the-sweet-taste-of-a-new-idea.md","5f9eba9fbe56a167",{"html":25,"metadata":23014},{"headings":23015,"localImagePaths":23016,"remoteImagePaths":23017,"frontmatter":23018,"imagePaths":23020},[],[],[],{"title":23006,"description":23007,"summary":23007,"pubDate":23019,"source":21340,"url":23009,"thumbnail":23010},"Mon, 19 May 2025 16:30:00 -0400",[],"2025-05-19-the-sweet-taste-of-a-new-idea.md","2025-05-20-advancing-geminis-security-safeguards",{"id":23022,"data":23024,"filePath":23030,"digest":23031,"rendered":23032,"legacyId":23040},{"title":23025,"description":23026,"summary":23026,"pubDate":23027,"source":6423,"url":23028,"thumbnail":23029},"Advancing Gemini's security safeguards","We’ve made Gemini 2.5 our most secure model family to date.",["Date","2025-05-20T09:45:00.000Z"],"https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/","https://lh3.googleusercontent.com/Uh_O6Nx1GWznAfODatYYz2sxiDekdb6HWnnSsy-cfmTxfjdUEEleh9w4cBdwUfBnyQBS-t1xW4UZXrMmC-rI6bz31hCrm5nHLt6Cp1FJAT7X9Upv5g=w1200-h630-n-nu","src/content/posts/2025-05-20-advancing-geminis-security-safeguards.md","bd10923ed892a839",{"html":25,"metadata":23033},{"headings":23034,"localImagePaths":23035,"remoteImagePaths":23036,"frontmatter":23037,"imagePaths":23039},[],[],[],{"title":23025,"description":23026,"summary":23026,"pubDate":23038,"source":6423,"url":23028,"thumbnail":23029},"Tue, 20 May 2025 09:45:00 +0000",[],"2025-05-20-advancing-geminis-security-safeguards.md","2025-05-20-announcing-gemma-3n-preview-powerful-efficient-mobile-first-ai",{"id":23041,"data":23043,"filePath":23049,"digest":23050,"rendered":23051,"legacyId":23058},{"title":23044,"description":23045,"summary":23045,"pubDate":23046,"source":6423,"url":23047,"thumbnail":23048},"Announcing Gemma 3n preview: Powerful, efficient, mobile-first AI","Gemma 3n is a cutting-edge open model designed for fast, multimodal AI on devices, featuring optimized performance, unique flexibility with a 2-in-1 model, and expanded multimodal understanding with audio, empowering developers to build live, interactive applications and sophisticated audio-centric experiences.",["Date","2025-05-20T09:45:00.000Z"],"https://deepmind.google/discover/blog/announcing-gemma-3n-preview-powerful-efficient-mobile-first-ai/","https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3n_Metadatal_RD2-V01.2e16d0ba.fill-1200x600.jpg","src/content/posts/2025-05-20-announcing-gemma-3n-preview-powerful-efficient-mobile-first-ai.md","7586e5cc85671a5c",{"html":25,"metadata":23052},{"headings":23053,"localImagePaths":23054,"remoteImagePaths":23055,"frontmatter":23056,"imagePaths":23057},[],[],[],{"title":23044,"description":23045,"summary":23045,"pubDate":23038,"source":6423,"url":23047,"thumbnail":23048},[],"2025-05-20-announcing-gemma-3n-preview-powerful-efficient-mobile-first-ai.md","2025-05-20-collaborators-healthcare-innovation-to-impact",{"id":23059,"data":23061,"filePath":23068,"digest":23069,"rendered":23070,"legacyId":23078},{"title":23062,"description":23063,"summary":23063,"pubDate":23064,"source":23065,"url":23066,"thumbnail":23067},"Collaborators: Healthcare Innovation to Impact","\u003Cp>In this discussion, Matthew Lungren, Jonathan Carlson, Smitha Saligrama, Will Guyman, and Cameron Runde explore how teams across Microsoft are working together to generate advanced AI capabilities and solutions for developers and clinicians around the globe. \u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/podcast/collaborators-healthcare-innovation-to-impact/'>Collaborators: Healthcare Innovation to Impact\u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-05-20T20:39:01.000Z"],"Microsoft Research Blog","https://www.microsoft.com/en-us/research/podcast/collaborators-healthcare-innovation-to-impact/","https://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE1Mu3b?ver=5c31&h=30","src/content/posts/2025-05-20-collaborators-healthcare-innovation-to-impact.md","774f03e03a251687",{"html":25,"metadata":23071},{"headings":23072,"localImagePaths":23073,"remoteImagePaths":23074,"frontmatter":23075,"imagePaths":23077},[],[],[],{"title":23062,"description":23063,"summary":23063,"pubDate":23076,"source":23065,"url":23066,"thumbnail":23067},"Tue, 20 May 2025 20:39:01 +0000",[],"2025-05-20-collaborators-healthcare-innovation-to-impact.md","2025-05-20-fuel-your-creativity-with-new-generative-media-models-and-tools",{"id":23079,"data":23081,"filePath":23087,"digest":23088,"rendered":23089,"legacyId":23096},{"title":23082,"description":23083,"summary":23083,"pubDate":23084,"source":6423,"url":23085,"thumbnail":23086},"Fuel your creativity with new generative media models and tools","Introducing Veo 3 and Imagen 4, and a new tool for filmmaking called Flow.",["Date","2025-05-20T09:45:00.000Z"],"https://deepmind.google/discover/blog/fuel-your-creativity-with-new-generative-media-models-and-tools/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_SS_1920x1080.width-1300.png","src/content/posts/2025-05-20-fuel-your-creativity-with-new-generative-media-models-and-tools.md","0244f5b321710add",{"html":25,"metadata":23090},{"headings":23091,"localImagePaths":23092,"remoteImagePaths":23093,"frontmatter":23094,"imagePaths":23095},[],[],[],{"title":23082,"description":23083,"summary":23083,"pubDate":23038,"source":6423,"url":23085,"thumbnail":23086},[],"2025-05-20-fuel-your-creativity-with-new-generative-media-models-and-tools.md","2025-05-20-gemini-25-our-most-intelligent-models-are-getting-even-better",{"id":23097,"data":23099,"filePath":23105,"digest":23106,"rendered":23107,"legacyId":23114},{"title":23100,"description":23101,"summary":23101,"pubDate":23102,"source":6423,"url":23103,"thumbnail":23104},"Gemini 2.5: Our most intelligent models are getting even better","Gemini 2.5 Pro continues to be loved by developers as the best model for coding, and 2.5 Flash is getting even better with a new update. We’re bringing new capabilities to our models, including Deep Think, an experimental enhanced reasoning mode for 2.5 Pro.",["Date","2025-05-20T09:45:00.000Z"],"https://deepmind.google/discover/blog/gemini-25-our-world-leading-model-is-getting-even-better/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/deep-think__key-art_16-9.width-1300.jpg","src/content/posts/2025-05-20-gemini-25-our-most-intelligent-models-are-getting-even-better.md","b6f30fe04ff0b1b7",{"html":25,"metadata":23108},{"headings":23109,"localImagePaths":23110,"remoteImagePaths":23111,"frontmatter":23112,"imagePaths":23113},[],[],[],{"title":23100,"description":23101,"summary":23101,"pubDate":23038,"source":6423,"url":23103,"thumbnail":23104},[],"2025-05-20-gemini-25-our-most-intelligent-models-are-getting-even-better.md","2025-05-20-our-vision-for-building-a-universal-ai-assistant",{"id":23115,"data":23117,"filePath":23123,"digest":23124,"rendered":23125,"legacyId":23132},{"title":23118,"description":23119,"summary":23119,"pubDate":23120,"source":6423,"url":23121,"thumbnail":23122},"Our vision for building a universal AI assistant","We’re extending Gemini to become a world model that can make plans and imagine new experiences by simulating aspects of the world.",["Date","2025-05-20T09:45:00.000Z"],"https://deepmind.google/discover/blog/our-vision-for-building-a-universal-ai-assistant/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_GeminiVision_SocialShare.width-1300.png","src/content/posts/2025-05-20-our-vision-for-building-a-universal-ai-assistant.md","bfcb0cdd2ce9f930",{"html":25,"metadata":23126},{"headings":23127,"localImagePaths":23128,"remoteImagePaths":23129,"frontmatter":23130,"imagePaths":23131},[],[],[],{"title":23118,"description":23119,"summary":23119,"pubDate":23038,"source":6423,"url":23121,"thumbnail":23122},[],"2025-05-20-our-vision-for-building-a-universal-ai-assistant.md","2025-05-20-synthid-detector-a-new-portal-to-help-identify-ai-generated-content",{"id":23133,"data":23135,"filePath":23141,"digest":23142,"rendered":23143,"legacyId":23150},{"title":23136,"description":23137,"summary":23137,"pubDate":23138,"source":6423,"url":23139,"thumbnail":23140},"SynthID Detector — a new portal to help identify AI-generated content","Learn about the new SynthID Detector portal we announced at I/O to help people understand how the content they see online was generated.",["Date","2025-05-20T09:45:00.000Z"],"https://deepmind.google/discover/blog/synthid-detector--a-new-portal-to-help-identify-ai-generated-content/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_Gemini_MOD_HEADER.width-1300.jpg","src/content/posts/2025-05-20-synthid-detector-a-new-portal-to-help-identify-ai-generated-content.md","c22f027f474e0943",{"html":25,"metadata":23144},{"headings":23145,"localImagePaths":23146,"remoteImagePaths":23147,"frontmatter":23148,"imagePaths":23149},[],[],[],{"title":23136,"description":23137,"summary":23137,"pubDate":23038,"source":6423,"url":23139,"thumbnail":23140},[],"2025-05-20-synthid-detector-a-new-portal-to-help-identify-ai-generated-content.md","2025-05-21-abstracts-aurora-with-megan-stanley-and-wessel-bruinsma",{"id":23151,"data":23153,"filePath":23158,"digest":23159,"rendered":23160,"legacyId":23168},{"title":23154,"description":23155,"summary":23155,"pubDate":23156,"source":23065,"url":23157,"thumbnail":23067},"Abstracts: Aurora with Megan Stanley and Wessel Bruinsma","\u003Cp>A new Nature paper explores Aurora, an AI model that redefines weather prediction with application to other environmental domains such as tropical cyclones. Hear from senior researchers Megan Stanley and Wessel Bruinsma as they share their groundbreaking work.\u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/podcast/abstracts-aurora-with-megan-stanley-and-wessel-bruinsma/'>Abstracts: Aurora with Megan Stanley and Wessel Bruinsma\u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-05-21T15:22:51.000Z"],"https://www.microsoft.com/en-us/research/podcast/abstracts-aurora-with-megan-stanley-and-wessel-bruinsma/","src/content/posts/2025-05-21-abstracts-aurora-with-megan-stanley-and-wessel-bruinsma.md","e4ec67be1f720a0a",{"html":25,"metadata":23161},{"headings":23162,"localImagePaths":23163,"remoteImagePaths":23164,"frontmatter":23165,"imagePaths":23167},[],[],[],{"title":23154,"description":23155,"summary":23155,"pubDate":23166,"source":23065,"url":23157,"thumbnail":23067},"Wed, 21 May 2025 15:22:51 +0000",[],"2025-05-21-abstracts-aurora-with-megan-stanley-and-wessel-bruinsma.md","2025-05-21-exploring-quantization-backends-in-diffusers",{"id":23169,"data":23171,"filePath":23177,"digest":23178,"rendered":23179,"legacyId":23187},{"title":23172,"description":25,"summary":23173,"pubDate":23174,"source":2720,"url":23175,"thumbnail":23176},"Exploring Quantization Backends in Diffusers","Exploring Quantization Backends in Diffusers Large diffusion models like Flux (a flow-based text-to-...",["Date","2025-05-21T00:00:00.000Z"],"https://huggingface.co/blog/diffusers-quantization","https://huggingface.co/blog/assets/diffusers-quantization/thumbnail.png","src/content/posts/2025-05-21-exploring-quantization-backends-in-diffusers.md","012ec5d08c343a17",{"html":25,"metadata":23180},{"headings":23181,"localImagePaths":23182,"remoteImagePaths":23183,"frontmatter":23184,"imagePaths":23186},[],[],[],{"title":23172,"description":25,"summary":23173,"pubDate":23185,"source":2720,"url":23175,"thumbnail":23176},"Wed, 21 May 2025 00:00:00 GMT",[],"2025-05-21-exploring-quantization-backends-in-diffusers.md","2025-05-21-learning-how-to-predict-rare-kinds-of-failures",{"id":23188,"data":23190,"filePath":23196,"digest":23197,"rendered":23198,"legacyId":23206},{"title":23191,"description":23192,"summary":23192,"pubDate":23193,"source":21340,"url":23194,"thumbnail":23195},"Learning how to predict rare kinds of failures","Researchers are developing algorithms to predict failures when automation meets the real world in areas like air traffic scheduling or autonomous vehicles.",["Date","2025-05-21T20:35:00.000Z"],"https://news.mit.edu/2025/learning-how-predict-rare-kinds-failures-0521","https://news.mit.edu/sites/default/files/images/202505/mit-rare-event-modeling.jpg","src/content/posts/2025-05-21-learning-how-to-predict-rare-kinds-of-failures.md","2dabe086ec850ab3",{"html":25,"metadata":23199},{"headings":23200,"localImagePaths":23201,"remoteImagePaths":23202,"frontmatter":23203,"imagePaths":23205},[],[],[],{"title":23191,"description":23192,"summary":23192,"pubDate":23204,"source":21340,"url":23194,"thumbnail":23195},"Wed, 21 May 2025 16:35:00 -0400",[],"2025-05-21-learning-how-to-predict-rare-kinds-of-failures.md","2025-05-21-nanovlm-the-simplest-repository-to-train-your-vlm-in-pure-pytorch",{"id":23207,"data":23209,"filePath":23215,"digest":23216,"rendered":23217,"legacyId":23224},{"title":23210,"description":25,"summary":23211,"pubDate":23212,"source":2720,"url":23213,"thumbnail":23214},"nanoVLM: The simplest repository to train your VLM in pure PyTorch","nanoVLM: The simplest repository to train your VLM in pure PyTorch nanoVLM is the simplest way to ge...",["Date","2025-05-21T00:00:00.000Z"],"https://huggingface.co/blog/nanovlm","https://huggingface.co/blog/assets/nanovlm/thumbnail.png","src/content/posts/2025-05-21-nanovlm-the-simplest-repository-to-train-your-vlm-in-pure-pytorch.md","7c517afe51791c56",{"html":25,"metadata":23218},{"headings":23219,"localImagePaths":23220,"remoteImagePaths":23221,"frontmatter":23222,"imagePaths":23223},[],[],[],{"title":23210,"description":25,"summary":23211,"pubDate":23185,"source":2720,"url":23213,"thumbnail":23214},[],"2025-05-21-nanovlm-the-simplest-repository-to-train-your-vlm-in-pure-pytorch.md","2025-05-21-new-tools-and-features-in-the-responses-api",{"id":23225,"data":23227,"filePath":23232,"digest":23233,"rendered":23234,"legacyId":23242},{"title":23228,"description":23229,"summary":23229,"pubDate":23230,"source":19,"url":23231,"thumbnail":21},"New tools and features in the Responses API","New features in the Responses API: Remote MCP, image gen, Code Interpreter, and more. Powering faster, smarter agents with GPT-4o & o-series models, plus new features for reliability and efficiency.",["Date","2025-05-21T08:00:00.000Z"],"https://openai.com/blog/new-tools-and-features-in-the-responses-api","src/content/posts/2025-05-21-new-tools-and-features-in-the-responses-api.md","d0bc39829d793d8d",{"html":25,"metadata":23235},{"headings":23236,"localImagePaths":23237,"remoteImagePaths":23238,"frontmatter":23239,"imagePaths":23241},[],[],[],{"title":23228,"description":23229,"summary":23229,"pubDate":23240,"source":19,"url":23231,"thumbnail":21},"Wed, 21 May 2025 08:00:00 GMT",[],"2025-05-21-new-tools-and-features-in-the-responses-api.md","2025-05-21-sam-jony",{"id":23243,"data":23245,"filePath":23249,"digest":23250,"rendered":23251,"legacyId":23258},{"title":23246,"description":23246,"summary":23246,"pubDate":23247,"source":19,"url":23248,"thumbnail":21},"Sam & Jony",["Date","2025-05-21T00:00:00.000Z"],"https://openai.com/sam-and-jony","src/content/posts/2025-05-21-sam-jony.md","a9c3dcbca1e6b1e2",{"html":25,"metadata":23252},{"headings":23253,"localImagePaths":23254,"remoteImagePaths":23255,"frontmatter":23256,"imagePaths":23257},[],[],[],{"title":23246,"description":23246,"summary":23246,"pubDate":23185,"source":19,"url":23248,"thumbnail":21},[],"2025-05-21-sam-jony.md","2025-05-22-abstracts-zero-shot-models-in-single-cell-biology-with-alex-lu",{"id":23259,"data":23261,"filePath":23266,"digest":23267,"rendered":23268,"legacyId":23276},{"title":23262,"description":23263,"summary":23263,"pubDate":23264,"source":23065,"url":23265,"thumbnail":23067},"Abstracts: Zero-shot models in single-cell biology with Alex Lu","\u003Cp>The emergence of foundation models has sparked interest in applications to single-cell biology, but when tested in zero-shot settings, they underperform compared to simpler methods. Alex Lu shares insights on why more research on AI models is needed in biological applications.\u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/podcast/abstracts-zero-shot-models-in-single-cell-biology-with-alex-lu/'>Abstracts: Zero-shot models in single-cell biology with Alex Lu\u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-05-22T15:58:00.000Z"],"https://www.microsoft.com/en-us/research/podcast/abstracts-zero-shot-models-in-single-cell-biology-with-alex-lu/","src/content/posts/2025-05-22-abstracts-zero-shot-models-in-single-cell-biology-with-alex-lu.md","9b89707aecc29568",{"html":25,"metadata":23269},{"headings":23270,"localImagePaths":23271,"remoteImagePaths":23272,"frontmatter":23273,"imagePaths":23275},[],[],[],{"title":23262,"description":23263,"summary":23263,"pubDate":23274,"source":23065,"url":23265,"thumbnail":23067},"Thu, 22 May 2025 15:58:00 +0000",[],"2025-05-22-abstracts-zero-shot-models-in-single-cell-biology-with-alex-lu.md","2025-05-22-ai-learns-how-vision-and-sound-are-connected-without-human-intervention",{"id":23277,"data":23279,"filePath":23285,"digest":23286,"rendered":23287,"legacyId":23295},{"title":23280,"description":23281,"summary":23281,"pubDate":23282,"source":21340,"url":23283,"thumbnail":23284},"AI learns how vision and sound are connected, without human intervention","This new machine-learning model can match corresponding audio and visual data, which could someday help robots interact in the real world.",["Date","2025-05-22T04:00:00.000Z"],"https://news.mit.edu/2025/ai-learns-how-vision-and-sound-are-connected-without-human-intervention-0522","https://news.mit.edu/sites/default/files/images/202505/MIT-AV-Learning-01-press.jpg","src/content/posts/2025-05-22-ai-learns-how-vision-and-sound-are-connected-without-human-intervention.md","6d19a8e934b4bf17",{"html":25,"metadata":23288},{"headings":23289,"localImagePaths":23290,"remoteImagePaths":23291,"frontmatter":23292,"imagePaths":23294},[],[],[],{"title":23280,"description":23281,"summary":23281,"pubDate":23293,"source":21340,"url":23283,"thumbnail":23284},"Thu, 22 May 2025 00:00:00 -0400",[],"2025-05-22-ai-learns-how-vision-and-sound-are-connected-without-human-intervention.md","2025-05-22-introducing-stargate-uae",{"id":23296,"data":23298,"filePath":23303,"digest":23304,"rendered":23305,"legacyId":23313},{"title":23299,"description":23300,"summary":23300,"pubDate":23301,"source":19,"url":23302,"thumbnail":21},"Introducing Stargate UAE","We’re launching Stargate UAE – the first international deployment of Stargate, OpenAI’s AI infrastructure platform.",["Date","2025-05-22T00:00:00.000Z"],"https://openai.com/blog/introducing-stargate-uae","src/content/posts/2025-05-22-introducing-stargate-uae.md","7ffc45aef33ce32c",{"html":25,"metadata":23306},{"headings":23307,"localImagePaths":23308,"remoteImagePaths":23309,"frontmatter":23310,"imagePaths":23312},[],[],[],{"title":23299,"description":23300,"summary":23300,"pubDate":23311,"source":19,"url":23302,"thumbnail":21},"Thu, 22 May 2025 00:00:00 GMT",[],"2025-05-22-introducing-stargate-uae.md","2025-05-22-openai-deutschland",{"id":23314,"data":23316,"filePath":23321,"digest":23322,"rendered":23323,"legacyId":23331},{"title":23317,"description":23318,"summary":23318,"pubDate":23319,"source":19,"url":23320,"thumbnail":21},"OpenAI Deutschland","OpenAI announces the opening of its first office in Germany, based in Munich.",["Date","2025-05-22T23:00:00.000Z"],"https://openai.com/blog/openai-deutschland","src/content/posts/2025-05-22-openai-deutschland.md","c6cb8b95bb5abb79",{"html":25,"metadata":23324},{"headings":23325,"localImagePaths":23326,"remoteImagePaths":23327,"frontmatter":23328,"imagePaths":23330},[],[],[],{"title":23317,"description":23318,"summary":23318,"pubDate":23329,"source":19,"url":23320,"thumbnail":21},"Thu, 22 May 2025 23:00:00 GMT",[],"2025-05-22-openai-deutschland.md","2025-05-22-shipping-code-faster-with-o3-o4-mini-and-gpt-41",{"id":23332,"data":23334,"filePath":23339,"digest":23340,"rendered":23341,"legacyId":23349},{"title":23335,"description":23336,"summary":23336,"pubDate":23337,"source":19,"url":23338,"thumbnail":21},"Shipping code faster with o3, o4-mini, and GPT-4.1","CodeRabbit uses OpenAI models to revolutionize code reviews—boosting accuracy, accelerating PR merges, and helping developers ship faster with fewer bugs and higher ROI.",["Date","2025-05-22T10:25:00.000Z"],"https://openai.com/blog/coderabbit","src/content/posts/2025-05-22-shipping-code-faster-with-o3-o4-mini-and-gpt-41.md","ab4ae4b36aa61a42",{"html":25,"metadata":23342},{"headings":23343,"localImagePaths":23344,"remoteImagePaths":23345,"frontmatter":23346,"imagePaths":23348},[],[],[],{"title":23335,"description":23336,"summary":23336,"pubDate":23347,"source":19,"url":23338,"thumbnail":21},"Thu, 22 May 2025 10:25:00 GMT",[],"2025-05-22-shipping-code-faster-with-o3-o4-mini-and-gpt-41.md","2025-05-23-addendum-to-openai-o3-and-o4-mini-system-card-openai-o3-operator",{"id":23350,"data":23352,"filePath":23357,"digest":23358,"rendered":23359,"legacyId":23367},{"title":23353,"description":23354,"summary":23354,"pubDate":23355,"source":19,"url":23356,"thumbnail":21},"Addendum to OpenAI o3 and o4-mini system card: OpenAI o3 Operator","We are replacing the existing GPT-4o-based model for Operator with a version based on OpenAI o3. The API version will remain based on 4o.",["Date","2025-05-23T00:00:00.000Z"],"https://openai.com/blog/o3-o4-mini-system-card-addendum-operator-o3","src/content/posts/2025-05-23-addendum-to-openai-o3-and-o4-mini-system-card-openai-o3-operator.md","a58832d2d159f824",{"html":25,"metadata":23360},{"headings":23361,"localImagePaths":23362,"remoteImagePaths":23363,"frontmatter":23364,"imagePaths":23366},[],[],[],{"title":23353,"description":23354,"summary":23354,"pubDate":23365,"source":19,"url":23356,"thumbnail":21},"Fri, 23 May 2025 00:00:00 GMT",[],"2025-05-23-addendum-to-openai-o3-and-o4-mini-system-card-openai-o3-operator.md","2025-05-23-dell-enterprise-hub-is-all-you-need-to-build-ai-on-premises",{"id":23368,"data":23370,"filePath":23376,"digest":23377,"rendered":23378,"legacyId":23385},{"title":23371,"description":25,"summary":23372,"pubDate":23373,"source":2720,"url":23374,"thumbnail":23375},"Dell Enterprise Hub is all you need to build AI on premises","Dell Enterprise Hub is all you need to build AI on premises This week at Dell Tech World, we announc...",["Date","2025-05-23T00:00:00.000Z"],"https://huggingface.co/blog/dell-ai-applications","https://huggingface.co/blog/assets/dell-ai-applications/dell-post-thumbnail.png","src/content/posts/2025-05-23-dell-enterprise-hub-is-all-you-need-to-build-ai-on-premises.md","d5741563ac600208",{"html":25,"metadata":23379},{"headings":23380,"localImagePaths":23381,"remoteImagePaths":23382,"frontmatter":23383,"imagePaths":23384},[],[],[],{"title":23371,"description":25,"summary":23372,"pubDate":23365,"source":2720,"url":23374,"thumbnail":23375},[],"2025-05-23-dell-enterprise-hub-is-all-you-need-to-build-ai-on-premises.md","2025-05-25-liger-grpo-meets-trl",{"id":23386,"data":23388,"filePath":23394,"digest":23395,"rendered":23396,"legacyId":23404},{"title":23389,"description":25,"summary":23390,"pubDate":23391,"source":2720,"url":23392,"thumbnail":23393},"🐯 Liger GRPO meets TRL","🐯 Liger GRPO meets TRL TL; DR Liger supercharges TRL’s Group Relative Policy Optimization GRPO Train...",["Date","2025-05-25T00:00:00.000Z"],"https://huggingface.co/blog/liger-grpo","https://huggingface.co/blog/assets/liger-grpo/thumbnail.png","src/content/posts/2025-05-25-liger-grpo-meets-trl.md","b4170b4143082a09",{"html":25,"metadata":23397},{"headings":23398,"localImagePaths":23399,"remoteImagePaths":23400,"frontmatter":23401,"imagePaths":23403},[],[],[],{"title":23389,"description":25,"summary":23390,"pubDate":23402,"source":2720,"url":23392,"thumbnail":23393},"Sun, 25 May 2025 00:00:00 GMT",[],"2025-05-25-liger-grpo-meets-trl.md","2025-05-23-tiny-agents-in-python-a-mcp-powered-agent-in-70-lines-of-code",{"id":23405,"data":23407,"filePath":23413,"digest":23414,"rendered":23415,"legacyId":23422},{"title":23408,"description":25,"summary":23409,"pubDate":23410,"source":2720,"url":23411,"thumbnail":23412},"Tiny Agents in Python: a MCP-powered agent in ~70 lines of code","Tiny Agents in Python: an MCP-powered agent in ~70 lines of code Inspired by Tiny Agents in JS, we p...",["Date","2025-05-23T00:00:00.000Z"],"https://huggingface.co/blog/python-tiny-agents","https://huggingface.co/blog/assets/python-tiny-agents/thumbnail.png","src/content/posts/2025-05-23-tiny-agents-in-python-a-mcp-powered-agent-in-70-lines-of-code.md","0c65125fd0a20ba9",{"html":25,"metadata":23416},{"headings":23417,"localImagePaths":23418,"remoteImagePaths":23419,"frontmatter":23420,"imagePaths":23421},[],[],[],{"title":23408,"description":25,"summary":23409,"pubDate":23365,"source":2720,"url":23411,"thumbnail":23412},[],"2025-05-23-tiny-agents-in-python-a-mcp-powered-agent-in-70-lines-of-code.md","2025-05-27-building-networks-of-data-science-talent",{"id":23423,"data":23425,"filePath":23431,"digest":23432,"rendered":23433,"legacyId":23441},{"title":23426,"description":23427,"summary":23427,"pubDate":23428,"source":21340,"url":23429,"thumbnail":23430},"Building networks of data science talent","Through collaborations with organizations like BREIT in Peru, the MIT Institute for Data, Systems, and Society is upskilling hundreds of learners around the world in data science and machine learning.",["Date","2025-05-27T20:11:00.000Z"],"https://news.mit.edu/2025/building-networks-data-science-talent-0527","https://news.mit.edu/sites/default/files/images/202504/mit-breit-idss-killian.jpg","src/content/posts/2025-05-27-building-networks-of-data-science-talent.md","bb511f69924d7e2e",{"html":25,"metadata":23434},{"headings":23435,"localImagePaths":23436,"remoteImagePaths":23437,"frontmatter":23438,"imagePaths":23440},[],[],[],{"title":23426,"description":23427,"summary":23427,"pubDate":23439,"source":21340,"url":23429,"thumbnail":23430},"Tue, 27 May 2025 16:11:00 -0400",[],"2025-05-27-building-networks-of-data-science-talent.md","2025-05-27-frodokem-a-conservative-quantum-safe-cryptographic-algorithm",{"id":23442,"data":23444,"filePath":23449,"digest":23450,"rendered":23451,"legacyId":23459},{"title":23445,"description":23446,"summary":23446,"pubDate":23447,"source":23065,"url":23448,"thumbnail":23067},"FrodoKEM: A conservative quantum-safe cryptographic algorithm","\u003Cp>The recent advances in quantum computing offer many advantages—but also challenge current cryptographic strategies. Learn how FrodoKEM could help strengthen security, even in a future with powerful quantum computers.\u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/blog/frodokem-a-conservative-quantum-safe-cryptographic-algorithm/'>FrodoKEM: A conservative quantum-safe cryptographic algorithm\u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-05-27T16:00:00.000Z"],"https://www.microsoft.com/en-us/research/blog/frodokem-a-conservative-quantum-safe-cryptographic-algorithm/","src/content/posts/2025-05-27-frodokem-a-conservative-quantum-safe-cryptographic-algorithm.md","7af3c852f081b23a",{"html":25,"metadata":23452},{"headings":23453,"localImagePaths":23454,"remoteImagePaths":23455,"frontmatter":23456,"imagePaths":23458},[],[],[],{"title":23445,"description":23446,"summary":23446,"pubDate":23457,"source":23065,"url":23448,"thumbnail":23067},"Tue, 27 May 2025 16:00:00 +0000",[],"2025-05-27-frodokem-a-conservative-quantum-safe-cryptographic-algorithm.md","2025-05-27-mit-announces-the-initiative-for-new-manufacturing",{"id":23460,"data":23462,"filePath":23468,"digest":23469,"rendered":23470,"legacyId":23478},{"title":23463,"description":23464,"summary":23464,"pubDate":23465,"source":21340,"url":23466,"thumbnail":23467},"MIT announces the Initiative for New Manufacturing","The Institute-wide effort aims to bolster industry and create jobs by driving innovation across vital manufacturing sectors.",["Date","2025-05-27T14:00:00.000Z"],"https://news.mit.edu/2025/mit-announces-initiative-for-new-manufacturing-0527","https://news.mit.edu/sites/default/files/images/202505/MIT-ManufacturingAnn-01-press.jpg","src/content/posts/2025-05-27-mit-announces-the-initiative-for-new-manufacturing.md","67a920135f5c62e7",{"html":25,"metadata":23471},{"headings":23472,"localImagePaths":23473,"remoteImagePaths":23474,"frontmatter":23475,"imagePaths":23477},[],[],[],{"title":23463,"description":23464,"summary":23464,"pubDate":23476,"source":21340,"url":23466,"thumbnail":23467},"Tue, 27 May 2025 10:00:00 -0400",[],"2025-05-27-mit-announces-the-initiative-for-new-manufacturing.md","2025-05-28-10-tips-for-10-years-of-google-photos",{"id":23479,"data":23481,"filePath":23488,"digest":23489,"rendered":23490,"legacyId":23498},{"title":23482,"description":23483,"summary":23483,"pubDate":23484,"source":23485,"url":23486,"thumbnail":23487},"10 tips for 10 years of Google Photos","Google Photos logo with the number ten and colorful confetti shapes.",["Date","2025-05-28T17:00:00.000Z"],"Google AI Blog","https://blog.google/products/photos/google-photos-10-years-tips-tricks/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/GooglePhotos-10year-Blog-header.width-1300.png","src/content/posts/2025-05-28-10-tips-for-10-years-of-google-photos.md","60265d736339926e",{"html":25,"metadata":23491},{"headings":23492,"localImagePaths":23493,"remoteImagePaths":23494,"frontmatter":23495,"imagePaths":23497},[],[],[],{"title":23482,"description":23483,"summary":23483,"pubDate":23496,"source":23485,"url":23486,"thumbnail":23487},"Wed, 28 May 2025 17:00:00 +0000",[],"2025-05-28-10-tips-for-10-years-of-google-photos.md","2025-05-28-an-anomaly-detection-framework-anyone-can-use",{"id":23499,"data":23501,"filePath":23507,"digest":23508,"rendered":23509,"legacyId":23517},{"title":23502,"description":23503,"summary":23503,"pubDate":23504,"source":21340,"url":23505,"thumbnail":23506},"An anomaly detection framework anyone can use","PhD student Sarah Alnegheimish wants to make machine learning systems accessible.",["Date","2025-05-28T20:00:00.000Z"],"https://news.mit.edu/2025/anomaly-detection-framework-anyone-can-use-sarah-alnegheimish-0528","https://news.mit.edu/sites/default/files/images/202505/mit-Sarah-Abdulaziz-Alnegheimish.JPG","src/content/posts/2025-05-28-an-anomaly-detection-framework-anyone-can-use.md","9ef0de02881f6adf",{"html":25,"metadata":23510},{"headings":23511,"localImagePaths":23512,"remoteImagePaths":23513,"frontmatter":23514,"imagePaths":23516},[],[],[],{"title":23502,"description":23503,"summary":23503,"pubDate":23515,"source":21340,"url":23505,"thumbnail":23506},"Wed, 28 May 2025 16:00:00 -0400",[],"2025-05-28-an-anomaly-detection-framework-anyone-can-use.md","2025-05-28-codeagents-structure-a-better-way-to-execute-actions",{"id":23518,"data":23520,"filePath":23526,"digest":23527,"rendered":23528,"legacyId":23536},{"title":23521,"description":25,"summary":23522,"pubDate":23523,"source":2720,"url":23524,"thumbnail":23525},"CodeAgents + Structure: A Better Way to Execute Actions","CodeAgents + Structure: A Better Way to Execute Actions Today we're sharing research that bridges tw...",["Date","2025-05-28T00:00:00.000Z"],"https://huggingface.co/blog/structured-codeagent","https://huggingface.co/blog/assets/structured-codeagent/thumbnail-codeagent.png","src/content/posts/2025-05-28-codeagents-structure-a-better-way-to-execute-actions.md","894eef78cb5cc291",{"html":25,"metadata":23529},{"headings":23530,"localImagePaths":23531,"remoteImagePaths":23532,"frontmatter":23533,"imagePaths":23535},[],[],[],{"title":23521,"description":25,"summary":23522,"pubDate":23534,"source":2720,"url":23524,"thumbnail":23525},"Wed, 28 May 2025 00:00:00 GMT",[],"2025-05-28-codeagents-structure-a-better-way-to-execute-actions.md","2025-05-28-rationale-engineering-generates-a-compact-new-tool-for-gene-therapy",{"id":23537,"data":23539,"filePath":23545,"digest":23546,"rendered":23547,"legacyId":23555},{"title":23540,"description":23541,"summary":23541,"pubDate":23542,"source":21340,"url":23543,"thumbnail":23544},"Rationale engineering generates a compact new tool for gene therapy","Researchers redesign a compact RNA-guided enzyme from bacteria, making it an efficient editor of human DNA.",["Date","2025-05-28T20:15:00.000Z"],"https://news.mit.edu/2025/rationale-engineering-generates-compact-new-tool-gene-therapy-0528","https://news.mit.edu/sites/default/files/images/202505/phylogenetic-tree.jpg","src/content/posts/2025-05-28-rationale-engineering-generates-a-compact-new-tool-for-gene-therapy.md","a7f414f2d7a4d596",{"html":25,"metadata":23548},{"headings":23549,"localImagePaths":23550,"remoteImagePaths":23551,"frontmatter":23552,"imagePaths":23554},[],[],[],{"title":23540,"description":23541,"summary":23541,"pubDate":23553,"source":21340,"url":23543,"thumbnail":23544},"Wed, 28 May 2025 16:15:00 -0400",[],"2025-05-28-rationale-engineering-generates-a-compact-new-tool-for-gene-therapy.md","2025-05-29-creating-websites-in-minutes-with-ai-website-builder",{"id":23556,"data":23558,"filePath":23563,"digest":23564,"rendered":23565,"legacyId":23573},{"title":23559,"description":23560,"summary":23560,"pubDate":23561,"source":19,"url":23562,"thumbnail":21},"Creating websites in minutes with AI Website Builder","Wix’s AI Website Builder, powered by OpenAI, lets anyone create a full website in minutes—just by describing their idea in a conversation.",["Date","2025-05-29T00:00:00.000Z"],"https://openai.com/blog/wix","src/content/posts/2025-05-29-creating-websites-in-minutes-with-ai-website-builder.md","f61f886089ec91e4",{"html":25,"metadata":23566},{"headings":23567,"localImagePaths":23568,"remoteImagePaths":23569,"frontmatter":23570,"imagePaths":23572},[],[],[],{"title":23559,"description":23560,"summary":23560,"pubDate":23571,"source":19,"url":23562,"thumbnail":21},"Thu, 29 May 2025 00:00:00 GMT",[],"2025-05-29-creating-websites-in-minutes-with-ai-website-builder.md","2025-05-29-what-ais-impact-on-individuals-means-for-the-health-workforce-and-industry",{"id":23574,"data":23576,"filePath":23581,"digest":23582,"rendered":23583,"legacyId":23591},{"title":23577,"description":23578,"summary":23578,"pubDate":23579,"source":23065,"url":23580,"thumbnail":23067},"What AI’s impact on individuals means for the health workforce and industry","\u003Cp>Ethan Mollick and Azeem Azhar, thought leaders at the forefront of AI’s influence on work, education, and society, discuss the impact of AI at the individual level and what that means for the healthcare workforce and the organizations and systems in medicine.\u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/podcast/what-ais-impact-on-individuals-means-for-the-health-workforce-and-industry/'>What AI&#8217;s impact on individuals means for the health workforce and industry\u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-05-29T15:13:48.000Z"],"https://www.microsoft.com/en-us/research/podcast/what-ais-impact-on-individuals-means-for-the-health-workforce-and-industry/","src/content/posts/2025-05-29-what-ais-impact-on-individuals-means-for-the-health-workforce-and-industry.md","094402cfd909b924",{"html":25,"metadata":23584},{"headings":23585,"localImagePaths":23586,"remoteImagePaths":23587,"frontmatter":23588,"imagePaths":23590},[],[],[],{"title":23577,"description":23578,"summary":23578,"pubDate":23589,"source":23065,"url":23580,"thumbnail":23067},"Thu, 29 May 2025 15:13:48 +0000",[],"2025-05-29-what-ais-impact-on-individuals-means-for-the-health-workforce-and-industry.md","2025-05-30-e2e音声対話api構築プラットフォーム最新動向の調査と自律型音声対話システムの展望",{"id":23592,"data":23594,"filePath":23600,"digest":23601,"rendered":23602,"legacyId":23610},{"title":23595,"description":23596,"summary":23596,"pubDate":23597,"source":20789,"url":23598,"thumbnail":23599},"E2E音声対話API・構築プラットフォーム最新動向の調査と自律型音声対話システムの展望","\u003Cp>はじめに こんにちは、AIチームの大竹です。 近年、音声対話アプリケーションの進化が目覚ましく、顧客対応の自動化や業務効率化への期待が高まっています。弊社のAI Messenger Voicebotも例外ではなく、最先端 [&#8230;]\u003C/p> \u003Cp>投稿 \u003Ca href='https://www.ai-shift.co.jp/techblog/5852' rel='nofollow'>E2E音声対話API・構築プラットフォーム最新動向の調査と自律型音声対話システムの展望\u003C/a> は \u003Ca href='https://www.ai-shift.co.jp' rel='nofollow'>株式会社AI Shift\u003C/a> に最初に表示されました。\u003C/p>",["Date","2025-05-30T01:38:01.000Z"],"https://www.ai-shift.co.jp/techblog/5852","https://www.ai-shift.co.jp/wp-content/uploads/2025/05/icon.png","src/content/posts/2025-05-30-e2e音声対話api構築プラットフォーム最新動向の調査と自律型音声対話システムの展望.md","c6001f91dba4735f",{"html":25,"metadata":23603},{"headings":23604,"localImagePaths":23605,"remoteImagePaths":23606,"frontmatter":23607,"imagePaths":23609},[],[],[],{"title":23595,"description":23596,"summary":23596,"pubDate":23608,"source":20789,"url":23598,"thumbnail":23599},"Fri, 30 May 2025 01:38:01 +0000",[],"2025-05-30-e2e音声対話api構築プラットフォーム最新動向の調査と自律型音声対話システムの展望.md","2025-05-30-how-google-is-driving-a-new-era-of-american-innovation-in-iowa",{"id":23611,"data":23613,"filePath":23619,"digest":23620,"rendered":23621,"legacyId":23629},{"title":23614,"description":23615,"summary":23615,"pubDate":23616,"source":23485,"url":23617,"thumbnail":23618},"How Google is driving a new era of American innovation in Iowa.","A group of six people, five women and one man, stand smiling on a white platform. They are positioned outdoors with a large data center featuring a Google logo and an American flag hanging from it in the background. Construction equipment, including a yellow crane, is visible next to the building.",["Date","2025-05-30T19:08:00.000Z"],"https://blog.google/feed/new-7-billion-investment-iowa/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Iowa_SS.max-1440x810.png","src/content/posts/2025-05-30-how-google-is-driving-a-new-era-of-american-innovation-in-iowa.md","42312bd0c7a202c2",{"html":25,"metadata":23622},{"headings":23623,"localImagePaths":23624,"remoteImagePaths":23625,"frontmatter":23626,"imagePaths":23628},[],[],[],{"title":23614,"description":23615,"summary":23615,"pubDate":23627,"source":23485,"url":23617,"thumbnail":23618},"Fri, 30 May 2025 19:08:00 +0000",[],"2025-05-30-how-google-is-driving-a-new-era-of-american-innovation-in-iowa.md","2025-06-02-3-questions-how-to-help-students-recognize-potential-bias-in-their-ai-datasets",{"id":23630,"data":23632,"filePath":23638,"digest":23639,"rendered":23640,"legacyId":23648},{"title":23633,"description":23634,"summary":23634,"pubDate":23635,"source":21340,"url":23636,"thumbnail":23637},"3 Questions: How to help students recognize potential bias in their AI datasets","Courses on developing AI models for health care need to focus more on identifying and addressing bias, says Leo Anthony Celi.",["Date","2025-06-02T14:30:00.000Z"],"https://news.mit.edu/2025/3-questions-recognizing-potential-bias-in-ai-datasets-0602","https://news.mit.edu/sites/default/files/images/202506/MIT_AI-Health-Data-01.jpg","src/content/posts/2025-06-02-3-questions-how-to-help-students-recognize-potential-bias-in-their-ai-datasets.md","2d7fbefd9fbaa3ea",{"html":25,"metadata":23641},{"headings":23642,"localImagePaths":23643,"remoteImagePaths":23644,"frontmatter":23645,"imagePaths":23647},[],[],[],{"title":23633,"description":23634,"summary":23634,"pubDate":23646,"source":21340,"url":23636,"thumbnail":23637},"Mon, 02 Jun 2025 10:30:00 -0400",[],"2025-06-02-3-questions-how-to-help-students-recognize-potential-bias-in-their-ai-datasets.md","2025-06-02-ai-stirs-up-the-recipe-for-concrete-in-mit-study",{"id":23649,"data":23651,"filePath":23657,"digest":23658,"rendered":23659,"legacyId":23667},{"title":23652,"description":23653,"summary":23653,"pubDate":23654,"source":21340,"url":23655,"thumbnail":23656},"AI stirs up the recipe for concrete in MIT study","With demand for cement alternatives rising, an MIT team uses machine learning to hunt for new ingredients across the scientific literature.",["Date","2025-06-02T19:45:00.000Z"],"https://news.mit.edu/2025/ai-stirs-recipe-for-concrete-0602","https://news.mit.edu/sites/default/files/images/202505/mit-Soroush-Mahjoubi.jpg","src/content/posts/2025-06-02-ai-stirs-up-the-recipe-for-concrete-in-mit-study.md","a061fb62dc3397e1",{"html":25,"metadata":23660},{"headings":23661,"localImagePaths":23662,"remoteImagePaths":23663,"frontmatter":23664,"imagePaths":23666},[],[],[],{"title":23652,"description":23653,"summary":23653,"pubDate":23665,"source":21340,"url":23655,"thumbnail":23656},"Mon, 02 Jun 2025 15:45:00 -0400",[],"2025-06-02-ai-stirs-up-the-recipe-for-concrete-in-mit-study.md","2025-06-02-teaching-ai-models-the-broad-strokes-to-sketch-more-like-humans-do",{"id":23668,"data":23670,"filePath":23676,"digest":23677,"rendered":23678,"legacyId":23686},{"title":23671,"description":23672,"summary":23672,"pubDate":23673,"source":21340,"url":23674,"thumbnail":23675},"Teaching AI models the broad strokes to sketch more like humans do","SketchAgent, a drawing system developed by MIT CSAIL researchers, sketches up concepts stroke-by-stroke, teaching language models to visually express concepts on their own and collaborate with humans.",["Date","2025-06-02T18:50:00.000Z"],"https://news.mit.edu/2025/teaching-ai-models-to-sketch-more-like-humans-0602","https://news.mit.edu/sites/default/files/images/202505/MIT-SketchAgent.jpg","src/content/posts/2025-06-02-teaching-ai-models-the-broad-strokes-to-sketch-more-like-humans-do.md","79b9dd1309b28428",{"html":25,"metadata":23679},{"headings":23680,"localImagePaths":23681,"remoteImagePaths":23682,"frontmatter":23683,"imagePaths":23685},[],[],[],{"title":23671,"description":23672,"summary":23672,"pubDate":23684,"source":21340,"url":23674,"thumbnail":23675},"Mon, 02 Jun 2025 14:50:00 -0400",[],"2025-06-02-teaching-ai-models-the-broad-strokes-to-sketch-more-like-humans-do.md","2025-06-02-拡散言語モデルの推論過程を眺めてみる",{"id":23687,"data":23689,"filePath":23695,"digest":23696,"rendered":23697,"legacyId":23705},{"title":23690,"description":23691,"summary":23691,"pubDate":23692,"source":20789,"url":23693,"thumbnail":23694},"拡散言語モデルの推論過程を眺めてみる","\u003Cp>こんにちはAIチームの戸田です。今回はGemini Diffusionの登場をきっかけに最近話題になった拡散言語モデルの推論過程に興味を持ち、その一例として拡散言語モデルのLLaDAの推論を実際に手元で確認してみた結果を [&#8230;]\u003C/p> \u003Cp>投稿 \u003Ca href='https://www.ai-shift.co.jp/techblog/5850' rel='nofollow'>拡散言語モデルの推論過程を眺めてみる\u003C/a> は \u003Ca href='https://www.ai-shift.co.jp' rel='nofollow'>株式会社AI Shift\u003C/a> に最初に表示されました。\u003C/p>",["Date","2025-06-02T00:13:43.000Z"],"https://www.ai-shift.co.jp/techblog/5850","https://www.ai-shift.co.jp/wp-content/uploads/2025/05/f81fd2e4c52864042852c112ce927ae2-1.png","src/content/posts/2025-06-02-拡散言語モデルの推論過程を眺めてみる.md","2a3d6a594e7a883f",{"html":25,"metadata":23698},{"headings":23699,"localImagePaths":23700,"remoteImagePaths":23701,"frontmatter":23702,"imagePaths":23704},[],[],[],{"title":23690,"description":23691,"summary":23691,"pubDate":23703,"source":20789,"url":23693,"thumbnail":23694},"Mon, 02 Jun 2025 00:13:43 +0000",[],"2025-06-02-拡散言語モデルの推論過程を眺めてみる.md","2025-06-03-advanced-audio-dialog-and-generation-with-gemini-25",{"id":23706,"data":23708,"filePath":23714,"digest":23715,"rendered":23716,"legacyId":23724},{"title":23709,"description":23710,"summary":23710,"pubDate":23711,"source":6423,"url":23712,"thumbnail":23713},"Advanced audio dialog and generation with Gemini 2.5","Gemini 2.5 has new capabilities in AI-powered audio dialog and generation.",["Date","2025-06-03T17:15:47.000Z"],"https://deepmind.google/discover/blog/advanced-audio-dialog-and-generation-with-gemini-25/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/capability__native-audio_16-9_121.width-1300.jpg","src/content/posts/2025-06-03-advanced-audio-dialog-and-generation-with-gemini-25.md","890405aeaa2e3521",{"html":25,"metadata":23717},{"headings":23718,"localImagePaths":23719,"remoteImagePaths":23720,"frontmatter":23721,"imagePaths":23723},[],[],[],{"title":23709,"description":23710,"summary":23710,"pubDate":23722,"source":6423,"url":23712,"thumbnail":23713},"Tue, 03 Jun 2025 17:15:47 +0000",[],"2025-06-03-advanced-audio-dialog-and-generation-with-gemini-25.md","2025-06-03-no-gpu-left-behind-unlocking-efficiency-with-co-located-vllm-in-trl",{"id":23725,"data":23727,"filePath":23732,"digest":23733,"rendered":23734,"legacyId":23742},{"title":23728,"description":25,"summary":23729,"pubDate":23730,"source":2720,"url":23731,"thumbnail":23393},"No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL","No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL 🚀 Introduction TRL supports tra...",["Date","2025-06-03T00:00:00.000Z"],"https://huggingface.co/blog/vllm-colocate","src/content/posts/2025-06-03-no-gpu-left-behind-unlocking-efficiency-with-co-located-vllm-in-trl.md","2c6c7572541a21ca",{"html":25,"metadata":23735},{"headings":23736,"localImagePaths":23737,"remoteImagePaths":23738,"frontmatter":23739,"imagePaths":23741},[],[],[],{"title":23728,"description":25,"summary":23729,"pubDate":23740,"source":2720,"url":23731,"thumbnail":23393},"Tue, 03 Jun 2025 00:00:00 GMT",[],"2025-06-03-no-gpu-left-behind-unlocking-efficiency-with-co-located-vllm-in-trl.md","2025-06-03-notebooklm-is-adding-a-new-way-to-share-your-own-notebooks-publicly",{"id":23743,"data":23745,"filePath":23751,"digest":23752,"rendered":23753,"legacyId":23761},{"title":23746,"description":23747,"summary":23747,"pubDate":23748,"source":23485,"url":23749,"thumbnail":23750},"NotebookLM is adding a new way to share your own notebooks publicly.","\u003Cimg src='https://storage.googleapis.com/gweb-uniblog-publish-prod/images/NotebookLMSharing_SS.max-600x600.format-webp.webp' />Many people who use NotebookLM already share their notebooks with classmates, coworkers, students and friends. Today, we're making sharing and curation easier — with pub…",["Date","2025-06-03T16:00:00.000Z"],"https://blog.google/technology/google-labs/notebooklm-public-notebooks/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/NotebookLMSharing_SS.max-1440x810.png","src/content/posts/2025-06-03-notebooklm-is-adding-a-new-way-to-share-your-own-notebooks-publicly.md","2e2a5bb7cb01ada9",{"html":25,"metadata":23754},{"headings":23755,"localImagePaths":23756,"remoteImagePaths":23757,"frontmatter":23758,"imagePaths":23760},[],[],[],{"title":23746,"description":23747,"summary":23747,"pubDate":23759,"source":23485,"url":23749,"thumbnail":23750},"Tue, 03 Jun 2025 16:00:00 +0000",[],"2025-06-03-notebooklm-is-adding-a-new-way-to-share-your-own-notebooks-publicly.md","2025-06-03-smolvla-efficient-vision-language-action-model-trained-on-lerobot-community-data",{"id":23762,"data":23764,"filePath":23770,"digest":23771,"rendered":23772,"legacyId":23779},{"title":23765,"description":25,"summary":23766,"pubDate":23767,"source":2720,"url":23768,"thumbnail":23769},"SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data","SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data 🧭TL;DR Today, we i...",["Date","2025-06-03T00:00:00.000Z"],"https://huggingface.co/blog/smolvla","https://huggingface.co/blog/assets/smolvla/SmolVLA_thumbnail.png","src/content/posts/2025-06-03-smolvla-efficient-vision-language-action-model-trained-on-lerobot-community-data.md","22b6557a9550948c",{"html":25,"metadata":23773},{"headings":23774,"localImagePaths":23775,"remoteImagePaths":23776,"frontmatter":23777,"imagePaths":23778},[],[],[],{"title":23765,"description":25,"summary":23766,"pubDate":23740,"source":2720,"url":23768,"thumbnail":23769},[],"2025-06-03-smolvla-efficient-vision-language-action-model-trained-on-lerobot-community-data.md","2025-06-03-teaching-ai-models-what-they-dont-know",{"id":23780,"data":23782,"filePath":23788,"digest":23789,"rendered":23790,"legacyId":23798},{"title":23783,"description":23784,"summary":23784,"pubDate":23785,"source":21340,"url":23786,"thumbnail":23787},"Teaching AI models what they don’t know","A team of MIT researchers founded Themis AI to quantify AI model uncertainty and address knowledge gaps.",["Date","2025-06-03T04:00:00.000Z"],"https://news.mit.edu/2025/themis-ai-teaches-ai-models-what-they-dont-know-0603","https://news.mit.edu/sites/default/files/images/202506/MIT-ThemisAI-01-Press.jpg","src/content/posts/2025-06-03-teaching-ai-models-what-they-dont-know.md","f419850847e6255c",{"html":25,"metadata":23791},{"headings":23792,"localImagePaths":23793,"remoteImagePaths":23794,"frontmatter":23795,"imagePaths":23797},[],[],[],{"title":23783,"description":23784,"summary":23784,"pubDate":23796,"source":21340,"url":23786,"thumbnail":23787},"Tue, 03 Jun 2025 00:00:00 -0400",[],"2025-06-03-teaching-ai-models-what-they-dont-know.md","2025-06-04-ai-breakthroughs-are-bringing-hope-to-cancer-research-and-treatment",{"id":23799,"data":23801,"filePath":23807,"digest":23808,"rendered":23809,"legacyId":23817},{"title":23802,"description":23803,"summary":23803,"pubDate":23804,"source":23485,"url":23805,"thumbnail":23806},"AI breakthroughs are bringing hope to cancer research and treatment","A presentation slide displays 'Why not?' in multiple languages, representing global communication. Smaller images show Ruth Porat on stage, medical professionals, and 3D virus models, connecting technology with healthcare.",["Date","2025-06-04T18:00:00.000Z"],"https://blog.google/technology/health/ruth-porat-remarks-asco/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/RuthAsco_Hero_2097x1182.width-1300.png","src/content/posts/2025-06-04-ai-breakthroughs-are-bringing-hope-to-cancer-research-and-treatment.md","9b5867013c99ef0e",{"html":25,"metadata":23810},{"headings":23811,"localImagePaths":23812,"remoteImagePaths":23813,"frontmatter":23814,"imagePaths":23816},[],[],[],{"title":23802,"description":23803,"summary":23803,"pubDate":23815,"source":23485,"url":23805,"thumbnail":23806},"Wed, 04 Jun 2025 18:00:00 +0000",[],"2025-06-04-ai-breakthroughs-are-bringing-hope-to-cancer-research-and-treatment.md","2025-06-04-kv-cache-from-scratch-in-nanovlm",{"id":23818,"data":23820,"filePath":23826,"digest":23827,"rendered":23828,"legacyId":23836},{"title":23821,"description":25,"summary":23822,"pubDate":23823,"source":2720,"url":23824,"thumbnail":23825},"KV Cache from scratch in nanoVLM","KV Cache from scratch in nanoVLM TL;DR We have implemented KV Caching from scratch in our nanoVLM re...",["Date","2025-06-04T00:00:00.000Z"],"https://huggingface.co/blog/kv-cache","https://huggingface.co/blog/assets/kv-cache/thumbnail.png","src/content/posts/2025-06-04-kv-cache-from-scratch-in-nanovlm.md","f2b3fdb2f31a15bb",{"html":25,"metadata":23829},{"headings":23830,"localImagePaths":23831,"remoteImagePaths":23832,"frontmatter":23833,"imagePaths":23835},[],[],[],{"title":23821,"description":25,"summary":23822,"pubDate":23834,"source":2720,"url":23824,"thumbnail":23825},"Wed, 04 Jun 2025 00:00:00 GMT",[],"2025-06-04-kv-cache-from-scratch-in-nanovlm.md","2025-06-05-a-closer-look-inside-ai-mode",{"id":23837,"data":23839,"filePath":23845,"digest":23846,"rendered":23847,"legacyId":23855},{"title":23840,"description":23841,"summary":23841,"pubDate":23842,"source":23485,"url":23843,"thumbnail":23844},"A closer look inside AI Mode","Two smartphones showing AI Mode. The left phone shows an AI Mode prompt being entered: ‘things to do in nashville this weekend with friends, we’re big foodies who like music but also more chill vibes and exploring off the beaten path’. The right phone sho",["Date","2025-06-05T18:30:00.000Z"],"https://blog.google/products/search/ai-mode-development/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Meet_AI_mode_ss.width-1300.png","src/content/posts/2025-06-05-a-closer-look-inside-ai-mode.md","3be5b5b29e43bcaa",{"html":25,"metadata":23848},{"headings":23849,"localImagePaths":23850,"remoteImagePaths":23851,"frontmatter":23852,"imagePaths":23854},[],[],[],{"title":23840,"description":23841,"summary":23841,"pubDate":23853,"source":23485,"url":23843,"thumbnail":23844},"Thu, 05 Jun 2025 18:30:00 +0000",[],"2025-06-05-a-closer-look-inside-ai-mode.md","2025-06-05-benchmarkqed-automated-benchmarking-of-rag-systems",{"id":23856,"data":23858,"filePath":23863,"digest":23864,"rendered":23865,"legacyId":23873},{"title":23859,"description":23860,"summary":23860,"pubDate":23861,"source":23065,"url":23862,"thumbnail":23067},"BenchmarkQED: Automated benchmarking of RAG systems","\u003Cp>BenchmarkQED is an open-source toolkit for benchmarking RAG systems using automated query generation, evaluation, and dataset prep. It shows that LazyGraphRAG outperforms standard methods, especially on complex, global queries.\u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/blog/benchmarkqed-automated-benchmarking-of-rag-systems/'>BenchmarkQED: Automated benchmarking of RAG systems\u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-06-05T16:00:00.000Z"],"https://www.microsoft.com/en-us/research/blog/benchmarkqed-automated-benchmarking-of-rag-systems/","src/content/posts/2025-06-05-benchmarkqed-automated-benchmarking-of-rag-systems.md","af2f9d9efa20a8ea",{"html":25,"metadata":23866},{"headings":23867,"localImagePaths":23868,"remoteImagePaths":23869,"frontmatter":23870,"imagePaths":23872},[],[],[],{"title":23859,"description":23860,"summary":23860,"pubDate":23871,"source":23065,"url":23862,"thumbnail":23067},"Thu, 05 Jun 2025 16:00:00 +0000",[],"2025-06-05-benchmarkqed-automated-benchmarking-of-rag-systems.md","2025-06-05-disrupting-malicious-uses-of-ai-june-2025",{"id":23874,"data":23876,"filePath":23881,"digest":23882,"rendered":23883,"legacyId":23891},{"title":23877,"description":23878,"summary":23878,"pubDate":23879,"source":19,"url":23880,"thumbnail":21},"Disrupting malicious uses of AI: June 2025","In our June 2025 update, we outline how we’re disrupting malicious uses of AI—through safety tools that detect and counter abuse, support democratic values, and promote responsible AI deployment for the benefit of all.",["Date","2025-06-05T02:00:00.000Z"],"https://openai.com/global-affairs/disrupting-malicious-uses-of-ai-june-2025","src/content/posts/2025-06-05-disrupting-malicious-uses-of-ai-june-2025.md","d339d45c0c290d25",{"html":25,"metadata":23884},{"headings":23885,"localImagePaths":23886,"remoteImagePaths":23887,"frontmatter":23888,"imagePaths":23890},[],[],[],{"title":23877,"description":23878,"summary":23878,"pubDate":23889,"source":19,"url":23880,"thumbnail":21},"Thu, 05 Jun 2025 02:00:00 GMT",[],"2025-06-05-disrupting-malicious-uses-of-ai-june-2025.md","2025-06-05-how-were-responding-to-the-new-york-times-data-demands-in-order-to-protect-user-privacy",{"id":23892,"data":23894,"filePath":23899,"digest":23900,"rendered":23901,"legacyId":23909},{"title":23895,"description":23896,"summary":23896,"pubDate":23897,"source":19,"url":23898,"thumbnail":21},"How we’re responding to The New York Times’ data demands in order to protect user privacy","OpenAI is fighting a court order at the demands of The New York Times and plaintiffs, which involves retention of consumer ChatGPT and API user data indefinitely. Learn how we’re working to uphold user privacy, address legal requirements, and stay true to our data protection commitments.",["Date","2025-06-05T16:30:00.000Z"],"https://openai.com/blog/response-to-nyt-data-demands","src/content/posts/2025-06-05-how-were-responding-to-the-new-york-times-data-demands-in-order-to-protect-user-privacy.md","0f2d4e49ab241282",{"html":25,"metadata":23902},{"headings":23903,"localImagePaths":23904,"remoteImagePaths":23905,"frontmatter":23906,"imagePaths":23908},[],[],[],{"title":23895,"description":23896,"summary":23896,"pubDate":23907,"source":19,"url":23898,"thumbnail":21},"Thu, 05 Jun 2025 16:30:00 GMT",[],"2025-06-05-how-were-responding-to-the-new-york-times-data-demands-in-order-to-protect-user-privacy.md","2025-06-05-portraits-personalized-ai-coaching-built-alongside-real-experts",{"id":23910,"data":23912,"filePath":23918,"digest":23919,"rendered":23920,"legacyId":23928},{"title":23913,"description":23914,"summary":23914,"pubDate":23915,"source":23485,"url":23916,"thumbnail":23917},"Portraits: personalized AI coaching built alongside real experts","Kim Scott Portrait chat example",["Date","2025-06-05T18:00:00.000Z"],"https://blog.google/technology/google-labs/portraits/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/portraits-hero.width-1300.png","src/content/posts/2025-06-05-portraits-personalized-ai-coaching-built-alongside-real-experts.md","af7165f509b97f28",{"html":25,"metadata":23921},{"headings":23922,"localImagePaths":23923,"remoteImagePaths":23924,"frontmatter":23925,"imagePaths":23927},[],[],[],{"title":23913,"description":23914,"summary":23914,"pubDate":23926,"source":23485,"url":23916,"thumbnail":23917},"Thu, 05 Jun 2025 18:00:00 +0000",[],"2025-06-05-portraits-personalized-ai-coaching-built-alongside-real-experts.md","2025-06-05-the-latest-ai-news-we-announced-in-may",{"id":23929,"data":23931,"filePath":23937,"digest":23938,"rendered":23939,"legacyId":23946},{"title":23932,"description":23933,"summary":23933,"pubDate":23934,"source":23485,"url":23935,"thumbnail":23936},"The latest AI news we announced in May","an mp4 showing a carousel of images including a collage of people, an illustrated graph with codes and the characters 'I/O'",["Date","2025-06-05T18:30:00.000Z"],"https://blog.google/technology/ai/google-ai-updates-may-2025/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/May_AI_roundup_ss.width-1300.png","src/content/posts/2025-06-05-the-latest-ai-news-we-announced-in-may.md","ebe1350440f10e40",{"html":25,"metadata":23940},{"headings":23941,"localImagePaths":23942,"remoteImagePaths":23943,"frontmatter":23944,"imagePaths":23945},[],[],[],{"title":23932,"description":23933,"summary":23933,"pubDate":23853,"source":23485,"url":23935,"thumbnail":23936},[],"2025-06-05-the-latest-ai-news-we-announced-in-may.md","2025-06-05-try-new-data-visualizations-and-graphs-for-finance-queries-in-ai-mode",{"id":23947,"data":23949,"filePath":23955,"digest":23956,"rendered":23957,"legacyId":23965},{"title":23950,"description":23951,"summary":23951,"pubDate":23952,"source":23485,"url":23953,"thumbnail":23954},"Try new data visualizations and graphs for finance queries in AI Mode.","\u003Cimg src='https://storage.googleapis.com/gweb-uniblog-publish-prod/images/BlueChip_1920x1080.max-600x600.format-webp.webp' />Today, we’re starting to roll out interactive chart visualizations in AI Mode in Labs to help bring financial data to life for questions on stocks and mutual funds.Now, …",["Date","2025-06-05T19:00:00.000Z"],"https://blog.google/products/search/ai-mode-data-visualization/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/BlueChip_1920x1080.max-1440x810.png","src/content/posts/2025-06-05-try-new-data-visualizations-and-graphs-for-finance-queries-in-ai-mode.md","d9dac1ab4f04cac0",{"html":25,"metadata":23958},{"headings":23959,"localImagePaths":23960,"remoteImagePaths":23961,"frontmatter":23962,"imagePaths":23964},[],[],[],{"title":23950,"description":23951,"summary":23951,"pubDate":23963,"source":23485,"url":23953,"thumbnail":23954},"Thu, 05 Jun 2025 19:00:00 +0000",[],"2025-06-05-try-new-data-visualizations-and-graphs-for-finance-queries-in-ai-mode.md","2025-06-05-try-the-latest-gemini-25-pro-before-general-availability",{"id":23966,"data":23968,"filePath":23974,"digest":23975,"rendered":23976,"legacyId":23983},{"title":23969,"description":23970,"summary":23970,"pubDate":23971,"source":23485,"url":23972,"thumbnail":23973},"Try the latest Gemini 2.5 Pro before general availability.","\u003Cimg src='https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_pro_preview_snippet_social_.max-600x600.format-webp.webp' />We’re introducing an upgraded preview of Gemini 2.5 Pro, our most intelligent model yet. Building on the version we released in May and showed at I/O, this model will be…",["Date","2025-06-05T16:00:00.000Z"],"https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_pro_preview_snippet_social_share.max-1440x810.jpg","src/content/posts/2025-06-05-try-the-latest-gemini-25-pro-before-general-availability.md","407810e70cf6f3ef",{"html":25,"metadata":23977},{"headings":23978,"localImagePaths":23979,"remoteImagePaths":23980,"frontmatter":23981,"imagePaths":23982},[],[],[],{"title":23969,"description":23970,"summary":23970,"pubDate":23871,"source":23485,"url":23972,"thumbnail":23973},[],"2025-06-05-try-the-latest-gemini-25-pro-before-general-availability.md","2025-06-06-screensuite---the-most-comprehensive-evaluation-suite-for-gui-agents",{"id":23984,"data":23986,"filePath":23992,"digest":23993,"rendered":23994,"legacyId":24002},{"title":23987,"description":25,"summary":23988,"pubDate":23989,"source":2720,"url":23990,"thumbnail":23991},"ScreenSuite - The most comprehensive evaluation suite for GUI Agents!","ScreenSuite - The most comprehensive evaluation suite for GUI Agents! Releasing ScreenSuite, the mos...",["Date","2025-06-06T00:00:00.000Z"],"https://huggingface.co/blog/screensuite","https://huggingface.co/blog/assets/screensuite/thumbnail.png","src/content/posts/2025-06-06-screensuite---the-most-comprehensive-evaluation-suite-for-gui-agents.md","270b5ccea1e9f726",{"html":25,"metadata":23995},{"headings":23996,"localImagePaths":23997,"remoteImagePaths":23998,"frontmatter":23999,"imagePaths":24001},[],[],[],{"title":23987,"description":25,"summary":23988,"pubDate":24000,"source":2720,"url":23990,"thumbnail":23991},"Fri, 06 Jun 2025 00:00:00 GMT",[],"2025-06-06-screensuite---the-most-comprehensive-evaluation-suite-for-gui-agents.md","2025-06-09-ai-enabled-control-system-helps-autonomous-drones-stay-on-target-in-uncertain-environments",{"id":24003,"data":24005,"filePath":24011,"digest":24012,"rendered":24013,"legacyId":24021},{"title":24006,"description":24007,"summary":24007,"pubDate":24008,"source":21340,"url":24009,"thumbnail":24010},"AI-enabled control system helps autonomous drones stay on target in uncertain environments","The system automatically learns to adapt to unknown disturbances such as gusting winds.",["Date","2025-06-09T20:40:00.000Z"],"https://news.mit.edu/2025/ai-enabled-control-system-helps-autonomous-drones-uncertain-environments-0609","https://news.mit.edu/sites/default/files/images/202506/MIT_MetaLearning-01.jpg","src/content/posts/2025-06-09-ai-enabled-control-system-helps-autonomous-drones-stay-on-target-in-uncertain-environments.md","19dbaadc8f483634",{"html":25,"metadata":24014},{"headings":24015,"localImagePaths":24016,"remoteImagePaths":24017,"frontmatter":24018,"imagePaths":24020},[],[],[],{"title":24006,"description":24007,"summary":24007,"pubDate":24019,"source":21340,"url":24009,"thumbnail":24010},"Mon, 09 Jun 2025 16:40:00 -0400",[],"2025-06-09-ai-enabled-control-system-helps-autonomous-drones-stay-on-target-in-uncertain-environments.md","2025-06-09-envisioning-a-future-where-health-care-tech-leaves-some-behind",{"id":24022,"data":24024,"filePath":24030,"digest":24031,"rendered":24032,"legacyId":24040},{"title":24025,"description":24026,"summary":24026,"pubDate":24027,"source":21340,"url":24028,"thumbnail":24029},"Envisioning a future where health care tech leaves some behind","The winning essay of the Envisioning the Future of Computing Prize puts health care disparities at the forefront.",["Date","2025-06-09T20:10:00.000Z"],"https://news.mit.edu/2025/envisioning-future-where-health-care-tech-leaves-some-behind-0609","https://news.mit.edu/sites/default/files/images/202506/Annaliese%20statue%20crop_v2.jpg","src/content/posts/2025-06-09-envisioning-a-future-where-health-care-tech-leaves-some-behind.md","1f8f0feb6f51b488",{"html":25,"metadata":24033},{"headings":24034,"localImagePaths":24035,"remoteImagePaths":24036,"frontmatter":24037,"imagePaths":24039},[],[],[],{"title":24025,"description":24026,"summary":24026,"pubDate":24038,"source":21340,"url":24028,"thumbnail":24029},"Mon, 09 Jun 2025 16:10:00 -0400",[],"2025-06-09-envisioning-a-future-where-health-care-tech-leaves-some-behind.md","2025-06-09-helping-machines-understand-visual-content-with-ai",{"id":24041,"data":24043,"filePath":24049,"digest":24050,"rendered":24051,"legacyId":24059},{"title":24044,"description":24045,"summary":24045,"pubDate":24046,"source":21340,"url":24047,"thumbnail":24048},"Helping machines understand visual content with AI","Coactive, founded by two MIT alumni, has built an AI-powered platform to unlock new insights from content of all types.",["Date","2025-06-09T19:45:00.000Z"],"https://news.mit.edu/2025/coactive-helps-machines-understand-visual-content-ai-0609","https://news.mit.edu/sites/default/files/images/202506/MIT-Coactive-AI-01-press.jpg","src/content/posts/2025-06-09-helping-machines-understand-visual-content-with-ai.md","3937cd6b31d11da2",{"html":25,"metadata":24052},{"headings":24053,"localImagePaths":24054,"remoteImagePaths":24055,"frontmatter":24056,"imagePaths":24058},[],[],[],{"title":24044,"description":24045,"summary":24045,"pubDate":24057,"source":21340,"url":24047,"thumbnail":24048},"Mon, 09 Jun 2025 15:45:00 -0400",[],"2025-06-09-helping-machines-understand-visual-content-with-ai.md","2025-06-09-heres-the-next-cohort-of-the-googleorg-accelerator-generative-ai",{"id":24060,"data":24062,"filePath":24068,"digest":24069,"rendered":24070,"legacyId":24078},{"title":24063,"description":24064,"summary":24064,"pubDate":24065,"source":23485,"url":24066,"thumbnail":24067},"Here’s the next cohort of the Google.org Accelerator: Generative AI","A collage of photos showing people using technology around the world, on a white background",["Date","2025-06-09T14:00:00.000Z"],"https://blog.google/outreach-initiatives/google-org/generative-ai-accelerator-cohort-2025/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gen_AI_Accelerator_ss.width-1300.png","src/content/posts/2025-06-09-heres-the-next-cohort-of-the-googleorg-accelerator-generative-ai.md","f8a63a0574775a76",{"html":25,"metadata":24071},{"headings":24072,"localImagePaths":24073,"remoteImagePaths":24074,"frontmatter":24075,"imagePaths":24077},[],[],[],{"title":24063,"description":24064,"summary":24064,"pubDate":24076,"source":23485,"url":24066,"thumbnail":24067},"Mon, 09 Jun 2025 14:00:00 +0000",[],"2025-06-09-heres-the-next-cohort-of-the-googleorg-accelerator-generative-ai.md","2025-06-09-how-we-built-one-of-the-most-ambitious-datasets-in-brain-activity-research",{"id":24079,"data":24081,"filePath":24087,"digest":24088,"rendered":24089,"legacyId":24097},{"title":24082,"description":24083,"summary":24083,"pubDate":24084,"source":23485,"url":24085,"thumbnail":24086},"How we built one of the most ambitious datasets in brain activity research","Four small, translucent zebrafish swim against a dark background",["Date","2025-06-09T16:00:00.000Z"],"https://blog.google/technology/research/zapbench-zebrafish-brain-mapping/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/SS_How-we-built-one-of-the-most-ambitious-dat.width-1300.png","src/content/posts/2025-06-09-how-we-built-one-of-the-most-ambitious-datasets-in-brain-activity-research.md","d02eace80acfcf5c",{"html":25,"metadata":24090},{"headings":24091,"localImagePaths":24092,"remoteImagePaths":24093,"frontmatter":24094,"imagePaths":24096},[],[],[],{"title":24082,"description":24083,"summary":24083,"pubDate":24095,"source":23485,"url":24085,"thumbnail":24086},"Mon, 09 Jun 2025 16:00:00 +0000",[],"2025-06-09-how-we-built-one-of-the-most-ambitious-datasets-in-brain-activity-research.md","2025-06-09-outbound-coordinated-vulnerability-disclosure-policy",{"id":24098,"data":24100,"filePath":24104,"digest":24105,"rendered":24106,"legacyId":24114},{"title":24101,"description":24101,"summary":24101,"pubDate":24102,"source":19,"url":24103,"thumbnail":21},"Outbound coordinated vulnerability disclosure policy",["Date","2025-06-09T00:00:00.000Z"],"https://openai.com/policies/outbound-coordinated-disclosure-policy","src/content/posts/2025-06-09-outbound-coordinated-vulnerability-disclosure-policy.md","8103c05e68a3befd",{"html":25,"metadata":24107},{"headings":24108,"localImagePaths":24109,"remoteImagePaths":24110,"frontmatter":24111,"imagePaths":24113},[],[],[],{"title":24101,"description":24101,"summary":24101,"pubDate":24112,"source":19,"url":24103,"thumbnail":21},"Mon, 09 Jun 2025 00:00:00 GMT",[],"2025-06-09-outbound-coordinated-vulnerability-disclosure-policy.md","2025-06-09-scaling-security-with-responsible-disclosure",{"id":24115,"data":24117,"filePath":24122,"digest":24123,"rendered":24124,"legacyId":24132},{"title":24118,"description":24119,"summary":24119,"pubDate":24120,"source":19,"url":24121,"thumbnail":21},"Scaling security with responsible disclosure","OpenAI introduces its Outbound Coordinated Disclosure Policy to guide how it responsibly reports vulnerabilities in third-party software—emphasizing integrity, collaboration, and proactive security at scale.",["Date","2025-06-09T10:00:00.000Z"],"https://openai.com/blog/scaling-coordinated-vulnerability-disclosure","src/content/posts/2025-06-09-scaling-security-with-responsible-disclosure.md","003b43c313729019",{"html":25,"metadata":24125},{"headings":24126,"localImagePaths":24127,"remoteImagePaths":24128,"frontmatter":24129,"imagePaths":24131},[],[],[],{"title":24118,"description":24119,"summary":24119,"pubDate":24130,"source":19,"url":24121,"thumbnail":21},"Mon, 09 Jun 2025 10:00:00 GMT",[],"2025-06-09-scaling-security-with-responsible-disclosure.md","2025-06-09-uk-government-harnesses-gemini-to-support-faster-planning-decisions",{"id":24133,"data":24135,"filePath":24141,"digest":24142,"rendered":24143,"legacyId":24151},{"title":24136,"description":24137,"summary":24137,"pubDate":24138,"source":23485,"url":24139,"thumbnail":24140},"UK government harnesses Gemini to support faster planning decisions","A summary of how Extract works",["Date","2025-06-09T11:00:00.000Z"],"https://blog.google/around-the-globe/google-europe/united-kingdom/uk-government-harnesses-gemini-to-support-faster-planning-decisions/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/overview.width-1300.png","src/content/posts/2025-06-09-uk-government-harnesses-gemini-to-support-faster-planning-decisions.md","73e32e0ba59c942b",{"html":25,"metadata":24144},{"headings":24145,"localImagePaths":24146,"remoteImagePaths":24147,"frontmatter":24148,"imagePaths":24150},[],[],[],{"title":24136,"description":24137,"summary":24137,"pubDate":24149,"source":23485,"url":24139,"thumbnail":24140},"Mon, 09 Jun 2025 11:00:00 +0000",[],"2025-06-09-uk-government-harnesses-gemini-to-support-faster-planning-decisions.md","2025-06-10-how-we-really-judge-ai",{"id":24152,"data":24154,"filePath":24160,"digest":24161,"rendered":24162,"legacyId":24170},{"title":24155,"description":24156,"summary":24156,"pubDate":24157,"source":21340,"url":24158,"thumbnail":24159},"How we really judge AI","Forget optimists vs. Luddites. Most people evaluate AI based on its perceived capability and their need for personalization.",["Date","2025-06-10T15:30:00.000Z"],"https://news.mit.edu/2025/how-we-really-judge-ai-0610","https://news.mit.edu/sites/default/files/images/202506/MIT-AI-Aversion-Appreciation-01.jpg","src/content/posts/2025-06-10-how-we-really-judge-ai.md","d30f3251280074dc",{"html":25,"metadata":24163},{"headings":24164,"localImagePaths":24165,"remoteImagePaths":24166,"frontmatter":24167,"imagePaths":24169},[],[],[],{"title":24155,"description":24156,"summary":24156,"pubDate":24168,"source":21340,"url":24158,"thumbnail":24159},"Tue, 10 Jun 2025 11:30:00 -0400",[],"2025-06-10-how-we-really-judge-ai.md","2025-06-10-how-we-used-generative-media-at-io-2025",{"id":24171,"data":24173,"filePath":24179,"digest":24180,"rendered":24181,"legacyId":24189},{"title":24174,"description":24175,"summary":24175,"pubDate":24176,"source":23485,"url":24177,"thumbnail":24178},"How we used generative media at I/O 2025","Video showing the I/O opening film.",["Date","2025-06-10T17:00:00.000Z"],"https://blog.google/technology/ai/generative-ai-io-keynote-2025/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/thumbnail_opener_hero.width-1300.png","src/content/posts/2025-06-10-how-we-used-generative-media-at-io-2025.md","b367065f90744296",{"html":25,"metadata":24182},{"headings":24183,"localImagePaths":24184,"remoteImagePaths":24185,"frontmatter":24186,"imagePaths":24188},[],[],[],{"title":24174,"description":24175,"summary":24175,"pubDate":24187,"source":23485,"url":24177,"thumbnail":24178},"Tue, 10 Jun 2025 17:00:00 +0000",[],"2025-06-10-how-we-used-generative-media-at-io-2025.md","2025-06-10-inroads-to-personalized-ai-trip-planning",{"id":24190,"data":24192,"filePath":24198,"digest":24199,"rendered":24200,"legacyId":24208},{"title":24193,"description":24194,"summary":24194,"pubDate":24195,"source":21340,"url":24196,"thumbnail":24197},"Inroads to personalized AI trip planning","A new framework from the MIT-IBM Watson AI Lab supercharges language models, so they can reason over, interactively develop, and verify valid, complex travel agendas.",["Date","2025-06-10T19:00:00.000Z"],"https://news.mit.edu/2025/inroads-personalized-ai-trip-planning-0610","https://news.mit.edu/sites/default/files/images/202505/mit-watson-travel-planning.jpg","src/content/posts/2025-06-10-inroads-to-personalized-ai-trip-planning.md","145796a69b729593",{"html":25,"metadata":24201},{"headings":24202,"localImagePaths":24203,"remoteImagePaths":24204,"frontmatter":24205,"imagePaths":24207},[],[],[],{"title":24193,"description":24194,"summary":24194,"pubDate":24206,"source":21340,"url":24196,"thumbnail":24197},"Tue, 10 Jun 2025 15:00:00 -0400",[],"2025-06-10-inroads-to-personalized-ai-trip-planning.md","2025-06-10-rewriting-symcrypt-in-rust-to-modernize-microsofts-cryptographic-library",{"id":24209,"data":24211,"filePath":24216,"digest":24217,"rendered":24218,"legacyId":24226},{"title":24212,"description":24213,"summary":24213,"pubDate":24214,"source":23065,"url":24215,"thumbnail":23067},"Rewriting SymCrypt in Rust to modernize Microsoft’s cryptographic library","\u003Cp>We're rewriting parts of Microsoft's SymCrypt cryptographic library in Rust to improve memory safety and defend against side-channel attacks, enabling formal verification while maintaining backward compatibility via a Rust-to-C compiler.\u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/blog/rewriting-symcrypt-in-rust-to-modernize-microsofts-cryptographic-library/'>Rewriting SymCrypt in Rust to modernize Microsoft’s cryptographic library \u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-06-10T16:00:00.000Z"],"https://www.microsoft.com/en-us/research/blog/rewriting-symcrypt-in-rust-to-modernize-microsofts-cryptographic-library/","src/content/posts/2025-06-10-rewriting-symcrypt-in-rust-to-modernize-microsofts-cryptographic-library.md","2065bf4058c8f6f0",{"html":25,"metadata":24219},{"headings":24220,"localImagePaths":24221,"remoteImagePaths":24222,"frontmatter":24223,"imagePaths":24225},[],[],[],{"title":24212,"description":24213,"summary":24213,"pubDate":24224,"source":23065,"url":24215,"thumbnail":23067},"Tue, 10 Jun 2025 16:00:00 +0000",[],"2025-06-10-rewriting-symcrypt-in-rust-to-modernize-microsofts-cryptographic-library.md","2025-06-10-melding-data-systems-and-society",{"id":24227,"data":24229,"filePath":24235,"digest":24236,"rendered":24237,"legacyId":24245},{"title":24230,"description":24231,"summary":24231,"pubDate":24232,"source":21340,"url":24233,"thumbnail":24234},"Melding data, systems, and society","A new book from Professor Munther Dahleh details the creation of a unique kind of transdisciplinary center, uniting many specialties through a common need for data science.",["Date","2025-06-10T18:25:00.000Z"],"https://news.mit.edu/2025/data-systems-and-society-0610","https://news.mit.edu/sites/default/files/images/202505/mit-Data-Systems-Dahleh-book.jpg","src/content/posts/2025-06-10-melding-data-systems-and-society.md","55a498fe92f36e6e",{"html":25,"metadata":24238},{"headings":24239,"localImagePaths":24240,"remoteImagePaths":24241,"frontmatter":24242,"imagePaths":24244},[],[],[],{"title":24230,"description":24231,"summary":24231,"pubDate":24243,"source":21340,"url":24233,"thumbnail":24234},"Tue, 10 Jun 2025 14:25:00 -0400",[],"2025-06-10-melding-data-systems-and-society.md","2025-06-11-allganizeノーコードで自社専用aiエージェントを構築できるagent-builderを提供開始",{"id":24246,"data":24248,"filePath":24255,"digest":24256,"rendered":24257,"legacyId":24265},{"title":24249,"description":24250,"summary":24250,"pubDate":24251,"source":24252,"url":24253,"thumbnail":24254},"Allganize、ノーコードで自社専用AIエージェントを構築できる「Agent Builder」を提供開始","\u003Cp>Allganizeは、企業が高度なセキュリティ環境下でAIエージェントをノーコードで簡単に構築できる「Agent Builder」の提供を開始しました。 このニュースのポイント Allganizeが、企業が高度なセキュリ [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/allganize_agent_builder/'>Allganize、ノーコードで自社専用AIエージェントを構築できる「Agent Builder」を提供開始\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-11T09:00:24.000Z"],"AI Smily","https://aismiley.co.jp/ai_news/allganize_agent_builder/","https://aismiley.co.jp/wp-content/uploads/2025/06/main-agentbuilder.png","src/content/posts/2025-06-11-allganizeノーコードで自社専用aiエージェントを構築できるagent-builderを提供開始.md","48ebaa86c1a5402c",{"html":25,"metadata":24258},{"headings":24259,"localImagePaths":24260,"remoteImagePaths":24261,"frontmatter":24262,"imagePaths":24264},[],[],[],{"title":24249,"description":24250,"summary":24250,"pubDate":24263,"source":24252,"url":24253,"thumbnail":24254},"Wed, 11 Jun 2025 09:00:24 +0000",[],"2025-06-11-allganizeノーコードで自社専用aiエージェントを構築できるagent-builderを提供開始.md","2025-06-11-bringing-meaning-into-technology-deployment",{"id":24266,"data":24268,"filePath":24274,"digest":24275,"rendered":24276,"legacyId":24284},{"title":24269,"description":24270,"summary":24270,"pubDate":24271,"source":21340,"url":24272,"thumbnail":24273},"Bringing meaning into technology deployment","The MIT Ethics of Computing Research Symposium showcases projects at the intersection of technology, ethics, and social responsibility.",["Date","2025-06-11T20:15:00.000Z"],"https://news.mit.edu/2025/bringing-meaning-technology-deployment-0611","https://news.mit.edu/sites/default/files/images/202505/mit-SERC-Symposium.jpg","src/content/posts/2025-06-11-bringing-meaning-into-technology-deployment.md","056bc310de20c953",{"html":25,"metadata":24277},{"headings":24278,"localImagePaths":24279,"remoteImagePaths":24280,"frontmatter":24281,"imagePaths":24283},[],[],[],{"title":24269,"description":24270,"summary":24270,"pubDate":24282,"source":21340,"url":24272,"thumbnail":24273},"Wed, 11 Jun 2025 16:15:00 -0400",[],"2025-06-11-bringing-meaning-into-technology-deployment.md","2025-06-11-google-for-nonprofits-will-expand-to-100-new-countries-and-launch-10-new-no-cost-ai-features",{"id":24285,"data":24287,"filePath":24293,"digest":24294,"rendered":24295,"legacyId":24303},{"title":24288,"description":24289,"summary":24289,"pubDate":24290,"source":23485,"url":24291,"thumbnail":24292},"Google for Nonprofits will expand to 100+ new countries and launch 10+ new no-cost AI features","Collage on a white background showing people in multiple different situations including two people in suits sitting on the back of an ambulance, and an adult and child using a laptop together",["Date","2025-06-11T16:00:00.000Z"],"https://blog.google/outreach-initiatives/google-org/google-nonprofits-updates-june-2025/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/GoogleforNonProfit_SS.width-1300.png","src/content/posts/2025-06-11-google-for-nonprofits-will-expand-to-100-new-countries-and-launch-10-new-no-cost-ai-features.md","49938fb5e3abe7eb",{"html":25,"metadata":24296},{"headings":24297,"localImagePaths":24298,"remoteImagePaths":24299,"frontmatter":24300,"imagePaths":24302},[],[],[],{"title":24288,"description":24289,"summary":24289,"pubDate":24301,"source":23485,"url":24291,"thumbnail":24292},"Wed, 11 Jun 2025 16:00:00 +0000",[],"2025-06-11-google-for-nonprofits-will-expand-to-100-new-countries-and-launch-10-new-no-cost-ai-features.md","2025-06-11-have-a-damaged-painting-restore-it-in-just-hours-with-an-ai-generated-mask",{"id":24304,"data":24306,"filePath":24312,"digest":24313,"rendered":24314,"legacyId":24322},{"title":24307,"description":24308,"summary":24308,"pubDate":24309,"source":21340,"url":24310,"thumbnail":24311},"Have a damaged painting? Restore it in just hours with an AI-generated “mask”","A new method can physically restore original paintings using digitally constructed films, which can be removed if desired.",["Date","2025-06-11T15:00:00.000Z"],"https://news.mit.edu/2025/restoring-damaged-paintings-using-ai-generated-mask-0611","https://news.mit.edu/sites/default/files/images/202506/MIT-Restoring-Paintings-01-press.jpg","src/content/posts/2025-06-11-have-a-damaged-painting-restore-it-in-just-hours-with-an-ai-generated-mask.md","42f0b9d4b6ea2449",{"html":25,"metadata":24315},{"headings":24316,"localImagePaths":24317,"remoteImagePaths":24318,"frontmatter":24319,"imagePaths":24321},[],[],[],{"title":24307,"description":24308,"summary":24308,"pubDate":24320,"source":21340,"url":24310,"thumbnail":24311},"Wed, 11 Jun 2025 11:00:00 -0400",[],"2025-06-11-have-a-damaged-painting-restore-it-in-just-hours-with-an-ai-generated-mask.md","2025-06-11-introducing-training-cluster-as-a-service---a-new-collaboration-with-nvidia",{"id":24323,"data":24325,"filePath":24331,"digest":24332,"rendered":24333,"legacyId":24341},{"title":24326,"description":25,"summary":24327,"pubDate":24328,"source":2720,"url":24329,"thumbnail":24330},"Introducing Training Cluster as a Service - a new collaboration with NVIDIA","Introducing Training Cluster as a Service - a new collaboration with NVIDIA Today at GTC Paris, we a...",["Date","2025-06-11T00:00:00.000Z"],"https://huggingface.co/blog/nvidia-training-cluster","https://huggingface.co/blog/assets/nvidia-training-cluster/nvidia-training-cluster-thumbnail-compressed.png","src/content/posts/2025-06-11-introducing-training-cluster-as-a-service---a-new-collaboration-with-nvidia.md","4d9bd7acd5f9f7d8",{"html":25,"metadata":24334},{"headings":24335,"localImagePaths":24336,"remoteImagePaths":24337,"frontmatter":24338,"imagePaths":24340},[],[],[],{"title":24326,"description":25,"summary":24327,"pubDate":24339,"source":2720,"url":24329,"thumbnail":24330},"Wed, 11 Jun 2025 00:00:00 GMT",[],"2025-06-11-introducing-training-cluster-as-a-service---a-new-collaboration-with-nvidia.md","2025-06-11-photonic-processor-could-streamline-6g-wireless-signal-processing",{"id":24342,"data":24344,"filePath":24350,"digest":24351,"rendered":24352,"legacyId":24360},{"title":24345,"description":24346,"summary":24346,"pubDate":24347,"source":21340,"url":24348,"thumbnail":24349},"Photonic processor could streamline 6G wireless signal processing","By performing deep learning at the speed of light, this chip could give edge devices new capabilities for real-time data analysis.",["Date","2025-06-11T18:00:00.000Z"],"https://news.mit.edu/2025/photonic-processor-could-streamline-6g-wireless-signal-processing-0611","https://news.mit.edu/sites/default/files/images/202506/MIT-Photonic-Process-01-press.jpg","src/content/posts/2025-06-11-photonic-processor-could-streamline-6g-wireless-signal-processing.md","b2f4d71bf9edd041",{"html":25,"metadata":24353},{"headings":24354,"localImagePaths":24355,"remoteImagePaths":24356,"frontmatter":24357,"imagePaths":24359},[],[],[],{"title":24345,"description":24346,"summary":24346,"pubDate":24358,"source":21340,"url":24348,"thumbnail":24349},"Wed, 11 Jun 2025 14:00:00 -0400",[],"2025-06-11-photonic-processor-could-streamline-6g-wireless-signal-processing.md","2025-06-11-tsuzumi活用医療文書作成支援aiで医療サービス向上を目指すntt東日本と新潟大学が共同研究",{"id":24361,"data":24363,"filePath":24369,"digest":24370,"rendered":24371,"legacyId":24379},{"title":24364,"description":24365,"summary":24365,"pubDate":24366,"source":24252,"url":24367,"thumbnail":24368},"tsuzumi活用、医療文書作成支援AIで医療サービス向上を目指す。NTT東日本と新潟大学が共同研究","\u003Cp>NTT東日本と新潟大学は、医師不足の解消と医療サービスの質向上を目的に、NTTが開発した大規模言語モデル「tsuzumi」を活用した医療文書作成支援AIモデルの実証事業を開始しました。 このニュースのポイント NTT東日 [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/ntt-east-niigata-university-ai/'>tsuzumi活用、医療文書作成支援AIで医療サービス向上を目指す。NTT東日本と新潟大学が共同研究\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-11T00:55:41.000Z"],"https://aismiley.co.jp/ai_news/ntt-east-niigata-university-ai/","https://aismiley.co.jp/wp-content/uploads/2025/05/ntt-east-niigata-university-ai.png","src/content/posts/2025-06-11-tsuzumi活用医療文書作成支援aiで医療サービス向上を目指すntt東日本と新潟大学が共同研究.md","56e0925c337b8882",{"html":25,"metadata":24372},{"headings":24373,"localImagePaths":24374,"remoteImagePaths":24375,"frontmatter":24376,"imagePaths":24378},[],[],[],{"title":24364,"description":24365,"summary":24365,"pubDate":24377,"source":24252,"url":24367,"thumbnail":24368},"Wed, 11 Jun 2025 00:55:41 +0000",[],"2025-06-11-tsuzumi活用医療文書作成支援aiで医療サービス向上を目指すntt東日本と新潟大学が共同研究.md","2025-06-11-生成ai社会実装に向けた最新成果とはgeniac第2期成果報告会レポート",{"id":24380,"data":24382,"filePath":24388,"digest":24389,"rendered":24390,"legacyId":24398},{"title":24383,"description":24384,"summary":24384,"pubDate":24385,"source":24252,"url":24386,"thumbnail":24387},"生成AI社会実装に向けた最新成果とは―GENIAC第2期成果報告会レポート","\u003Cp>日本の生成AI開発力を底上げする取り組みとして注目されている「GENIAC（Generative AI Accelerator Challenge）」の第2期成果報告会が開催されました。 GENIACは、経済産業省とNE [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/report-geniac-second-results-presentation/'>生成AI社会実装に向けた最新成果とは―GENIAC第2期成果報告会レポート\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-11T07:45:26.000Z"],"https://aismiley.co.jp/ai_news/report-geniac-second-results-presentation/","https://aismiley.co.jp/wp-content/uploads/2025/06/geniac-main.png","src/content/posts/2025-06-11-生成ai社会実装に向けた最新成果とはgeniac第2期成果報告会レポート.md","ffe3e2e38a82e414",{"html":25,"metadata":24391},{"headings":24392,"localImagePaths":24393,"remoteImagePaths":24394,"frontmatter":24395,"imagePaths":24397},[],[],[],{"title":24383,"description":24384,"summary":24384,"pubDate":24396,"source":24252,"url":24386,"thumbnail":24387},"Wed, 11 Jun 2025 07:45:26 +0000",[],"2025-06-11-生成ai社会実装に向けた最新成果とはgeniac第2期成果報告会レポート.md","2025-06-12-5-things-from-io-to-try-right-now",{"id":24399,"data":24401,"filePath":24407,"digest":24408,"rendered":24409,"legacyId":24417},{"title":24402,"description":24403,"summary":24403,"pubDate":24404,"source":23485,"url":24405,"thumbnail":24406},"5 things from I/O to try right now","Collage on a dark background showing AI-generated media including humpback whales jumping from the water and the very detailed face of a chameleon",["Date","2025-06-12T16:00:00.000Z"],"https://blog.google/technology/ai/io-2025-tools-to-try-globally/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5_I_O_tools_ss.width-1300.png","src/content/posts/2025-06-12-5-things-from-io-to-try-right-now.md","cfe5a30c0656cf49",{"html":25,"metadata":24410},{"headings":24411,"localImagePaths":24412,"remoteImagePaths":24413,"frontmatter":24414,"imagePaths":24416},[],[],[],{"title":24402,"description":24403,"summary":24403,"pubDate":24415,"source":23485,"url":24405,"thumbnail":24406},"Thu, 12 Jun 2025 16:00:00 +0000",[],"2025-06-12-5-things-from-io-to-try-right-now.md","2025-06-12-bringing-the-magic-of-ai-to-mattels-iconic-brands",{"id":24418,"data":24420,"filePath":24425,"digest":24426,"rendered":24427,"legacyId":24435},{"title":24421,"description":24422,"summary":24422,"pubDate":24423,"source":19,"url":24424,"thumbnail":21},"Bringing the Magic of AI to Mattel’s Iconic Brands","OpenAI and Mattel are partnering to integrate AI into iconic brands such as Barbie and Hot Wheels, aiming to enhance creative development, streamline workflows, and create new ways for fans to engage.",["Date","2025-06-12T00:00:00.000Z"],"https://openai.com/blog/mattels-iconic-brands","src/content/posts/2025-06-12-bringing-the-magic-of-ai-to-mattels-iconic-brands.md","d003659e35bfdaf5",{"html":25,"metadata":24428},{"headings":24429,"localImagePaths":24430,"remoteImagePaths":24431,"frontmatter":24432,"imagePaths":24434},[],[],[],{"title":24421,"description":24422,"summary":24422,"pubDate":24433,"source":19,"url":24424,"thumbnail":21},"Thu, 12 Jun 2025 00:00:00 GMT",[],"2025-06-12-bringing-the-magic-of-ai-to-mattels-iconic-brands.md","2025-06-12-enhance-your-models-in-5-minutes-with-the-hugging-face-kernel-hub",{"id":24436,"data":24438,"filePath":24444,"digest":24445,"rendered":24446,"legacyId":24453},{"title":24439,"description":25,"summary":24440,"pubDate":24441,"source":2720,"url":24442,"thumbnail":24443},"Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub","🏎️ Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub Boost your model performance wi...",["Date","2025-06-12T00:00:00.000Z"],"https://huggingface.co/blog/hello-hf-kernels","https://huggingface.co/blog/assets/hello-hf-kernels/kernel-hub-five-mins-short.png","src/content/posts/2025-06-12-enhance-your-models-in-5-minutes-with-the-hugging-face-kernel-hub.md","3c157f97dffa52ae",{"html":25,"metadata":24447},{"headings":24448,"localImagePaths":24449,"remoteImagePaths":24450,"frontmatter":24451,"imagePaths":24452},[],[],[],{"title":24439,"description":25,"summary":24440,"pubDate":24433,"source":2720,"url":24442,"thumbnail":24443},[],"2025-06-12-enhance-your-models-in-5-minutes-with-the-hugging-face-kernel-hub.md","2025-06-12-eques製薬特化llmjpharmatron-7bを開発薬文書作成など製薬業務における活用を目指す",{"id":24454,"data":24456,"filePath":24462,"digest":24463,"rendered":24464,"legacyId":24472},{"title":24457,"description":24458,"summary":24458,"pubDate":24459,"source":24252,"url":24460,"thumbnail":24461},"EQUES、製薬特化LLM「JPharmatron-7B」を開発。薬文書作成など、製薬業務における活用を目指す","\u003Cp>EQUESは、経済産業省/NEDOによる生成AI研究支援プログラム「GENIAC」の一環として、製薬業界向けの新しいLLM「JPharmatron-7B」を発表しました。 このニュースのポイント 製薬業界向けLLM「JP [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/eques-develops-pharmaceutical-specific-llm/'>EQUES、製薬特化LLM「JPharmatron-7B」を開発。薬文書作成など、製薬業務における活用を目指す\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-12T09:25:20.000Z"],"https://aismiley.co.jp/ai_news/eques-develops-pharmaceutical-specific-llm/","https://aismiley.co.jp/wp-content/uploads/2025/06/eques-develops-pharmaceutical-specific-llm1.png","src/content/posts/2025-06-12-eques製薬特化llmjpharmatron-7bを開発薬文書作成など製薬業務における活用を目指す.md","822a0abdc4f573c8",{"html":25,"metadata":24465},{"headings":24466,"localImagePaths":24467,"remoteImagePaths":24468,"frontmatter":24469,"imagePaths":24471},[],[],[],{"title":24457,"description":24458,"summary":24458,"pubDate":24470,"source":24252,"url":24460,"thumbnail":24461},"Thu, 12 Jun 2025 09:25:20 +0000",[],"2025-06-12-eques製薬特化llmjpharmatron-7bを開発薬文書作成など製薬業務における活用を目指す.md","2025-06-12-how-ai-is-reshaping-the-future-of-healthcare-and-medical-research",{"id":24473,"data":24475,"filePath":24480,"digest":24481,"rendered":24482,"legacyId":24490},{"title":24476,"description":24477,"summary":24477,"pubDate":24478,"source":23065,"url":24479,"thumbnail":23067},"How AI is reshaping the future of healthcare and medical research","\u003Cp>Technologists Bill Gates and Sébastien Bubeck discuss the state of generative AI in medicine, how access to “medical intelligence” might help empower people across healthcare, and how AI’s accelerating improvements are likely to affect both delivery and discovery.\u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/podcast/how-ai-is-reshaping-the-future-of-healthcare-and-medical-research/'>How AI is reshaping the future of healthcare and medical research\u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-06-12T16:17:04.000Z"],"https://www.microsoft.com/en-us/research/podcast/how-ai-is-reshaping-the-future-of-healthcare-and-medical-research/","src/content/posts/2025-06-12-how-ai-is-reshaping-the-future-of-healthcare-and-medical-research.md","2ddd8028c9087f07",{"html":25,"metadata":24483},{"headings":24484,"localImagePaths":24485,"remoteImagePaths":24486,"frontmatter":24487,"imagePaths":24489},[],[],[],{"title":24476,"description":24477,"summary":24477,"pubDate":24488,"source":23065,"url":24479,"thumbnail":23067},"Thu, 12 Jun 2025 16:17:04 +0000",[],"2025-06-12-how-ai-is-reshaping-the-future-of-healthcare-and-medical-research.md","2025-06-12-featherless-ai-on-hugging-face-inference-providers",{"id":24491,"data":24493,"filePath":24499,"digest":24500,"rendered":24501,"legacyId":24508},{"title":24494,"description":25,"summary":24495,"pubDate":24496,"source":2720,"url":24497,"thumbnail":24498},"Featherless AI on Hugging Face Inference Providers 🔥","Featherless AI on Hugging Face Inference Providers 🔥 We're thrilled to share that Featherless AI is ...",["Date","2025-06-12T00:00:00.000Z"],"https://huggingface.co/blog/inference-providers-featherless","https://huggingface.co/blog/assets/inference-providers/welcome-featherless.jpg","src/content/posts/2025-06-12-featherless-ai-on-hugging-face-inference-providers.md","a43284d15797e832",{"html":25,"metadata":24502},{"headings":24503,"localImagePaths":24504,"remoteImagePaths":24505,"frontmatter":24506,"imagePaths":24507},[],[],[],{"title":24494,"description":25,"summary":24495,"pubDate":24433,"source":2720,"url":24497,"thumbnail":24498},[],"2025-06-12-featherless-ai-on-hugging-face-inference-providers.md","2025-06-12-how-were-supporting-better-tropical-cyclone-prediction-with-ai",{"id":24509,"data":24511,"filePath":24517,"digest":24518,"rendered":24519,"legacyId":24527},{"title":24512,"description":24513,"summary":24513,"pubDate":24514,"source":6423,"url":24515,"thumbnail":24516},"How we're supporting better tropical cyclone prediction with AI","We’re launching Weather Lab, featuring our experimental cyclone predictions, and we’re partnering with the U.S. National Hurricane Center to support their forecasts and warnings this cyclone season.",["Date","2025-06-12T15:00:00.000Z"],"https://deepmind.google/discover/blog/weather-lab-cyclone-predictions-with-ai/","https://lh3.googleusercontent.com/4emGMNrEdaydebppYDiyQMNhXtgUFr8VvrKhVItMHENrxeWmWO9yqhteSj2fe25lxkiZAu7vOZZcsXPDLg0O-LPSvk6CS1I8E2-GdjtoN_2ViJOY=w1200-h630-n-nu","src/content/posts/2025-06-12-how-were-supporting-better-tropical-cyclone-prediction-with-ai.md","48a537bd6bac94ce",{"html":25,"metadata":24520},{"headings":24521,"localImagePaths":24522,"remoteImagePaths":24523,"frontmatter":24524,"imagePaths":24526},[],[],[],{"title":24512,"description":24513,"summary":24513,"pubDate":24525,"source":6423,"url":24515,"thumbnail":24516},"Thu, 12 Jun 2025 15:00:00 +0000",[],"2025-06-12-how-were-supporting-better-tropical-cyclone-prediction-with-ai.md","2025-06-12-openai-o3-proとは最新aiモデルの実力を徹底解説",{"id":24528,"data":24530,"filePath":24536,"digest":24537,"rendered":24538,"legacyId":24546},{"title":24531,"description":24532,"summary":24532,"pubDate":24533,"source":24252,"url":24534,"thumbnail":24535},"OpenAI o3-proとは？最新AIモデルの実力を徹底解説","\u003Cp>2025年6月に、OpenAIの最新モデル「o3-pro」がリリースされました。主に科学や数学など深い推論を得意として、自然言語処理、画像理解、音声認識といったマルチモーダル対応が特徴となっています。 本記事では、o3- [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/o3-pro/'>OpenAI o3-proとは？最新AIモデルの実力を徹底解説\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-12T06:31:40.000Z"],"https://aismiley.co.jp/ai_news/o3-pro/","https://aismiley.co.jp/wp-content/uploads/2025/06/o3-pro.png","src/content/posts/2025-06-12-openai-o3-proとは最新aiモデルの実力を徹底解説.md","798a7e5b5c0e946a",{"html":25,"metadata":24539},{"headings":24540,"localImagePaths":24541,"remoteImagePaths":24542,"frontmatter":24543,"imagePaths":24545},[],[],[],{"title":24531,"description":24532,"summary":24532,"pubDate":24544,"source":24252,"url":24534,"thumbnail":24535},"Thu, 12 Jun 2025 06:31:40 +0000",[],"2025-06-12-openai-o3-proとは最新aiモデルの実力を徹底解説.md","2025-06-13-behind-ancestra-combining-veo-with-live-action-filmmaking",{"id":24547,"data":24549,"filePath":24555,"digest":24556,"rendered":24557,"legacyId":24565},{"title":24550,"description":24551,"summary":24551,"pubDate":24552,"source":6423,"url":24553,"thumbnail":24554},"Behind “ANCESTRA”: combining Veo with live-action filmmaking","We partnered with Darren Aronofsky, Eliza McNitt and a team of more than 200 people to make a film using Veo and live-action filmmaking.",["Date","2025-06-13T13:30:00.000Z"],"https://deepmind.google/discover/blog/behind-ancestra-combining-veo-with-live-action-filmmaking/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ancestra-YTThumbnail.width-1300.png","src/content/posts/2025-06-13-behind-ancestra-combining-veo-with-live-action-filmmaking.md","2d77fb1d6cec74b0",{"html":25,"metadata":24558},{"headings":24559,"localImagePaths":24560,"remoteImagePaths":24561,"frontmatter":24562,"imagePaths":24564},[],[],[],{"title":24550,"description":24551,"summary":24551,"pubDate":24563,"source":6423,"url":24553,"thumbnail":24554},"Fri, 13 Jun 2025 13:30:00 +0000",[],"2025-06-13-behind-ancestra-combining-veo-with-live-action-filmmaking.md","2025-06-13-get-an-audio-overview-of-search-results-in-labs-then-click-through-to-learn-more",{"id":24566,"data":24568,"filePath":24574,"digest":24575,"rendered":24576,"legacyId":24584},{"title":24569,"description":24570,"summary":24570,"pubDate":24571,"source":23485,"url":24572,"thumbnail":24573},"Get an audio overview of Search results in Labs, then click through to learn more.","A phone screen showing Google search results with a section titled 'Search Labs | Audio Overviews' and an audio player.",["Date","2025-06-13T15:30:00.000Z"],"https://blog.google/products/search/audio-overviews-search-labs/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AudioOverview_SS.max-1440x810.png","src/content/posts/2025-06-13-get-an-audio-overview-of-search-results-in-labs-then-click-through-to-learn-more.md","d1f8737a2fd91808",{"html":25,"metadata":24577},{"headings":24578,"localImagePaths":24579,"remoteImagePaths":24580,"frontmatter":24581,"imagePaths":24583},[],[],[],{"title":24569,"description":24570,"summary":24570,"pubDate":24582,"source":23485,"url":24572,"thumbnail":24573},"Fri, 13 Jun 2025 15:30:00 +0000",[],"2025-06-13-get-an-audio-overview-of-search-results-in-labs-then-click-through-to-learn-more.md","2025-06-13-kms業務効率化サービスdaiverseにgpt-41とweb情報登録機能を実装",{"id":24585,"data":24587,"filePath":24593,"digest":24594,"rendered":24595,"legacyId":24603},{"title":24588,"description":24589,"summary":24589,"pubDate":24590,"source":24252,"url":24591,"thumbnail":24592},"KMS、業務効率化サービス「DAIVERSE」にGPT-4.1とWeb情報登録機能を実装","\u003Cp>KMSは、業務効率化サービス「DAIVERSE」に、Azure OpenAI ServiceのGPT-4.1モデルとWebサイト情報登録機能を実装しました。 このニュースのポイント 「DAIVERSE」にGPT-4.1モ [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/kms-azure-openai-service/'>KMS、業務効率化サービス「DAIVERSE」にGPT-4.1とWeb情報登録機能を実装\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-13T02:00:38.000Z"],"https://aismiley.co.jp/ai_news/kms-azure-openai-service/","https://aismiley.co.jp/wp-content/uploads/2025/06/kms-azure-openai-service1.png","src/content/posts/2025-06-13-kms業務効率化サービスdaiverseにgpt-41とweb情報登録機能を実装.md","5c6cd6f0cccf7578",{"html":25,"metadata":24596},{"headings":24597,"localImagePaths":24598,"remoteImagePaths":24599,"frontmatter":24600,"imagePaths":24602},[],[],[],{"title":24588,"description":24589,"summary":24589,"pubDate":24601,"source":24252,"url":24591,"thumbnail":24592},"Fri, 13 Jun 2025 02:00:38 +0000",[],"2025-06-13-kms業務効率化サービスdaiverseにgpt-41とweb情報登録機能を実装.md","2025-06-16-celebrating-an-academic-industry-collaboration-to-advance-vehicle-technology",{"id":24604,"data":24606,"filePath":24612,"digest":24613,"rendered":24614,"legacyId":24622},{"title":24607,"description":24608,"summary":24608,"pubDate":24609,"source":21340,"url":24610,"thumbnail":24611},"Celebrating an academic-industry collaboration to advance vehicle technology","MIT Advanced Vehicle Technology Consortium marks a decade of developing data that improve understanding of how drivers use and respond to increasingly sophisticated automotive features.",["Date","2025-06-16T18:45:00.000Z"],"https://news.mit.edu/2025/celebrating-academic-industry-collaboration-advance-vehicle-technology-0616","https://news.mit.edu/sites/default/files/images/202505/MIT-AVT-conference.jpg","src/content/posts/2025-06-16-celebrating-an-academic-industry-collaboration-to-advance-vehicle-technology.md","f99f069e8a8ef312",{"html":25,"metadata":24615},{"headings":24616,"localImagePaths":24617,"remoteImagePaths":24618,"frontmatter":24619,"imagePaths":24621},[],[],[],{"title":24607,"description":24608,"summary":24608,"pubDate":24620,"source":21340,"url":24610,"thumbnail":24611},"Mon, 16 Jun 2025 14:45:00 -0400",[],"2025-06-16-celebrating-an-academic-industry-collaboration-to-advance-vehicle-technology.md","2025-06-16-79開催生成ai品質改善ウェビナー-本番品質の生成aiをどう作るllmops高精度データで実現する改善プロセス",{"id":24623,"data":24625,"filePath":24631,"digest":24632,"rendered":24633,"legacyId":24641},{"title":24626,"description":24627,"summary":24627,"pubDate":24628,"source":24252,"url":24629,"thumbnail":24630},"7/9開催【生成AI品質改善ウェビナー】 本番品質の生成AIをどう作る？LLMOps×高精度データで実現する改善プロセス","\u003Cp>AIポータルメディア「AIsmiley」は、2025年7月9日（水）12時から生成AI活用をテーマにウェビナーを開催します。 本ウェビナーでは、生成AI活用のポイントについてご紹介。生成AIを本番品質へ改善するポイントか [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/20250709webinar/'>7/9開催【生成AI品質改善ウェビナー】 本番品質の生成AIをどう作る？LLMOps×高精度データで実現する改善プロセス\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-16T01:37:40.000Z"],"https://aismiley.co.jp/ai_news/20250709webinar/","https://aismiley.co.jp/wp-content/uploads/2025/06/699_1200x628_2.jpg","src/content/posts/2025-06-16-79開催生成ai品質改善ウェビナー-本番品質の生成aiをどう作るllmops高精度データで実現する改善プロセス.md","8f55a5c98f26af1a",{"html":25,"metadata":24634},{"headings":24635,"localImagePaths":24636,"remoteImagePaths":24637,"frontmatter":24638,"imagePaths":24640},[],[],[],{"title":24626,"description":24627,"summary":24627,"pubDate":24639,"source":24252,"url":24629,"thumbnail":24630},"Mon, 16 Jun 2025 01:37:40 +0000",[],"2025-06-16-79開催生成ai品質改善ウェビナー-本番品質の生成aiをどう作るllmops高精度データで実現する改善プロセス.md","2025-06-16-groq-on-hugging-face-inference-providers",{"id":24642,"data":24644,"filePath":24650,"digest":24651,"rendered":24652,"legacyId":24660},{"title":24645,"description":25,"summary":24646,"pubDate":24647,"source":2720,"url":24648,"thumbnail":24649},"Groq on Hugging Face Inference Providers 🔥","Groq on Hugging Face Inference Providers 🔥 We're thrilled to share that Groq is now a supported Infe...",["Date","2025-06-16T00:00:00.000Z"],"https://huggingface.co/blog/inference-providers-groq","https://huggingface.co/blog/assets/inference-providers/welcome-groq.jpg","src/content/posts/2025-06-16-groq-on-hugging-face-inference-providers.md","d7c8f6227678feb1",{"html":25,"metadata":24653},{"headings":24654,"localImagePaths":24655,"remoteImagePaths":24656,"frontmatter":24657,"imagePaths":24659},[],[],[],{"title":24645,"description":25,"summary":24646,"pubDate":24658,"source":2720,"url":24648,"thumbnail":24649},"Mon, 16 Jun 2025 00:00:00 GMT",[],"2025-06-16-groq-on-hugging-face-inference-providers.md","2025-06-16-introducing-openai-for-government",{"id":24661,"data":24663,"filePath":24668,"digest":24669,"rendered":24670,"legacyId":24677},{"title":24664,"description":24665,"summary":24665,"pubDate":24666,"source":19,"url":24667,"thumbnail":21},"Introducing OpenAI for Government","We’re launching OpenAI for Government, a new initiative focused on bringing our most advanced AI tools to public servants across the United States. We're supporting the U.S. government's efforts in adopting best-in-class technology and deploying these tools in service of the public good.",["Date","2025-06-16T00:00:00.000Z"],"https://openai.com/global-affairs/introducing-openai-for-government","src/content/posts/2025-06-16-introducing-openai-for-government.md","fd1de1b6909869c1",{"html":25,"metadata":24671},{"headings":24672,"localImagePaths":24673,"remoteImagePaths":24674,"frontmatter":24675,"imagePaths":24676},[],[],[],{"title":24664,"description":24665,"summary":24665,"pubDate":24658,"source":19,"url":24667,"thumbnail":21},[],"2025-06-16-introducing-openai-for-government.md","2025-06-17-a-sounding-board-for-strengthening-the-student-experience",{"id":24678,"data":24680,"filePath":24686,"digest":24687,"rendered":24688,"legacyId":24696},{"title":24681,"description":24682,"summary":24682,"pubDate":24683,"source":21340,"url":24684,"thumbnail":24685},"A sounding board for strengthening the student experience","Composed of “computing bilinguals,” the Undergraduate Advisory Group provides vital input to help advance the mission of the MIT Schwarzman College of Computing.",["Date","2025-06-17T20:00:00.000Z"],"https://news.mit.edu/2025/sounding-board-for-strengthening-student-experience-0617","https://news.mit.edu/sites/default/files/images/202505/mit-SCC-UAG.jpg","src/content/posts/2025-06-17-a-sounding-board-for-strengthening-the-student-experience.md","88a5135d934ef25b",{"html":25,"metadata":24689},{"headings":24690,"localImagePaths":24691,"remoteImagePaths":24692,"frontmatter":24693,"imagePaths":24695},[],[],[],{"title":24681,"description":24682,"summary":24682,"pubDate":24694,"source":21340,"url":24684,"thumbnail":24685},"Tue, 17 Jun 2025 16:00:00 -0400",[],"2025-06-17-a-sounding-board-for-strengthening-the-student-experience.md","2025-06-17-combining-technology-education-and-human-connection-to-improve-online-learning",{"id":24697,"data":24699,"filePath":24705,"digest":24706,"rendered":24707,"legacyId":24715},{"title":24700,"description":24701,"summary":24701,"pubDate":24702,"source":21340,"url":24703,"thumbnail":24704},"Combining technology, education, and human connection to improve online learning","Caitlin Morris, a PhD student and 2024 MAD Fellow affiliated with the MIT Media Lab, designs digital learning platforms that make room for the “social magic” that influences curiosity and motivation.",["Date","2025-06-17T20:25:00.000Z"],"https://news.mit.edu/2025/caitlin-morris-combines-tech-education-human-connection-improve-online-learning-0617","https://news.mit.edu/sites/default/files/images/202506/mit-Caitlin-Morris.jpg","src/content/posts/2025-06-17-combining-technology-education-and-human-connection-to-improve-online-learning.md","0c14c307052b9d60",{"html":25,"metadata":24708},{"headings":24709,"localImagePaths":24710,"remoteImagePaths":24711,"frontmatter":24712,"imagePaths":24714},[],[],[],{"title":24700,"description":24701,"summary":24701,"pubDate":24713,"source":21340,"url":24703,"thumbnail":24704},"Tue, 17 Jun 2025 16:25:00 -0400",[],"2025-06-17-combining-technology-education-and-human-connection-to-improve-online-learning.md","2025-06-17-gemini-25-updates-to-our-family-of-thinking-models",{"id":24716,"data":24718,"filePath":24724,"digest":24725,"rendered":24726,"legacyId":24734},{"title":24719,"description":24720,"summary":24720,"pubDate":24721,"source":6423,"url":24722,"thumbnail":24723},"Gemini 2.5: Updates to our family of thinking models","Explore the latest Gemini 2.5 model updates with enhanced performance and accuracy: Gemini 2.5 Pro now stable, Flash generally available, and the new Flash-Lite in preview.",["Date","2025-06-17T16:03:39.000Z"],"https://deepmind.google/discover/blog/gemini-25-updates-to-our-family-of-thinking-models/","https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini-2-5-pro-meta_1.2e16d0ba.fill-1200x600.png","src/content/posts/2025-06-17-gemini-25-updates-to-our-family-of-thinking-models.md","6d7a3ce5b932f4e6",{"html":25,"metadata":24727},{"headings":24728,"localImagePaths":24729,"remoteImagePaths":24730,"frontmatter":24731,"imagePaths":24733},[],[],[],{"title":24719,"description":24720,"summary":24720,"pubDate":24732,"source":6423,"url":24722,"thumbnail":24723},"Tue, 17 Jun 2025 16:03:39 +0000",[],"2025-06-17-gemini-25-updates-to-our-family-of-thinking-models.md","2025-06-17-new-methods-boost-reasoning-in-small-and-large-language-models",{"id":24735,"data":24737,"filePath":24742,"digest":24743,"rendered":24744,"legacyId":24752},{"title":24738,"description":24739,"summary":24739,"pubDate":24740,"source":23065,"url":24741,"thumbnail":23067},"New methods boost reasoning in small and large language models","\u003Cp>New techniques are reimagining how LLMs reason. By combining symbolic logic, mathematical rigor, and adaptive planning, these methods enable models to tackle complex, real-world problems across a variety of fields.\u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/blog/new-methods-boost-reasoning-in-small-and-large-language-models/'>New methods boost reasoning in small and large language models\u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-06-17T16:00:00.000Z"],"https://www.microsoft.com/en-us/research/blog/new-methods-boost-reasoning-in-small-and-large-language-models/","src/content/posts/2025-06-17-new-methods-boost-reasoning-in-small-and-large-language-models.md","b7fafa6eb0a325cc",{"html":25,"metadata":24745},{"headings":24746,"localImagePaths":24747,"remoteImagePaths":24748,"frontmatter":24749,"imagePaths":24751},[],[],[],{"title":24738,"description":24739,"summary":24739,"pubDate":24750,"source":23065,"url":24741,"thumbnail":23067},"Tue, 17 Jun 2025 16:00:00 +0000",[],"2025-06-17-new-methods-boost-reasoning-in-small-and-large-language-models.md","2025-06-17-unpacking-the-bias-of-large-language-models",{"id":24753,"data":24755,"filePath":24761,"digest":24762,"rendered":24763,"legacyId":24770},{"title":24756,"description":24757,"summary":24757,"pubDate":24758,"source":21340,"url":24759,"thumbnail":24760},"Unpacking the bias of large language models","In a new study, researchers discover the root cause of a type of bias in LLMs, paving the way for more accurate and reliable AI systems.",["Date","2025-06-17T20:00:00.000Z"],"https://news.mit.edu/2025/unpacking-large-language-model-bias-0617","https://news.mit.edu/sites/default/files/images/202506/MIT-transform-bias-01-press.jpg","src/content/posts/2025-06-17-unpacking-the-bias-of-large-language-models.md","b4b7596daa2f1d92",{"html":25,"metadata":24764},{"headings":24765,"localImagePaths":24766,"remoteImagePaths":24767,"frontmatter":24768,"imagePaths":24769},[],[],[],{"title":24756,"description":24757,"summary":24757,"pubDate":24694,"source":21340,"url":24759,"thumbnail":24760},[],"2025-06-17-unpacking-the-bias-of-large-language-models.md","2025-06-17-were-expanding-our-gemini-25-family-of-models",{"id":24771,"data":24773,"filePath":24779,"digest":24780,"rendered":24781,"legacyId":24789},{"title":24774,"description":24775,"summary":24775,"pubDate":24776,"source":6423,"url":24777,"thumbnail":24778},"We’re expanding our Gemini 2.5 family of models","Gemini 2.5 Flash and Pro are now generally available, and we’re introducing 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet.",["Date","2025-06-17T16:01:00.000Z"],"https://deepmind.google/discover/blog/were-expanding-our-gemini-25-family-of-models/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_bundle_keyword_social-share_1920-1080.width-1300.png","src/content/posts/2025-06-17-were-expanding-our-gemini-25-family-of-models.md","ebf50daf2ac7e27b",{"html":25,"metadata":24782},{"headings":24783,"localImagePaths":24784,"remoteImagePaths":24785,"frontmatter":24786,"imagePaths":24788},[],[],[],{"title":24774,"description":24775,"summary":24775,"pubDate":24787,"source":6423,"url":24777,"thumbnail":24778},"Tue, 17 Jun 2025 16:01:00 +0000",[],"2025-06-17-were-expanding-our-gemini-25-family-of-models.md","2025-06-18-716開催ウェビナーllm開発におけるgpuクラウドとオンプレの徹底比較-独自llmの開発秘話からgpuコストを削減するための具体的なポイントまで一挙大公開",{"id":24790,"data":24792,"filePath":24798,"digest":24799,"rendered":24800,"legacyId":24808},{"title":24793,"description":24794,"summary":24794,"pubDate":24795,"source":24252,"url":24796,"thumbnail":24797},"【7/16開催ウェビナー】LLM開発におけるGPUクラウドとオンプレの徹底比較！  ~独自LLMの開発秘話からGPUコストを削減するための具体的なポイントまで一挙大公開~","\u003Cp>AIポータルメディア「AIsmiley」は、2025年7月16日（水）12時からLLM開発に関するウェビナーを開催します。 本ウェビナーでは、他社のGPUクラウドサービスやオンプレミス環境との比較を通じて、GPUコストを [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/20250716webinar/'>【7/16開催ウェビナー】LLM開発におけるGPUクラウドとオンプレの徹底比較！  ~独自LLMの開発秘話からGPUコストを削減するための具体的なポイントまで一挙大公開~\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-18T01:00:37.000Z"],"https://aismiley.co.jp/ai_news/20250716webinar/","https://aismiley.co.jp/wp-content/uploads/2025/06/700_1200x628_2.jpg","src/content/posts/2025-06-18-716開催ウェビナーllm開発におけるgpuクラウドとオンプレの徹底比較-独自llmの開発秘話からgpuコストを削減するための具体的なポイントまで一挙大公開.md","4de190e6730aaef4",{"html":25,"metadata":24801},{"headings":24802,"localImagePaths":24803,"remoteImagePaths":24804,"frontmatter":24805,"imagePaths":24807},[],[],[],{"title":24793,"description":24794,"summary":24794,"pubDate":24806,"source":24252,"url":24796,"thumbnail":24797},"Wed, 18 Jun 2025 01:00:37 +0000",[],"2025-06-18-716開催ウェビナーllm開発におけるgpuクラウドとオンプレの徹底比較-独自llmの開発秘話からgpuコストを削減するための具体的なポイントまで一挙大公開.md","2025-06-18-archaic日本語特化型ragシステムaiを開発製造業分野でトップクラスの正答率",{"id":24809,"data":24811,"filePath":24817,"digest":24818,"rendered":24819,"legacyId":24827},{"title":24812,"description":24813,"summary":24813,"pubDate":24814,"source":24252,"url":24815,"thumbnail":24816},"Archaic、日本語特化型RAGシステムAIを開発。製造業分野でトップクラスの正答率","\u003Cp>Archaicは日本語業務文書に特化したRAGシステムAIを開発。公開ベンチマークの評価データセットで製造業カテゴリと全体平均でトップクラスの正答率を記録しました。 このニュースのポイント 日本語業務文書に特化したRAG [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/archaic-develops-japanese-specific-rag/'>Archaic、日本語特化型RAGシステムAIを開発。製造業分野でトップクラスの正答率\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-18T05:51:09.000Z"],"https://aismiley.co.jp/ai_news/archaic-develops-japanese-specific-rag/","https://aismiley.co.jp/wp-content/uploads/2025/06/Archaic-develops-Japanese-specific-RAG1.png","src/content/posts/2025-06-18-archaic日本語特化型ragシステムaiを開発製造業分野でトップクラスの正答率.md","05aac81c12acb892",{"html":25,"metadata":24820},{"headings":24821,"localImagePaths":24822,"remoteImagePaths":24823,"frontmatter":24824,"imagePaths":24826},[],[],[],{"title":24812,"description":24813,"summary":24813,"pubDate":24825,"source":24252,"url":24815,"thumbnail":24816},"Wed, 18 Jun 2025 05:51:09 +0000",[],"2025-06-18-archaic日本語特化型ragシステムaiを開発製造業分野でトップクラスの正答率.md","2025-06-18-hear-a-podcast-discussion-about-geminis-coding-capabilities",{"id":24828,"data":24830,"filePath":24836,"digest":24837,"rendered":24838,"legacyId":24846},{"title":24831,"description":24832,"summary":24832,"pubDate":24833,"source":23485,"url":24834,"thumbnail":24835},"Hear a podcast discussion about Gemini’s coding capabilities.","\u003Cimg src='https://storage.googleapis.com/gweb-uniblog-publish-prod/images/ep8_thumbnail.max-600x600.format-webp.webp' />The latest episode of the Google AI: Release Notes podcast focuses on how the Gemini team built one of the world’s leading AI coding models.Host Logan Kilpatrick chats w…",["Date","2025-06-18T10:28:00.000Z"],"https://blog.google/products/gemini/gemini-coding-podcast/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/ep8_thumbnail.max-1440x810.png","src/content/posts/2025-06-18-hear-a-podcast-discussion-about-geminis-coding-capabilities.md","96211bdac223021f",{"html":25,"metadata":24839},{"headings":24840,"localImagePaths":24841,"remoteImagePaths":24842,"frontmatter":24843,"imagePaths":24845},[],[],[],{"title":24831,"description":24832,"summary":24832,"pubDate":24844,"source":23485,"url":24834,"thumbnail":24835},"Wed, 18 Jun 2025 10:28:00 +0000",[],"2025-06-18-hear-a-podcast-discussion-about-geminis-coding-capabilities.md","2025-06-18-preparing-for-future-ai-risks-in-biology",{"id":24847,"data":24849,"filePath":24854,"digest":24855,"rendered":24856,"legacyId":24864},{"title":24850,"description":24851,"summary":24851,"pubDate":24852,"source":19,"url":24853,"thumbnail":21},"Preparing for future AI risks in biology","Advanced AI can transform biology and medicine—but also raises biosecurity risks. We’re proactively assessing capabilities and implementing safeguards to prevent misuse.",["Date","2025-06-18T10:00:00.000Z"],"https://openai.com/blog/preparing-for-future-ai-capabilities-in-biology","src/content/posts/2025-06-18-preparing-for-future-ai-risks-in-biology.md","19fff23ab063e36a",{"html":25,"metadata":24857},{"headings":24858,"localImagePaths":24859,"remoteImagePaths":24860,"frontmatter":24861,"imagePaths":24863},[],[],[],{"title":24850,"description":24851,"summary":24851,"pubDate":24862,"source":19,"url":24853,"thumbnail":21},"Wed, 18 Jun 2025 10:00:00 GMT",[],"2025-06-18-preparing-for-future-ai-risks-in-biology.md","2025-06-18-search-live-talk-listen-and-explore-in-real-time-with-ai-mode",{"id":24865,"data":24867,"filePath":24873,"digest":24874,"rendered":24875,"legacyId":24883},{"title":24868,"description":24869,"summary":24869,"pubDate":24870,"source":23485,"url":24871,"thumbnail":24872},"Search Live: Talk, listen and explore in real time with AI Mode","Logos for AI Mode in Search and Search Live in front of a black background",["Date","2025-06-18T16:00:00.000Z"],"https://blog.google/products/search/search-live-ai-mode/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/SearchLive_SS.width-1300.png","src/content/posts/2025-06-18-search-live-talk-listen-and-explore-in-real-time-with-ai-mode.md","0665eb3920d00306",{"html":25,"metadata":24876},{"headings":24877,"localImagePaths":24878,"remoteImagePaths":24879,"frontmatter":24880,"imagePaths":24882},[],[],[],{"title":24868,"description":24869,"summary":24869,"pubDate":24881,"source":23485,"url":24871,"thumbnail":24872},"Wed, 18 Jun 2025 16:00:00 +0000",[],"2025-06-18-search-live-talk-listen-and-explore-in-real-time-with-ai-mode.md","2025-06-18-toward-understanding-and-preventing-misalignment-generalization",{"id":24884,"data":24886,"filePath":24891,"digest":24892,"rendered":24893,"legacyId":24900},{"title":24887,"description":24888,"summary":24888,"pubDate":24889,"source":19,"url":24890,"thumbnail":21},"Toward understanding and preventing misalignment generalization","We study how training on incorrect responses can cause broader misalignment in language models and identify an internal feature driving this behavior—one that can be reversed with minimal fine-tuning.",["Date","2025-06-18T10:00:00.000Z"],"https://openai.com/blog/emergent-misalignment","src/content/posts/2025-06-18-toward-understanding-and-preventing-misalignment-generalization.md","7d6a94f4147e931b",{"html":25,"metadata":24894},{"headings":24895,"localImagePaths":24896,"remoteImagePaths":24897,"frontmatter":24898,"imagePaths":24899},[],[],[],{"title":24887,"description":24888,"summary":24888,"pubDate":24862,"source":19,"url":24890,"thumbnail":21},[],"2025-06-18-toward-understanding-and-preventing-misalignment-generalization.md","2025-06-19-lora-fine-tuning-flux1-dev-on-consumer-hardware",{"id":24901,"data":24903,"filePath":24909,"digest":24910,"rendered":24911,"legacyId":24919},{"title":24904,"description":25,"summary":24905,"pubDate":24906,"source":2720,"url":24907,"thumbnail":24908},"(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware","(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware In our previous post, Exploring Quantization Back...",["Date","2025-06-19T00:00:00.000Z"],"https://huggingface.co/blog/flux-qlora","https://huggingface.co/blog/assets/flux-qlora/thumbnail.png","src/content/posts/2025-06-19-lora-fine-tuning-flux1-dev-on-consumer-hardware.md","7f89b14e047c9a11",{"html":25,"metadata":24912},{"headings":24913,"localImagePaths":24914,"remoteImagePaths":24915,"frontmatter":24916,"imagePaths":24918},[],[],[],{"title":24904,"description":25,"summary":24905,"pubDate":24917,"source":2720,"url":24907,"thumbnail":24908},"Thu, 19 Jun 2025 00:00:00 GMT",[],"2025-06-19-lora-fine-tuning-flux1-dev-on-consumer-hardware.md","2025-06-18-breaking-bonds-breaking-ground-advancing-the-accuracy-of-computational-chemistry-with-deep-learning",{"id":24920,"data":24922,"filePath":24927,"digest":24928,"rendered":24929,"legacyId":24937},{"title":24923,"description":24924,"summary":24924,"pubDate":24925,"source":23065,"url":24926,"thumbnail":23067},"Breaking bonds, breaking ground: Advancing the accuracy of computational chemistry with deep learning","\u003Cp>Microsoft researchers achieved a breakthrough in the accuracy of DFT, a method for predicting the properties of molecules and materials, by using deep learning. This work can lead to better batteries, green fertilizers, precision drug discovery, and more.\u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/blog/breaking-bonds-breaking-ground-advancing-the-accuracy-of-computational-chemistry-with-deep-learning/'>Breaking bonds, breaking ground: Advancing the accuracy of computational chemistry with deep learning\u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-06-18T10:01:47.000Z"],"https://www.microsoft.com/en-us/research/blog/breaking-bonds-breaking-ground-advancing-the-accuracy-of-computational-chemistry-with-deep-learning/","src/content/posts/2025-06-18-breaking-bonds-breaking-ground-advancing-the-accuracy-of-computational-chemistry-with-deep-learning.md","d34961018ef7f7c4",{"html":25,"metadata":24930},{"headings":24931,"localImagePaths":24932,"remoteImagePaths":24933,"frontmatter":24934,"imagePaths":24936},[],[],[],{"title":24923,"description":24924,"summary":24924,"pubDate":24935,"source":23065,"url":24926,"thumbnail":23067},"Wed, 18 Jun 2025 10:01:47 +0000",[],"2025-06-18-breaking-bonds-breaking-ground-advancing-the-accuracy-of-computational-chemistry-with-deep-learning.md","2025-06-19-microsoftまたaiレイオフか-営業職中心に数千人を削減-bloomberg報道",{"id":24938,"data":24940,"filePath":24947,"digest":24948,"rendered":24949,"legacyId":24957},{"title":24941,"description":24942,"summary":24942,"pubDate":24943,"source":24944,"url":24945,"thumbnail":24946},"Microsoft、また“AIレイオフ”か　営業職中心に数千人を削減　Bloomberg報道","米Microsoftが、営業部門を中心に数千人規模の人員削減を計画していると、米Bloombergが報じた。AIに関する支出増に伴う施策という。",["Date","2025-06-19T03:40:00.000Z"],"ITmedia AI","https://www.itmedia.co.jp/news/articles/2506/19/news077.html","https://image.itmedia.co.jp/news/articles/2506/19/cover_news077.jpg","src/content/posts/2025-06-19-microsoftまたaiレイオフか-営業職中心に数千人を削減-bloomberg報道.md","3282e6457b2ba984",{"html":25,"metadata":24950},{"headings":24951,"localImagePaths":24952,"remoteImagePaths":24953,"frontmatter":24954,"imagePaths":24956},[],[],[],{"title":24941,"description":24942,"summary":24942,"pubDate":24955,"source":24944,"url":24945,"thumbnail":24946},"Thu, 19 Jun 2025 12:40:00 +0900",[],"2025-06-19-microsoftまたaiレイオフか-営業職中心に数千人を削減-bloomberg報道.md","2025-06-19-napkin-aiナプキンaiとは使い方からかかる料金まで詳しく解説",{"id":24958,"data":24960,"filePath":24966,"digest":24967,"rendered":24968,"legacyId":24976},{"title":24961,"description":24962,"summary":24962,"pubDate":24963,"source":24252,"url":24964,"thumbnail":24965},"Napkin AI（ナプキンAI）とは？使い方からかかる料金まで詳しく解説","\u003Cp>仕事で使う文章やアイデアをまとめてわかりやすい形で整理したいので、何か良いツールはないか探しているけれど、なかなか見つからず困っている人はいませんか？ 仕事で周囲の人に物事をわかりやすく説明するためには、文章だけではなく [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/what-is-napkinai/'>Napkin AI（ナプキンAI）とは？使い方からかかる料金まで詳しく解説\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-19T05:40:52.000Z"],"https://aismiley.co.jp/ai_news/what-is-napkinai/","https://aismiley.co.jp/wp-content/uploads/2025/02/what-is-dify.jpg","src/content/posts/2025-06-19-napkin-aiナプキンaiとは使い方からかかる料金まで詳しく解説.md","c578b9b8074ceb9c",{"html":25,"metadata":24969},{"headings":24970,"localImagePaths":24971,"remoteImagePaths":24972,"frontmatter":24973,"imagePaths":24975},[],[],[],{"title":24961,"description":24962,"summary":24962,"pubDate":24974,"source":24252,"url":24964,"thumbnail":24965},"Thu, 19 Jun 2025 05:40:52 +0000",[],"2025-06-19-napkin-aiナプキンaiとは使い方からかかる料金まで詳しく解説.md","2025-06-19-youtubeショートに動画生成aiveo-3統合へ-今夏",{"id":24977,"data":24979,"filePath":24985,"digest":24986,"rendered":24987,"legacyId":24995},{"title":24980,"description":24981,"summary":24981,"pubDate":24982,"source":24944,"url":24983,"thumbnail":24984},"YouTubeショートに動画生成AI「Veo 3」統合へ　今夏","米YouTubeが、YouTube Shortsに米Googleの動画生成AI「Veo 3」を統合すると発表した。",["Date","2025-06-19T04:53:00.000Z"],"https://www.itmedia.co.jp/news/articles/2506/19/news085.html","https://image.itmedia.co.jp/news/articles/2506/19/cover_news085.jpg","src/content/posts/2025-06-19-youtubeショートに動画生成aiveo-3統合へ-今夏.md","bb6ff45e13e890c6",{"html":25,"metadata":24988},{"headings":24989,"localImagePaths":24990,"remoteImagePaths":24991,"frontmatter":24992,"imagePaths":24994},[],[],[],{"title":24980,"description":24981,"summary":24981,"pubDate":24993,"source":24944,"url":24983,"thumbnail":24984},"Thu, 19 Jun 2025 13:53:00 +0900",[],"2025-06-19-youtubeショートに動画生成aiveo-3統合へ-今夏.md","2025-06-19-サイバーエージェントaiエージェントに年4億円投資-開発エンジニアに月200ドル支援-業務外でも試せる",{"id":24996,"data":24998,"filePath":25004,"digest":25005,"rendered":25006,"legacyId":25014},{"title":24999,"description":25000,"summary":25000,"pubDate":25001,"source":24944,"url":25002,"thumbnail":25003},"サイバーエージェント、AIエージェントに年4億円投資　開発エンジニアに月200ドル支援　業務外でも試せる","サイバーエージェントは、エンジニア約1200人に1人当たり月200ドル、開発AIエージェント導入費用をサポートする。",["Date","2025-06-19T06:59:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/19/news100.html","https://image.itmedia.co.jp/aiplus/articles/2506/19/cover_news100.jpg","src/content/posts/2025-06-19-サイバーエージェントaiエージェントに年4億円投資-開発エンジニアに月200ドル支援-業務外でも試せる.md","267cfddb3f6179bb",{"html":25,"metadata":25007},{"headings":25008,"localImagePaths":25009,"remoteImagePaths":25010,"frontmatter":25011,"imagePaths":25013},[],[],[],{"title":24999,"description":25000,"summary":25000,"pubDate":25012,"source":24944,"url":25002,"thumbnail":25003},"Thu, 19 Jun 2025 15:59:00 +0900",[],"2025-06-19-サイバーエージェントaiエージェントに年4億円投資-開発エンジニアに月200ドル支援-業務外でも試せる.md","2025-06-19-ディズニーらと係争中のmidjourney動画生成aiv1発表-月10ドルから-どんな動画が作れる",{"id":25015,"data":25017,"filePath":25023,"digest":25024,"rendered":25025,"legacyId":25033},{"title":25018,"description":25019,"summary":25019,"pubDate":25020,"source":24944,"url":25021,"thumbnail":25022},"ディズニーらと係争中のMidjourney、動画生成AI「V1」発表　月10ドルから　どんな動画が作れる？","画像生成AIサービス「Midjourney」を提供する米Midjourney社は、同社初の動画生成AI「V1」を発表した。画像を入力すると、5秒の動画を生成する。まずは有料ユーザー向けに、MidjourneyのWeb版で提供を始める。",["Date","2025-06-19T06:38:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/19/news073.html","https://image.itmedia.co.jp/aiplus/articles/2506/19/cover_news073.jpg","src/content/posts/2025-06-19-ディズニーらと係争中のmidjourney動画生成aiv1発表-月10ドルから-どんな動画が作れる.md","3f66cbade1a72807",{"html":25,"metadata":25026},{"headings":25027,"localImagePaths":25028,"remoteImagePaths":25029,"frontmatter":25030,"imagePaths":25032},[],[],[],{"title":25018,"description":25019,"summary":25019,"pubDate":25031,"source":24944,"url":25021,"thumbnail":25022},"Thu, 19 Jun 2025 15:38:00 +0900",[],"2025-06-19-ディズニーらと係争中のmidjourney動画生成aiv1発表-月10ドルから-どんな動画が作れる.md","2025-06-19-映画岸辺露伴は動かない劇伴制作元aiの利用について声明-1万6000文字超で訴えたaiを使う理由は",{"id":25034,"data":25036,"filePath":25042,"digest":25043,"rendered":25044,"legacyId":25052},{"title":25037,"description":25038,"summary":25038,"pubDate":25039,"source":24944,"url":25040,"thumbnail":25041},"映画「岸辺露伴は動かない」劇伴制作元、AIの利用について声明　“1万6000文字超”で訴えた、AIを使う理由は？","作曲家・菊地成孔さんが率いる楽曲制作グループ「新音楽制作工房」は、生成AIの利用に関する声明を出した。同グループを巡っては、映画「岸辺露伴は動かない 懺悔室」の劇伴をAIを活用して制作。Xで賛否を巻き起こしていた。",["Date","2025-06-19T00:00:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/17/news107.html","https://image.itmedia.co.jp/aiplus/articles/2506/17/cover_news107.jpg","src/content/posts/2025-06-19-映画岸辺露伴は動かない劇伴制作元aiの利用について声明-1万6000文字超で訴えたaiを使う理由は.md","ac1839a7b0ccf4a1",{"html":25,"metadata":25045},{"headings":25046,"localImagePaths":25047,"remoteImagePaths":25048,"frontmatter":25049,"imagePaths":25051},[],[],[],{"title":25037,"description":25038,"summary":25038,"pubDate":25050,"source":24944,"url":25040,"thumbnail":25041},"Thu, 19 Jun 2025 09:00:00 +0900",[],"2025-06-19-映画岸辺露伴は動かない劇伴制作元aiの利用について声明-1万6000文字超で訴えたaiを使う理由は.md","2025-06-20-mixiの会話ロボasdadhd当事者向けの新モデル発売-会話を通じ自身の特性理解するサポート",{"id":25053,"data":25055,"filePath":25061,"digest":25062,"rendered":25063,"legacyId":25071},{"title":25056,"description":25057,"summary":25057,"pubDate":25058,"source":24944,"url":25059,"thumbnail":25060},"MIXIの会話ロボ、ASD・ADHD当事者向けの新モデル発売　会話を通じ、自身の特性理解するサポート","MIXIは、自閉スペクトラム症（ASD）と注意欠如・多動症（ADHD）当事者の社会人向けに、AI会話ロボット「Romi」の新モデル「ライフスキルトレーニングモデル」を発売した。Romiとの会話を通じて、ASD・ADHD当事者が自身の特性を理解する手助けをするという。",["Date","2025-06-20T09:36:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/20/news099.html","https://image.itmedia.co.jp/aiplus/articles/2506/20/cover_news099.jpg","src/content/posts/2025-06-20-mixiの会話ロボasdadhd当事者向けの新モデル発売-会話を通じ自身の特性理解するサポート.md","5d0eaea3cace07f0",{"html":25,"metadata":25064},{"headings":25065,"localImagePaths":25066,"remoteImagePaths":25067,"frontmatter":25068,"imagePaths":25070},[],[],[],{"title":25056,"description":25057,"summary":25057,"pubDate":25069,"source":24944,"url":25059,"thumbnail":25060},"Fri, 20 Jun 2025 18:36:00 +0900",[],"2025-06-20-mixiの会話ロボasdadhd当事者向けの新モデル発売-会話を通じ自身の特性理解するサポート.md","2025-06-20-openaiaiによる生物兵器開発リスクに警鐘",{"id":25072,"data":25074,"filePath":25080,"digest":25081,"rendered":25082,"legacyId":25090},{"title":25075,"description":25076,"summary":25076,"pubDate":25077,"source":24944,"url":25078,"thumbnail":25079},"OpenAI、AIによる生物兵器開発リスクに警鐘","OpenAIは、AIが悪用され生物兵器開発につながる深刻なリスクがあると警告した。同社の将来のAIモデルは専門知識のない人物による生物学的脅威の作成を可能にする恐れがあるという。有害リクエストの拒否や専門家との連携、疑わしい行為の監視などの多角的な対策を講じ、社会全体の防御力向上も提唱している。",["Date","2025-06-19T22:04:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/20/news054.html","https://image.itmedia.co.jp/aiplus/articles/2506/20/cover_news054.jpg","src/content/posts/2025-06-20-openaiaiによる生物兵器開発リスクに警鐘.md","cbed9ab81970ac7d",{"html":25,"metadata":25083},{"headings":25084,"localImagePaths":25085,"remoteImagePaths":25086,"frontmatter":25087,"imagePaths":25089},[],[],[],{"title":25075,"description":25076,"summary":25076,"pubDate":25088,"source":24944,"url":25078,"thumbnail":25079},"Fri, 20 Jun 2025 07:04:00 +0900",[],"2025-06-20-openaiaiによる生物兵器開発リスクに警鐘.md","2025-06-19-銅線泥棒を許さない-kddi系企業がawsで一計案じる",{"id":25091,"data":25093,"filePath":25099,"digest":25100,"rendered":25101,"legacyId":25109},{"title":25094,"description":25095,"summary":25095,"pubDate":25096,"source":24944,"url":25097,"thumbnail":25098},"銅線泥棒を許さない　KDDI系企業がAWSで一計案じる","KDDIスマートドローンは「Amazon SageMaker」と連携したAI解析ドローンシステムを開発し、太陽光発電施設の夜間警備に実装した。遠隔運航とリアルタイム解析を組み合わせ、人的負担を軽減しつつ盗難対策を強化している。",["Date","2025-06-19T01:00:00.000Z"],"https://kn.itmedia.co.jp/kn/articles/2506/19/news033.html","https://image.itmedia.co.jp/kn/articles/2506/19/cover_news033.png","src/content/posts/2025-06-19-銅線泥棒を許さない-kddi系企業がawsで一計案じる.md","0cb48a6838389c3a",{"html":25,"metadata":25102},{"headings":25103,"localImagePaths":25104,"remoteImagePaths":25105,"frontmatter":25106,"imagePaths":25108},[],[],[],{"title":25094,"description":25095,"summary":25095,"pubDate":25107,"source":24944,"url":25097,"thumbnail":25098},"Thu, 19 Jun 2025 10:00:00 +0900",[],"2025-06-19-銅線泥棒を許さない-kddi系企業がawsで一計案じる.md","2025-06-20-openaiaiモデルに潜む悪ガキペルソナの更生について説明",{"id":25110,"data":25112,"filePath":25118,"digest":25119,"rendered":25120,"legacyId":25128},{"title":25113,"description":25114,"summary":25114,"pubDate":25115,"source":24944,"url":25116,"thumbnail":25117},"OpenAI、AIモデルに潜む“悪ガキペルソナ”の更生について説明","OpenAIは、AIモデルが意図せず「悪ガキペルソナ」のような望ましくない振る舞いをする「誤アラインメント」に関する論文を公開した。不適切な学習が特定のペルソナを増幅させることが原因だという。対策として高品質なデータの使用が重要で、発生後も少量の良質なデータで再調整すれば修復可能としている。",["Date","2025-06-20T01:58:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/20/news062.html","https://image.itmedia.co.jp/aiplus/articles/2506/20/cover_news062.jpg","src/content/posts/2025-06-20-openaiaiモデルに潜む悪ガキペルソナの更生について説明.md","6236666676195214",{"html":25,"metadata":25121},{"headings":25122,"localImagePaths":25123,"remoteImagePaths":25124,"frontmatter":25125,"imagePaths":25127},[],[],[],{"title":25113,"description":25114,"summary":25114,"pubDate":25126,"source":24944,"url":25116,"thumbnail":25117},"Fri, 20 Jun 2025 10:58:00 +0900",[],"2025-06-20-openaiaiモデルに潜む悪ガキペルソナの更生について説明.md","2025-06-20-researchers-present-bold-ideas-for-ai-at-mit-generative-ai-impact-consortium-kickoff-event",{"id":25129,"data":25131,"filePath":25137,"digest":25138,"rendered":25139,"legacyId":25147},{"title":25132,"description":25133,"summary":25133,"pubDate":25134,"source":21340,"url":25135,"thumbnail":25136},"Researchers present bold ideas for AI at MIT Generative AI Impact Consortium kickoff event","Presentations targeted high-impact intersections of AI and other areas, such as health care, business, and education.",["Date","2025-06-20T20:45:00.000Z"],"https://news.mit.edu/2025/researchers-present-bold-ideas-ai-mit-generative-ai-impact-consortium-event-0620","https://news.mit.edu/sites/default/files/images/202506/mit-Anantha.jpg","src/content/posts/2025-06-20-researchers-present-bold-ideas-for-ai-at-mit-generative-ai-impact-consortium-kickoff-event.md","7b6a4f4fd6f97848",{"html":25,"metadata":25140},{"headings":25141,"localImagePaths":25142,"remoteImagePaths":25143,"frontmatter":25144,"imagePaths":25146},[],[],[],{"title":25132,"description":25133,"summary":25133,"pubDate":25145,"source":21340,"url":25135,"thumbnail":25136},"Fri, 20 Jun 2025 16:45:00 -0400",[],"2025-06-20-researchers-present-bold-ideas-for-ai-at-mit-generative-ai-impact-consortium-kickoff-event.md","2025-06-20-アングラaiwormgptの亜種が登場-grokなどのモデルを改造か",{"id":25148,"data":25150,"filePath":25156,"digest":25157,"rendered":25158,"legacyId":25166},{"title":25151,"description":25152,"summary":25152,"pubDate":25153,"source":24944,"url":25154,"thumbnail":25155},"アングラAI「WormGPT」の亜種が登場　Grokなどのモデルを改造か","Cato Networksは、地下フォーラムで流通するWormGPT派生2モデルの存在を報告した。GrokとMixtralを悪用したkeanu-WormGPTとxzin0vich-WormGPTが検閲回避で犯罪支援に利用されている実態が明らかになっている。",["Date","2025-06-19T22:00:00.000Z"],"https://www.itmedia.co.jp/enterprise/articles/2506/20/news022.html","https://image.itmedia.co.jp/enterprise/articles/2506/20/cover_news022.jpg","src/content/posts/2025-06-20-アングラaiwormgptの亜種が登場-grokなどのモデルを改造か.md","569259b51f67478f",{"html":25,"metadata":25159},{"headings":25160,"localImagePaths":25161,"remoteImagePaths":25162,"frontmatter":25163,"imagePaths":25165},[],[],[],{"title":25151,"description":25152,"summary":25152,"pubDate":25164,"source":24944,"url":25154,"thumbnail":25155},"Fri, 20 Jun 2025 07:00:00 +0900",[],"2025-06-20-アングラaiwormgptの亜種が登場-grokなどのモデルを改造か.md","2025-06-20-プロジェクトマネジャーが生成ai導入のために今すぐできる3つのこと-googleがブログで紹介",{"id":25167,"data":25169,"filePath":25175,"digest":25176,"rendered":25177,"legacyId":25184},{"title":25170,"description":25171,"summary":25171,"pubDate":25172,"source":24944,"url":25173,"thumbnail":25174},"プロジェクトマネジャーが生成AI導入のために“今すぐできる”3つのこと　Googleがブログで紹介","プロジェクトマネジャー（PM）が組織に生成AIを導入するため“今すぐできる”3つのこと――米Googleが公式ブログで、こんなチェックリストを公開している。",["Date","2025-06-19T22:00:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/19/news110.html","https://image.itmedia.co.jp/aiplus/articles/2506/19/cover_news110.jpg","src/content/posts/2025-06-20-プロジェクトマネジャーが生成ai導入のために今すぐできる3つのこと-googleがブログで紹介.md","3cfa2469f022f102",{"html":25,"metadata":25178},{"headings":25179,"localImagePaths":25180,"remoteImagePaths":25181,"frontmatter":25182,"imagePaths":25183},[],[],[],{"title":25170,"description":25171,"summary":25171,"pubDate":25164,"source":24944,"url":25173,"thumbnail":25174},[],"2025-06-20-プロジェクトマネジャーが生成ai導入のために今すぐできる3つのこと-googleがブログで紹介.md","2025-06-23-a-brain-to-population-graph-learning-framework-for-diagnosing-brain-disorders",{"id":25185,"data":25187,"filePath":25194,"digest":25195,"rendered":25196,"legacyId":25204},{"title":25188,"description":25189,"summary":25189,"pubDate":25190,"source":25191,"url":25192,"thumbnail":25193},"A Brain-to-Population Graph Learning Framework for Diagnosing Brain Disorders","arXiv:2506.16096v1 Announce Type: cross Abstract: Recent developed graph-based methods for diagnosing brain disorders using functional connectivity highly rely on predefined brain atlases, but overlook the rich information embedded within atlases and the confounding effects of site and phenotype variability. To address these challenges, we propose a two-stage Brain-to-Population Graph Learning (B2P-GL) framework that integrates the semantic similarity of brain regions and condition-based population graph modeling. In the first stage, termed brain representation learning, we leverage brain atlas knowledge from GPT-4 to enrich the graph representation and refine the brain graph through an adaptive node reassignment graph attention network. In the second stage, termed population disorder diagnosis, phenotypic data is incorporated into population graph construction and feature fusion to mitigate confounding effects and enhance diagnosis performance. Experiments on the ABIDE I, ADHD-200, and Rest-meta-MDD datasets show that B2P-GL outperforms state-of-the-art methods in prediction accuracy while enhancing interpretability. Overall, our proposed framework offers a reliable and personalized approach to brain disorder diagnosis, advancing clinical applicability.",["Date","2025-06-23T04:00:00.000Z"],"arXiv AI","https://arxiv.org/abs/2506.16096","https://raisex-llc.github.io/ai-news-curation-site/assets/arxiv.png","src/content/posts/2025-06-23-a-brain-to-population-graph-learning-framework-for-diagnosing-brain-disorders.md","dc1e0df3c631ead8",{"html":25,"metadata":25197},{"headings":25198,"localImagePaths":25199,"remoteImagePaths":25200,"frontmatter":25201,"imagePaths":25203},[],[],[],{"title":25188,"description":25189,"summary":25189,"pubDate":25202,"source":25191,"url":25192,"thumbnail":25193},"Mon, 23 Jun 2025 00:00:00 -0400",[],"2025-06-23-a-brain-to-population-graph-learning-framework-for-diagnosing-brain-disorders.md","2025-06-23-a-community-driven-vision-for-a-new-knowledge-resource-for-ai",{"id":25205,"data":25207,"filePath":25212,"digest":25213,"rendered":25214,"legacyId":25221},{"title":25208,"description":25209,"summary":25209,"pubDate":25210,"source":25191,"url":25211,"thumbnail":25193},"A Community-driven vision for a new Knowledge Resource for AI","arXiv:2506.16596v1 Announce Type: new Abstract: The long-standing goal of creating a comprehensive, multi-purpose knowledge resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and other commercial knowledge graphs, verifiable, general-purpose widely available sources of knowledge remain a critical deficiency in AI infrastructure. Large language models struggle due to knowledge gaps; robotic planning lacks necessary world knowledge; and the detection of factually false information relies heavily on human expertise. What kind of knowledge resource is most needed in AI today? How can modern technology shape its development and evaluation? A recent AAAI workshop gathered over 50 researchers to explore these questions. This paper synthesizes our findings and outlines a community-driven vision for a new knowledge infrastructure. In addition to leveraging contemporary advances in knowledge representation and reasoning, one promising idea is to build an open engineering framework to exploit knowledge modules effectively within the context of practical applications. Such a framework should include sets of conventions and social structures that are adopted by contributors.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16596","src/content/posts/2025-06-23-a-community-driven-vision-for-a-new-knowledge-resource-for-ai.md","3c71b67a473f9cd4",{"html":25,"metadata":25215},{"headings":25216,"localImagePaths":25217,"remoteImagePaths":25218,"frontmatter":25219,"imagePaths":25220},[],[],[],{"title":25208,"description":25209,"summary":25209,"pubDate":25202,"source":25191,"url":25211,"thumbnail":25193},[],"2025-06-23-a-community-driven-vision-for-a-new-knowledge-resource-for-ai.md","2025-06-23-a-deep-learning-and-machine-learning-approach-to-predict-neonatal-death-in-the-context-of-sao-paulo",{"id":25222,"data":25224,"filePath":25229,"digest":25230,"rendered":25231,"legacyId":25238},{"title":25225,"description":25226,"summary":25226,"pubDate":25227,"source":25191,"url":25228,"thumbnail":25193},"A deep learning and machine learning approach to predict neonatal death in the context of S~ao Paulo","arXiv:2506.16929v1 Announce Type: cross Abstract: Neonatal death is still a concerning reality for underdeveloped and even some developed countries. Worldwide data indicate that 26.693 babies out of 1,000 births die, according to Macro Trades. To reduce this number, early prediction of endangered babies is crucial. Such prediction enables the opportunity to take ample care of the child and mother so that early child death can be avoided. In this context, machine learning was used to determine whether a newborn baby is at risk. To train the predictive model, historical data of 1.4 million newborns was used. Machine learning and deep learning techniques such as logical regression, K-nearest neighbor, random forest classifier, extreme gradient boosting (XGBoost), convolutional neural network, and long short-term memory (LSTM) were implemented using the dataset to identify the most accurate model for predicting neonatal mortality. Among the machine learning algorithms, XGBoost and random forest classifier achieved the best accuracy with 94%, while among the deep learning models, LSTM delivered the highest accuracy with 99%. Therefore, using LSTM appears to be the most suitable approach to predict whether precautionary measures for a child are necessary.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16929","src/content/posts/2025-06-23-a-deep-learning-and-machine-learning-approach-to-predict-neonatal-death-in-the-context-of-sao-paulo.md","89e10fcfddacb774",{"html":25,"metadata":25232},{"headings":25233,"localImagePaths":25234,"remoteImagePaths":25235,"frontmatter":25236,"imagePaths":25237},[],[],[],{"title":25225,"description":25226,"summary":25226,"pubDate":25202,"source":25191,"url":25228,"thumbnail":25193},[],"2025-06-23-a-deep-learning-and-machine-learning-approach-to-predict-neonatal-death-in-the-context-of-sao-paulo.md","2025-06-23-a-hybrid-deberta-and-gated-broad-learning-system-for-cyberbullying-detection-in-english-text",{"id":25239,"data":25241,"filePath":25246,"digest":25247,"rendered":25248,"legacyId":25255},{"title":25242,"description":25243,"summary":25243,"pubDate":25244,"source":25191,"url":25245,"thumbnail":25193},"A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text","arXiv:2506.16052v1 Announce Type: cross Abstract: The proliferation of online communication platforms has created unprecedented opportunities for global connectivity while simultaneously enabling harmful behaviors such as cyberbullying, which affects approximately 54.4% of teenagers according to recent research. This paper presents a hybrid architecture that combines the contextual understanding capabilities of transformer-based models with the pattern recognition strengths of broad learning systems for effective cyberbullying detection. This approach integrates a modified DeBERTa model augmented with Squeeze-and-Excitation blocks and sentiment analysis capabilities with a Gated Broad Learning System (GBLS) classifier, creating a synergistic framework that outperforms existing approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa + GBLS model achieved good performance on four English datasets: 79.3% accuracy on HateXplain, 95.41% accuracy on SOSNet, 91.37% accuracy on Mendeley-I, and 94.67% accuracy on Mendeley-II. Beyond performance gains, the framework incorporates comprehensive explainability mechanisms including token-level attribution analysis, LIME-based local interpretations, and confidence calibration, addressing critical transparency requirements in automated content moderation. Ablation studies confirm the meaningful contribution of each architectural component, while failure case analysis reveals specific challenges in detecting implicit bias and sarcastic content, providing valuable insights for future improvements in cyberbullying detection systems.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16052","src/content/posts/2025-06-23-a-hybrid-deberta-and-gated-broad-learning-system-for-cyberbullying-detection-in-english-text.md","220f72c75656f673",{"html":25,"metadata":25249},{"headings":25250,"localImagePaths":25251,"remoteImagePaths":25252,"frontmatter":25253,"imagePaths":25254},[],[],[],{"title":25242,"description":25243,"summary":25243,"pubDate":25202,"source":25191,"url":25245,"thumbnail":25193},[],"2025-06-23-a-hybrid-deberta-and-gated-broad-learning-system-for-cyberbullying-detection-in-english-text.md","2025-06-23-a-implies-b-circuit-analysis-in-llms-for-propositional-logical-reasoning",{"id":25256,"data":25258,"filePath":25263,"digest":25264,"rendered":25265,"legacyId":25272},{"title":25259,"description":25260,"summary":25260,"pubDate":25261,"source":25191,"url":25262,"thumbnail":25193},"A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning","arXiv:2411.04105v4 Announce Type: replace-cross Abstract: Due to the size and complexity of modern large language models (LLMs), it has proven challenging to uncover the underlying mechanisms that models use to solve reasoning problems. For instance, is their reasoning for a specific problem localized to certain parts of the network? Do they break down the reasoning problem into modular components that are then executed as sequential steps as we go deeper in the model? To better understand the reasoning capability of LLMs, we study a minimal propositional logic problem that requires combining multiple facts to arrive at a solution. By studying this problem on Mistral and Gemma models, up to 27B parameters, we illuminate the core components the models use to solve such logic problems. From a mechanistic interpretability point of view, we use causal mediation analysis to uncover the pathways and components of the LLMs' reasoning processes. Then, we offer fine-grained insights into the functions of attention heads in different layers. We not only find a sparse circuit that computes the answer, but we decompose it into sub-circuits that have four distinct and modular uses. Finally, we reveal that three distinct models -- Mistral-7B, Gemma-2-9B and Gemma-2-27B -- contain analogous but not identical mechanisms.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2411.04105","src/content/posts/2025-06-23-a-implies-b-circuit-analysis-in-llms-for-propositional-logical-reasoning.md","55ac56de892292ef",{"html":25,"metadata":25266},{"headings":25267,"localImagePaths":25268,"remoteImagePaths":25269,"frontmatter":25270,"imagePaths":25271},[],[],[],{"title":25259,"description":25260,"summary":25260,"pubDate":25202,"source":25191,"url":25262,"thumbnail":25193},[],"2025-06-23-a-implies-b-circuit-analysis-in-llms-for-propositional-logical-reasoning.md","2025-06-23-a-minimalist-method-for-fine-tuning-text-to-image-diffusion-models",{"id":25273,"data":25275,"filePath":25280,"digest":25281,"rendered":25282,"legacyId":25289},{"title":25276,"description":25277,"summary":25277,"pubDate":25278,"source":25191,"url":25279,"thumbnail":25193},"A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models","arXiv:2506.12036v2 Announce Type: replace-cross Abstract: Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the 'golden noise' hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.12036","src/content/posts/2025-06-23-a-minimalist-method-for-fine-tuning-text-to-image-diffusion-models.md","da11b5c3c0d60ce9",{"html":25,"metadata":25283},{"headings":25284,"localImagePaths":25285,"remoteImagePaths":25286,"frontmatter":25287,"imagePaths":25288},[],[],[],{"title":25276,"description":25277,"summary":25277,"pubDate":25202,"source":25191,"url":25279,"thumbnail":25193},[],"2025-06-23-a-minimalist-method-for-fine-tuning-text-to-image-diffusion-models.md","2025-06-23-a-minimalist-optimizer-design-for-llm-pretraining",{"id":25290,"data":25292,"filePath":25297,"digest":25298,"rendered":25299,"legacyId":25306},{"title":25293,"description":25294,"summary":25294,"pubDate":25295,"source":25191,"url":25296,"thumbnail":25193},"A Minimalist Optimizer Design for LLM Pretraining","arXiv:2506.16659v1 Announce Type: cross Abstract: Training large language models (LLMs) typically relies on adaptive optimizers such as Adam, which require significant memory to maintain first- and second-moment matrices, known as optimizer states. While recent works such as GaLore, Fira, and APOLLO have proposed state-compressed variants to reduce memory consumption, a fundamental question remains: What is the minimal amount of optimizer state that is truly necessary to retain state-of-the-art performance in LLM pretraining? In this work, we systematically investigate this question using a bottom-up approach. We find that two memory- and compute-efficient optimization techniques are particularly effective: (1) column-wise gradient normalization significantly boosts the performance of plain SGD without requiring momentum; and (2) adding first-order momentum only to the output layer - where gradient variance is highest - yields performance competitive with fully adaptive methods such as Muon. Based on these insights, we propose SCALE (Stochastic Column-normalized Last-layer Momentum), a new optimizer that combines column-normalized SGD with last-layer momentum, where column normalization refers to normalizing the gradient along the output dimension. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the performance of Adam while using only 35-45% of the total memory. It also consistently outperforms memory-efficient optimizers such as GaLore, Fira, and APOLLO, making it a strong candidate for large-scale pretraining under memory constraints. For the LLaMA 7B model, SCALE outperforms the state-of-the-art method APOLLO in terms of both perplexity and memory consumption. In addition, our method serves as a minimalist baseline for more sophisticated optimizer design.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16659","src/content/posts/2025-06-23-a-minimalist-optimizer-design-for-llm-pretraining.md","4430e18a42345a29",{"html":25,"metadata":25300},{"headings":25301,"localImagePaths":25302,"remoteImagePaths":25303,"frontmatter":25304,"imagePaths":25305},[],[],[],{"title":25293,"description":25294,"summary":25294,"pubDate":25202,"source":25191,"url":25296,"thumbnail":25193},[],"2025-06-23-a-minimalist-optimizer-design-for-llm-pretraining.md","2025-06-23-a-quantile-regression-approach-for-remaining-useful-life-estimation-with-state-space-models",{"id":25307,"data":25309,"filePath":25314,"digest":25315,"rendered":25316,"legacyId":25323},{"title":25310,"description":25311,"summary":25311,"pubDate":25312,"source":25191,"url":25313,"thumbnail":25193},"A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models","arXiv:2506.17018v1 Announce Type: new Abstract: Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0, proactively enhancing efficiency through accurate equipment Remaining Useful Life (RUL) prediction, thus optimizing maintenance scheduling and reducing unexpected failures and premature interventions. This paper introduces a novel RUL estimation approach leveraging State Space Models (SSM) for efficient long-term sequence modeling. To handle model uncertainty, Simoultaneous Quantile Regression (SQR) is integrated into the SSM, enabling multiple quantile estimations. The proposed method is benchmarked against traditional sequence modelling techniques (LSTM, Transformer, Informer) using the C-MAPSS dataset. Results demonstrate superior accuracy and computational efficiency of SSM models, underscoring their potential for high-stakes industrial applications.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17018","src/content/posts/2025-06-23-a-quantile-regression-approach-for-remaining-useful-life-estimation-with-state-space-models.md","c9baf402bd450503",{"html":25,"metadata":25317},{"headings":25318,"localImagePaths":25319,"remoteImagePaths":25320,"frontmatter":25321,"imagePaths":25322},[],[],[],{"title":25310,"description":25311,"summary":25311,"pubDate":25202,"source":25191,"url":25313,"thumbnail":25193},[],"2025-06-23-a-quantile-regression-approach-for-remaining-useful-life-estimation-with-state-space-models.md","2025-06-23-a-simple-contrastive-framework-of-item-tokenization-for-generative-recommendation",{"id":25324,"data":25326,"filePath":25331,"digest":25332,"rendered":25333,"legacyId":25340},{"title":25327,"description":25328,"summary":25328,"pubDate":25329,"source":25191,"url":25330,"thumbnail":25193},"A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation","arXiv:2506.16683v1 Announce Type: cross Abstract: Generative retrieval-based recommendation has emerged as a promising paradigm aiming at directly generating the identifiers of the target candidates. However, in large-scale recommendation systems, this approach becomes increasingly cumbersome due to the redundancy and sheer scale of the token space. To overcome these limitations, recent research has explored the use of semantic tokens as an alternative to ID tokens, which typically leveraged reconstruction-based strategies, like RQ-VAE, to quantize content embeddings and significantly reduce the embedding size. However, reconstructive quantization aims for the precise reconstruction of each item embedding independently, which conflicts with the goal of generative retrieval tasks focusing more on differentiating among items. Moreover, multi-modal side information of items, such as descriptive text and images, geographical knowledge in location-based recommendation services, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Nevertheless, effectively integrating such complementary knowledge into existing generative recommendation frameworks remains challenging. To overcome these challenges, we propose a novel unsupervised deep quantization exclusively based on contrastive learning, named SimCIT (a Simple Contrastive Item Tokenization framework). Specifically, different from existing reconstruction-based strategies, SimCIT propose to use a learnable residual quantization module to align with the signals from different modalities of the items, which combines multi-modal knowledge alignment and semantic tokenization in a mutually beneficial contrastive learning framework. Extensive experiments across public datasets and a large-scale industrial dataset from various domains demonstrate SimCIT's effectiveness in LLM-based generative recommendation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16683","src/content/posts/2025-06-23-a-simple-contrastive-framework-of-item-tokenization-for-generative-recommendation.md","802db6ea12053e8f",{"html":25,"metadata":25334},{"headings":25335,"localImagePaths":25336,"remoteImagePaths":25337,"frontmatter":25338,"imagePaths":25339},[],[],[],{"title":25327,"description":25328,"summary":25328,"pubDate":25202,"source":25191,"url":25330,"thumbnail":25193},[],"2025-06-23-a-simple-contrastive-framework-of-item-tokenization-for-generative-recommendation.md","2025-06-23-a-study-of-hybrid-and-evolutionary-metaheuristics-for-single-hidden-layer-feedforward-neural-network-architecture",{"id":25341,"data":25343,"filePath":25348,"digest":25349,"rendered":25350,"legacyId":25357},{"title":25344,"description":25345,"summary":25345,"pubDate":25346,"source":25191,"url":25347,"thumbnail":25193},"A Study of Hybrid and Evolutionary Metaheuristics for Single Hidden Layer Feedforward Neural Network Architecture","arXiv:2506.15737v1 Announce Type: cross Abstract: Training Artificial Neural Networks (ANNs) with Stochastic Gradient Descent (SGD) frequently encounters difficulties, including substantial computing expense and the risk of converging to local optima, attributable to its dependence on partial weight gradients. Therefore, this work investigates Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) - two population-based Metaheuristic Optimizers (MHOs) - as alternatives to SGD to mitigate these constraints. A hybrid PSO-SGD strategy is developed to improve local search efficiency. The findings indicate that the hybrid PSO-SGD technique decreases the median training MSE by 90 to 95 percent relative to conventional GA and PSO across various network sizes (e.g., from around 0.02 to approximately 0.001 in the Sphere function). RMHC attains substantial enhancements, reducing MSE by roughly 85 to 90 percent compared to GA. Simultaneously, RS consistently exhibits errors exceeding 0.3, signifying subpar performance. These findings underscore that hybrid and evolutionary procedures significantly improve training efficiency and accuracy compared to conventional optimization methods and imply that the Building Block Hypothesis (BBH) may still be valid, indicating that advantageous weight structures are retained during evolutionary search.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15737","src/content/posts/2025-06-23-a-study-of-hybrid-and-evolutionary-metaheuristics-for-single-hidden-layer-feedforward-neural-network-architecture.md","d3deaec1b1834a25",{"html":25,"metadata":25351},{"headings":25352,"localImagePaths":25353,"remoteImagePaths":25354,"frontmatter":25355,"imagePaths":25356},[],[],[],{"title":25344,"description":25345,"summary":25345,"pubDate":25202,"source":25191,"url":25347,"thumbnail":25193},[],"2025-06-23-a-study-of-hybrid-and-evolutionary-metaheuristics-for-single-hidden-layer-feedforward-neural-network-architecture.md","2025-06-23-a-survey-of-automatic-hallucination-evaluation-on-natural-language-generation",{"id":25358,"data":25360,"filePath":25365,"digest":25366,"rendered":25367,"legacyId":25374},{"title":25361,"description":25362,"summary":25362,"pubDate":25363,"source":25191,"url":25364,"thumbnail":25193},"A Survey of Automatic Hallucination Evaluation on Natural Language Generation","arXiv:2404.12041v3 Announce Type: replace-cross Abstract: The proliferation of Large Language Models (LLMs) has introduced a critical challenge: accurate hallucination evaluation that ensures model reliability. While Automatic Hallucination Evaluation (AHE) has emerged as essential, the field suffers from methodological fragmentation, hindering both theoretical understanding and practical advancement. This survey addresses this critical gap through a comprehensive analysis of 74 evaluation methods, revealing that 74% specifically target LLMs, a paradigm shift that demands new evaluation frameworks. We formulate a unified evaluation pipeline encompassing datasets and benchmarks, evidence collection strategies, and comparison mechanisms, systematically documenting the evolution from pre-LLM to post-LLM methodologies. Beyond taxonomical organization, we identify fundamental limitations in current approaches and their implications for real-world deployment. To guide future research, we delineate key challenges and propose strategic directions, including enhanced interpretability mechanisms and integration of application-specific evaluation criteria, ultimately providing a roadmap for developing more robust and practical hallucination evaluation systems.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2404.12041","src/content/posts/2025-06-23-a-survey-of-automatic-hallucination-evaluation-on-natural-language-generation.md","0e903bf4705c37b9",{"html":25,"metadata":25368},{"headings":25369,"localImagePaths":25370,"remoteImagePaths":25371,"frontmatter":25372,"imagePaths":25373},[],[],[],{"title":25361,"description":25362,"summary":25362,"pubDate":25202,"source":25191,"url":25364,"thumbnail":25193},[],"2025-06-23-a-survey-of-automatic-hallucination-evaluation-on-natural-language-generation.md","2025-06-23-a-technical-study-into-05b-reasoning-language-models",{"id":25375,"data":25377,"filePath":25382,"digest":25383,"rendered":25384,"legacyId":25391},{"title":25378,"description":25379,"summary":25379,"pubDate":25380,"source":25191,"url":25381,"thumbnail":25193},"A Technical Study into 0.5B Reasoning Language Models","arXiv:2506.13404v2 Announce Type: replace Abstract: The ongoing evolution of language models has led to the development of large-scale architectures that demonstrate exceptional performance across a wide range of tasks. However, these models come with significant computational and energy demands, as well as potential privacy implications. In this context, Small Reasoning Language Models (SRLMs) with approximately 0.5 billion parameters present a compelling alternative due to their remarkable computational efficiency and cost effectiveness, particularly in resource-constrained environments. Despite these advantages, the limited capacity of 0.5 billion parameter models poses challenges in handling complex tasks such as mathematical reasoning and code generation. This research investigates various training strategies, including supervised fine-tuning (SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as their hybrid implementations, to enhance the performance of 0.5B SRLMs. We analyze effective methodologies to bridge the performance gap between SRLMS and larger models and present insights into optimal training pipelines tailored for these smaller architectures. Through extensive experimental validation and analysis, our work aims to provide actionable recommendations for maximizing the reasoning capabilities of 0.5B models.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.13404","src/content/posts/2025-06-23-a-technical-study-into-05b-reasoning-language-models.md","c85d4220308166f6",{"html":25,"metadata":25385},{"headings":25386,"localImagePaths":25387,"remoteImagePaths":25388,"frontmatter":25389,"imagePaths":25390},[],[],[],{"title":25378,"description":25379,"summary":25379,"pubDate":25202,"source":25191,"url":25381,"thumbnail":25193},[],"2025-06-23-a-technical-study-into-05b-reasoning-language-models.md","2025-06-23-adapting-while-learning-grounding-llms-for-scientific-problems-with-intelligent-tool-usage-adaptation",{"id":25392,"data":25394,"filePath":25399,"digest":25400,"rendered":25401,"legacyId":25408},{"title":25395,"description":25396,"summary":25396,"pubDate":25397,"source":25191,"url":25398,"thumbnail":25193},"Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation","arXiv:2411.00412v4 Announce Type: replace-cross Abstract: Large Language Models (LLMs) demonstrate promising capabilities in solving scientific problems but often suffer from the issue of hallucination. While integrating LLMs with tools can mitigate this issue, models fine-tuned on tool usage become overreliant on them and incur unnecessary costs. Inspired by how human experts assess problem complexity before selecting solutions, we propose a novel two-component fine-tuning method, Adapting While Learning (AWL). In the first component, World Knowledge Learning (WKL), LLMs internalize scientific knowledge by learning from tool-generated solutions. In the second component, Tool Usage Adaptation (TUA), we categorize problems as easy or hard based on the model's accuracy, and train it to maintain direct reasoning for easy problems while switching to tools for hard ones. We validate our method on six scientific benchmark datasets across climate science, epidemiology, physics, and other domains. Compared to the original instruct model (8B), models post-trained with AWL achieve 29.11% higher answer accuracy and 12.72% better tool usage accuracy, even surpassing state-of-the-art models including GPT-4o and Claude-3.5 on four custom-created datasets. Our code is open-source at https://github.com/Rose-STL-Lab/Adapting-While-Learning.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2411.00412","src/content/posts/2025-06-23-adapting-while-learning-grounding-llms-for-scientific-problems-with-intelligent-tool-usage-adaptation.md","711c64d6021ab3b5",{"html":25,"metadata":25402},{"headings":25403,"localImagePaths":25404,"remoteImagePaths":25405,"frontmatter":25406,"imagePaths":25407},[],[],[],{"title":25395,"description":25396,"summary":25396,"pubDate":25202,"source":25191,"url":25398,"thumbnail":25193},[],"2025-06-23-adapting-while-learning-grounding-llms-for-scientific-problems-with-intelligent-tool-usage-adaptation.md","2025-06-23-adaptive-experimental-design-for-policy-learning",{"id":25409,"data":25411,"filePath":25416,"digest":25417,"rendered":25418,"legacyId":25425},{"title":25412,"description":25413,"summary":25413,"pubDate":25414,"source":25191,"url":25415,"thumbnail":25193},"Adaptive Experimental Design for Policy Learning","arXiv:2401.03756v4 Announce Type: replace-cross Abstract: This study investigates the contextual best arm identification (BAI) problem, aiming to design an adaptive experiment to identify the best treatment arm conditioned on contextual information (covariates). We consider a decision-maker who assigns treatment arms to experimental units during an experiment and recommends the estimated best treatment arm based on the contexts at the end of the experiment. The decision-maker uses a policy for recommendations, which is a function that provides the estimated best treatment arm given the contexts. In our evaluation, we focus on the worst-case expected regret, a relative measure between the expected outcomes of an optimal policy and our proposed policy. We derive a lower bound for the expected simple regret and then propose a strategy called Adaptive Sampling-Policy Learning (PLAS). We prove that this strategy is minimax rate-optimal in the sense that its leading factor in the regret upper bound matches the lower bound as the number of experimental units increases.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2401.03756","src/content/posts/2025-06-23-adaptive-experimental-design-for-policy-learning.md","5861acad6f04b2c0",{"html":25,"metadata":25419},{"headings":25420,"localImagePaths":25421,"remoteImagePaths":25422,"frontmatter":25423,"imagePaths":25424},[],[],[],{"title":25412,"description":25413,"summary":25413,"pubDate":25202,"source":25191,"url":25415,"thumbnail":25193},[],"2025-06-23-adaptive-experimental-design-for-policy-learning.md","2025-06-23-a-vietnamese-dataset-for-text-segmentation-and-multiple-choices-reading-comprehension",{"id":25426,"data":25428,"filePath":25433,"digest":25434,"rendered":25435,"legacyId":25442},{"title":25429,"description":25430,"summary":25430,"pubDate":25431,"source":25191,"url":25432,"thumbnail":25193},"A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension","arXiv:2506.15978v1 Announce Type: cross Abstract: Vietnamese, the 20th most spoken language with over 102 million native speakers, lacks robust resources for key natural language processing tasks such as text segmentation and machine reading comprehension (MRC). To address this gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset includes 15,942 documents for text segmentation and 16,347 synthetic multiple-choice question-answer pairs generated with human quality assurance, ensuring a reliable and diverse resource. Experiments show that mBERT consistently outperforms monolingual models on both tasks, achieving an accuracy of 88.01% on MRC test set and an F1 score of 63.15% on text segmentation test set. Our analysis reveals that multilingual models excel in NLP tasks for Vietnamese, suggesting potential applications to other under-resourced languages. VSMRC is available at HuggingFace",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15978","src/content/posts/2025-06-23-a-vietnamese-dataset-for-text-segmentation-and-multiple-choices-reading-comprehension.md","955df0aaa0baada6",{"html":25,"metadata":25436},{"headings":25437,"localImagePaths":25438,"remoteImagePaths":25439,"frontmatter":25440,"imagePaths":25441},[],[],[],{"title":25429,"description":25430,"summary":25430,"pubDate":25202,"source":25191,"url":25432,"thumbnail":25193},[],"2025-06-23-a-vietnamese-dataset-for-text-segmentation-and-multiple-choices-reading-comprehension.md","2025-06-23-adaptive-guidance-accelerates-reinforcement-learning-of-reasoning-models",{"id":25443,"data":25445,"filePath":25450,"digest":25451,"rendered":25452,"legacyId":25459},{"title":25446,"description":25447,"summary":25447,"pubDate":25448,"source":25191,"url":25449,"thumbnail":25193},"Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models","arXiv:2506.13923v2 Announce Type: replace-cross Abstract: We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance in two main ways: (1) by compressing pass@$k$ into pass@1 and (2) via 'capability gain' in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B parameters on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $text{Guide}$ -- a new class of online training algorithms. $text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the 'off-policy' trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $text{Guide}$'s components and theoretically analyze Guide's learning efficiency.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.13923","src/content/posts/2025-06-23-adaptive-guidance-accelerates-reinforcement-learning-of-reasoning-models.md","4c4f36b07b54b563",{"html":25,"metadata":25453},{"headings":25454,"localImagePaths":25455,"remoteImagePaths":25456,"frontmatter":25457,"imagePaths":25458},[],[],[],{"title":25446,"description":25447,"summary":25447,"pubDate":25202,"source":25191,"url":25449,"thumbnail":25193},[],"2025-06-23-adaptive-guidance-accelerates-reinforcement-learning-of-reasoning-models.md","2025-06-23-advanced-sign-language-video-generation-with-compressed-and-quantized-multi-condition-tokenization",{"id":25460,"data":25462,"filePath":25467,"digest":25468,"rendered":25469,"legacyId":25476},{"title":25463,"description":25464,"summary":25464,"pubDate":25465,"source":25191,"url":25466,"thumbnail":25193},"Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization","arXiv:2506.15980v1 Announce Type: cross Abstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (eg, skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporates multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (ie, fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at https://github.com/umnooob/signvip/.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15980","src/content/posts/2025-06-23-advanced-sign-language-video-generation-with-compressed-and-quantized-multi-condition-tokenization.md","1e79b9a3569cc09f",{"html":25,"metadata":25470},{"headings":25471,"localImagePaths":25472,"remoteImagePaths":25473,"frontmatter":25474,"imagePaths":25475},[],[],[],{"title":25463,"description":25464,"summary":25464,"pubDate":25202,"source":25191,"url":25466,"thumbnail":25193},[],"2025-06-23-advanced-sign-language-video-generation-with-compressed-and-quantized-multi-condition-tokenization.md","2025-06-23-advancing-embodied-agent-security-from-safety-benchmarks-to-input-moderation",{"id":25477,"data":25479,"filePath":25484,"digest":25485,"rendered":25486,"legacyId":25493},{"title":25480,"description":25481,"summary":25481,"pubDate":25482,"source":25191,"url":25483,"thumbnail":25193},"Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation","arXiv:2504.15699v3 Announce Type: replace Abstract: Embodied agents exhibit immense potential across a multitude of domains, making the assurance of their behavioral safety a fundamental prerequisite for their widespread deployment. However, existing research predominantly concentrates on the security of general large language models, lacking specialized methodologies for establishing safety benchmarks and input moderation tailored to embodied agents. To bridge this gap, this paper introduces a novel input moderation framework, meticulously designed to safeguard embodied agents. This framework encompasses the entire pipeline, including taxonomy definition, dataset curation, moderator architecture, model training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a meticulously crafted safety benchmark engineered to facilitate both the training and stringent assessment of moderators specifically designed for embodied agents. Furthermore, we propose Pinpoint, an innovative prompt-decoupled input moderation scheme that harnesses a masked attention mechanism to effectively isolate and mitigate the influence of functional prompts on moderation tasks. Extensive experiments conducted on diverse benchmark datasets and models validate the feasibility and efficacy of the proposed approach. The results demonstrate that our methodologies achieve an impressive average detection accuracy of 94.58%, surpassing the performance of existing state-of-the-art techniques, alongside an exceptional moderation processing time of merely 0.002 seconds per instance.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2504.15699","src/content/posts/2025-06-23-advancing-embodied-agent-security-from-safety-benchmarks-to-input-moderation.md","5f31e6d472316b1b",{"html":25,"metadata":25487},{"headings":25488,"localImagePaths":25489,"remoteImagePaths":25490,"frontmatter":25491,"imagePaths":25492},[],[],[],{"title":25480,"description":25481,"summary":25481,"pubDate":25202,"source":25191,"url":25483,"thumbnail":25193},[],"2025-06-23-advancing-embodied-agent-security-from-safety-benchmarks-to-input-moderation.md","2025-06-23-advancing-harmful-content-detection-in-organizational-research-integrating-large-language-models-with-elo-rating-system",{"id":25494,"data":25496,"filePath":25501,"digest":25502,"rendered":25503,"legacyId":25510},{"title":25497,"description":25498,"summary":25498,"pubDate":25499,"source":25191,"url":25500,"thumbnail":25193},"Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System","arXiv:2506.16575v1 Announce Type: new Abstract: Large language models (LLMs) offer promising opportunities for organizational research. However, their built-in moderation systems can create problems when researchers try to analyze harmful content, often refusing to follow certain instructions or producing overly cautious responses that undermine validity of the results. This is particularly problematic when analyzing organizational conflicts such as microaggressions or hate speech. This paper introduces an Elo rating-based method that significantly improves LLM performance for harmful content analysis In two datasets, one focused on microaggression detection and the other on hate speech, we find that our method outperforms traditional LLM prompting techniques and conventional machine learning models on key measures such as accuracy, precision, and F1 scores. Advantages include better reliability when analyzing harmful content, fewer false positives, and greater scalability for large-scale datasets. This approach supports organizational applications, including detecting workplace harassment, assessing toxic communication, and fostering safer and more inclusive work environments.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16575","src/content/posts/2025-06-23-advancing-harmful-content-detection-in-organizational-research-integrating-large-language-models-with-elo-rating-system.md","ce43a03651d0afb5",{"html":25,"metadata":25504},{"headings":25505,"localImagePaths":25506,"remoteImagePaths":25507,"frontmatter":25508,"imagePaths":25509},[],[],[],{"title":25497,"description":25498,"summary":25498,"pubDate":25202,"source":25191,"url":25500,"thumbnail":25193},[],"2025-06-23-advancing-harmful-content-detection-in-organizational-research-integrating-large-language-models-with-elo-rating-system.md","2025-06-23-advancing-stochastic-3-sat-solvers-by-dissipating-oversatisfied-constraints",{"id":25511,"data":25513,"filePath":25518,"digest":25519,"rendered":25520,"legacyId":25527},{"title":25514,"description":25515,"summary":25515,"pubDate":25516,"source":25191,"url":25517,"thumbnail":25193},"Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints","arXiv:2506.15774v1 Announce Type: new Abstract: We introduce and benchmark a stochastic local search heuristic for the NP-complete satisfiability problem 3-SAT that drastically outperforms existing solvers in the notoriously difficult realm of critically hard instances. Our construction is based on the crucial observation that well established previous approaches such as WalkSAT are prone to get stuck in local minima that are distinguished from true solutions by a larger number of oversatisfied combinatorial constraints. To address this issue, the proposed algorithm, coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their unfavorable abundance so as to render them critical. We analyze and benchmark our algorithm on a randomly generated sample of hard but satisfiable 3-SAT instances with varying problem sizes up to N=15000. Quite remarkably, we find that DOCSAT outperforms both WalkSAT and other well known algorithms including the complete solver Kissat, even when comparing its ability to solve the hardest quintile of the sample to the average performance of its competitors. The essence of DOCSAT may be seen as a way of harnessing statistical structure beyond the primary cost function of a combinatorial problem to avoid or escape local minima traps in stochastic local search, which opens avenues for generalization to other optimization problems.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15774","src/content/posts/2025-06-23-advancing-stochastic-3-sat-solvers-by-dissipating-oversatisfied-constraints.md","050174f0a5fb0153",{"html":25,"metadata":25521},{"headings":25522,"localImagePaths":25523,"remoteImagePaths":25524,"frontmatter":25525,"imagePaths":25526},[],[],[],{"title":25514,"description":25515,"summary":25515,"pubDate":25202,"source":25191,"url":25517,"thumbnail":25193},[],"2025-06-23-advancing-stochastic-3-sat-solvers-by-dissipating-oversatisfied-constraints.md","2025-06-23-aerialvg-a-challenging-benchmark-for-aerial-visual-grounding-by-exploring-positional-relations",{"id":25528,"data":25530,"filePath":25535,"digest":25536,"rendered":25537,"legacyId":25544},{"title":25531,"description":25532,"summary":25532,"pubDate":25533,"source":25191,"url":25534,"thumbnail":25193},"AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations","arXiv:2504.07836v3 Announce Type: replace-cross Abstract: Visual grounding (VG) aims to localize target objects in an image based on natural language descriptions. In this paper, we propose AerialVG, a new task focusing on visual grounding from aerial views. Compared to traditional VG, AerialVG poses new challenges, emph{e.g.}, appearance-based grounding is insufficient to distinguish among multiple visually similar objects, and positional relations should be emphasized. Besides, existing VG models struggle when applied to aerial imagery, where high-resolution images cause significant difficulties. To address these challenges, we introduce the first AerialVG dataset, consisting of 5K real-world aerial images, 50K manually annotated descriptions, and 103K objects. Particularly, each annotation in AerialVG dataset contains multiple target objects annotated with relative spatial relations, requiring models to perform comprehensive spatial reasoning. Furthermore, we propose an innovative model especially for the AerialVG task, where a Hierarchical Cross-Attention is devised to focus on target regions, and a Relation-Aware Grounding module is designed to infer positional relations. Experimental results validate the effectiveness of our dataset and method, highlighting the importance of spatial reasoning in aerial visual grounding. The code and dataset will be released.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2504.07836","src/content/posts/2025-06-23-aerialvg-a-challenging-benchmark-for-aerial-visual-grounding-by-exploring-positional-relations.md","0529d475e0261410",{"html":25,"metadata":25538},{"headings":25539,"localImagePaths":25540,"remoteImagePaths":25541,"frontmatter":25542,"imagePaths":25543},[],[],[],{"title":25531,"description":25532,"summary":25532,"pubDate":25202,"source":25191,"url":25534,"thumbnail":25193},[],"2025-06-23-aerialvg-a-challenging-benchmark-for-aerial-visual-grounding-by-exploring-positional-relations.md","2025-06-23-agentic-personalisation-of-cross-channel-marketing-experiences",{"id":25545,"data":25547,"filePath":25552,"digest":25553,"rendered":25554,"legacyId":25561},{"title":25548,"description":25549,"summary":25549,"pubDate":25550,"source":25191,"url":25551,"thumbnail":25193},"Agentic Personalisation of Cross-Channel Marketing Experiences","arXiv:2506.16429v1 Announce Type: new Abstract: Consumer applications provide ample opportunities to surface and communicate various forms of content to users. From promotional campaigns for new features or subscriptions, to evergreen nudges for engagement, or personalised recommendations; across e-mails, push notifications, and in-app surfaces. The conventional approach to orchestration for communication relies heavily on labour-intensive manual marketer work, and inhibits effective personalisation of content, timing, frequency, and copy-writing. We formulate this task under a sequential decision-making framework, where we aim to optimise a modular decision-making policy that maximises incremental engagement for any funnel event. Our approach leverages a Difference-in-Differences design for Individual Treatment Effect estimation, and Thompson sampling to balance the explore-exploit trade-off. We present results from a multi-service application, where our methodology has resulted in significant increases to a variety of goal events across several product features, and is currently deployed across 150 million users.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16429","src/content/posts/2025-06-23-agentic-personalisation-of-cross-channel-marketing-experiences.md","203034de77881f02",{"html":25,"metadata":25555},{"headings":25556,"localImagePaths":25557,"remoteImagePaths":25558,"frontmatter":25559,"imagePaths":25560},[],[],[],{"title":25548,"description":25549,"summary":25549,"pubDate":25202,"source":25191,"url":25551,"thumbnail":25193},[],"2025-06-23-agentic-personalisation-of-cross-channel-marketing-experiences.md","2025-06-23-agi-driven-generative-semantic-communications-principles-and-practices",{"id":25562,"data":25564,"filePath":25569,"digest":25570,"rendered":25571,"legacyId":25578},{"title":25565,"description":25566,"summary":25566,"pubDate":25567,"source":25191,"url":25568,"thumbnail":25193},"AGI-Driven Generative Semantic Communications: Principles and Practices","arXiv:2504.14947v2 Announce Type: replace Abstract: Semantic communications leverage artificial intelligence (AI) technologies to extract semantic information for efficient data delivery, thereby significantly reducing communication cost. With the evolution towards artificial general intelligence (AGI), the increasing demands for AGI services pose new challenges to semantic communications. In this context, an AGI application is typically defined on a general-sense task, covering a broad, even unforeseen, set of objectives, as well as driven by the need for a human-friendly interface in forms (e.g., videos, images, or text) easily understood by human users.In response, we introduce an AGI-driven communication paradigm for supporting AGI applications, called generative semantic communication (GSC). We first describe the basic concept of GSC and its difference from existing semantic communications, and then introduce a general framework of GSC based on advanced AI technologies including foundation models and generative models. Two case studies are presented to verify the advantages of GSC. Finally, open challenges and new research directions are discussed to stimulate this line of research and pave the way for practical applications.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2504.14947","src/content/posts/2025-06-23-agi-driven-generative-semantic-communications-principles-and-practices.md","8b5ea042054b4030",{"html":25,"metadata":25572},{"headings":25573,"localImagePaths":25574,"remoteImagePaths":25575,"frontmatter":25576,"imagePaths":25577},[],[],[],{"title":25565,"description":25566,"summary":25566,"pubDate":25202,"source":25191,"url":25568,"thumbnail":25193},[],"2025-06-23-agi-driven-generative-semantic-communications-principles-and-practices.md","2025-06-23-ai-driven-tools-in-modern-software-quality-assurance-an-assessment-of-benefits-challenges-and-future-directions",{"id":25579,"data":25581,"filePath":25586,"digest":25587,"rendered":25588,"legacyId":25595},{"title":25582,"description":25583,"summary":25583,"pubDate":25584,"source":25191,"url":25585,"thumbnail":25193},"AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions","arXiv:2506.16586v1 Announce Type: cross Abstract: Traditional quality assurance (QA) methods face significant challenges in addressing the complexity, scale, and rapid iteration cycles of modern software systems and are strained by limited resources available, leading to substantial costs associated with poor quality. The object of this research is the Quality Assurance processes for modern distributed software applications. The subject of the research is the assessment of the benefits, challenges, and prospects of integrating modern AI-oriented tools into quality assurance processes. We performed comprehensive analysis of implications on both verification and validation processes covering exploratory test analyses, equivalence partitioning and boundary analyses, metamorphic testing, finding inconsistencies in acceptance criteria (AC), static analyses, test case generation, unit test generation, test suit optimization and assessment, end to end scenario execution. End to end regression of sample enterprise application utilizing AI-agents over generated test scenarios was implemented as a proof of concept highlighting practical use of the study. The results, with only 8.3% flaky executions of generated test cases, indicate significant potential for the proposed approaches. However, the study also identified substantial challenges for practical adoption concerning generation of semantically identical coverage, 'black box' nature and lack of explainability from state-of-the-art Large Language Models (LLMs), the tendency to correct mutated test cases to match expected results, underscoring the necessity for thorough verification of both generated artifacts and test execution results. The research demonstrates AI's transformative potential for QA but highlights the importance of a strategic approach to implementing these technologies, considering the identified limitations and the need for developing appropriate verification methodologies.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16586","src/content/posts/2025-06-23-ai-driven-tools-in-modern-software-quality-assurance-an-assessment-of-benefits-challenges-and-future-directions.md","114fff06181d5417",{"html":25,"metadata":25589},{"headings":25590,"localImagePaths":25591,"remoteImagePaths":25592,"frontmatter":25593,"imagePaths":25594},[],[],[],{"title":25582,"description":25583,"summary":25583,"pubDate":25202,"source":25191,"url":25585,"thumbnail":25193},[],"2025-06-23-ai-driven-tools-in-modern-software-quality-assurance-an-assessment-of-benefits-challenges-and-future-directions.md","2025-06-23-ai-testing-and-evaluation-learnings-from-science-and-industry",{"id":25596,"data":25598,"filePath":25603,"digest":25604,"rendered":25605,"legacyId":25613},{"title":25599,"description":25600,"summary":25600,"pubDate":25601,"source":23065,"url":25602,"thumbnail":23067},"AI Testing and Evaluation: Learnings from Science and Industry","\u003Cp>In the introductory episode of this new series, host Kathleen Sullivan and Senior Director Amanda Craig Deckard explore Microsoft’s efforts to draw on the experience of other domains to help advance the role of AI testing and evaluation as a governance tool.\u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/podcast/ai-testing-and-evaluation-learnings-from-science-and-industry/'>AI Testing and Evaluation: Learnings from Science and Industry\u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-06-23T16:38:09.000Z"],"https://www.microsoft.com/en-us/research/podcast/ai-testing-and-evaluation-learnings-from-science-and-industry/","src/content/posts/2025-06-23-ai-testing-and-evaluation-learnings-from-science-and-industry.md","2319691148e1bedd",{"html":25,"metadata":25606},{"headings":25607,"localImagePaths":25608,"remoteImagePaths":25609,"frontmatter":25610,"imagePaths":25612},[],[],[],{"title":25599,"description":25600,"summary":25600,"pubDate":25611,"source":23065,"url":25602,"thumbnail":23067},"Mon, 23 Jun 2025 16:38:09 +0000",[],"2025-06-23-ai-testing-and-evaluation-learnings-from-science-and-industry.md","2025-06-23-ais-blind-spots-geographic-knowledge-and-diversity-deficit-in-generated-urban-scenario",{"id":25614,"data":25616,"filePath":25621,"digest":25622,"rendered":25623,"legacyId":25630},{"title":25617,"description":25618,"summary":25618,"pubDate":25619,"source":25191,"url":25620,"thumbnail":25193},"AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario","arXiv:2506.16898v1 Announce Type: new Abstract: Image generation models are revolutionizing many domains, and urban analysis and design is no exception. While such models are widely adopted, there is a limited literature exploring their geographic knowledge, along with the biases they embed. In this work, we generated 150 synthetic images for each state in the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two state-of-the-art models for image generation. We embed each image using DINO-v2 ViT-S/14 and the Fr'echet Inception Distances to measure the similarity between the generated images. We found that while these models have implicitly learned aspects of USA geography, if we prompt the models to generate an image for 'United States' instead of specific cities or states, the models exhibit a strong representative bias toward metropolis-like areas, excluding rural states and smaller cities. {color{black} In addition, we found that models systematically exhibit some entity-disambiguation issues with European-sounding names like Frankfort or Devon.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16898","src/content/posts/2025-06-23-ais-blind-spots-geographic-knowledge-and-diversity-deficit-in-generated-urban-scenario.md","eefb71a27cfb4cd6",{"html":25,"metadata":25624},{"headings":25625,"localImagePaths":25626,"remoteImagePaths":25627,"frontmatter":25628,"imagePaths":25629},[],[],[],{"title":25617,"description":25618,"summary":25618,"pubDate":25202,"source":25191,"url":25620,"thumbnail":25193},[],"2025-06-23-ais-blind-spots-geographic-knowledge-and-diversity-deficit-in-generated-urban-scenario.md","2025-06-23-aiイラストにファンアートタグ使わないでvtuberぶいすぽっ運営が声明-aiイラストの利用を必須化",{"id":25631,"data":25633,"filePath":25639,"digest":25640,"rendered":25641,"legacyId":25649},{"title":25634,"description":25635,"summary":25635,"pubDate":25636,"source":24944,"url":25637,"thumbnail":25638},"AIイラストに“ファンアートタグ”使わないで──VTuber「ぶいすぽっ！」運営が声明　「#AIイラスト」の利用を必須化","AIイラストには“ファンアートタグ”は使わないで──VTuberプロジェクト「ぶいすぽっ！」を運営するバーチャルエンターテイメントは、公式Xアカウント（＠Vspo77）でそんな声明を発表した。",["Date","2025-06-23T10:11:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/23/news098.html","https://image.itmedia.co.jp/aiplus/articles/2506/23/cover_news098.jpg","src/content/posts/2025-06-23-aiイラストにファンアートタグ使わないでvtuberぶいすぽっ運営が声明-aiイラストの利用を必須化.md","209fec1e56d4a03e",{"html":25,"metadata":25642},{"headings":25643,"localImagePaths":25644,"remoteImagePaths":25645,"frontmatter":25646,"imagePaths":25648},[],[],[],{"title":25634,"description":25635,"summary":25635,"pubDate":25647,"source":24944,"url":25637,"thumbnail":25638},"Mon, 23 Jun 2025 19:11:00 +0900",[],"2025-06-23-aiイラストにファンアートタグ使わないでvtuberぶいすぽっ運営が声明-aiイラストの利用を必須化.md","2025-06-23-ai作成の偽サイトをai用いて捜査フィッシングサイト作成の男2人を逮捕-大阪府警",{"id":25650,"data":25652,"filePath":25658,"digest":25659,"rendered":25660,"legacyId":25668},{"title":25653,"description":25654,"summary":25654,"pubDate":25655,"source":24944,"url":25656,"thumbnail":25657},"AI作成の偽サイトをAI用いて捜査、フィッシングサイト作成の男2人を逮捕　大阪府警","ECサイトになりすましたフィッシングサイトを作成し、インターネットに公開したなどとして、大阪府警サイバー犯罪捜査課は不正アクセス禁止法違反の疑いで2人を逮捕したと発表した。",["Date","2025-06-23T08:27:00.000Z"],"https://www.itmedia.co.jp/news/articles/2506/23/news093.html","https://image.itmedia.co.jp/news/articles/2506/23/cover_news093.jpg","src/content/posts/2025-06-23-ai作成の偽サイトをai用いて捜査フィッシングサイト作成の男2人を逮捕-大阪府警.md","0723a25f9c21df3d",{"html":25,"metadata":25661},{"headings":25662,"localImagePaths":25663,"remoteImagePaths":25664,"frontmatter":25665,"imagePaths":25667},[],[],[],{"title":25653,"description":25654,"summary":25654,"pubDate":25666,"source":24944,"url":25656,"thumbnail":25657},"Mon, 23 Jun 2025 17:27:00 +0900",[],"2025-06-23-ai作成の偽サイトをai用いて捜査フィッシングサイト作成の男2人を逮捕-大阪府警.md","2025-06-23-aiによる生産要件の自動判定機能が好評な3dプラットフォーム",{"id":25669,"data":25671,"filePath":25677,"digest":25678,"rendered":25679,"legacyId":25687},{"title":25672,"description":25673,"summary":25673,"pubDate":25674,"source":24944,"url":25675,"thumbnail":25676},"AIによる生産要件の自動判定機能が好評な3Dプラットフォーム","LIGHTzは、製造業の設計開発における開発速度や生産性の向上を支援する3Dプラットフォーム「blooplinter」の導入が製造業で進んでいると発表した。特に「AI要件チェック機能」が活用されているという。",["Date","2025-06-23T00:00:00.000Z"],"https://monoist.itmedia.co.jp/mn/articles/2506/23/news006.html","https://image.itmedia.co.jp/mn/articles/2506/23/cover_news006.jpg","src/content/posts/2025-06-23-aiによる生産要件の自動判定機能が好評な3dプラットフォーム.md","ff92aadfeb02a816",{"html":25,"metadata":25680},{"headings":25681,"localImagePaths":25682,"remoteImagePaths":25683,"frontmatter":25684,"imagePaths":25686},[],[],[],{"title":25672,"description":25673,"summary":25673,"pubDate":25685,"source":24944,"url":25675,"thumbnail":25676},"Mon, 23 Jun 2025 09:00:00 +0900",[],"2025-06-23-aiによる生産要件の自動判定機能が好評な3dプラットフォーム.md","2025-06-23-alta-compiler-based-analysis-of-transformers",{"id":25688,"data":25690,"filePath":25695,"digest":25696,"rendered":25697,"legacyId":25704},{"title":25691,"description":25692,"summary":25692,"pubDate":25693,"source":25191,"url":25694,"thumbnail":25193},"ALTA: Compiler-Based Analysis of Transformers","arXiv:2410.18077v2 Announce Type: replace-cross Abstract: We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights. ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights. ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages. ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps. We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm. To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal. This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings. We make the ALTA framework -- language specification, symbolic interpreter, and weight compiler -- available to the community to enable further applications and insights.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2410.18077","src/content/posts/2025-06-23-alta-compiler-based-analysis-of-transformers.md","2b2e1cf59eb98daa",{"html":25,"metadata":25698},{"headings":25699,"localImagePaths":25700,"remoteImagePaths":25701,"frontmatter":25702,"imagePaths":25703},[],[],[],{"title":25691,"description":25692,"summary":25692,"pubDate":25202,"source":25191,"url":25694,"thumbnail":25193},[],"2025-06-23-alta-compiler-based-analysis-of-transformers.md","2025-06-23-aligndistil-token-level-language-model-alignment-as-adaptive-policy-distillation",{"id":25705,"data":25707,"filePath":25712,"digest":25713,"rendered":25714,"legacyId":25721},{"title":25708,"description":25709,"summary":25709,"pubDate":25710,"source":25191,"url":25711,"thumbnail":25193},"AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation","arXiv:2503.02832v2 Announce Type: replace-cross Abstract: In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2503.02832","src/content/posts/2025-06-23-aligndistil-token-level-language-model-alignment-as-adaptive-policy-distillation.md","fdf078cec6574a38",{"html":25,"metadata":25715},{"headings":25716,"localImagePaths":25717,"remoteImagePaths":25718,"frontmatter":25719,"imagePaths":25720},[],[],[],{"title":25708,"description":25709,"summary":25709,"pubDate":25202,"source":25191,"url":25711,"thumbnail":25193},[],"2025-06-23-aligndistil-token-level-language-model-alignment-as-adaptive-policy-distillation.md","2025-06-23-alternates-assemble-selecting-optimal-alternates-for-citizens-assemblies",{"id":25722,"data":25724,"filePath":25729,"digest":25730,"rendered":25731,"legacyId":25738},{"title":25725,"description":25726,"summary":25726,"pubDate":25727,"source":25191,"url":25728,"thumbnail":25193},"Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies","arXiv:2506.15716v1 Announce Type: cross Abstract: An increasingly influential form of deliberative democracy centers on citizens' assemblies, where randomly selected people discuss policy questions. The legitimacy of these panels hinges on their representation of the broader population, but panelists often drop out, leading to an unbalanced composition. Although participant attrition is mitigated in practice by alternates, their selection is not taken into account by existing methods. To address this gap, we introduce an optimization framework for alternate selection. Our algorithmic approach, which leverages learning-theoretic machinery, estimates dropout probabilities using historical data and selects alternates to minimize expected misrepresentation. We establish theoretical guarantees for our approach, including worst-case bounds on sample complexity (with implications for computational efficiency) and on loss when panelists' probabilities of dropping out are mis-estimated. Empirical evaluation using real-world data demonstrates that, compared to the status quo, our method significantly improves representation while requiring fewer alternates.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15716","src/content/posts/2025-06-23-alternates-assemble-selecting-optimal-alternates-for-citizens-assemblies.md","d0e5ca618a3da3d4",{"html":25,"metadata":25732},{"headings":25733,"localImagePaths":25734,"remoteImagePaths":25735,"frontmatter":25736,"imagePaths":25737},[],[],[],{"title":25725,"description":25726,"summary":25726,"pubDate":25202,"source":25191,"url":25728,"thumbnail":25193},[],"2025-06-23-alternates-assemble-selecting-optimal-alternates-for-citizens-assemblies.md","2025-06-23-alto-orchestrating-distributed-compound-ai-systems-with-nested-ancestry",{"id":25739,"data":25741,"filePath":25746,"digest":25747,"rendered":25748,"legacyId":25755},{"title":25742,"description":25743,"summary":25743,"pubDate":25744,"source":25191,"url":25745,"thumbnail":25193},"Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry","arXiv:2403.04311v2 Announce Type: replace Abstract: Compound AI applications chain together subcomponents such as generative language models, document retrievers, and embedding models. Applying traditional systems optimizations such as parallelism and pipelining in compound AI systems is difficult because each component has different constraints in terms of the granularity and type of data that it ingests. New data is often generated during intermediate computations, and text streams may be split into smaller, independent fragments (such as documents to sentences) which may then be re-aggregated at later parts of the computation. Due to this complexity, existing systems to serve compound AI queries do not fully take advantage of parallelism and pipelining opportunities. We present Alto, a framework that automatically optimizes execution of compound AI queries through streaming and parallelism. Bento introduces a new abstraction called nested ancestry, a metadata hierarchy that allows the system to correctly track partial outputs and aggregate data across the heterogeneous constraints of the components of compound AI applications. This metadata is automatically inferred from the programming model, allowing developers to express complex dataflow patterns without needing to reason manually about the details of routing and aggregation. Implementations of four applications in Alto outperform or match implementations in LangGraph, a popular existing AI programming framework. Alto implementations match or improve latency by between 10-30%.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2403.04311","src/content/posts/2025-06-23-alto-orchestrating-distributed-compound-ai-systems-with-nested-ancestry.md","dd715750afce4a0a",{"html":25,"metadata":25749},{"headings":25750,"localImagePaths":25751,"remoteImagePaths":25752,"frontmatter":25753,"imagePaths":25754},[],[],[],{"title":25742,"description":25743,"summary":25743,"pubDate":25202,"source":25191,"url":25745,"thumbnail":25193},[],"2025-06-23-alto-orchestrating-distributed-compound-ai-systems-with-nested-ancestry.md","2025-06-23-analyzing-the-influence-of-knowledge-graph-information-on-relation-extraction",{"id":25756,"data":25758,"filePath":25763,"digest":25764,"rendered":25765,"legacyId":25772},{"title":25759,"description":25760,"summary":25760,"pubDate":25761,"source":25191,"url":25762,"thumbnail":25193},"Analyzing the Influence of Knowledge Graph Information on Relation Extraction","arXiv:2506.16343v1 Announce Type: cross Abstract: We examine the impact of incorporating knowledge graph information on the performance of relation extraction models across a range of datasets. Our hypothesis is that the positions of entities within a knowledge graph provide important insights for relation extraction tasks. We conduct experiments on multiple datasets, each varying in the number of relations, training examples, and underlying knowledge graphs. Our results demonstrate that integrating knowledge graph information significantly enhances performance, especially when dealing with an imbalance in the number of training examples for each relation. We evaluate the contribution of knowledge graph-based features by combining established relation extraction methods with graph-aware Neural Bellman-Ford networks. These features are tested in both supervised and zero-shot settings, demonstrating consistent performance improvements across various datasets.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16343","src/content/posts/2025-06-23-analyzing-the-influence-of-knowledge-graph-information-on-relation-extraction.md","136800ecf920c1c3",{"html":25,"metadata":25766},{"headings":25767,"localImagePaths":25768,"remoteImagePaths":25769,"frontmatter":25770,"imagePaths":25771},[],[],[],{"title":25759,"description":25760,"summary":25760,"pubDate":25202,"source":25191,"url":25762,"thumbnail":25193},[],"2025-06-23-analyzing-the-influence-of-knowledge-graph-information-on-relation-extraction.md","2025-06-23-anytraverse-an-off-road-traversability-framework-with-vlm-and-human-operator-in-the-loop",{"id":25773,"data":25775,"filePath":25780,"digest":25781,"rendered":25782,"legacyId":25789},{"title":25776,"description":25777,"summary":25777,"pubDate":25778,"source":25191,"url":25779,"thumbnail":25193},"AnyTraverse: An off-road traversability framework with VLM and human operator in the loop","arXiv:2506.16826v1 Announce Type: cross Abstract: Off-road traversability segmentation enables autonomous navigation with applications in search-and-rescue, military operations, wildlife exploration, and agriculture. Current frameworks struggle due to significant variations in unstructured environments and uncertain scene changes, and are not adaptive to be used for different robot types. We present AnyTraverse, a framework combining natural language-based prompts with human-operator assistance to determine navigable regions for diverse robotic vehicles. The system segments scenes for a given set of prompts and calls the operator only when encountering previously unexplored scenery or unknown class not part of the prompt in its region-of-interest, thus reducing active supervision load while adapting to varying outdoor scenes. Our zero-shot learning approach eliminates the need for extensive data collection or retraining. Our experimental validation includes testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate real-world deployment on multiple robot platforms. The results show that AnyTraverse performs better than GA-NAV and Off-seg while offering a vehicle-agnostic approach to off-road traversability that balances automation with targeted human supervision.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16826","src/content/posts/2025-06-23-anytraverse-an-off-road-traversability-framework-with-vlm-and-human-operator-in-the-loop.md","50de5c2e331bc4d3",{"html":25,"metadata":25783},{"headings":25784,"localImagePaths":25785,"remoteImagePaths":25786,"frontmatter":25787,"imagePaths":25788},[],[],[],{"title":25776,"description":25777,"summary":25777,"pubDate":25202,"source":25191,"url":25779,"thumbnail":25193},[],"2025-06-23-anytraverse-an-off-road-traversability-framework-with-vlm-and-human-operator-in-the-loop.md","2025-06-23-approximation-fixpoint-theory-with-refined-approximation-spaces",{"id":25790,"data":25792,"filePath":25797,"digest":25798,"rendered":25799,"legacyId":25806},{"title":25793,"description":25794,"summary":25794,"pubDate":25795,"source":25191,"url":25796,"thumbnail":25193},"Approximation Fixpoint Theory with Refined Approximation Spaces","arXiv:2506.16294v1 Announce Type: new Abstract: Approximation Fixpoint Theory (AFT) is a powerful theory covering various semantics of non-monotonic reasoning formalisms in knowledge representation such as Logic Programming and Answer Set Programming. Many semantics of such non-monotonic formalisms can be characterized as suitable fixpoints of a non-monotonic operator on a suitable lattice. Instead of working on the original lattice, AFT operates on intervals in such lattice to approximate or construct the fixpoints of interest. While AFT has been applied successfully across a broad range of non-monotonic reasoning formalisms, it is confronted by its limitations in other, relatively simple, examples. In this paper, we overcome those limitations by extending consistent AFT to deal with approximations that are more refined than intervals. Therefore, we introduce a more general notion of approximation spaces, showcase the improved expressiveness and investigate relations between different approximation spaces.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16294","src/content/posts/2025-06-23-approximation-fixpoint-theory-with-refined-approximation-spaces.md","211f063c31e9a478",{"html":25,"metadata":25800},{"headings":25801,"localImagePaths":25802,"remoteImagePaths":25803,"frontmatter":25804,"imagePaths":25805},[],[],[],{"title":25793,"description":25794,"summary":25794,"pubDate":25202,"source":25191,"url":25796,"thumbnail":25193},[],"2025-06-23-approximation-fixpoint-theory-with-refined-approximation-spaces.md","2025-06-23-aqa-bench-an-interactive-benchmark-for-evaluating-llms-sequential-reasoning-ability",{"id":25807,"data":25809,"filePath":25814,"digest":25815,"rendered":25816,"legacyId":25823},{"title":25810,"description":25811,"summary":25811,"pubDate":25812,"source":25191,"url":25813,"thumbnail":25193},"AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability","arXiv:2402.09404v2 Announce Type: replace-cross Abstract: This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol - for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves considering the possible environmental feedback in the future steps. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 14 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show much stronger sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing in-context examples may inadvertently hurt few-shot performance in an interactive environment due to over-fitting to examples. (3) Instead of using optimal steps from another test case as the in-context example, a very limited number of predecessor steps in the current test case following the optimal policy can substantially boost small models' performance. (4) The performance gap between weak models and strong models is greatly due to the incapability of weak models to start well. (5) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2402.09404","src/content/posts/2025-06-23-aqa-bench-an-interactive-benchmark-for-evaluating-llms-sequential-reasoning-ability.md","4eed0666bc94c473",{"html":25,"metadata":25817},{"headings":25818,"localImagePaths":25819,"remoteImagePaths":25820,"frontmatter":25821,"imagePaths":25822},[],[],[],{"title":25810,"description":25811,"summary":25811,"pubDate":25202,"source":25191,"url":25813,"thumbnail":25193},[],"2025-06-23-aqa-bench-an-interactive-benchmark-for-evaluating-llms-sequential-reasoning-ability.md","2025-06-23-are-bias-evaluation-methods-biased",{"id":25824,"data":25826,"filePath":25831,"digest":25832,"rendered":25833,"legacyId":25840},{"title":25827,"description":25828,"summary":25828,"pubDate":25829,"source":25191,"url":25830,"thumbnail":25193},"Are Bias Evaluation Methods Biased ?","arXiv:2506.17111v1 Announce Type: new Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is one of the key activities within the trusted AI community. These benchmarks allow models to be compared for different aspects of safety such as toxicity, bias, harmful behavior etc. Independent benchmarks adopt different approaches with distinct data sets and evaluation methods. We investigate how robust such benchmarks are by using different approaches to rank a set of representative models for bias and compare how similar are the overall rankings. We show that different but widely used bias evaluations methods result in disparate model rankings. We conclude with recommendations for the community in the usage of such benchmarks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17111","src/content/posts/2025-06-23-are-bias-evaluation-methods-biased.md","d53d1db464ce7487",{"html":25,"metadata":25834},{"headings":25835,"localImagePaths":25836,"remoteImagePaths":25837,"frontmatter":25838,"imagePaths":25839},[],[],[],{"title":25827,"description":25828,"summary":25828,"pubDate":25202,"source":25191,"url":25830,"thumbnail":25193},[],"2025-06-23-are-bias-evaluation-methods-biased.md","2025-06-23-artificial-intelligence-for-atmospheric-sciences-a-research-roadmap",{"id":25841,"data":25843,"filePath":25848,"digest":25849,"rendered":25850,"legacyId":25857},{"title":25844,"description":25845,"summary":25845,"pubDate":25846,"source":25191,"url":25847,"thumbnail":25193},"Artificial Intelligence for Atmospheric Sciences: A Research Roadmap","arXiv:2506.16281v1 Announce Type: cross Abstract: Atmospheric sciences are crucial for understanding environmental phenomena ranging from air quality to extreme weather events, and climate change. Recent breakthroughs in sensing, communication, computing, and Artificial Intelligence (AI) have significantly advanced atmospheric sciences, enabling the generation of vast amounts of data through long-term Earth observations and providing powerful tools for analyzing atmospheric phenomena and predicting natural disasters. This paper contributes a critical interdisciplinary overview that bridges the fields of atmospheric science and computer science, highlighting the transformative potential of AI in atmospheric research. We identify key challenges associated with integrating AI into atmospheric research, including issues related to big data and infrastructure, and provide a detailed research roadmap that addresses both current and emerging challenges.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16281","src/content/posts/2025-06-23-artificial-intelligence-for-atmospheric-sciences-a-research-roadmap.md","f8aca4674c340762",{"html":25,"metadata":25851},{"headings":25852,"localImagePaths":25853,"remoteImagePaths":25854,"frontmatter":25855,"imagePaths":25856},[],[],[],{"title":25844,"description":25845,"summary":25845,"pubDate":25202,"source":25191,"url":25847,"thumbnail":25193},[],"2025-06-23-artificial-intelligence-for-atmospheric-sciences-a-research-roadmap.md","2025-06-23-ask-a-techspert-what-is-inference",{"id":25858,"data":25860,"filePath":25866,"digest":25867,"rendered":25868,"legacyId":25876},{"title":25861,"description":25862,"summary":25862,"pubDate":25863,"source":23485,"url":25864,"thumbnail":25865},"Ask a Techspert: What is inference?","Illustration of a computer chip surrounded by elements representing AI and data, including a cat's head, a wireframe cat, puzzle pieces, a bar graph, gears, and text bubbles.",["Date","2025-06-23T17:30:00.000Z"],"https://blog.google/technology/ai/ask-a-techspert-what-is-inference/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/InferenceHero_v3.width-1300.png","src/content/posts/2025-06-23-ask-a-techspert-what-is-inference.md","1e4cde47d55d483b",{"html":25,"metadata":25869},{"headings":25870,"localImagePaths":25871,"remoteImagePaths":25872,"frontmatter":25873,"imagePaths":25875},[],[],[],{"title":25861,"description":25862,"summary":25862,"pubDate":25874,"source":23485,"url":25864,"thumbnail":25865},"Mon, 23 Jun 2025 17:30:00 +0000",[],"2025-06-23-ask-a-techspert-what-is-inference.md","2025-06-23-assessing-tenstorrents-risc-v-matmul-acceleration-capabilities",{"id":25877,"data":25879,"filePath":25884,"digest":25885,"rendered":25886,"legacyId":25893},{"title":25880,"description":25881,"summary":25881,"pubDate":25882,"source":25191,"url":25883,"thumbnail":25193},"Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities","arXiv:2505.06085v3 Announce Type: replace-cross Abstract: The increasing demand for generative AI as Large Language Models (LLMs) services has driven the need for specialized hardware architectures that optimize computational efficiency and energy consumption. This paper evaluates the performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic linear algebra kernels at reduced numerical precision, a fundamental operation in LLM computations. We present a detailed characterization of Grayskull's execution model, gridsize, matrix dimensions, data formats, and numerical precision impact computational efficiency. Furthermore, we compare Grayskull's performance against state-of-the-art architectures with tensor acceleration, including Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100). Whilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a competitive trade-off between power consumption and computational throughput, reaching a peak of 1.55 TFLOPs/Watt with BF16.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.06085","src/content/posts/2025-06-23-assessing-tenstorrents-risc-v-matmul-acceleration-capabilities.md","b14847819c0e4e8d",{"html":25,"metadata":25887},{"headings":25888,"localImagePaths":25889,"remoteImagePaths":25890,"frontmatter":25891,"imagePaths":25892},[],[],[],{"title":25880,"description":25881,"summary":25881,"pubDate":25202,"source":25191,"url":25883,"thumbnail":25193},[],"2025-06-23-assessing-tenstorrents-risc-v-matmul-acceleration-capabilities.md","2025-06-23-assistantx-an-llm-powered-proactive-assistant-in-collaborative-human-populated-environment",{"id":25894,"data":25896,"filePath":25901,"digest":25902,"rendered":25903,"legacyId":25910},{"title":25897,"description":25898,"summary":25898,"pubDate":25899,"source":25191,"url":25900,"thumbnail":25193},"AssistantX: An LLM-Powered Proactive Assistant in Collaborative Human-Populated Environment","arXiv:2409.17655v3 Announce Type: replace-cross Abstract: Current service robots suffer from limited natural language communication abilities, heavy reliance on predefined commands, ongoing human intervention, and, most notably, a lack of proactive collaboration awareness in human-populated environments. This results in narrow applicability and low utility. In this paper, we introduce AssistantX, an LLM-powered proactive assistant designed for autonomous operation in realworld scenarios with high accuracy. AssistantX employs a multi-agent framework consisting of 4 specialized LLM agents, each dedicated to perception, planning, decision-making, and reflective review, facilitating advanced inference capabilities and comprehensive collaboration awareness, much like a human assistant by your side. We built a dataset of 210 real-world tasks to validate AssistantX, which includes instruction content and status information on whether relevant personnel are available. Extensive experiments were conducted in both text-based simulations and a real office environment over the course of a month and a half. Our experiments demonstrate the effectiveness of the proposed framework, showing that AssistantX can reactively respond to user instructions, actively adjust strategies to adapt to contingencies, and proactively seek assistance from humans to ensure successful task completion. More details and videos can be found at https://assistantx-agent.github.io/AssistantX/.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2409.17655","src/content/posts/2025-06-23-assistantx-an-llm-powered-proactive-assistant-in-collaborative-human-populated-environment.md","2dd4b68cc403dd9f",{"html":25,"metadata":25904},{"headings":25905,"localImagePaths":25906,"remoteImagePaths":25907,"frontmatter":25908,"imagePaths":25909},[],[],[],{"title":25897,"description":25898,"summary":25898,"pubDate":25202,"source":25191,"url":25900,"thumbnail":25193},[],"2025-06-23-assistantx-an-llm-powered-proactive-assistant-in-collaborative-human-populated-environment.md","2025-06-23-automated-skill-discovery-for-language-agents-through-exploration-and-iterative-feedback",{"id":25911,"data":25913,"filePath":25918,"digest":25919,"rendered":25920,"legacyId":25927},{"title":25914,"description":25915,"summary":25915,"pubDate":25916,"source":25191,"url":25917,"thumbnail":25193},"Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback","arXiv:2506.04287v2 Announce Type: replace Abstract: Training large language model (LLM) agents to acquire necessary skills and perform diverse tasks within an environment is gaining interest as a means to enable open-endedness. However, creating the training dataset for their skill acquisition faces several challenges. Manual trajectory collection requires significant human effort. Another approach, where LLMs directly propose tasks to learn, is often invalid, as the LLMs lack knowledge of which tasks are actually feasible. Moreover, the generated data may not provide a meaningful learning signal, as agents often already perform well on the proposed tasks. To address this, we propose a novel automatic skill discovery framework EXIF for LLM-powered agents, designed to improve the feasibility of generated target behaviors while accounting for the agents' capabilities. Our method adopts an exploration-first strategy by employing an exploration agent (Alice) to train the target agent (Bob) to learn essential skills in the environment. Specifically, Alice first interacts with the environment to retrospectively generate a feasible, environment-grounded skill dataset, which is then used to train Bob. Crucially, we incorporate an iterative feedback loop, where Alice evaluates Bob's performance to identify areas for improvement. This feedback then guides Alice's next round of exploration, forming a closed-loop data generation process. Experiments on Webshop and Crafter demonstrate EXIF's ability to effectively discover meaningful skills and iteratively expand the capabilities of the trained agent without any human intervention, achieving substantial performance improvements. Interestingly, we observe that setting Alice to the same model as Bob also notably improves performance, demonstrating EXIF's potential for building a self-evolving system.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.04287","src/content/posts/2025-06-23-automated-skill-discovery-for-language-agents-through-exploration-and-iterative-feedback.md","71fc17084810f8ee",{"html":25,"metadata":25921},{"headings":25922,"localImagePaths":25923,"remoteImagePaths":25924,"frontmatter":25925,"imagePaths":25926},[],[],[],{"title":25914,"description":25915,"summary":25915,"pubDate":25202,"source":25191,"url":25917,"thumbnail":25193},[],"2025-06-23-automated-skill-discovery-for-language-agents-through-exploration-and-iterative-feedback.md","2025-06-23-automatic-dataset-shift-identification-to-support-safe-deployment-of-medical-imaging-ai",{"id":25928,"data":25930,"filePath":25935,"digest":25936,"rendered":25937,"legacyId":25944},{"title":25931,"description":25932,"summary":25932,"pubDate":25933,"source":25191,"url":25934,"thumbnail":25193},"Automatic dataset shift identification to support safe deployment of medical imaging AI","arXiv:2411.07940v3 Announce Type: replace Abstract: Shifts in data distribution can substantially harm the performance of clinical AI models and lead to misdiagnosis. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, the root causes of dataset shifts are diverse, and the choice of shift mitigation strategies is highly dependent on the precise type of shift encountered at test time. As such, detecting test-time dataset shift is not sufficient: precisely identifying which type of shift has occurred is critical. In this work, we propose the first unsupervised dataset shift identification framework for imaging datasets, effectively distinguishing between prevalence shift (caused by a change in the label distribution), covariate shift (caused by a change in input characteristics) and mixed shifts (simultaneous prevalence and covariate shifts). We discuss the importance of self-supervised encoders for detecting subtle covariate shifts and propose a novel shift detector leveraging both self-supervised encoders and task model outputs for improved shift detection. We show the effectiveness of the proposed shift identification framework across three different imaging modalities (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts using five large publicly available datasets.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2411.07940","src/content/posts/2025-06-23-automatic-dataset-shift-identification-to-support-safe-deployment-of-medical-imaging-ai.md","fa1819896f0430a1",{"html":25,"metadata":25938},{"headings":25939,"localImagePaths":25940,"remoteImagePaths":25941,"frontmatter":25942,"imagePaths":25943},[],[],[],{"title":25931,"description":25932,"summary":25932,"pubDate":25202,"source":25191,"url":25934,"thumbnail":25193},[],"2025-06-23-automatic-dataset-shift-identification-to-support-safe-deployment-of-medical-imaging-ai.md","2025-06-23-autohformer-efficient-hierarchical-autoregressive-transformer-for-time-series-prediction",{"id":25945,"data":25947,"filePath":25952,"digest":25953,"rendered":25954,"legacyId":25961},{"title":25948,"description":25949,"summary":25949,"pubDate":25950,"source":25191,"url":25951,"thumbnail":25193},"AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction","arXiv:2506.16001v1 Announce Type: cross Abstract: Time series forecasting requires architectures that simultaneously achieve three competing objectives: (1) strict temporal causality for reliable predictions, (2) sub-quadratic complexity for practical scalability, and (3) multi-scale pattern recognition for accurate long-horizon forecasting. We introduce AutoHFormer, a hierarchical autoregressive transformer that addresses these challenges through three key innovations: 1) Hierarchical Temporal Modeling: Our architecture decomposes predictions into segment-level blocks processed in parallel, followed by intra-segment sequential refinement. This dual-scale approach maintains temporal coherence while enabling efficient computation. 2) Dynamic Windowed Attention: The attention mechanism employs learnable causal windows with exponential decay, reducing complexity while preserving precise temporal relationships. This design avoids both the anti-causal violations of standard transformers and the sequential bottlenecks of RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system is adopted to capture time patterns at multiple scales. It combines fixed oscillating patterns for short-term variations with learnable decay rates for long-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X faster training and 6.06X memory reduction compared to PatchTST on PEMS08, while maintaining consistent accuracy across 96-720 step horizons in most of cases. These breakthroughs establish new benchmarks for efficient and precise time series modeling. Implementations of our method and all baselines in hierarchical autoregressive mechanism are available at https://github.com/lizzyhku/Autotime.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16001","src/content/posts/2025-06-23-autohformer-efficient-hierarchical-autoregressive-transformer-for-time-series-prediction.md","99893dc1fcf201ba",{"html":25,"metadata":25955},{"headings":25956,"localImagePaths":25957,"remoteImagePaths":25958,"frontmatter":25959,"imagePaths":25960},[],[],[],{"title":25948,"description":25949,"summary":25949,"pubDate":25202,"source":25191,"url":25951,"thumbnail":25193},[],"2025-06-23-autohformer-efficient-hierarchical-autoregressive-transformer-for-time-series-prediction.md","2025-06-23-autonomous-computer-vision-development-with-agentic-ai",{"id":25962,"data":25964,"filePath":25969,"digest":25970,"rendered":25971,"legacyId":25978},{"title":25965,"description":25966,"summary":25966,"pubDate":25967,"source":25191,"url":25968,"thumbnail":25193},"Autonomous Computer Vision Development with Agentic AI","arXiv:2506.11140v3 Announce Type: replace-cross Abstract: Agentic Artificial Intelligence (AI) systems leveraging Large Language Models (LLMs) exhibit significant potential for complex reasoning, planning, and tool utilization. We demonstrate that a specialized computer vision system can be built autonomously from a natural language prompt using Agentic AI methods. This involved extending SimpleMind (SM), an open-source Cognitive AI environment with configurable tools for medical image analysis, with an LLM-based agent, implemented using OpenManus, to automate the planning (tool configuration) for a particular computer vision task. We provide a proof-of-concept demonstration that an agentic system can interpret a computer vision task prompt, plan a corresponding SimpleMind workflow by decomposing the task and configuring appropriate tools. From the user input prompt, 'provide sm (SimpleMind) config for lungs, heart, and ribs segmentation for cxr (chest x-ray)'), the agent LLM was able to generate the plan (tool configuration file in YAML format), and execute SM-Learn (training) and SM-Think (inference) scripts autonomously. The computer vision agent automatically configured, trained, and tested itself on 50 chest x-ray images, achieving mean dice scores of 0.96, 0.82, 0.83, for lungs, heart, and ribs, respectively. This work shows the potential for autonomous planning and tool configuration that has traditionally been performed by a data scientist in the development of computer vision applications.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.11140","src/content/posts/2025-06-23-autonomous-computer-vision-development-with-agentic-ai.md","7d11d7e7ba235732",{"html":25,"metadata":25972},{"headings":25973,"localImagePaths":25974,"remoteImagePaths":25975,"frontmatter":25976,"imagePaths":25977},[],[],[],{"title":25965,"description":25966,"summary":25966,"pubDate":25202,"source":25191,"url":25968,"thumbnail":25193},[],"2025-06-23-autonomous-computer-vision-development-with-agentic-ai.md","2025-06-23-autosculpt-a-pattern-based-model-auto-pruning-framework-using-reinforcement-learning-and-graph-learning",{"id":25979,"data":25981,"filePath":25986,"digest":25987,"rendered":25988,"legacyId":25995},{"title":25982,"description":25983,"summary":25983,"pubDate":25984,"source":25191,"url":25985,"thumbnail":25193},"AutoSculpt: A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning","arXiv:2412.18091v2 Announce Type: replace Abstract: As deep neural networks (DNNs) are increasingly deployed on edge devices, optimizing models for constrained computational resources is critical. Existing auto-pruning methods face challenges due to the diversity of DNN models, various operators (e.g., filters), and the difficulty in balancing pruning granularity with model accuracy. To address these limitations, we introduce AutoSculpt, a pattern-based automated pruning framework designed to enhance efficiency and accuracy by leveraging graph learning and deep reinforcement learning (DRL). AutoSculpt automatically identifies and prunes regular patterns within DNN architectures that can be recognized by existing inference engines, enabling runtime acceleration. Three key steps in AutoSculpt include: (1) Constructing DNNs as graphs to encode their topology and parameter dependencies, (2) embedding computationally efficient pruning patterns, and (3) utilizing DRL to iteratively refine auto-pruning strategies until the optimal balance between compression and accuracy is achieved. Experimental results demonstrate the effectiveness of AutoSculpt across various architectures, including ResNet, MobileNet, VGG, and Vision Transformer, achieving pruning rates of up to 90% and nearly 18% improvement in FLOPs reduction, outperforming all baselines. The codes can be available at https://anonymous.4open.science/r/AutoSculpt-DDA0",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2412.18091","src/content/posts/2025-06-23-autosculpt-a-pattern-based-model-auto-pruning-framework-using-reinforcement-learning-and-graph-learning.md","b9b37f79e3ba8457",{"html":25,"metadata":25989},{"headings":25990,"localImagePaths":25991,"remoteImagePaths":25992,"frontmatter":25993,"imagePaths":25994},[],[],[],{"title":25982,"description":25983,"summary":25983,"pubDate":25202,"source":25191,"url":25985,"thumbnail":25193},[],"2025-06-23-autosculpt-a-pattern-based-model-auto-pruning-framework-using-reinforcement-learning-and-graph-learning.md","2025-06-23-bandwidth-selectors-on-semiparametric-bayesian-networks",{"id":25996,"data":25998,"filePath":26003,"digest":26004,"rendered":26005,"legacyId":26012},{"title":25999,"description":26000,"summary":26000,"pubDate":26001,"source":25191,"url":26002,"thumbnail":25193},"Bandwidth Selectors on Semiparametric Bayesian Networks","arXiv:2506.16844v1 Announce Type: cross Abstract: Semiparametric Bayesian networks (SPBNs) integrate parametric and non-parametric probabilistic models, offering flexibility in learning complex data distributions from samples. In particular, kernel density estimators (KDEs) are employed for the non-parametric component. Under the assumption of data normality, the normal rule is used to learn the bandwidth matrix for the KDEs in SPBNs. This matrix is the key hyperparameter that controls the trade-off between bias and variance. However, real-world data often deviates from normality, potentially leading to suboptimal density estimation and reduced predictive performance. This paper first establishes the theoretical framework for the application of state-of-the-art bandwidth selectors and subsequently evaluates their impact on SPBN performance. We explore the approaches of cross-validation and plug-in selectors, assessing their effectiveness in enhancing the learning capability and applicability of SPBNs. To support this investigation, we have extended the open-source package PyBNesian for SPBNs with the additional bandwidth selection techniques and conducted extensive experimental analyses. Our results demonstrate that the proposed bandwidth selectors leverage increasing information more effectively than the normal rule, which, despite its robustness, stagnates with more data. In particular, unbiased cross-validation generally outperforms the normal rule, highlighting its advantage in high sample size scenarios.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16844","src/content/posts/2025-06-23-bandwidth-selectors-on-semiparametric-bayesian-networks.md","fd6b2cdd2de1f5b7",{"html":25,"metadata":26006},{"headings":26007,"localImagePaths":26008,"remoteImagePaths":26009,"frontmatter":26010,"imagePaths":26011},[],[],[],{"title":25999,"description":26000,"summary":26000,"pubDate":25202,"source":25191,"url":26002,"thumbnail":25193},[],"2025-06-23-bandwidth-selectors-on-semiparametric-bayesian-networks.md","2025-06-23-base-q-bias-and-asymmetric-scaling-enhanced-rotational-quantization-for-large-language-models",{"id":26013,"data":26015,"filePath":26020,"digest":26021,"rendered":26022,"legacyId":26029},{"title":26016,"description":26017,"summary":26017,"pubDate":26018,"source":25191,"url":26019,"thumbnail":25193},"BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models","arXiv:2506.15689v1 Announce Type: cross Abstract: Rotations have become essential to state-of-the-art quantization pipelines for large language models (LLMs) by effectively smoothing outliers in weights and activations. However, further optimizing the rotation parameters offers only limited performance gains and introduces significant training overhead: due to rotation parameter sharing, full-model must be loaded simultaneously to enable backpropagation, resulting in substantial memory consumption and limited practical utility. In this work, we identify two fundamental limitations of current rotational quantization methods: (i) rotation fails to align channel means, resulting in wider quantization bounds and increased rounding errors; and (ii) rotation makes the activation distribution more Gaussian-like, increasing energy loss caused by clipping errors. To address these issues, we introduce textbf{BASE-Q}, a simple yet powerful approach that combines bias correction and asymmetric scaling to effectively reduce rounding and clipping errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the need for memory-intensive full-model backpropagation. Extensive experiments on various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing the accuracy gap to full-precision models by 50.5%, 42.9%, and 29.2% compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be released soon.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15689","src/content/posts/2025-06-23-base-q-bias-and-asymmetric-scaling-enhanced-rotational-quantization-for-large-language-models.md","f624b6c1490896c6",{"html":25,"metadata":26023},{"headings":26024,"localImagePaths":26025,"remoteImagePaths":26026,"frontmatter":26027,"imagePaths":26028},[],[],[],{"title":26016,"description":26017,"summary":26017,"pubDate":25202,"source":25191,"url":26019,"thumbnail":25193},[],"2025-06-23-base-q-bias-and-asymmetric-scaling-enhanced-rotational-quantization-for-large-language-models.md","2025-06-23-batayan-a-filipino-nlp-benchmark-for-evaluating-large-language-models",{"id":26030,"data":26032,"filePath":26037,"digest":26038,"rendered":26039,"legacyId":26046},{"title":26033,"description":26034,"summary":26034,"pubDate":26035,"source":25191,"url":26036,"thumbnail":25193},"Batayan: A Filipino NLP benchmark for evaluating Large Language Models","arXiv:2502.14911v2 Announce Type: replace-cross Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable capabilities on widely benchmarked high-resource languages. However, linguistic nuances of under-resourced languages remain unexplored. We introduce Batayan, a holistic Filipino benchmark that systematically evaluates LLMs across three key natural language processing (NLP) competencies: understanding, reasoning, and generation. Batayan consolidates eight tasks, three of which have not existed prior for Filipino corpora, covering both Tagalog and code-switched Taglish utterances. Our rigorous, native-speaker-driven adaptation and validation processes ensures fluency and authenticity to the complex morphological and syntactic structures of Filipino, alleviating the pervasive translationese bias in existing Filipino corpora. We report empirical results on a variety of open-source and commercial LLMs, highlighting significant performance gaps that signal the under-representation of Filipino in pre-training corpora, the unique hurdles in modeling Filipino's rich morphology and construction, and the importance of explicit Filipino language support. Moreover, we discuss the practical challenges encountered in dataset construction and propose principled solutions for building culturally and linguistically-faithful resources in under-represented languages. We also provide a public evaluation suite as a clear foundation for iterative, community-driven progress in Filipino NLP.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2502.14911","src/content/posts/2025-06-23-batayan-a-filipino-nlp-benchmark-for-evaluating-large-language-models.md","8073d60fa53c16f1",{"html":25,"metadata":26040},{"headings":26041,"localImagePaths":26042,"remoteImagePaths":26043,"frontmatter":26044,"imagePaths":26045},[],[],[],{"title":26033,"description":26034,"summary":26034,"pubDate":25202,"source":25191,"url":26036,"thumbnail":25193},[],"2025-06-23-batayan-a-filipino-nlp-benchmark-for-evaluating-large-language-models.md","2025-06-23-batterybert-for-realistic-battery-fault-detection-using-point-masked-signal-modeling",{"id":26047,"data":26049,"filePath":26054,"digest":26055,"rendered":26056,"legacyId":26063},{"title":26050,"description":26051,"summary":26051,"pubDate":26052,"source":25191,"url":26053,"thumbnail":25193},"BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling","arXiv:2506.15712v1 Announce Type: cross Abstract: Accurate fault detection in lithium-ion batteries is essential for the safe and reliable operation of electric vehicles and energy storage systems. However, existing methods often struggle to capture complex temporal dependencies and cannot fully leverage abundant unlabeled data. Although large language models (LLMs) exhibit strong representation capabilities, their architectures are not directly suited to the numerical time-series data common in industrial settings. To address these challenges, we propose a novel framework that adapts BERT-style pretraining for battery fault detection by extending the standard BERT architecture with a customized time-series-to-token representation module and a point-level Masked Signal Modeling (point-MSM) pretraining task tailored to battery applications. This approach enables self-supervised learning on sequential current, voltage, and other charge-discharge cycle data, yielding distributionally robust, context-aware temporal embeddings. We then concatenate these embeddings with battery metadata and feed them into a downstream classifier for accurate fault classification. Experimental results on a large-scale real-world dataset show that models initialized with our pretrained parameters significantly improve both representation quality and classification accuracy, achieving an AUROC of 0.945 and substantially outperforming existing approaches. These findings validate the effectiveness of BERT-style pretraining for time-series fault detection.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15712","src/content/posts/2025-06-23-batterybert-for-realistic-battery-fault-detection-using-point-masked-signal-modeling.md","57363ffb36fbb6a9",{"html":25,"metadata":26057},{"headings":26058,"localImagePaths":26059,"remoteImagePaths":26060,"frontmatter":26061,"imagePaths":26062},[],[],[],{"title":26050,"description":26051,"summary":26051,"pubDate":25202,"source":25191,"url":26053,"thumbnail":25193},[],"2025-06-23-batterybert-for-realistic-battery-fault-detection-using-point-masked-signal-modeling.md","2025-06-23-bayesian-epistemology-with-weighted-authority-a-formal-architecture-for-truth-promoting-autonomous-scientific-reasoning",{"id":26064,"data":26066,"filePath":26071,"digest":26072,"rendered":26073,"legacyId":26080},{"title":26067,"description":26068,"summary":26068,"pubDate":26069,"source":25191,"url":26070,"thumbnail":25193},"Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning","arXiv:2506.16015v1 Announce Type: new Abstract: The exponential expansion of scientific literature has surpassed the epistemic processing capabilities of both human experts and current artificial intelligence systems. This paper introduces Bayesian Epistemology with Weighted Authority (BEWA), a formally structured architecture that operationalises belief as a dynamic, probabilistically coherent function over structured scientific claims. Each claim is contextualised, author-attributed, and evaluated through a system of replication scores, citation weighting, and temporal decay. Belief updates are performed via evidence-conditioned Bayesian inference, contradiction processing, and epistemic decay mechanisms. The architecture supports graph-based claim propagation, authorial credibility modelling, cryptographic anchoring, and zero-knowledge audit verification. By formalising scientific reasoning into a computationally verifiable epistemic network, BEWA advances the foundation for machine reasoning systems that promote truth utility, rational belief convergence, and audit-resilient integrity across dynamic scientific domains.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16015","src/content/posts/2025-06-23-bayesian-epistemology-with-weighted-authority-a-formal-architecture-for-truth-promoting-autonomous-scientific-reasoning.md","c6cfa2db679ebebd",{"html":25,"metadata":26074},{"headings":26075,"localImagePaths":26076,"remoteImagePaths":26077,"frontmatter":26078,"imagePaths":26079},[],[],[],{"title":26067,"description":26068,"summary":26068,"pubDate":25202,"source":25191,"url":26070,"thumbnail":25193},[],"2025-06-23-bayesian-epistemology-with-weighted-authority-a-formal-architecture-for-truth-promoting-autonomous-scientific-reasoning.md","2025-06-23-beads-bias-evaluation-across-domains",{"id":26081,"data":26083,"filePath":26088,"digest":26089,"rendered":26090,"legacyId":26097},{"title":26084,"description":26085,"summary":26085,"pubDate":26086,"source":25191,"url":26087,"thumbnail":25193},"BEADs: Bias Evaluation Across Domains","arXiv:2406.04220v5 Announce Type: replace-cross Abstract: Recent advancements in large language models (LLMs) have significantly improved natural language processing (NLP) applications. However, these models often inherit biases from their training data. While several datasets exist for bias detection, most are limited to one or two NLP tasks, typically classification or evaluation, and lack comprehensive coverage across a broader range of tasks. To address this gap, we introduce the Bias Evaluations Across Domains (BEADs) dataset, designed to support a wide range of NLP tasks, including text classification, token classification, bias quantification, and benign language generation. A key contribution of this work is the gold-standard annotation provided by GPT-4 for scalability, with expert verification to ensure high reliability. BEADs can be used for both fine-tuning models (for classification and generation tasks) and evaluating LLM behavior. Our findings show that BEADs effectively surfaces various biases during model fine-tuning and helps reduce biases in language generation tasks while maintaining output quality. The dataset also highlights prevalent demographic biases in LLMs during evaluation. We release BEADs as a practical resource for detecting and mitigating bias across domains, supporting the development of responsible AI systems. Project: https://vectorinstitute.github.io/BEAD/ Data: https://huggingface.co/datasets/shainar/BEAD",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2406.04220","src/content/posts/2025-06-23-beads-bias-evaluation-across-domains.md","fcae61e3659a5782",{"html":25,"metadata":26091},{"headings":26092,"localImagePaths":26093,"remoteImagePaths":26094,"frontmatter":26095,"imagePaths":26096},[],[],[],{"title":26084,"description":26085,"summary":26085,"pubDate":25202,"source":25191,"url":26087,"thumbnail":25193},[],"2025-06-23-beads-bias-evaluation-across-domains.md","2025-06-23-behaviour-planning-a-toolkit-for-diverse-planning",{"id":26098,"data":26100,"filePath":26105,"digest":26106,"rendered":26107,"legacyId":26114},{"title":26101,"description":26102,"summary":26102,"pubDate":26103,"source":25191,"url":26104,"thumbnail":25193},"Behaviour Planning: A Toolkit for Diverse Planning","arXiv:2405.04300v3 Announce Type: replace Abstract: Diverse planning approaches are utilised in real-world applications like risk management, automated streamed data analysis, and malware detection. The current diverse planning formulations encode the diversity model as a distance function, which is computational inexpensive when comparing two plans. However, such modelling approach limits what can be encoded as measure of diversity, as well as the ability to explain why two plans are different. This paper introduces a novel approach to the diverse planning problem, allowing for more expressive modelling of diversity using a n-dimensional grid representation, where each dimension corresponds to a user-defined feature. Furthermore, we present a novel toolkit that generates diverse plans based on such customisable diversity models, called emph{Behaviour Planning}. We provide an implementation for behaviour planning using planning-as-satisfiability. An empirical evaluation of our implementation shows that behaviour planning significantly outperforms the current diverse planning method in generating diverse plans measured on our new customisable diversity models. Our implementation is the first diverse planning approach to support planning categories beyond classical planning, such as over-subscription and numerical planning.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2405.04300","src/content/posts/2025-06-23-behaviour-planning-a-toolkit-for-diverse-planning.md","4d23ca78dbdcee19",{"html":25,"metadata":26108},{"headings":26109,"localImagePaths":26110,"remoteImagePaths":26111,"frontmatter":26112,"imagePaths":26113},[],[],[],{"title":26101,"description":26102,"summary":26102,"pubDate":25202,"source":25191,"url":26104,"thumbnail":25193},[],"2025-06-23-behaviour-planning-a-toolkit-for-diverse-planning.md","2025-06-23-beyond-audio-and-pose-a-general-purpose-framework-for-video-synchronization",{"id":26115,"data":26117,"filePath":26122,"digest":26123,"rendered":26124,"legacyId":26131},{"title":26118,"description":26119,"summary":26119,"pubDate":26120,"source":25191,"url":26121,"thumbnail":25193},"Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization","arXiv:2506.15937v1 Announce Type: cross Abstract: Video synchronization-aligning multiple video streams capturing the same event from different angles-is crucial for applications such as reality TV show production, sports analysis, surveillance, and autonomous systems. Prior work has heavily relied on audio cues or specific visual events, limiting applicability in diverse settings where such signals may be unreliable or absent. Additionally, existing benchmarks for video synchronization lack generality and reproducibility, restricting progress in the field. In this work, we introduce VideoSync, a video synchronization framework that operates independently of specific feature extraction methods, such as human pose estimation, enabling broader applicability across different content types. We evaluate our system on newly composed datasets covering single-human, multi-human, and non-human scenarios, providing both the methodology and code for dataset creation to establish reproducible benchmarks. Our analysis reveals biases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline, leading to inflated performance claims. We correct these biases and propose a more rigorous evaluation framework, demonstrating that VideoSync outperforms existing approaches, including SeSyn-Net, under fair experimental conditions. Additionally, we explore various synchronization offset prediction methods, identifying a convolutional neural network (CNN)-based model as the most effective. Our findings advance video synchronization beyond domain-specific constraints, making it more generalizable and robust for real-world applications.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15937","src/content/posts/2025-06-23-beyond-audio-and-pose-a-general-purpose-framework-for-video-synchronization.md","7f434b46c0efb78b",{"html":25,"metadata":26125},{"headings":26126,"localImagePaths":26127,"remoteImagePaths":26128,"frontmatter":26129,"imagePaths":26130},[],[],[],{"title":26118,"description":26119,"summary":26119,"pubDate":25202,"source":25191,"url":26121,"thumbnail":25193},[],"2025-06-23-beyond-audio-and-pose-a-general-purpose-framework-for-video-synchronization.md","2025-06-23-bida-a-bi-level-interaction-decision-making-algorithm-for-autonomous-vehicles-in-dynamic-traffic-scenarios",{"id":26132,"data":26134,"filePath":26139,"digest":26140,"rendered":26141,"legacyId":26148},{"title":26135,"description":26136,"summary":26136,"pubDate":26137,"source":25191,"url":26138,"thumbnail":25193},"BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios","arXiv:2506.16546v1 Announce Type: cross Abstract: In complex real-world traffic environments, autonomous vehicles (AVs) need to interact with other traffic participants while making real-time and safety-critical decisions accordingly. The unpredictability of human behaviors poses significant challenges, particularly in dynamic scenarios, such as multi-lane highways and unsignalized T-intersections. To address this gap, we design a bi-level interaction decision-making algorithm (BIDA) that integrates interactive Monte Carlo tree search (MCTS) with deep reinforcement learning (DRL), aiming to enhance interaction rationality, efficiency and safety of AVs in dynamic key traffic scenarios. Specifically, we adopt three types of DRL algorithms to construct a reliable value network and policy network, which guide the online deduction process of interactive MCTS by assisting in value update and node selection. Then, a dynamic trajectory planner and a trajectory tracking controller are designed and implemented in CARLA to ensure smooth execution of planned maneuvers. Experimental evaluations demonstrate that our BIDA not only enhances interactive deduction and reduces computational costs, but also outperforms other latest benchmarks, which exhibits superior safety, efficiency and interaction rationality under varying traffic conditions.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16546","src/content/posts/2025-06-23-bida-a-bi-level-interaction-decision-making-algorithm-for-autonomous-vehicles-in-dynamic-traffic-scenarios.md","1d6a4a9fc6b9b6ef",{"html":25,"metadata":26142},{"headings":26143,"localImagePaths":26144,"remoteImagePaths":26145,"frontmatter":26146,"imagePaths":26147},[],[],[],{"title":26135,"description":26136,"summary":26136,"pubDate":25202,"source":25191,"url":26138,"thumbnail":25193},[],"2025-06-23-bida-a-bi-level-interaction-decision-making-algorithm-for-autonomous-vehicles-in-dynamic-traffic-scenarios.md","2025-06-23-blur-a-benchmark-for-llm-unlearning-robust-to-forget-retain-overlap",{"id":26149,"data":26151,"filePath":26156,"digest":26157,"rendered":26158,"legacyId":26165},{"title":26152,"description":26153,"summary":26153,"pubDate":26154,"source":25191,"url":26155,"thumbnail":25193},"BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap","arXiv:2506.15699v1 Announce Type: cross Abstract: Machine unlearning has the potential to improve the safety of large language models (LLMs) by removing sensitive or harmful information post hoc. A key challenge in unlearning involves balancing between forget quality (effectively unlearning undesirable information) and retain quality (maintaining good performance on other, general tasks). Unfortunately, as we show, current LLM unlearning benchmarks contain highly disparate forget and retain sets -- painting a false picture of the effectiveness of LLM unlearning methods. This can be particularly problematic because it opens the door for benign perturbations, such as relearning attacks, to easily reveal supposedly unlearned knowledge once models are deployed. To address this, we present $texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic scenarios of forget-retain overlap. $texttt{BLUR}$ significantly expands on existing unlearning benchmarks by providing extended evaluation tasks, combined forget/retain queries, and relearning datasets of varying degrees of difficulty. Despite the benign nature of the queries considered, we find that the performance of existing methods drops significantly when evaluated on $texttt{BLUR}$, with simple approaches performing better on average than more recent methods. These results highlight the importance of robust evaluation and suggest several important directions of future study. Our benchmark is publicly available at: https://huggingface.co/datasets/forgelab/BLUR",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15699","src/content/posts/2025-06-23-blur-a-benchmark-for-llm-unlearning-robust-to-forget-retain-overlap.md","2e968d2993fc7a7e",{"html":25,"metadata":26159},{"headings":26160,"localImagePaths":26161,"remoteImagePaths":26162,"frontmatter":26163,"imagePaths":26164},[],[],[],{"title":26152,"description":26153,"summary":26153,"pubDate":25202,"source":25191,"url":26155,"thumbnail":25193},[],"2025-06-23-blur-a-benchmark-for-llm-unlearning-robust-to-forget-retain-overlap.md","2025-06-23-boosting-multi-demographic-federated-learning-for-chest-radiograph-analysis-using-general-purpose-self-supervised-representations",{"id":26166,"data":26168,"filePath":26173,"digest":26174,"rendered":26175,"legacyId":26182},{"title":26169,"description":26170,"summary":26170,"pubDate":26171,"source":25191,"url":26172,"thumbnail":25193},"Boosting multi-demographic federated learning for chest radiograph analysis using general-purpose self-supervised representations","arXiv:2504.08584v2 Announce Type: replace-cross Abstract: Reliable artificial intelligence (AI) models for medical image analysis often depend on large and diverse labeled datasets. Federated learning (FL) offers a decentralized and privacy-preserving approach to training but struggles in highly non-independent and identically distributed (non-IID) settings, where institutions with more representative data may experience degraded performance. Moreover, existing large-scale FL studies have been limited to adult datasets, neglecting the unique challenges posed by pediatric data, which introduces additional non-IID variability. To address these limitations, we analyzed n=398,523 adult chest radiographs from diverse institutions across multiple countries and n=9,125 pediatric images, leveraging transfer learning from general-purpose self-supervised image representations to classify pneumonia and cases with no abnormality. Using state-of-the-art vision transformers, we found that FL improved performance only for smaller adult datasets (P\u003C0.001) but degraded performance for larger datasets (P\u003C0.064) and pediatric cases (P=0.242). However, equipping FL with self-supervised weights significantly enhanced outcomes across pediatric cases (P=0.031) and most adult datasets (P\u003C0.008), except the largest dataset (P=0.052). These findings underscore the potential of easily deployable general-purpose self-supervised image representations to address non-IID challenges in clinical FL applications and highlight their promise for enhancing patient outcomes and advancing pediatric healthcare, where data scarcity and variability remain persistent obstacles.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2504.08584","src/content/posts/2025-06-23-boosting-multi-demographic-federated-learning-for-chest-radiograph-analysis-using-general-purpose-self-supervised-representations.md","8200d7a5a741eb02",{"html":25,"metadata":26176},{"headings":26177,"localImagePaths":26178,"remoteImagePaths":26179,"frontmatter":26180,"imagePaths":26181},[],[],[],{"title":26169,"description":26170,"summary":26170,"pubDate":25202,"source":25191,"url":26172,"thumbnail":25193},[],"2025-06-23-boosting-multi-demographic-federated-learning-for-chest-radiograph-analysis-using-general-purpose-self-supervised-representations.md","2025-06-23-breaking-the-compression-ceiling-data-free-pipeline-for-ultra-efficient-delta-compression",{"id":26183,"data":26185,"filePath":26190,"digest":26191,"rendered":26192,"legacyId":26199},{"title":26186,"description":26187,"summary":26187,"pubDate":26188,"source":25191,"url":26189,"thumbnail":25193},"Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression","arXiv:2505.13563v2 Announce Type: replace-cross Abstract: With the rise of the fine-tuned--pretrained paradigm, storing numerous fine-tuned models for multi-tasking creates significant storage overhead. Delta compression alleviates this by storing only the pretrained model and the highly compressed delta weights (the differences between fine-tuned and pretrained model weights). However, existing methods fail to maintain both high compression and performance, and often rely on data. To address these challenges, we propose UltraDelta, the first data-free delta compression pipeline that achieves both ultra-high compression and strong performance. UltraDelta is designed to minimize redundancy, maximize information, and stabilize performance across inter-layer, intra-layer, and global dimensions, using three key components: (1) Variance-Based Mixed Sparsity Allocation assigns sparsity based on variance, giving lower sparsity to high-variance layers to preserve inter-layer information. (2) Distribution-Aware Compression applies uniform quantization and then groups parameters by value, followed by group-wise pruning, to better preserve intra-layer distribution. (3) Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a global rescaling factor, improving model stability under higher compression. Extensive experiments across (a) large language models (fine-tuned on LLaMA-2 7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base) with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and (d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that UltraDelta consistently outperforms existing methods, especially under ultra-high compression.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.13563","src/content/posts/2025-06-23-breaking-the-compression-ceiling-data-free-pipeline-for-ultra-efficient-delta-compression.md","22dee12b25c0a3c5",{"html":25,"metadata":26193},{"headings":26194,"localImagePaths":26195,"remoteImagePaths":26196,"frontmatter":26197,"imagePaths":26198},[],[],[],{"title":26186,"description":26187,"summary":26187,"pubDate":25202,"source":25191,"url":26189,"thumbnail":25193},[],"2025-06-23-breaking-the-compression-ceiling-data-free-pipeline-for-ultra-efficient-delta-compression.md","2025-06-23-breastdcedl-curating-a-comprehensive-dce-mri-dataset-and-developing-a-transformer-implementation-for-breast-cancer-treatment-response-prediction",{"id":26200,"data":26202,"filePath":26207,"digest":26208,"rendered":26209,"legacyId":26216},{"title":26203,"description":26204,"summary":26204,"pubDate":26205,"source":25191,"url":26206,"thumbnail":25193},"BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a Transformer Implementation for Breast Cancer Treatment Response Prediction","arXiv:2506.12190v2 Announce Type: replace-cross Abstract: Breast cancer remains a leading cause of cancer-related mortality worldwide, making early detection and accurate treatment response monitoring critical priorities. We present BreastDCEDL, a curated, deep learning-ready dataset comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from 2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts, all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were rigorously converted into standardized 3D NIfTI volumes with preserved signal integrity, accompanied by unified tumor annotations and harmonized clinical metadata including pathologic complete response (pCR), hormone receptor (HR), and HER2 status. Although DCE-MRI provides essential diagnostic information and deep learning offers tremendous potential for analyzing such complex data, progress has been limited by lack of accessible, public, multicenter datasets. BreastDCEDL addresses this gap by enabling development of advanced models, including state-of-the-art transformer architectures that require substantial training data. To demonstrate its capacity for robust modeling, we developed the first transformer-based model for breast DCE-MRI, leveraging Vision Transformer (ViT) architecture trained on RGB-fused images from three contrast phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT model achieved state-of-the-art pCR prediction performance in HR+/HER2- patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark splits, offering a framework for reproducible research and enabling clinically meaningful modeling in breast cancer imaging.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.12190","src/content/posts/2025-06-23-breastdcedl-curating-a-comprehensive-dce-mri-dataset-and-developing-a-transformer-implementation-for-breast-cancer-treatment-response-prediction.md","85335e7142494c8b",{"html":25,"metadata":26210},{"headings":26211,"localImagePaths":26212,"remoteImagePaths":26213,"frontmatter":26214,"imagePaths":26215},[],[],[],{"title":26203,"description":26204,"summary":26204,"pubDate":25202,"source":25191,"url":26206,"thumbnail":25193},[],"2025-06-23-breastdcedl-curating-a-comprehensive-dce-mri-dataset-and-developing-a-transformer-implementation-for-breast-cancer-treatment-response-prediction.md","2025-06-23-calibrating-pre-trained-language-classifiers-on-llm-generated-noisy-labels-via-iterative-refinement",{"id":26217,"data":26219,"filePath":26224,"digest":26225,"rendered":26226,"legacyId":26233},{"title":26220,"description":26221,"summary":26221,"pubDate":26222,"source":25191,"url":26223,"thumbnail":25193},"Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement","arXiv:2505.19675v2 Announce Type: replace-cross Abstract: The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the model's generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifier's prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks. Our code is available on Github.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.19675","src/content/posts/2025-06-23-calibrating-pre-trained-language-classifiers-on-llm-generated-noisy-labels-via-iterative-refinement.md","5154b33abd2af40d",{"html":25,"metadata":26227},{"headings":26228,"localImagePaths":26229,"remoteImagePaths":26230,"frontmatter":26231,"imagePaths":26232},[],[],[],{"title":26220,"description":26221,"summary":26221,"pubDate":25202,"source":25191,"url":26223,"thumbnail":25193},[],"2025-06-23-calibrating-pre-trained-language-classifiers-on-llm-generated-noisy-labels-via-iterative-refinement.md","2025-06-23-can-large-language-models-replace-human-subjects-a-large-scale-replication-of-scenario-based-experiments-in-psychology-and-management",{"id":26234,"data":26236,"filePath":26241,"digest":26242,"rendered":26243,"legacyId":26250},{"title":26237,"description":26238,"summary":26238,"pubDate":26239,"source":25191,"url":26240,"thumbnail":25193},"Can Large Language Models Replace Human Subjects? A Large-Scale Replication of Scenario-Based Experiments in Psychology and Management","arXiv:2409.00128v3 Announce Type: replace-cross Abstract: Artificial Intelligence (AI) is increasingly being integrated into scientific research, particularly in the social sciences, where understanding human behavior is critical. Large Language Models (LLMs) have shown promise in replicating human-like responses in various psychological experiments. We conducted a large-scale study replicating 156 psychological experiments from top social science journals using three state-of-the-art LLMs (GPT-4, Claude 3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate high replication rates for main effects (73-81%) and moderate to strong success with interaction effects (46-63%), They consistently produce larger effect sizes than human studies, with Fisher Z values approximately 2-3 times higher than human studies. Notably, LLMs show significantly lower replication rates for studies involving socially sensitive topics such as race, gender and ethics. When original studies reported null findings, LLMs produced significant results at remarkably high rates (68-83%) - while this could reflect cleaner data with less noise, as evidenced by narrower confidence intervals, it also suggests potential risks of effect size overestimation. Our results demonstrate both the promise and challenges of LLMs in psychological research, offering efficient tools for pilot testing and rapid hypothesis validation while enriching rather than replacing traditional human subject studies, yet requiring more nuanced interpretation and human validation for complex social phenomena and culturally sensitive research questions.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2409.00128","src/content/posts/2025-06-23-can-large-language-models-replace-human-subjects-a-large-scale-replication-of-scenario-based-experiments-in-psychology-and-management.md","71f2a77282031be9",{"html":25,"metadata":26244},{"headings":26245,"localImagePaths":26246,"remoteImagePaths":26247,"frontmatter":26248,"imagePaths":26249},[],[],[],{"title":26237,"description":26238,"summary":26238,"pubDate":25202,"source":25191,"url":26240,"thumbnail":25193},[],"2025-06-23-can-large-language-models-replace-human-subjects-a-large-scale-replication-of-scenario-based-experiments-in-psychology-and-management.md","2025-06-23-can-structural-correspondences-ground-real-world-representational-content-in-large-language-models",{"id":26251,"data":26253,"filePath":26258,"digest":26259,"rendered":26260,"legacyId":26267},{"title":26254,"description":26255,"summary":26255,"pubDate":26256,"source":25191,"url":26257,"thumbnail":25193},"Can structural correspondences ground real world representational content in Large Language Models?","arXiv:2506.16370v1 Announce Type: cross Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a wide range of prompts. But their representational capacities are uncertain. Many LLMs have no direct contact with extra-linguistic reality: their inputs, outputs and training data consist solely of text, raising the questions (1) can LLMs represent anything and (2) if so, what? In this paper, I explore what it would take to answer these questions according to a structural-correspondence based account of representation, and make an initial survey of this evidence. I argue that the mere existence of structural correspondences between LLMs and worldly entities is insufficient to ground representation of those entities. However, if these structural correspondences play an appropriate role - they are exploited in a way that explains successful task performance - then they could ground real world contents. This requires overcoming a challenge: the text-boundedness of LLMs appears, on the face of it, to prevent them engaging in the right sorts of tasks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16370","src/content/posts/2025-06-23-can-structural-correspondences-ground-real-world-representational-content-in-large-language-models.md","f9850175f9fb71de",{"html":25,"metadata":26261},{"headings":26262,"localImagePaths":26263,"remoteImagePaths":26264,"frontmatter":26265,"imagePaths":26266},[],[],[],{"title":26254,"description":26255,"summary":26255,"pubDate":25202,"source":25191,"url":26257,"thumbnail":25193},[],"2025-06-23-can-structural-correspondences-ground-real-world-representational-content-in-large-language-models.md","2025-06-23-can-we-detect-failures-without-failure-data-uncertainty-aware-runtime-failure-detection-for-imitation-learning-policies",{"id":26268,"data":26270,"filePath":26275,"digest":26276,"rendered":26277,"legacyId":26284},{"title":26271,"description":26272,"summary":26272,"pubDate":26273,"source":25191,"url":26274,"thumbnail":25193},"Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies","arXiv:2503.08558v3 Announce Type: replace-cross Abstract: Recent years have witnessed impressive robotic manipulation systems driven by advances in imitation learning and generative modeling, such as diffusion- and flow-based approaches. As robot policy performance increases, so does the complexity and time horizon of achievable tasks, inducing unexpected and diverse failure modes that are difficult to predict a priori. To enable trustworthy policy deployment in safety-critical human environments, reliable runtime failure detection becomes important during policy inference. However, most existing failure detection approaches rely on prior knowledge of failure modes and require failure data during training, which imposes a significant challenge in practicality and scalability. In response to these limitations, we present FAIL-Detect, a modular two-stage approach for failure detection in imitation learning-based robotic manipulation. To accurately identify failures from successful training data alone, we frame the problem as sequential out-of-distribution (OOD) detection. We first distill policy inputs and outputs into scalar signals that correlate with policy failures and capture epistemic uncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile framework for uncertainty quantification with statistical guarantees. Empirically, we thoroughly investigate both learned and post-hoc scalar signal candidates on diverse robotic manipulation tasks. Our experiments show learned signals to be mostly consistently effective, particularly when using our novel flow-based density estimator. Furthermore, our method detects failures more accurately and faster than state-of-the-art (SOTA) failure detection baselines. These results highlight the potential of FAIL-Detect to enhance the safety and reliability of imitation learning-based robotic systems as they progress toward real-world deployment.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2503.08558","src/content/posts/2025-06-23-can-we-detect-failures-without-failure-data-uncertainty-aware-runtime-failure-detection-for-imitation-learning-policies.md","fe50ae46d31b5e94",{"html":25,"metadata":26278},{"headings":26279,"localImagePaths":26280,"remoteImagePaths":26281,"frontmatter":26282,"imagePaths":26283},[],[],[],{"title":26271,"description":26272,"summary":26272,"pubDate":25202,"source":25191,"url":26274,"thumbnail":25193},[],"2025-06-23-can-we-detect-failures-without-failure-data-uncertainty-aware-runtime-failure-detection-for-imitation-learning-policies.md","2025-06-23-capsdt-diffusion-transformer-for-capsule-robot-manipulation",{"id":26285,"data":26287,"filePath":26292,"digest":26293,"rendered":26294,"legacyId":26301},{"title":26288,"description":26289,"summary":26289,"pubDate":26290,"source":25191,"url":26291,"thumbnail":25193},"CapsDT: Diffusion-Transformer for Capsule Robot Manipulation","arXiv:2506.16263v1 Announce Type: cross Abstract: Vision-Language-Action (VLA) models have emerged as a prominent research area, showcasing significant potential across a variety of applications. However, their performance in endoscopy robotics, particularly endoscopy capsule robots that perform actions within the digestive system, remains unexplored. The integration of VLA models into endoscopy robots allows more intuitive and efficient interactions between human operators and medical devices, improving both diagnostic accuracy and treatment outcomes. In this work, we design CapsDT, a Diffusion Transformer model for capsule robot manipulation in the stomach. By processing interleaved visual inputs, and textual instructions, CapsDT can infer corresponding robotic control signals to facilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot system, a capsule robot controlled by a robotic arm-held magnet, addressing different levels of four endoscopy tasks and creating corresponding capsule robot datasets within the stomach simulator. Comprehensive evaluations on various robotic tasks indicate that CapsDT can serve as a robust vision-language generalist, achieving state-of-the-art performance in various levels of endoscopy tasks while achieving a 26.25% success rate in real-world simulation manipulation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16263","src/content/posts/2025-06-23-capsdt-diffusion-transformer-for-capsule-robot-manipulation.md","210a68ffb119597c",{"html":25,"metadata":26295},{"headings":26296,"localImagePaths":26297,"remoteImagePaths":26298,"frontmatter":26299,"imagePaths":26300},[],[],[],{"title":26288,"description":26289,"summary":26289,"pubDate":25202,"source":25191,"url":26291,"thumbnail":25193},[],"2025-06-23-capsdt-diffusion-transformer-for-capsule-robot-manipulation.md","2025-06-23-cast-enhancing-code-retrieval-augmented-generation-with-structural-chunking-via-abstract-syntax-tree",{"id":26302,"data":26304,"filePath":26309,"digest":26310,"rendered":26311,"legacyId":26318},{"title":26305,"description":26306,"summary":26306,"pubDate":26307,"source":25191,"url":26308,"thumbnail":25193},"cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree","arXiv:2506.15655v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale code generation, grounding predictions in external code corpora to improve actuality. However, a critical yet underexplored aspect of RAG pipelines is chunking -- the process of dividing documents into retrievable units. Existing line-based chunking heuristics often break semantic structures, splitting functions or merging unrelated code, which can degrade generation quality. We propose chunking via Abstract Syntax Trees (ourwork), a structure-aware method that recursively breaks large AST nodes into smaller chunks and merges sibling nodes while respecting size limits. This approach generates self-contained, semantically coherent units across programming languages and tasks, improving performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3 points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation. Our work highlights the importance of structure-aware chunking for scaling retrieval-enhanced code intelligence.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15655","src/content/posts/2025-06-23-cast-enhancing-code-retrieval-augmented-generation-with-structural-chunking-via-abstract-syntax-tree.md","baa64b305d9b3250",{"html":25,"metadata":26312},{"headings":26313,"localImagePaths":26314,"remoteImagePaths":26315,"frontmatter":26316,"imagePaths":26317},[],[],[],{"title":26305,"description":26306,"summary":26306,"pubDate":25202,"source":25191,"url":26308,"thumbnail":25193},[],"2025-06-23-cast-enhancing-code-retrieval-augmented-generation-with-structural-chunking-via-abstract-syntax-tree.md","2025-06-23-category-based-galaxy-image-generation-via-diffusion-models",{"id":26319,"data":26321,"filePath":26326,"digest":26327,"rendered":26328,"legacyId":26335},{"title":26322,"description":26323,"summary":26323,"pubDate":26324,"source":25191,"url":26325,"thumbnail":25193},"Category-based Galaxy Image Generation via Diffusion Models","arXiv:2506.16255v1 Announce Type: cross Abstract: Conventional galaxy generation methods rely on semi-analytical models and hydrodynamic simulations, which are highly dependent on physical assumptions and parameter tuning. In contrast, data-driven generative models do not have explicit physical parameters pre-determined, and instead learn them efficiently from observational data, making them alternative solutions to galaxy generation. Among these, diffusion models outperform Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) in quality and diversity. Leveraging physical prior knowledge to these models can further enhance their capabilities. In this work, we present GalCatDiff, the first framework in astronomy to leverage both galaxy image features and astrophysical properties in the network design of diffusion models. GalCatDiff incorporates an enhanced U-Net and a novel block entitled Astro-RAB (Residual Attention Block), which dynamically combines attention mechanisms with convolution operations to ensure global consistency and local feature fidelity. Moreover, GalCatDiff uses category embeddings for class-specific galaxy generation, avoiding the high computational costs of training separate models for each category. Our experimental results demonstrate that GalCatDiff significantly outperforms existing methods in terms of the consistency of sample color and size distributions, and the generated galaxies are both visually realistic and physically consistent. This framework will enhance the reliability of galaxy simulations and can potentially serve as a data augmentor to support future galaxy classification algorithm development.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16255","src/content/posts/2025-06-23-category-based-galaxy-image-generation-via-diffusion-models.md","65430485e43dacc6",{"html":25,"metadata":26329},{"headings":26330,"localImagePaths":26331,"remoteImagePaths":26332,"frontmatter":26333,"imagePaths":26334},[],[],[],{"title":26322,"description":26323,"summary":26323,"pubDate":25202,"source":25191,"url":26325,"thumbnail":25193},[],"2025-06-23-category-based-galaxy-image-generation-via-diffusion-models.md","2025-06-23-cds-knowledge-component-driven-data-synthesis-guided-by-cognitive-diagnosis-theory",{"id":26336,"data":26338,"filePath":26343,"digest":26344,"rendered":26345,"legacyId":26352},{"title":26339,"description":26340,"summary":26340,"pubDate":26341,"source":25191,"url":26342,"thumbnail":25193},"CDS: Knowledge Component-Driven Data Synthesis Guided by Cognitive Diagnosis Theory","arXiv:2501.07674v3 Announce Type: replace Abstract: Large Language Models (LLMs) have achieved significant advancements, but the increasing complexity of tasks and higher performance demands highlight the need for continuous improvement. Some approaches utilize synthetic data generated by advanced LLMs based on evaluation results to train models. However, conventional evaluation methods fail to provide detailed, fine-grained profiles of LLMs, limiting their guidance for data synthesis. In this paper, we introduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a diagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine evaluation results and characterize model profiles at the knowledge component level. Based on these diagnostics, we propose two diagnosis-synthesis strategies for weakness-targeted data synthesis. Additionally, we present an enhanced data augmentation and selection pipeline to improve the quality and diversity of synthesized data. Our experiments with several open-source models show significant improvements across multiple benchmarks, achieving up to 6.00% improvement in code generation, 13.10% in mathematical reasoning, and 5.43% in academic exams. Code and data are available on GitHub.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2501.07674","src/content/posts/2025-06-23-cds-knowledge-component-driven-data-synthesis-guided-by-cognitive-diagnosis-theory.md","197d12e1a88837f0",{"html":25,"metadata":26346},{"headings":26347,"localImagePaths":26348,"remoteImagePaths":26349,"frontmatter":26350,"imagePaths":26351},[],[],[],{"title":26339,"description":26340,"summary":26340,"pubDate":25202,"source":25191,"url":26342,"thumbnail":25193},[],"2025-06-23-cds-knowledge-component-driven-data-synthesis-guided-by-cognitive-diagnosis-theory.md","2025-06-23-capturing-polysemanticity-with-prism-a-multi-concept-feature-description-framework",{"id":26353,"data":26355,"filePath":26360,"digest":26361,"rendered":26362,"legacyId":26369},{"title":26356,"description":26357,"summary":26357,"pubDate":26358,"source":25191,"url":26359,"thumbnail":25193},"Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework","arXiv:2506.15538v2 Announce Type: replace-cross Abstract: Automated interpretability research aims to identify concepts encoded in neural network features to enhance human understanding of model behavior. Current feature description methods face two critical challenges: limited robustness and the flawed assumption that each neuron encodes only a single concept (monosemanticity), despite growing evidence that neurons are often polysemantic. This assumption restricts the expressiveness of feature descriptions and limits their ability to capture the full range of behaviors encoded in model internals. To address this, we introduce Polysemantic FeatuRe Identification and Scoring Method (PRISM), a novel framework that captures the inherent complexity of neural network features. Unlike prior approaches that assign a single description per feature, PRISM provides more nuanced descriptions for both polysemantic and monosemantic features. We apply PRISM to language models and, through extensive benchmarking against existing methods, demonstrate that our approach produces more accurate and faithful feature descriptions, improving both overall description quality (via a description score) and the ability to capture distinct concepts when polysemanticity is present (via a polysemanticity score).",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15538","src/content/posts/2025-06-23-capturing-polysemanticity-with-prism-a-multi-concept-feature-description-framework.md","36750dcf1d4126a5",{"html":25,"metadata":26363},{"headings":26364,"localImagePaths":26365,"remoteImagePaths":26366,"frontmatter":26367,"imagePaths":26368},[],[],[],{"title":26356,"description":26357,"summary":26357,"pubDate":25202,"source":25191,"url":26359,"thumbnail":25193},[],"2025-06-23-capturing-polysemanticity-with-prism-a-multi-concept-feature-description-framework.md","2025-06-23-cellular-traffic-prediction-via-deep-state-space-models-with-attention-mechanism",{"id":26370,"data":26372,"filePath":26377,"digest":26378,"rendered":26379,"legacyId":26386},{"title":26373,"description":26374,"summary":26374,"pubDate":26375,"source":25191,"url":26376,"thumbnail":25193},"Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism","arXiv:2506.15688v1 Announce Type: cross Abstract: Cellular traffic prediction is of great importance for operators to manage network resources and make decisions. Traffic is highly dynamic and influenced by many exogenous factors, which would lead to the degradation of traffic prediction accuracy. This paper proposes an end-to-end framework with two variants to explicitly characterize the spatiotemporal patterns of cellular traffic among neighboring cells. It uses convolutional neural networks with an attention mechanism to capture the spatial dynamics and Kalman filter for temporal modelling. Besides, we can fully exploit the auxiliary information such as social activities to improve prediction performance. We conduct extensive experiments on three real-world datasets. The results show that our proposed models outperform the state-of-the-art machine learning techniques in terms of prediction accuracy.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15688","src/content/posts/2025-06-23-cellular-traffic-prediction-via-deep-state-space-models-with-attention-mechanism.md","ab7ffa133f18267d",{"html":25,"metadata":26380},{"headings":26381,"localImagePaths":26382,"remoteImagePaths":26383,"frontmatter":26384,"imagePaths":26385},[],[],[],{"title":26373,"description":26374,"summary":26374,"pubDate":25202,"source":25191,"url":26376,"thumbnail":25193},[],"2025-06-23-cellular-traffic-prediction-via-deep-state-space-models-with-attention-mechanism.md","2025-06-23-cf-seg-counterfactuals-meet-segmentation",{"id":26387,"data":26389,"filePath":26394,"digest":26395,"rendered":26396,"legacyId":26403},{"title":26390,"description":26391,"summary":26391,"pubDate":26392,"source":25191,"url":26393,"thumbnail":25193},"CF-Seg: Counterfactuals meet Segmentation","arXiv:2506.16213v1 Announce Type: cross Abstract: Segmenting anatomical structures in medical images plays an important role in the quantitative assessment of various diseases. However, accurate segmentation becomes significantly more challenging in the presence of disease. Disease patterns can alter the appearance of surrounding healthy tissues, introduce ambiguous boundaries, or even obscure critical anatomical structures. As such, segmentation models trained on real-world datasets may struggle to provide good anatomical segmentation, leading to potential misdiagnosis. In this paper, we generate counterfactual (CF) images to simulate how the same anatomy would appear in the absence of disease without altering the underlying structure. We then use these CF images to segment structures of interest, without requiring any changes to the underlying segmentation model. Our experiments on two real-world clinical chest X-ray datasets show that the use of counterfactual images improves anatomical segmentation, thereby aiding downstream clinical decision-making.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16213","src/content/posts/2025-06-23-cf-seg-counterfactuals-meet-segmentation.md","ce2055a54a16de10",{"html":25,"metadata":26397},{"headings":26398,"localImagePaths":26399,"remoteImagePaths":26400,"frontmatter":26401,"imagePaths":26402},[],[],[],{"title":26390,"description":26391,"summary":26391,"pubDate":25202,"source":25191,"url":26393,"thumbnail":25193},[],"2025-06-23-cf-seg-counterfactuals-meet-segmentation.md","2025-06-23-chain-of-trust-a-progressive-trust-evaluation-framework-enabled-by-generative-ai",{"id":26404,"data":26406,"filePath":26411,"digest":26412,"rendered":26413,"legacyId":26420},{"title":26407,"description":26408,"summary":26408,"pubDate":26409,"source":25191,"url":26410,"thumbnail":25193},"Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI","arXiv:2506.17130v1 Announce Type: new Abstract: In collaborative systems with complex tasks relying on distributed resources, trust evaluation of potential collaborators has emerged as an effective mechanism for task completion. However, due to the network dynamics and varying information gathering latencies, it is extremely challenging to observe and collect all trust attributes of a collaborating device concurrently for a comprehensive trust assessment. In this paper, a novel progressive trust evaluation framework, namely chain-of-trust, is proposed to make better use of misaligned device attribute data. This framework, designed for effective task completion, divides the trust evaluation process into multiple chained stages based on task decomposition. At each stage, based on the task completion process, the framework only gathers the latest device attribute data relevant to that stage, leading to reduced trust evaluation complexity and overhead. By leveraging advanced in-context learning, few-shot learning, and reasoning capabilities, generative AI is then employed to analyze and interpret the collected data to produce correct evaluation results quickly. Only devices deemed trustworthy at this stage proceed to the next round of trust evaluation. The framework ultimately determines devices that remain trustworthy across all stages. Experimental results demonstrate that the proposed framework achieves high accuracy in trust evaluation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17130","src/content/posts/2025-06-23-chain-of-trust-a-progressive-trust-evaluation-framework-enabled-by-generative-ai.md","cd6e5a255c938b02",{"html":25,"metadata":26414},{"headings":26415,"localImagePaths":26416,"remoteImagePaths":26417,"frontmatter":26418,"imagePaths":26419},[],[],[],{"title":26407,"description":26408,"summary":26408,"pubDate":25202,"source":25191,"url":26410,"thumbnail":25193},[],"2025-06-23-chain-of-trust-a-progressive-trust-evaluation-framework-enabled-by-generative-ai.md","2025-06-23-chatdbg-augmenting-debugging-with-large-language-models",{"id":26421,"data":26423,"filePath":26428,"digest":26429,"rendered":26430,"legacyId":26437},{"title":26424,"description":26425,"summary":26425,"pubDate":26426,"source":25191,"url":26427,"thumbnail":25193},"ChatDBG: Augmenting Debugging with Large Language Models","arXiv:2403.16354v5 Announce Type: replace-cross Abstract: Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like 'why is x null?'. To handle these queries, ChatDBG grants the LLM autonomy to 'take the wheel': it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2403.16354","src/content/posts/2025-06-23-chatdbg-augmenting-debugging-with-large-language-models.md","b57f5539b50ad608",{"html":25,"metadata":26431},{"headings":26432,"localImagePaths":26433,"remoteImagePaths":26434,"frontmatter":26435,"imagePaths":26436},[],[],[],{"title":26424,"description":26425,"summary":26425,"pubDate":25202,"source":25191,"url":26427,"thumbnail":25193},[],"2025-06-23-chatdbg-augmenting-debugging-with-large-language-models.md","2025-06-23-claude-4-モデル登場新機能や他社との比較料金など詳しく解説",{"id":26438,"data":26440,"filePath":26446,"digest":26447,"rendered":26448,"legacyId":26456},{"title":26441,"description":26442,"summary":26442,"pubDate":26443,"source":24252,"url":26444,"thumbnail":26445},"Claude 4 モデル登場！新機能や他社との比較・料金など詳しく解説","\u003Cp>Anthropic 社は2025年5月22日、Claude の最新版「Claude 4」の提供を開始しました。プログラミングや推論をはじめ、AIエージェント構築といったシーンで最高水準の能力を実現しています。また、コーデ [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/claude-4-model-new-features/'>Claude 4 モデル登場！新機能や他社との比較・料金など詳しく解説\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-23T03:33:57.000Z"],"https://aismiley.co.jp/ai_news/claude-4-model-new-features/","https://aismiley.co.jp/wp-content/uploads/2025/06/claude4.png","src/content/posts/2025-06-23-claude-4-モデル登場新機能や他社との比較料金など詳しく解説.md","f9e8b66707c875d4",{"html":25,"metadata":26449},{"headings":26450,"localImagePaths":26451,"remoteImagePaths":26452,"frontmatter":26453,"imagePaths":26455},[],[],[],{"title":26441,"description":26442,"summary":26442,"pubDate":26454,"source":24252,"url":26444,"thumbnail":26445},"Mon, 23 Jun 2025 03:33:57 +0000",[],"2025-06-23-claude-4-モデル登場新機能や他社との比較料金など詳しく解説.md","2025-06-23-clip-mg-guiding-semantic-attention-with-skeletal-pose-features-and-rgb-data-for-micro-gesture-recognition-on-the-imigue-dataset",{"id":26457,"data":26459,"filePath":26464,"digest":26465,"rendered":26466,"legacyId":26473},{"title":26460,"description":26461,"summary":26461,"pubDate":26462,"source":25191,"url":26463,"thumbnail":25193},"CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset","arXiv:2506.16385v1 Announce Type: cross Abstract: Micro-gesture recognition is a challenging task in affective computing due to the subtle, involuntary nature of the gestures and their low movement amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP model tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG integrates human pose (skeleton) information into the CLIP-based recognition pipeline through pose-guided semantic query generation and a gated multi-modal fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These results demonstrate both the potential of our approach and the remaining difficulty in fully adapting vision-language models like CLIP for micro-gesture recognition.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16385","src/content/posts/2025-06-23-clip-mg-guiding-semantic-attention-with-skeletal-pose-features-and-rgb-data-for-micro-gesture-recognition-on-the-imigue-dataset.md","7815f08357f72f5d",{"html":25,"metadata":26467},{"headings":26468,"localImagePaths":26469,"remoteImagePaths":26470,"frontmatter":26471,"imagePaths":26472},[],[],[],{"title":26460,"description":26461,"summary":26461,"pubDate":25202,"source":25191,"url":26463,"thumbnail":25193},[],"2025-06-23-clip-mg-guiding-semantic-attention-with-skeletal-pose-features-and-rgb-data-for-micro-gesture-recognition-on-the-imigue-dataset.md","2025-06-23-compiler-r1-towards-agentic-compiler-auto-tuning-with-reinforcement-learning",{"id":26474,"data":26476,"filePath":26481,"digest":26482,"rendered":26483,"legacyId":26490},{"title":26477,"description":26478,"summary":26478,"pubDate":26479,"source":25191,"url":26480,"thumbnail":25193},"Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning","arXiv:2506.15701v1 Announce Type: cross Abstract: Compiler auto-tuning optimizes pass sequences to improve performance metrics such as Intermediate Representation (IR) instruction count. Although recent advances leveraging Large Language Models (LLMs) have shown promise in automating compiler tuning, two significant challenges still remain: the absence of high-quality reasoning datasets for agents training, and limited effective interactions with the compilation environment. In this work, we introduce Compiler-R1, the first reinforcement learning (RL)-driven framework specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1 features a curated, high-quality reasoning dataset and a novel two-stage end-to-end RL training pipeline, enabling efficient environment exploration and learning through an outcome-based reward. Extensive experiments across seven datasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction count reduction compared to opt -Oz, showcasing the strong potential of RL-trained LLMs for compiler optimization. Our code and datasets are publicly available at https://github.com/Panhaolin2001/Compiler-R1.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15701","src/content/posts/2025-06-23-compiler-r1-towards-agentic-compiler-auto-tuning-with-reinforcement-learning.md","0d8812396afd97e6",{"html":25,"metadata":26484},{"headings":26485,"localImagePaths":26486,"remoteImagePaths":26487,"frontmatter":26488,"imagePaths":26489},[],[],[],{"title":26477,"description":26478,"summary":26478,"pubDate":25202,"source":25191,"url":26480,"thumbnail":25193},[],"2025-06-23-compiler-r1-towards-agentic-compiler-auto-tuning-with-reinforcement-learning.md","2025-06-23-conformal-inference-under-high-dimensional-covariate-shifts-via-likelihood-ratio-regularization",{"id":26491,"data":26493,"filePath":26498,"digest":26499,"rendered":26500,"legacyId":26507},{"title":26494,"description":26495,"summary":26495,"pubDate":26496,"source":25191,"url":26497,"thumbnail":25193},"Conformal Inference under High-Dimensional Covariate Shifts via Likelihood-Ratio Regularization","arXiv:2502.13030v4 Announce Type: replace-cross Abstract: We consider the problem of conformal prediction under covariate shift. Given labeled data from a source domain and unlabeled data from a covariate shifted target domain, we seek to construct prediction sets with valid marginal coverage in the target domain. Most existing methods require estimating the unknown likelihood ratio function, which can be prohibitive for high-dimensional data such as images. To address this challenge, we introduce the likelihood ratio regularized quantile regression (LR-QR) algorithm, which combines the pinball loss with a novel choice of regularization in order to construct a threshold function without directly estimating the unknown likelihood ratio. We show that the LR-QR method has coverage at the desired level in the target domain, up to a small error term that we can control. Our proofs draw on a novel analysis of coverage via stability bounds from learning theory. Our experiments demonstrate that the LR-QR algorithm outperforms existing methods on high-dimensional prediction tasks, including a regression task for the Communities and Crime dataset, an image classification task from the WILDS repository, and an LLM question-answering task on the MMLU benchmark.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2502.13030","src/content/posts/2025-06-23-conformal-inference-under-high-dimensional-covariate-shifts-via-likelihood-ratio-regularization.md","034e06d0856ee969",{"html":25,"metadata":26501},{"headings":26502,"localImagePaths":26503,"remoteImagePaths":26504,"frontmatter":26505,"imagePaths":26506},[],[],[],{"title":26494,"description":26495,"summary":26495,"pubDate":25202,"source":25191,"url":26497,"thumbnail":25193},[],"2025-06-23-conformal-inference-under-high-dimensional-covariate-shifts-via-likelihood-ratio-regularization.md","2025-06-23-consistency-verification-in-ontology-based-process-models-with-parameter-interdependencies",{"id":26508,"data":26510,"filePath":26515,"digest":26516,"rendered":26517,"legacyId":26524},{"title":26511,"description":26512,"summary":26512,"pubDate":26513,"source":25191,"url":26514,"thumbnail":25193},"Consistency Verification in Ontology-Based Process Models with Parameter Interdependencies","arXiv:2506.16087v1 Announce Type: new Abstract: The formalization of process knowledge using ontologies enables consistent modeling of parameter interdependencies in manufacturing. These interdependencies are typically represented as mathematical expressions that define relations between process parameters, supporting tasks such as calculation, validation, and simulation. To support cross-context application and knowledge reuse, such expressions are often defined in a generic form and applied across multiple process contexts. This highlights the necessity of a consistent and semantically coherent model to ensure the correctness of data retrieval and interpretation. Consequently, dedicated mechanisms are required to address key challenges such as selecting context-relevant data, ensuring unit compatibility between variables and data elements, and verifying the completeness of input data required for evaluating mathematical expressions. This paper presents a set of verification mechanisms for a previously developed ontology-based process model that integrates standardized process semantics, data element definitions, and formal mathematical constructs. The approach includes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a unit consistency check based on expected-unit annotations and semantic classification, and (iii) a data completeness check to validate the evaluability of interdependencies. The applicability of the approach is demonstrated with a use case from Resin Transfer Molding (RTM), supporting the development of machine-interpretable and verifiable engineering models.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16087","src/content/posts/2025-06-23-consistency-verification-in-ontology-based-process-models-with-parameter-interdependencies.md","7a2ef5b54d62004c",{"html":25,"metadata":26518},{"headings":26519,"localImagePaths":26520,"remoteImagePaths":26521,"frontmatter":26522,"imagePaths":26523},[],[],[],{"title":26511,"description":26512,"summary":26512,"pubDate":25202,"source":25191,"url":26514,"thumbnail":25193},[],"2025-06-23-consistency-verification-in-ontology-based-process-models-with-parameter-interdependencies.md","2025-06-23-consistent-sampling-and-simulation-molecular-dynamics-with-energy-based-diffusion-models",{"id":26525,"data":26527,"filePath":26532,"digest":26533,"rendered":26534,"legacyId":26541},{"title":26528,"description":26529,"summary":26529,"pubDate":26530,"source":25191,"url":26531,"thumbnail":25193},"Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models","arXiv:2506.17139v1 Announce Type: cross Abstract: Diffusion models have recently gained significant attention due to their effectiveness in various scientific domains, including biochemistry. When trained on equilibrium molecular distributions, diffusion models provide both: a generative procedure to sample equilibrium conformations and associated forces derived from the model's scores. However, using the forces for coarse-grained molecular dynamics simulations uncovers inconsistencies in the samples generated via classical diffusion inference and simulation, despite both originating from the same model. Particularly at the small diffusion timesteps required for simulations, diffusion models fail to satisfy the Fokker-Planck equation, which governs how the score should evolve over time. We interpret this deviation as an indication of the observed inconsistencies and propose an energy-based diffusion model with a Fokker-Planck-derived regularization term enforcing consistency. We demonstrate the effectiveness of our approach on toy systems, alanine dipeptide, and introduce a state-of-the-art transferable Boltzmann emulator for dipeptides that supports simulation and demonstrates enhanced consistency and efficient sampling.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17139","src/content/posts/2025-06-23-consistent-sampling-and-simulation-molecular-dynamics-with-energy-based-diffusion-models.md","b22c0b69c8f7e9d5",{"html":25,"metadata":26535},{"headings":26536,"localImagePaths":26537,"remoteImagePaths":26538,"frontmatter":26539,"imagePaths":26540},[],[],[],{"title":26528,"description":26529,"summary":26529,"pubDate":25202,"source":25191,"url":26531,"thumbnail":25193},[],"2025-06-23-consistent-sampling-and-simulation-molecular-dynamics-with-energy-based-diffusion-models.md","2025-06-23-consumer-friendly-eeg-based-emotion-recognition-system-a-multi-scale-convolutional-neural-network-approach",{"id":26542,"data":26544,"filePath":26549,"digest":26550,"rendered":26551,"legacyId":26558},{"title":26545,"description":26546,"summary":26546,"pubDate":26547,"source":25191,"url":26548,"thumbnail":25193},"Consumer-friendly EEG-based Emotion Recognition System: A Multi-scale Convolutional Neural Network Approach","arXiv:2506.16448v1 Announce Type: cross Abstract: EEG is a non-invasive, safe, and low-risk method to record electrophysiological signals inside the brain. Especially with recent technology developments like dry electrodes, consumer-grade EEG devices, and rapid advances in machine learning, EEG is commonly used as a resource for automatic emotion recognition. With the aim to develop a deep learning model that can perform EEG-based emotion recognition in a real-life context, we propose a novel approach to utilize multi-scale convolutional neural networks to accomplish such tasks. By implementing feature extraction kernels with many ratio coefficients as well as a new type of kernel that learns key information from four separate areas of the brain, our model consistently outperforms the state-of-the-art TSception model in predicting valence, arousal, and dominance scores across many performance evaluation metrics.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16448","src/content/posts/2025-06-23-consumer-friendly-eeg-based-emotion-recognition-system-a-multi-scale-convolutional-neural-network-approach.md","18696a5dcfeb6ed9",{"html":25,"metadata":26552},{"headings":26553,"localImagePaths":26554,"remoteImagePaths":26555,"frontmatter":26556,"imagePaths":26557},[],[],[],{"title":26545,"description":26546,"summary":26546,"pubDate":25202,"source":25191,"url":26548,"thumbnail":25193},[],"2025-06-23-consumer-friendly-eeg-based-emotion-recognition-system-a-multi-scale-convolutional-neural-network-approach.md","2025-06-23-context-matters-relaxing-goals-with-llms-for-feasible-3d-scene-planning",{"id":26559,"data":26561,"filePath":26566,"digest":26567,"rendered":26568,"legacyId":26575},{"title":26562,"description":26563,"summary":26563,"pubDate":26564,"source":25191,"url":26565,"thumbnail":25193},"Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning","arXiv:2506.15828v1 Announce Type: cross Abstract: Classical planning in AI and Robotics addresses complex tasks by shifting from imperative to declarative approaches (e.g., PDDL). However, these methods often fail in real scenarios due to limited robot perception and the need to ground perceptions to planning predicates. This often results in heavily hard-coded behaviors that struggle to adapt, even with scenarios where goals can be achieved through relaxed planning. Meanwhile, Large Language Models (LLMs) lead to planning systems that leverage commonsense reasoning but often at the cost of generating unfeasible and/or unsafe plans. To address these limitations, we present an approach integrating classical planning with LLMs, leveraging their ability to extract commonsense knowledge and ground actions. We propose a hierarchical formulation that enables robots to make unfeasible tasks tractable by defining functionally equivalent goals through gradual relaxation. This mechanism supports partial achievement of the intended objective, suited to the agent's specific context. Our method demonstrates its ability to adapt and execute tasks effectively within environments modeled using 3D Scene Graphs through comprehensive qualitative and quantitative evaluations. We also show how this method succeeds in complex scenarios where other benchmark methods are more likely to fail. Code, dataset, and additional material are released to the community.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15828","src/content/posts/2025-06-23-context-matters-relaxing-goals-with-llms-for-feasible-3d-scene-planning.md","ff5d715b0cc6ffd0",{"html":25,"metadata":26569},{"headings":26570,"localImagePaths":26571,"remoteImagePaths":26572,"frontmatter":26573,"imagePaths":26574},[],[],[],{"title":26562,"description":26563,"summary":26563,"pubDate":25202,"source":25191,"url":26565,"thumbnail":25193},[],"2025-06-23-context-matters-relaxing-goals-with-llms-for-feasible-3d-scene-planning.md","2025-06-23-contextbench-modifying-contexts-for-targeted-latent-activation",{"id":26576,"data":26578,"filePath":26583,"digest":26584,"rendered":26585,"legacyId":26592},{"title":26579,"description":26580,"summary":26580,"pubDate":26581,"source":25191,"url":26582,"thumbnail":25193},"ContextBench: Modifying Contexts for Targeted Latent Activation","arXiv:2506.15735v1 Announce Type: new Abstract: Identifying inputs that trigger specific behaviours or latent features in language models could have a wide range of safety use cases. We investigate a class of methods capable of generating targeted, linguistically fluent inputs that activate specific latent features or elicit model behaviours. We formalise this approach as context modification and present ContextBench -- a benchmark with tasks assessing core method capabilities and potential safety applications. Our evaluation framework measures both elicitation strength (activation of latent features or behaviours) and linguistic fluency, highlighting how current state-of-the-art methods struggle to balance these objectives. We enhance Evolutionary Prompt Optimisation (EPO) with LLM-assistance and diffusion model inpainting, and demonstrate that these variants achieve state-of-the-art performance in balancing elicitation effectiveness and fluency.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15735","src/content/posts/2025-06-23-contextbench-modifying-contexts-for-targeted-latent-activation.md","c465a1c678eb855d",{"html":25,"metadata":26586},{"headings":26587,"localImagePaths":26588,"remoteImagePaths":26589,"frontmatter":26590,"imagePaths":26591},[],[],[],{"title":26579,"description":26580,"summary":26580,"pubDate":25202,"source":25191,"url":26582,"thumbnail":25193},[],"2025-06-23-contextbench-modifying-contexts-for-targeted-latent-activation.md","2025-06-23-continual-learning-with-columnar-spiking-neural-networks",{"id":26593,"data":26595,"filePath":26600,"digest":26601,"rendered":26602,"legacyId":26609},{"title":26596,"description":26597,"summary":26597,"pubDate":26598,"source":25191,"url":26599,"thumbnail":25193},"Continual Learning with Columnar Spiking Neural Networks","arXiv:2506.17169v1 Announce Type: cross Abstract: This study investigates columnar-organized spiking neural networks (SNNs) for continual learning and catastrophic forgetting. Using CoLaNET (Columnar Layered Network), we show that microcolumns adapt most efficiently to new tasks when they lack shared structure with prior learning. We demonstrate how CoLaNET hyperparameters govern the trade-off between retaining old knowledge (stability) and acquiring new information (plasticity). Our optimal configuration learns ten sequential MNIST tasks effectively, maintaining 92% accuracy on each. It shows low forgetting, with only 4% performance degradation on the first task after training on nine subsequent tasks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17169","src/content/posts/2025-06-23-continual-learning-with-columnar-spiking-neural-networks.md","4e301bc791dc135f",{"html":25,"metadata":26603},{"headings":26604,"localImagePaths":26605,"remoteImagePaths":26606,"frontmatter":26607,"imagePaths":26608},[],[],[],{"title":26596,"description":26597,"summary":26597,"pubDate":25202,"source":25191,"url":26599,"thumbnail":25193},[],"2025-06-23-continual-learning-with-columnar-spiking-neural-networks.md","2025-06-23-convergent-linear-representations-of-emergent-misalignment",{"id":26610,"data":26612,"filePath":26617,"digest":26618,"rendered":26619,"legacyId":26626},{"title":26613,"description":26614,"summary":26614,"pubDate":26615,"source":25191,"url":26616,"thumbnail":25193},"Convergent Linear Representations of Emergent Misalignment","arXiv:2506.11618v2 Announce Type: replace-cross Abstract: Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a 'misalignment direction' from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.11618","src/content/posts/2025-06-23-convergent-linear-representations-of-emergent-misalignment.md","64dfbae3e4c31b78",{"html":25,"metadata":26620},{"headings":26621,"localImagePaths":26622,"remoteImagePaths":26623,"frontmatter":26624,"imagePaths":26625},[],[],[],{"title":26613,"description":26614,"summary":26614,"pubDate":25202,"source":25191,"url":26616,"thumbnail":25193},[],"2025-06-23-convergent-linear-representations-of-emergent-misalignment.md","2025-06-23-core-knowledge-deficits-in-multi-modal-language-models",{"id":26627,"data":26629,"filePath":26634,"digest":26635,"rendered":26636,"legacyId":26643},{"title":26630,"description":26631,"summary":26631,"pubDate":26632,"source":25191,"url":26633,"thumbnail":25193},"Core Knowledge Deficits in Multi-Modal Language Models","arXiv:2410.10855v4 Announce Type: replace-cross Abstract: While Multi-modal Large Language Models (MLLMs) demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild remains limited, often falling short on tasks that are intuitive and effortless for humans. We examine the hypothesis that these deficiencies stem from the absence of core knowledge--rudimentary cognitive abilities innate to humans from early childhood. To explore the core knowledge representation in MLLMs, we introduce CoreCognition, a large-scale benchmark encompassing 12 core knowledge concepts grounded in developmental cognitive science. We evaluate 230 models with 11 different prompts, leading to a total of 2,530 data points for analysis. Our experiments uncover four key findings, collectively demonstrating core knowledge deficits in MLLMs: they consistently underperform and show reduced, or even absent, scalability on low-level abilities relative to high-level ones. Finally, we propose Concept Hacking, a novel controlled evaluation method that reveals MLLMs fail to progress toward genuine core knowledge understanding, but instead rely on shortcut learning as they scale.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2410.10855","src/content/posts/2025-06-23-core-knowledge-deficits-in-multi-modal-language-models.md","ae68ecf73fa6cd33",{"html":25,"metadata":26637},{"headings":26638,"localImagePaths":26639,"remoteImagePaths":26640,"frontmatter":26641,"imagePaths":26642},[],[],[],{"title":26630,"description":26631,"summary":26631,"pubDate":25202,"source":25191,"url":26633,"thumbnail":25193},[],"2025-06-23-core-knowledge-deficits-in-multi-modal-language-models.md","2025-06-23-cost-effective-instruction-learning-for-pathology-vision-and-language-analysis",{"id":26644,"data":26646,"filePath":26651,"digest":26652,"rendered":26653,"legacyId":26660},{"title":26647,"description":26648,"summary":26648,"pubDate":26649,"source":25191,"url":26650,"thumbnail":25193},"Cost-effective Instruction Learning for Pathology Vision and Language Analysis","arXiv:2407.17734v2 Announce Type: replace Abstract: The advent of vision-language models fosters the interactive conversations between AI-enabled models and humans. Yet applying these models into clinics must deal with daunting challenges around large-scale training data, financial, and computational resources. Here we propose a cost-effective instruction learning framework for conversational pathology named as CLOVER. CLOVER only trains a lightweight module and uses instruction tuning while freezing the parameters of the large language model. Instead of using costly GPT-4, we propose well-designed prompts on GPT-3.5 for building generation-based instructions, emphasizing the utility of pathological knowledge derived from the Internet source. To augment the use of instructions, we construct a high-quality set of template-based instructions in the context of digital pathology. From two benchmark datasets, our findings reveal the strength of hybrid-form instructions in the visual question-answer in pathology. Extensive results show the cost-effectiveness of CLOVER in answering both open-ended and closed-ended questions, where CLOVER outperforms strong baselines that possess 37 times more training parameters and use instruction data generated from GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot learning in the external clinical dataset. These findings demonstrate that cost-effective modeling of CLOVER could accelerate the adoption of rapid conversational applications in the landscape of digital pathology.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2407.17734","src/content/posts/2025-06-23-cost-effective-instruction-learning-for-pathology-vision-and-language-analysis.md","1d625be574715d31",{"html":25,"metadata":26654},{"headings":26655,"localImagePaths":26656,"remoteImagePaths":26657,"frontmatter":26658,"imagePaths":26659},[],[],[],{"title":26647,"description":26648,"summary":26648,"pubDate":25202,"source":25191,"url":26650,"thumbnail":25193},[],"2025-06-23-cost-effective-instruction-learning-for-pathology-vision-and-language-analysis.md","2025-06-23-contraction-actor-critic-contraction-metric-guided-reinforcement-learning-for-robust-path-tracking",{"id":26661,"data":26663,"filePath":26668,"digest":26669,"rendered":26670,"legacyId":26677},{"title":26664,"description":26665,"summary":26665,"pubDate":26666,"source":25191,"url":26667,"thumbnail":25193},"Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking","arXiv:2506.15700v1 Announce Type: cross Abstract: Control contraction metrics (CCMs) provide a framework to co-synthesize a controller and a corresponding contraction metric -- a positive-definite Riemannian metric under which a closed-loop system is guaranteed to be incrementally exponentially stable. However, the synthesized controller only ensures that all the trajectories of the system converge to one single trajectory and, as such, does not impose any notion of optimality across an entire trajectory. Furthermore, constructing CCMs requires a known dynamics model and non-trivial effort in solving an infinite-dimensional convex feasibility problem, which limits its scalability to complex systems featuring high dimensionality with uncertainty. To address these issues, we propose to integrate CCMs into reinforcement learning (RL), where CCMs provide dynamics-informed feedback for learning control policies that minimize cumulative tracking error under unknown dynamics. We show that our algorithm, called contraction actor-critic (CAC), formally enhances the capability of CCMs to provide a set of contracting policies with the long-term optimality of RL in a fully automated setting. Given a pre-trained dynamics model, CAC simultaneously learns a contraction metric generator (CMG) -- which generates a contraction metric -- and uses an actor-critic algorithm to learn an optimal tracking policy guided by that metric. We demonstrate the effectiveness of our algorithm relative to established baselines through extensive empirical studies, including simulated and real-world robot experiments, and provide a theoretical rationale for incorporating contraction theory into RL.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15700","src/content/posts/2025-06-23-contraction-actor-critic-contraction-metric-guided-reinforcement-learning-for-robust-path-tracking.md","c7b789fa6b204794",{"html":25,"metadata":26671},{"headings":26672,"localImagePaths":26673,"remoteImagePaths":26674,"frontmatter":26675,"imagePaths":26676},[],[],[],{"title":26664,"description":26665,"summary":26665,"pubDate":25202,"source":25191,"url":26667,"thumbnail":25193},[],"2025-06-23-contraction-actor-critic-contraction-metric-guided-reinforcement-learning-for-robust-path-tracking.md","2025-06-23-cp2-leveraging-geometry-for-conformal-prediction-via-canonicalization",{"id":26678,"data":26680,"filePath":26685,"digest":26686,"rendered":26687,"legacyId":26694},{"title":26681,"description":26682,"summary":26682,"pubDate":26683,"source":25191,"url":26684,"thumbnail":25193},"CP$^2$: Leveraging Geometry for Conformal Prediction via Canonicalization","arXiv:2506.16189v1 Announce Type: cross Abstract: We study the problem of conformal prediction (CP) under geometric data shifts, where data samples are susceptible to transformations such as rotations or flips. While CP endows prediction models with post-hoc uncertainty quantification and formal coverage guarantees, their practicality breaks under distribution shifts that deteriorate model performance. To address this issue, we propose integrating geometric information--such as geometric pose--into the conformal procedure to reinstate its guarantees and ensure robustness under geometric shifts. In particular, we explore recent advancements on pose canonicalization as a suitable information extractor for this purpose. Evaluating the combined approach across discrete and continuous shifts and against equivariant and augmentation-based baselines, we find that integrating geometric information with CP yields a principled way to address geometric shifts while maintaining broad applicability to black-box predictors.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16189","src/content/posts/2025-06-23-cp2-leveraging-geometry-for-conformal-prediction-via-canonicalization.md","658f87df206b219e",{"html":25,"metadata":26688},{"headings":26689,"localImagePaths":26690,"remoteImagePaths":26691,"frontmatter":26692,"imagePaths":26693},[],[],[],{"title":26681,"description":26682,"summary":26682,"pubDate":25202,"source":25191,"url":26684,"thumbnail":25193},[],"2025-06-23-cp2-leveraging-geometry-for-conformal-prediction-via-canonicalization.md","2025-06-23-cria-a-cross-view-interaction-and-instance-adapted-pre-training-framework-for-generalizable-eeg-representations",{"id":26695,"data":26697,"filePath":26702,"digest":26703,"rendered":26704,"legacyId":26711},{"title":26698,"description":26699,"summary":26699,"pubDate":26700,"source":25191,"url":26701,"thumbnail":25193},"CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations","arXiv:2506.16056v1 Announce Type: cross Abstract: The difficulty of extracting deep features from EEG data and effectively integrating information from multiple views presents significant challenges for developing a generalizable pretraining framework for EEG representation learning. However, most existing pre-training methods rely solely on the contextual semantics of a single view, failing to capture the complex and synergistic interactions among different perspectives, limiting the expressiveness and generalization of learned representations. To address these issues, this paper proposes CRIA, an adaptive framework that utilizes variable-length and variable-channel coding to achieve a unified representation of EEG data across different datasets. In this work, we define cross-view information as the integrated representation that emerges from the interaction among temporal, spectral, and spatial views of EEG signals. The model employs a cross-attention mechanism to fuse temporal, spectral, and spatial features effectively, and combines an attention matrix masking strategy based on the information bottleneck principle with a novel viewpoint masking pre-training scheme. Experimental results on the Temple University EEG corpus and the CHB-MIT dataset show that CRIA outperforms existing methods with the same pre-training conditions, achieving a balanced accuracy of 57.02% for multi-class event classification and 80.03% for anomaly detection, highlighting its strong generalization ability.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16056","src/content/posts/2025-06-23-cria-a-cross-view-interaction-and-instance-adapted-pre-training-framework-for-generalizable-eeg-representations.md","1e6d8e6a5262e122",{"html":25,"metadata":26705},{"headings":26706,"localImagePaths":26707,"remoteImagePaths":26708,"frontmatter":26709,"imagePaths":26710},[],[],[],{"title":26698,"description":26699,"summary":26699,"pubDate":25202,"source":25191,"url":26701,"thumbnail":25193},[],"2025-06-23-cria-a-cross-view-interaction-and-instance-adapted-pre-training-framework-for-generalizable-eeg-representations.md","2025-06-23-cross-modality-learning-for-predicting-ihc-biomarkers-from-he-stained-whole-slide-images",{"id":26712,"data":26714,"filePath":26719,"digest":26720,"rendered":26721,"legacyId":26728},{"title":26715,"description":26716,"summary":26716,"pubDate":26717,"source":25191,"url":26718,"thumbnail":25193},"Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide Images","arXiv:2506.15853v1 Announce Type: cross Abstract: Hematoxylin and Eosin (H&amp;E) staining is a cornerstone of pathological analysis, offering reliable visualization of cellular morphology and tissue architecture for cancer diagnosis, subtyping, and grading. Immunohistochemistry (IHC) staining provides molecular insights by detecting specific proteins within tissues, enhancing diagnostic accuracy, and improving treatment planning. However, IHC staining is costly, time-consuming, and resource-intensive, requiring specialized expertise. To address these limitations, this study proposes HistoStainAlign, a novel deep learning framework that predicts IHC staining patterns directly from H&amp;E whole-slide images (WSIs) by learning joint representations of morphological and molecular features. The framework integrates paired H&amp;E and IHC embeddings through a contrastive training strategy, capturing complementary features across staining modalities without patch-level annotations or tissue registration. The model was evaluated on gastrointestinal and lung tissue WSIs with three commonly used IHC stains: P53, PD-L1, and Ki-67. HistoStainAlign achieved weighted F1 scores of 0.735 [95% Confidence Interval (CI): 0.670-0.799], 0.830 [95% CI: 0.772-0.886], and 0.723 [95% CI: 0.607-0.836], respectively for these three IHC stains. Embedding analyses demonstrated the robustness of the contrastive alignment in capturing meaningful cross-stain relationships. Comparisons with a baseline model further highlight the advantage of incorporating contrastive learning for improved stain pattern prediction. This study demonstrates the potential of computational approaches to serve as a pre-screening tool, helping prioritize cases for IHC staining and improving workflow efficiency.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15853","src/content/posts/2025-06-23-cross-modality-learning-for-predicting-ihc-biomarkers-from-he-stained-whole-slide-images.md","363a519d693e7b29",{"html":25,"metadata":26722},{"headings":26723,"localImagePaths":26724,"remoteImagePaths":26725,"frontmatter":26726,"imagePaths":26727},[],[],[],{"title":26715,"description":26716,"summary":26716,"pubDate":25202,"source":25191,"url":26718,"thumbnail":25193},[],"2025-06-23-cross-modality-learning-for-predicting-ihc-biomarkers-from-he-stained-whole-slide-images.md","2025-06-23-cryoccd-conditional-cycle-consistent-diffusion-with-biophysical-modeling-for-cryo-em-synthesis",{"id":26729,"data":26731,"filePath":26736,"digest":26737,"rendered":26738,"legacyId":26745},{"title":26732,"description":26733,"summary":26733,"pubDate":26734,"source":25191,"url":26735,"thumbnail":25193},"CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis","arXiv:2505.23444v2 Announce Type: replace-cross Abstract: Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of macromolecules, but developing robust models for downstream analysis is hindered by the scarcity of high-quality annotated data. While synthetic data generation has emerged as a potential solution, existing methods often fail to capture both the structural diversity of biological specimens and the complex, spatially varying noise inherent in cryo-EM imaging. To overcome these limitations, we propose CryoCCD, a synthesis framework that integrates biophysical modeling with generative techniques. Specifically, CryoCCD produces multi-scale cryo-EM micrographs that reflect realistic biophysical variability through compositional heterogeneity, cellular context, and physics-informed imaging. To generate realistic noise, we employ a conditional diffusion model, enhanced by cycle consistency to preserve structural fidelity and mask-aware contrastive learning to capture spatially adaptive noise patterns. Extensive experiments show that CryoCCD generates structurally accurate micrographs and enhances performance in downstream tasks, outperforming state-of-the-art baselines in both particle picking and reconstruction.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.23444","src/content/posts/2025-06-23-cryoccd-conditional-cycle-consistent-diffusion-with-biophysical-modeling-for-cryo-em-synthesis.md","3715781a35daf59a",{"html":25,"metadata":26739},{"headings":26740,"localImagePaths":26741,"remoteImagePaths":26742,"frontmatter":26743,"imagePaths":26744},[],[],[],{"title":26732,"description":26733,"summary":26733,"pubDate":25202,"source":25191,"url":26735,"thumbnail":25193},[],"2025-06-23-cryoccd-conditional-cycle-consistent-diffusion-with-biophysical-modeling-for-cryo-em-synthesis.md","2025-06-23-cyclic-vision-language-manipulator-towards-reliable-and-fine-grained-image-interpretation-for-automated-report-generation",{"id":26746,"data":26748,"filePath":26753,"digest":26754,"rendered":26755,"legacyId":26762},{"title":26749,"description":26750,"summary":26750,"pubDate":26751,"source":25191,"url":26752,"thumbnail":25193},"Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation","arXiv:2411.05261v3 Announce Type: replace-cross Abstract: Despite significant advancements in automated report generation, the opaqueness of text interpretability continues to cast doubt on the reliability of the content produced. This paper introduces a novel approach to identify specific image features in X-ray images that influence the outputs of report generation models. Specifically, we propose Cyclic Vision-Language Manipulator CVLM, a module to generate a manipulated X-ray from an original X-ray and its report from a designated report generator. The essence of CVLM is that cycling manipulated X-rays to the report generator produces altered reports aligned with the alterations pre-injected into the reports for X-ray generation, achieving the term 'cyclic manipulation'. This process allows direct comparison between original and manipulated X-rays, clarifying the critical image features driving changes in reports and enabling model users to assess the reliability of the generated texts. Empirical evaluations demonstrate that CVLM can identify more precise and reliable features compared to existing explanation methods, significantly enhancing the transparency and applicability of AI-generated reports.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2411.05261","src/content/posts/2025-06-23-cyclic-vision-language-manipulator-towards-reliable-and-fine-grained-image-interpretation-for-automated-report-generation.md","42031e8780b8eec0",{"html":25,"metadata":26756},{"headings":26757,"localImagePaths":26758,"remoteImagePaths":26759,"frontmatter":26760,"imagePaths":26761},[],[],[],{"title":26749,"description":26750,"summary":26750,"pubDate":25202,"source":25191,"url":26752,"thumbnail":25193},[],"2025-06-23-cyclic-vision-language-manipulator-towards-reliable-and-fine-grained-image-interpretation-for-automated-report-generation.md","2025-06-23-dadpo-distribution-aware-dpo-for-distilling-conversational-abilities",{"id":26763,"data":26765,"filePath":26770,"digest":26771,"rendered":26772,"legacyId":26779},{"title":26766,"description":26767,"summary":26767,"pubDate":26768,"source":25191,"url":26769,"thumbnail":25193},"daDPO: Distribution-Aware DPO for Distilling Conversational Abilities","arXiv:2506.15717v1 Announce Type: cross Abstract: Large language models (LLMs) have demonstrated exceptional performance across various applications, but their conversational abilities decline sharply as model size decreases, presenting a barrier to their deployment in resource-constrained environments. Knowledge distillation with Direct Preference Optimization (dDPO) has emerged as a promising approach to enhancing the conversational abilities of smaller models using a larger teacher model. However, current methods primarily focus on 'black-box' KD, which only uses the teacher's responses, overlooking the output distribution offered by the teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware DPO), a unified method for preference optimization and distribution-based distillation. We provide rigorous theoretical analysis and empirical validation, showing that daDPO outperforms existing methods in restoring performance for pruned models and enhancing smaller LLM models. Notably, in in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve near-teacher performance (-7.3% preference rate compared to that of dDPO's -31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model (14.0% win rate).",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15717","src/content/posts/2025-06-23-dadpo-distribution-aware-dpo-for-distilling-conversational-abilities.md","ba30564cb3afb600",{"html":25,"metadata":26773},{"headings":26774,"localImagePaths":26775,"remoteImagePaths":26776,"frontmatter":26777,"imagePaths":26778},[],[],[],{"title":26766,"description":26767,"summary":26767,"pubDate":25202,"source":25191,"url":26769,"thumbnail":25193},[],"2025-06-23-dadpo-distribution-aware-dpo-for-distilling-conversational-abilities.md","2025-06-23-deep-reinforcement-learning-xiangqi-player-with-monte-carlo-tree-search",{"id":26780,"data":26782,"filePath":26787,"digest":26788,"rendered":26789,"legacyId":26796},{"title":26783,"description":26784,"summary":26784,"pubDate":26785,"source":25191,"url":26786,"thumbnail":25193},"Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search","arXiv:2506.15880v1 Announce Type: new Abstract: This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi (Chinese Chess) that integrates neural networks with Monte Carlo Tree Search (MCTS) to enable strategic self-play and self-improvement. Addressing the underexplored complexity of Xiangqi, including its unique board layout, piece movement constraints, and victory conditions, our approach combines policy-value networks with MCTS to simulate move consequences and refine decision-making. By overcoming challenges such as Xiangqi's high branching factor and asymmetrical piece dynamics, our work advances AI capabilities in culturally significant strategy games while providing insights for adapting DRL-MCTS frameworks to domain-specific rule systems.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15880","src/content/posts/2025-06-23-deep-reinforcement-learning-xiangqi-player-with-monte-carlo-tree-search.md","9cdffa524eac1d1d",{"html":25,"metadata":26790},{"headings":26791,"localImagePaths":26792,"remoteImagePaths":26793,"frontmatter":26794,"imagePaths":26795},[],[],[],{"title":26783,"description":26784,"summary":26784,"pubDate":25202,"source":25191,"url":26786,"thumbnail":25193},[],"2025-06-23-deep-reinforcement-learning-xiangqi-player-with-monte-carlo-tree-search.md","2025-06-23-decentralized-collective-world-model-for-emergent-communication-and-coordination",{"id":26797,"data":26799,"filePath":26804,"digest":26805,"rendered":26806,"legacyId":26813},{"title":26800,"description":26801,"summary":26801,"pubDate":26802,"source":25191,"url":26803,"thumbnail":25193},"Decentralized Collective World Model for Emergent Communication and Coordination","arXiv:2504.03353v2 Announce Type: replace-cross Abstract: We propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our decentralized approach with constraints preventing direct access to other agents' internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2504.03353","src/content/posts/2025-06-23-decentralized-collective-world-model-for-emergent-communication-and-coordination.md","82e34f0a6664fb7f",{"html":25,"metadata":26807},{"headings":26808,"localImagePaths":26809,"remoteImagePaths":26810,"frontmatter":26811,"imagePaths":26812},[],[],[],{"title":26800,"description":26801,"summary":26801,"pubDate":25202,"source":25191,"url":26803,"thumbnail":25193},[],"2025-06-23-decentralized-collective-world-model-for-emergent-communication-and-coordination.md","2025-06-23-decoupled-classifier-free-guidance-for-counterfactual-diffusion-models",{"id":26814,"data":26816,"filePath":26821,"digest":26822,"rendered":26823,"legacyId":26830},{"title":26817,"description":26818,"summary":26818,"pubDate":26819,"source":25191,"url":26820,"thumbnail":25193},"Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models","arXiv:2506.14399v2 Announce Type: replace-cross Abstract: Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.14399","src/content/posts/2025-06-23-decoupled-classifier-free-guidance-for-counterfactual-diffusion-models.md","de328b620545b372",{"html":25,"metadata":26824},{"headings":26825,"localImagePaths":26826,"remoteImagePaths":26827,"frontmatter":26828,"imagePaths":26829},[],[],[],{"title":26817,"description":26818,"summary":26818,"pubDate":25202,"source":25191,"url":26820,"thumbnail":25193},[],"2025-06-23-decoupled-classifier-free-guidance-for-counterfactual-diffusion-models.md","2025-06-23-deepselective-interpretable-prognosis-prediction-via-feature-selection-and-compression-in-ehr-data",{"id":26831,"data":26833,"filePath":26838,"digest":26839,"rendered":26840,"legacyId":26847},{"title":26834,"description":26835,"summary":26835,"pubDate":26836,"source":25191,"url":26837,"thumbnail":25193},"DeepSelective: Interpretable Prognosis Prediction via Feature Selection and Compression in EHR Data","arXiv:2504.11264v2 Announce Type: replace-cross Abstract: The rapid accumulation of Electronic Health Records (EHRs) has transformed healthcare by providing valuable data that enhance clinical predictions and diagnoses. While conventional machine learning models have proven effective, they often lack robust representation learning and depend heavily on expert-crafted features. Although deep learning offers powerful solutions, it is often criticized for its lack of interpretability. To address these challenges, we propose DeepSelective, a novel end to end deep learning framework for predicting patient prognosis using EHR data, with a strong emphasis on enhancing model interpretability. DeepSelective combines data compression techniques with an innovative feature selection approach, integrating custom-designed modules that work together to improve both accuracy and interpretability. Our experiments demonstrate that DeepSelective not only enhances predictive accuracy but also significantly improves interpretability, making it a valuable tool for clinical decision-making. The source code is freely available at http://www.healthinformaticslab.org/supp/resources.php .",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2504.11264","src/content/posts/2025-06-23-deepselective-interpretable-prognosis-prediction-via-feature-selection-and-compression-in-ehr-data.md","482e8392982769fc",{"html":25,"metadata":26841},{"headings":26842,"localImagePaths":26843,"remoteImagePaths":26844,"frontmatter":26845,"imagePaths":26846},[],[],[],{"title":26834,"description":26835,"summary":26835,"pubDate":25202,"source":25191,"url":26837,"thumbnail":25193},[],"2025-06-23-deepselective-interpretable-prognosis-prediction-via-feature-selection-and-compression-in-ehr-data.md","2025-06-23-digmapper-a-modular-system-for-automated-geologic-map-digitization",{"id":26848,"data":26850,"filePath":26855,"digest":26856,"rendered":26857,"legacyId":26864},{"title":26851,"description":26852,"summary":26852,"pubDate":26853,"source":25191,"url":26854,"thumbnail":25193},"DIGMAPPER: A Modular System for Automated Geologic Map Digitization","arXiv:2506.16006v1 Announce Type: cross Abstract: Historical geologic maps contain rich geospatial information, such as rock units, faults, folds, and bedding planes, that is critical for assessing mineral resources essential to renewable energy, electric vehicles, and national security. However, digitizing maps remains a labor-intensive and time-consuming task. We present DIGMAPPER, a modular, scalable system developed in collaboration with the United States Geological Survey (USGS) to automate the digitization of geologic maps. DIGMAPPER features a fully dockerized, workflow-orchestrated architecture that integrates state-of-the-art deep learning models for map layout analysis, feature extraction, and georeferencing. To overcome challenges such as limited training data and complex visual content, our system employs innovative techniques, including in-context learning with large language models, synthetic data generation, and transformer-based models. Evaluations on over 100 annotated maps from the DARPA-USGS dataset demonstrate high accuracy across polygon, line, and point feature extraction, and reliable georeferencing performance. Deployed at USGS, DIGMAPPER significantly accelerates the creation of analysis-ready geospatial datasets, supporting national-scale critical mineral assessments and broader geoscientific applications.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16006","src/content/posts/2025-06-23-digmapper-a-modular-system-for-automated-geologic-map-digitization.md","ef3ff3cee81ae767",{"html":25,"metadata":26858},{"headings":26859,"localImagePaths":26860,"remoteImagePaths":26861,"frontmatter":26862,"imagePaths":26863},[],[],[],{"title":26851,"description":26852,"summary":26852,"pubDate":25202,"source":25191,"url":26854,"thumbnail":25193},[],"2025-06-23-digmapper-a-modular-system-for-automated-geologic-map-digitization.md","2025-06-23-dispositions-and-roles-of-generically-dependent-entities",{"id":26865,"data":26867,"filePath":26872,"digest":26873,"rendered":26874,"legacyId":26881},{"title":26868,"description":26869,"summary":26869,"pubDate":26870,"source":25191,"url":26871,"thumbnail":25193},"Dispositions and Roles of Generically Dependent Entities","arXiv:2506.17085v1 Announce Type: new Abstract: BFO 2020 does not support functions, dispositions, and roles of generically dependent continuants (like software or datasets). In this paper, we argue that this is a severe limitation, which prevents, for example, the adequate representation of the functions of computer models or the various roles of datasets during the execution of these models. We discuss the aspects of BFO 2020 that prevent the representation of realizable entities of generically dependent continuants. Two approaches to address the issue are presented: (a) the use of defined classes and (b) a proposal of changes that allow BFO to support functions, dispositions, and roles of generically dependent continuants.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17085","src/content/posts/2025-06-23-dispositions-and-roles-of-generically-dependent-entities.md","0ca3e5ce619e4d11",{"html":25,"metadata":26875},{"headings":26876,"localImagePaths":26877,"remoteImagePaths":26878,"frontmatter":26879,"imagePaths":26880},[],[],[],{"title":26868,"description":26869,"summary":26869,"pubDate":25202,"source":25191,"url":26871,"thumbnail":25193},[],"2025-06-23-dispositions-and-roles-of-generically-dependent-entities.md","2025-06-23-dissecting-the-swe-bench-leaderboards-profiling-submitters-and-architectures-of-llm--and-agent-based-repair-systems",{"id":26882,"data":26884,"filePath":26889,"digest":26890,"rendered":26891,"legacyId":26898},{"title":26885,"description":26886,"summary":26886,"pubDate":26887,"source":25191,"url":26888,"thumbnail":25193},"Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems","arXiv:2506.17208v1 Announce Type: cross Abstract: The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench Verified, have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries) leaderboards, analyzing 67 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17208","src/content/posts/2025-06-23-dissecting-the-swe-bench-leaderboards-profiling-submitters-and-architectures-of-llm--and-agent-based-repair-systems.md","d799c52e5f54816e",{"html":25,"metadata":26892},{"headings":26893,"localImagePaths":26894,"remoteImagePaths":26895,"frontmatter":26896,"imagePaths":26897},[],[],[],{"title":26885,"description":26886,"summary":26886,"pubDate":25202,"source":25191,"url":26888,"thumbnail":25193},[],"2025-06-23-dissecting-the-swe-bench-leaderboards-profiling-submitters-and-architectures-of-llm--and-agent-based-repair-systems.md","2025-06-23-distribution-parameter-actor-critic-shifting-the-agent-environment-boundary-for-diverse-action-spaces",{"id":26899,"data":26901,"filePath":26906,"digest":26907,"rendered":26908,"legacyId":26915},{"title":26902,"description":26903,"summary":26903,"pubDate":26904,"source":25191,"url":26905,"thumbnail":25193},"Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces","arXiv:2506.16608v1 Announce Type: cross Abstract: We introduce a novel reinforcement learning (RL) framework that treats distribution parameters as actions, redefining the boundary between agent and environment. This reparameterization makes the new action space continuous, regardless of the original action type (discrete, continuous, mixed, etc.). Under this new parameterization, we develop a generalized deterministic policy gradient estimator, Distribution Parameter Policy Gradient (DPPG), which has lower variance than the gradient in the original action space. Although learning the critic over distribution parameters poses new challenges, we introduce interpolated critic learning (ICL), a simple yet effective strategy to enhance learning, supported by insights from bandit settings. Building on TD3, a strong baseline for continuous control, we propose a practical DPPG-based actor-critic algorithm, Distribution Parameter Actor-Critic (DPAC). Empirically, DPAC outperforms TD3 in MuJoCo continuous control tasks from OpenAI Gym and DeepMind Control Suite, and demonstrates competitive performance on the same environments with discretized action spaces.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16608","src/content/posts/2025-06-23-distribution-parameter-actor-critic-shifting-the-agent-environment-boundary-for-diverse-action-spaces.md","82facdb3af14184b",{"html":25,"metadata":26909},{"headings":26910,"localImagePaths":26911,"remoteImagePaths":26912,"frontmatter":26913,"imagePaths":26914},[],[],[],{"title":26902,"description":26903,"summary":26903,"pubDate":25202,"source":25191,"url":26905,"thumbnail":25193},[],"2025-06-23-distribution-parameter-actor-critic-shifting-the-agent-environment-boundary-for-diverse-action-spaces.md","2025-06-23-do-we-need-large-vlms-for-spotting-soccer-actions",{"id":26916,"data":26918,"filePath":26923,"digest":26924,"rendered":26925,"legacyId":26932},{"title":26919,"description":26920,"summary":26920,"pubDate":26921,"source":25191,"url":26922,"thumbnail":25193},"Do We Need Large VLMs for Spotting Soccer Actions?","arXiv:2506.17144v1 Announce Type: cross Abstract: Traditional video-based tasks like soccer action spotting rely heavily on visual inputs, often requiring complex and computationally expensive models to process dense video data. In this work, we propose a shift from this video-centric approach to a text-based task, making it lightweight and scalable by utilizing Large Language Models (LLMs) instead of Vision-Language Models (VLMs). We posit that expert commentary, which provides rich, fine-grained descriptions and contextual cues such as excitement and tactical insights, contains enough information to reliably spot key actions in a match. To demonstrate this, we use the SoccerNet Echoes dataset, which provides timestamped commentary, and employ a system of three LLMs acting as judges specializing in outcome, excitement, and tactics. Each LLM evaluates sliding windows of commentary to identify actions like goals, cards, and substitutions, generating accurate timestamps for these events. Our experiments show that this language-centric approach performs effectively in detecting critical match events, providing a lightweight and training-free alternative to traditional video-based methods for action spotting.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17144","src/content/posts/2025-06-23-do-we-need-large-vlms-for-spotting-soccer-actions.md","01f8579a6c917dfe",{"html":25,"metadata":26926},{"headings":26927,"localImagePaths":26928,"remoteImagePaths":26929,"frontmatter":26930,"imagePaths":26931},[],[],[],{"title":26919,"description":26920,"summary":26920,"pubDate":25202,"source":25191,"url":26922,"thumbnail":25193},[],"2025-06-23-do-we-need-large-vlms-for-spotting-soccer-actions.md","2025-06-23-do-we-talk-to-robots-like-therapists-and-do-they-respond-accordingly-language-alignment-in-ai-emotional-support",{"id":26933,"data":26935,"filePath":26940,"digest":26941,"rendered":26942,"legacyId":26949},{"title":26936,"description":26937,"summary":26937,"pubDate":26938,"source":25191,"url":26939,"thumbnail":25193},"Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support","arXiv:2506.16473v1 Announce Type: cross Abstract: As conversational agents increasingly engage in emotionally supportive dialogue, it is important to understand how closely their interactions resemble those in traditional therapy settings. This study investigates whether the concerns shared with a robot align with those shared in human-to-human (H2H) therapy sessions, and whether robot responses semantically mirror those of human therapists. We analyzed two datasets: one of interactions between users and professional therapists (Hugging Face's NLP Mental Health Conversations), and another involving supportive conversations with a social robot (QTrobot from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence embeddings and K-means clustering, we assessed cross-agent thematic alignment by applying a distance-based cluster-fitting method that evaluates whether responses from one agent type map to clusters derived from the other, and validated it using Euclidean distances. Results showed that 90.88% of robot conversation disclosures could be mapped to clusters from the human therapy dataset, suggesting shared topical structure. For matched clusters, we compared the subjects as well as therapist and robot responses using Transformer, Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects' disclosures in both datasets, as well as in the responses given to similar human disclosure themes across agent types (robot vs. human therapist). These findings highlight both the parallels and boundaries of robot-led support conversations and their potential for augmenting mental health interventions.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16473","src/content/posts/2025-06-23-do-we-talk-to-robots-like-therapists-and-do-they-respond-accordingly-language-alignment-in-ai-emotional-support.md","db4b6b76c8bc9915",{"html":25,"metadata":26943},{"headings":26944,"localImagePaths":26945,"remoteImagePaths":26946,"frontmatter":26947,"imagePaths":26948},[],[],[],{"title":26936,"description":26937,"summary":26937,"pubDate":25202,"source":25191,"url":26939,"thumbnail":25193},[],"2025-06-23-do-we-talk-to-robots-like-therapists-and-do-they-respond-accordingly-language-alignment-in-ai-emotional-support.md","2025-06-23-double-entendre-robust-audio-based-ai-generated-lyrics-detection-via-multi-view-fusion",{"id":26950,"data":26952,"filePath":26957,"digest":26958,"rendered":26959,"legacyId":26966},{"title":26953,"description":26954,"summary":26954,"pubDate":26955,"source":25191,"url":26956,"thumbnail":25193},"Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion","arXiv:2506.15981v1 Announce Type: cross Abstract: The rapid advancement of AI-based music generation tools is revolutionizing the music industry but also posing challenges to artists, copyright holders, and providers alike. This necessitates reliable methods for detecting such AI-generated content. However, existing detectors, relying on either audio or lyrics, face key practical limitations: audio-based detectors fail to generalize to new or unseen generators and are vulnerable to audio perturbations; lyrics-based methods require cleanly formatted and accurate lyrics, unavailable in practice. To overcome these limitations, we propose a novel, practically grounded approach: a multimodal, modular late-fusion pipeline that combines automatically transcribed sung lyrics and speech features capturing lyrics-related information within the audio. By relying on lyrical aspects directly from audio, our method enhances robustness, mitigates susceptibility to low-level artifacts, and enables practical applicability. Experiments show that our method, DE-detect, outperforms existing lyrics-based detectors while also being more robust to audio perturbations. Thus, it offers an effective, robust solution for detecting AI-generated music in real-world scenarios. Our code is available at https://github.com/deezer/robust-AI-lyrics-detection.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15981","src/content/posts/2025-06-23-double-entendre-robust-audio-based-ai-generated-lyrics-detection-via-multi-view-fusion.md","4f98a2de68eb7cc1",{"html":25,"metadata":26960},{"headings":26961,"localImagePaths":26962,"remoteImagePaths":26963,"frontmatter":26964,"imagePaths":26965},[],[],[],{"title":26953,"description":26954,"summary":26954,"pubDate":25202,"source":25191,"url":26956,"thumbnail":25193},[],"2025-06-23-double-entendre-robust-audio-based-ai-generated-lyrics-detection-via-multi-view-fusion.md","2025-06-23-drag-and-drop-llms-zero-shot-prompt-to-weights",{"id":26967,"data":26969,"filePath":26974,"digest":26975,"rendered":26976,"legacyId":26983},{"title":26970,"description":26971,"summary":26971,"pubDate":26972,"source":25191,"url":26973,"thumbnail":25193},"Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights","arXiv:2506.16406v1 Announce Type: cross Abstract: Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce textbf{Drag-and-Drop LLMs (textit{DnD})}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to textbf{12,000$times$} lower overhead than full fine-tuning, ii) average gains up to textbf{30%} in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization despite never seeing the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs. Our project is available at href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16406","src/content/posts/2025-06-23-drag-and-drop-llms-zero-shot-prompt-to-weights.md","030c67ee3b622ad4",{"html":25,"metadata":26977},{"headings":26978,"localImagePaths":26979,"remoteImagePaths":26980,"frontmatter":26981,"imagePaths":26982},[],[],[],{"title":26970,"description":26971,"summary":26971,"pubDate":25202,"source":25191,"url":26973,"thumbnail":25193},[],"2025-06-23-drag-and-drop-llms-zero-shot-prompt-to-weights.md","2025-06-23-dual-objective-reinforcement-learning-with-novel-hamilton-jacobi-bellman-formulations",{"id":26984,"data":26986,"filePath":26991,"digest":26992,"rendered":26993,"legacyId":27000},{"title":26987,"description":26988,"summary":26988,"pubDate":26989,"source":25191,"url":26990,"thumbnail":25193},"Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations","arXiv:2506.16016v1 Announce Type: new Abstract: Hard constraints in reinforcement learning (RL), whether imposed via the reward function or the model architecture, often degrade policy performance. Lagrangian methods offer a way to blend objectives with constraints, but often require intricate reward engineering and parameter tuning. In this work, we extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to propose two novel value functions for dual-objective satisfaction. Namely, we address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds of two distinct rewards. In contrast with temporal logic approaches, which typically involve representing an automaton, we derive explicit, tractable Bellman forms in this context by decomposing our problem into reach, avoid, and reach-avoid problems, as to leverage these aforementioned recent advances. From a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are complementary and fundamentally different from standard sum-of-rewards problems and temporal logic problems, providing a new perspective on constrained decision-making. We leverage our analysis to propose a variation of Proximal Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of tasks for safe-arrival and multi-target achievement, we demonstrate that DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches and out-competes a number of baselines in various metrics.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16016","src/content/posts/2025-06-23-dual-objective-reinforcement-learning-with-novel-hamilton-jacobi-bellman-formulations.md","8bd931f68cb1eef2",{"html":25,"metadata":26994},{"headings":26995,"localImagePaths":26996,"remoteImagePaths":26997,"frontmatter":26998,"imagePaths":26999},[],[],[],{"title":26987,"description":26988,"summary":26988,"pubDate":25202,"source":25191,"url":26990,"thumbnail":25193},[],"2025-06-23-dual-objective-reinforcement-learning-with-novel-hamilton-jacobi-bellman-formulations.md","2025-06-23-dual-thinking-and-logical-processing----are-multi-modal-large-language-models-closing-the-gap-with-human-vision",{"id":27001,"data":27003,"filePath":27008,"digest":27009,"rendered":27010,"legacyId":27017},{"title":27004,"description":27005,"summary":27005,"pubDate":27006,"source":25191,"url":27007,"thumbnail":25193},"Dual Thinking and Logical Processing -- Are Multi-modal Large Language Models Closing the Gap with Human Vision ?","arXiv:2406.06967v4 Announce Type: replace-cross Abstract: The dual thinking framework considers fast, intuitive, and slower logical processing. The perception of dual thinking in vision requires images where inferences from intuitive and logical processing differ, and the latter is under-explored in current studies. We introduce a novel adversarial dataset to provide evidence for the dual thinking framework in human vision, which also facilitates the study of the qualitative behavior of deep learning models. Our psychophysical studies show the presence of multiple inferences in rapid succession, and analysis of errors shows that the early stopping of visual processing can result in missing relevant information. MLLMs (Multi-modal Large Language Models) and VLMs (Vision Language Models) have made significant progress in correcting errors in intuitive processing in human vision and showed enhanced performance on images requiring logical processing. However, their improvements in logical processing have not kept pace with their advancements in intuitive processing. In contrast, segmentation models exhibit errors similar to those seen in intuitive human processing and lack understanding of sub-structures, as indicated by errors related to sub-components in identified instances. As AI (Artificial Intelligence)-based systems find increasing applications in safety-critical domains like autonomous driving, the integration of logical processing capabilities becomes essential. This not only enhances performance but also addresses the limitations of scaling-based approaches while ensuring robustness and reliability in real-world environments.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2406.06967","src/content/posts/2025-06-23-dual-thinking-and-logical-processing----are-multi-modal-large-language-models-closing-the-gap-with-human-vision.md","5ba63d2f674ea0dc",{"html":25,"metadata":27011},{"headings":27012,"localImagePaths":27013,"remoteImagePaths":27014,"frontmatter":27015,"imagePaths":27016},[],[],[],{"title":27004,"description":27005,"summary":27005,"pubDate":25202,"source":25191,"url":27007,"thumbnail":25193},[],"2025-06-23-dual-thinking-and-logical-processing----are-multi-modal-large-language-models-closing-the-gap-with-human-vision.md","2025-06-23-dxの本質は技術導入にあらず-インドネシア小売大手に学ぶ顧客データの徹底活用",{"id":27018,"data":27020,"filePath":27026,"digest":27027,"rendered":27028,"legacyId":27036},{"title":27021,"description":27022,"summary":27022,"pubDate":27023,"source":24944,"url":27024,"thumbnail":27025},"DXの本質は技術導入にあらず　インドネシア小売大手に学ぶ、顧客データの徹底活用","インドネシアの大手小売業アルファマートは、25年間で1万2000店舗から2万2000店舗へと拡大し、国内認知度98%を誇る。同社の戦略とビジネスモデルから、日本企業が学ぶべきこととは。",["Date","2025-06-22T22:00:00.000Z"],"https://www.itmedia.co.jp/business/articles/2506/23/news010.html","https://image.itmedia.co.jp/business/articles/2506/23/cover_news010.jpg","src/content/posts/2025-06-23-dxの本質は技術導入にあらず-インドネシア小売大手に学ぶ顧客データの徹底活用.md","8f32c9399fa8d944",{"html":25,"metadata":27029},{"headings":27030,"localImagePaths":27031,"remoteImagePaths":27032,"frontmatter":27033,"imagePaths":27035},[],[],[],{"title":27021,"description":27022,"summary":27022,"pubDate":27034,"source":24944,"url":27024,"thumbnail":27025},"Mon, 23 Jun 2025 07:00:00 +0900",[],"2025-06-23-dxの本質は技術導入にあらず-インドネシア小売大手に学ぶ顧客データの徹底活用.md","2025-06-23-dynamic-knowledge-integration-for-evidence-driven-counter-argument-generation-with-large-language-models",{"id":27037,"data":27039,"filePath":27044,"digest":27045,"rendered":27046,"legacyId":27053},{"title":27040,"description":27041,"summary":27041,"pubDate":27042,"source":25191,"url":27043,"thumbnail":25193},"Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models","arXiv:2503.05328v2 Announce Type: replace-cross Abstract: This paper investigates the role of dynamic external knowledge integration in improving counter-argument generation using Large Language Models (LLMs). While LLMs have shown promise in argumentative tasks, their tendency to generate lengthy, potentially unfactual responses highlights the need for more controlled and evidence-based approaches. We introduce a new manually curated dataset of argument and counter-argument pairs specifically designed to balance argumentative complexity with evaluative feasibility. We also propose a new LLM-as-a-Judge evaluation methodology that shows a stronger correlation with human judgments compared to traditional reference-based metrics. Our experimental results demonstrate that integrating dynamic external knowledge from the web significantly improves the quality of generated counter-arguments, particularly in terms of relatedness, persuasiveness, and factuality. The findings suggest that combining LLMs with real-time external knowledge retrieval offers a promising direction for developing more effective and reliable counter-argumentation systems.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2503.05328","src/content/posts/2025-06-23-dynamic-knowledge-integration-for-evidence-driven-counter-argument-generation-with-large-language-models.md","96ed14fd3370740a",{"html":25,"metadata":27047},{"headings":27048,"localImagePaths":27049,"remoteImagePaths":27050,"frontmatter":27051,"imagePaths":27052},[],[],[],{"title":27040,"description":27041,"summary":27041,"pubDate":25202,"source":25191,"url":27043,"thumbnail":25193},[],"2025-06-23-dynamic-knowledge-integration-for-evidence-driven-counter-argument-generation-with-large-language-models.md","2025-06-23-dynamic-risk-assessments-for-offensive-cybersecurity-agents",{"id":27054,"data":27056,"filePath":27061,"digest":27062,"rendered":27063,"legacyId":27070},{"title":27057,"description":27058,"summary":27058,"pubDate":27059,"source":25191,"url":27060,"thumbnail":25193},"Dynamic Risk Assessments for Offensive Cybersecurity Agents","arXiv:2505.18384v2 Announce Type: replace-cross Abstract: Foundation models are increasingly becoming better autonomous programmers, raising the prospect that they could also automate dangerous offensive cyber-operations. Current frontier model audits probe the cybersecurity risks of such agents, but most fail to account for the degrees of freedom available to adversaries in the real world. In particular, with strong verifiers and financial incentives, agents for offensive cybersecurity are amenable to iterative improvement by would-be adversaries. We argue that assessments should take into account an expanded threat model in the context of cybersecurity, emphasizing the varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget. We show that even with a relatively small compute budget (8 H100 GPU Hours in our study), adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40% relative to the baseline -- without any external assistance. These results highlight the need to evaluate agents' cybersecurity risk in a dynamic manner, painting a more representative picture of risk.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.18384","src/content/posts/2025-06-23-dynamic-risk-assessments-for-offensive-cybersecurity-agents.md","28f9a52abd803505",{"html":25,"metadata":27064},{"headings":27065,"localImagePaths":27066,"remoteImagePaths":27067,"frontmatter":27068,"imagePaths":27069},[],[],[],{"title":27057,"description":27058,"summary":27058,"pubDate":25202,"source":25191,"url":27060,"thumbnail":25193},[],"2025-06-23-dynamic-risk-assessments-for-offensive-cybersecurity-agents.md","2025-06-23-dynscaling-efficient-verifier-free-inference-scaling-via-dynamic-and-integrated-sampling",{"id":27071,"data":27073,"filePath":27078,"digest":27079,"rendered":27080,"legacyId":27087},{"title":27074,"description":27075,"summary":27075,"pubDate":27076,"source":25191,"url":27077,"thumbnail":25193},"DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling","arXiv:2506.16043v1 Announce Type: cross Abstract: Inference-time scaling has proven effective in boosting large language model (LLM) performance through increased test-time computation. Yet, its practical application is often hindered by reliance on external verifiers or a lack of optimization for realistic computational constraints. We propose DynScaling, which addresses these limitations through two primary innovations: an integrated parallel-sequential sampling strategy and a bandit-based dynamic budget allocation framework. The integrated sampling strategy unifies parallel and sequential sampling by constructing synthetic sequential reasoning chains from initially independent parallel responses, promoting diverse and coherent reasoning trajectories. The dynamic budget allocation framework formulates the allocation of computational resources as a multi-armed bandit problem, adaptively distributing the inference budget across queries based on the uncertainty of previously sampled responses, thereby maximizing computational efficiency. By combining these components, DynScaling effectively improves LLM performance under practical resource constraints without the need for external verifiers. Experimental results demonstrate that DynScaling consistently surpasses existing verifier-free inference scaling baselines in both task performance and computational cost.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16043","src/content/posts/2025-06-23-dynscaling-efficient-verifier-free-inference-scaling-via-dynamic-and-integrated-sampling.md","9287a242f9f12851",{"html":25,"metadata":27081},{"headings":27082,"localImagePaths":27083,"remoteImagePaths":27084,"frontmatter":27085,"imagePaths":27086},[],[],[],{"title":27074,"description":27075,"summary":27075,"pubDate":25202,"source":25191,"url":27077,"thumbnail":25193},[],"2025-06-23-dynscaling-efficient-verifier-free-inference-scaling-via-dynamic-and-integrated-sampling.md","2025-06-23-each-rank-could-be-an-expert-single-ranked-mixture-of-experts-lora-for-multi-task-learning",{"id":27088,"data":27090,"filePath":27095,"digest":27096,"rendered":27097,"legacyId":27104},{"title":27091,"description":27092,"summary":27092,"pubDate":27093,"source":25191,"url":27094,"thumbnail":25193},"Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning","arXiv:2501.15103v2 Announce Type: replace-cross Abstract: Low-Rank Adaptation (LoRA) is widely used for adapting large language models (LLMs) to specific domains due to its efficiency and modularity. Meanwhile, vanilla LoRA struggles with task conflicts in multi-task scenarios. Recent works adopt Mixture of Experts (MoE) by treating each LoRA module as an expert, thereby mitigating task interference through multiple specialized LoRA modules. While effective, these methods often isolate knowledge within individual tasks, failing to fully exploit the shared knowledge across related tasks. In this paper, we establish a connection between single LoRA and multi-LoRA MoE, integrating them into a unified framework. We demonstrate that the dynamic routing of multiple LoRAs is functionally equivalent to rank partitioning and block-level activation within a single LoRA. We further empirically demonstrate that finer-grained LoRA partitioning, within the same total and activated parameter constraints, leads to better performance gains across heterogeneous tasks. Building on these findings, we propose Single-ranked Mixture of Experts LoRA (textbf{SMoRA}), which embeds MoE into LoRA by textit{treating each rank as an independent expert}. With a textit{dynamic rank-wise activation} mechanism, SMoRA promotes finer-grained knowledge sharing while mitigating task conflicts. Experiments demonstrate that SMoRA activates fewer parameters yet achieves better performance in multi-task scenarios.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2501.15103","src/content/posts/2025-06-23-each-rank-could-be-an-expert-single-ranked-mixture-of-experts-lora-for-multi-task-learning.md","246d499f9ebe963c",{"html":25,"metadata":27098},{"headings":27099,"localImagePaths":27100,"remoteImagePaths":27101,"frontmatter":27102,"imagePaths":27103},[],[],[],{"title":27091,"description":27092,"summary":27092,"pubDate":25202,"source":25191,"url":27094,"thumbnail":25193},[],"2025-06-23-each-rank-could-be-an-expert-single-ranked-mixture-of-experts-lora-for-multi-task-learning.md","2025-06-23-eau-de-q-network-adaptive-distillation-of-neural-networks-in-deep-reinforcement-learning",{"id":27105,"data":27107,"filePath":27112,"digest":27113,"rendered":27114,"legacyId":27121},{"title":27108,"description":27109,"summary":27109,"pubDate":27110,"source":25191,"url":27111,"thumbnail":25193},"Eau De $Q$-Network: Adaptive Distillation of Neural Networks in Deep Reinforcement Learning","arXiv:2503.01437v2 Announce Type: replace-cross Abstract: Recent works have successfully demonstrated that sparse deep reinforcement learning agents can be competitive against their dense counterparts. This opens up opportunities for reinforcement learning applications in fields where inference time and memory requirements are cost-sensitive or limited by hardware. Until now, dense-to-sparse methods have relied on hand-designed sparsity schedules that are not synchronized with the agent's learning pace. Crucially, the final sparsity level is chosen as a hyperparameter, which requires careful tuning as setting it too high might lead to poor performances. In this work, we address these shortcomings by crafting a dense-to-sparse algorithm that we name Eau De $Q$-Network (EauDeQN). To increase sparsity at the agent's learning pace, we consider multiple online networks with different sparsity levels, where each online network is trained from a shared target network. At each target update, the online network with the smallest loss is chosen as the next target network, while the other networks are replaced by a pruned version of the chosen network. We evaluate the proposed approach on the Atari $2600$ benchmark and the MuJoCo physics simulator, showing that EauDeQN reaches high sparsity levels while keeping performances high.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2503.01437","src/content/posts/2025-06-23-eau-de-q-network-adaptive-distillation-of-neural-networks-in-deep-reinforcement-learning.md","5dc68d3318e94fba",{"html":25,"metadata":27115},{"headings":27116,"localImagePaths":27117,"remoteImagePaths":27118,"frontmatter":27119,"imagePaths":27120},[],[],[],{"title":27108,"description":27109,"summary":27109,"pubDate":25202,"source":25191,"url":27111,"thumbnail":25193},[],"2025-06-23-eau-de-q-network-adaptive-distillation-of-neural-networks-in-deep-reinforcement-learning.md","2025-06-23-efficient-and-flexible-neural-network-training-through-layer-wise-feedback-propagation",{"id":27122,"data":27124,"filePath":27129,"digest":27130,"rendered":27131,"legacyId":27138},{"title":27125,"description":27126,"summary":27126,"pubDate":27127,"source":25191,"url":27128,"thumbnail":25193},"Efficient and Flexible Neural Network Training through Layer-wise Feedback Propagation","arXiv:2308.12053v3 Announce Type: replace-cross Abstract: Gradient-based optimization has been a cornerstone of machine learning that enabled the vast advances of Artificial Intelligence (AI) development over the past decades. However, this type of optimization requires differentiation, and with recent evidence of the benefits of non-differentiable (e.g. neuromorphic) architectures over classical models w.r.t. efficiency, such constraints can become limiting in the future. We present Layer-wise Feedback Propagation (LFP), a novel training principle for neural network-like predictors that utilizes methods from the domain of explainability to decompose a reward to individual neurons based on their respective contributions. Leveraging these neuron-wise rewards, our method then implements a greedy approach reinforcing helpful parts of the network and weakening harmful ones. While having comparable computational complexity to gradient descent, LFP does not require gradient computation and generates sparse and thereby memory- and energy-efficient parameter updates and models. We establish the convergence of LFP theoretically and empirically, demonstrating its effectiveness on various models and datasets. Via two applications - neural network pruning and the approximation-free training of Spiking Neural Networks (SNNs) - we demonstrate that LFP combines increased efficiency in terms of computation and representation with flexibility w.r.t. choice of model architecture and objective function. Our code is available at https://github.com/leanderweber/layerwise-feedback-propagation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2308.12053","src/content/posts/2025-06-23-efficient-and-flexible-neural-network-training-through-layer-wise-feedback-propagation.md","906711a516332a25",{"html":25,"metadata":27132},{"headings":27133,"localImagePaths":27134,"remoteImagePaths":27135,"frontmatter":27136,"imagePaths":27137},[],[],[],{"title":27125,"description":27126,"summary":27126,"pubDate":25202,"source":25191,"url":27128,"thumbnail":25193},[],"2025-06-23-efficient-and-flexible-neural-network-training-through-layer-wise-feedback-propagation.md","2025-06-23-efficient-but-vulnerable-benchmarking-and-defending-llm-batch-prompting-attack",{"id":27139,"data":27141,"filePath":27146,"digest":27147,"rendered":27148,"legacyId":27155},{"title":27142,"description":27143,"summary":27143,"pubDate":27144,"source":25191,"url":27145,"thumbnail":25193},"Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack","arXiv:2503.15551v2 Announce Type: replace-cross Abstract: Batch prompting, which combines a batch of multiple queries sharing the same context in one inference, has emerged as a promising solution to reduce inference costs. However, our study reveals a significant security vulnerability in batch prompting: malicious users can inject attack instructions into a batch, leading to unwanted interference across all queries, which can result in the inclusion of harmful content, such as phishing links, or the disruption of logical reasoning. In this paper, we construct BATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of two types and 8k batch instances, to study the batch prompting vulnerability systematically. Our evaluation of both closed-source and open-weight LLMs demonstrates that all LLMs are susceptible to batch-prompting attacks. We then explore multiple defending approaches. While the prompting-based defense shows limited effectiveness for smaller LLMs, the probing-based approach achieves about 95% accuracy in detecting attacks. Additionally, we perform a mechanistic analysis to understand the attack and identify attention heads that are responsible for it.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2503.15551","src/content/posts/2025-06-23-efficient-but-vulnerable-benchmarking-and-defending-llm-batch-prompting-attack.md","2f081075e7bd3363",{"html":25,"metadata":27149},{"headings":27150,"localImagePaths":27151,"remoteImagePaths":27152,"frontmatter":27153,"imagePaths":27154},[],[],[],{"title":27142,"description":27143,"summary":27143,"pubDate":25202,"source":25191,"url":27145,"thumbnail":25193},[],"2025-06-23-efficient-but-vulnerable-benchmarking-and-defending-llm-batch-prompting-attack.md","2025-06-23-efficient-event-based-object-detection-a-hybrid-neural-network-with-spatial-and-temporal-attention",{"id":27156,"data":27158,"filePath":27163,"digest":27164,"rendered":27165,"legacyId":27172},{"title":27159,"description":27160,"summary":27160,"pubDate":27161,"source":25191,"url":27162,"thumbnail":25193},"Efficient Event-Based Object Detection: A Hybrid Neural Network with Spatial and Temporal Attention","arXiv:2403.10173v4 Announce Type: replace-cross Abstract: Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for robust object detection. While Spiking Neural Networks (SNNs) on neuromorphic hardware are often considered for energy-efficient and low latency event-based data processing, they often fall short of Artificial Neural Networks (ANNs) in accuracy and flexibility. Here, we introduce Attention-based Hybrid SNN-ANN backbones for event-based object detection to leverage the strengths of both SNN and ANN architectures. A novel Attention-based SNN-ANN bridge module captures sparse spatial and temporal relations from the SNN layer and converts them into dense feature maps for the ANN part of the backbone. Additionally, we present a variant that integrates DWConvL-STMs to the ANN blocks to capture slower dynamics. This multi-timescale network combines fast SNN processing for short timesteps with long-term dense RNN processing, effectively capturing both fast and slow dynamics. Experimental results demonstrate that our proposed method surpasses SNN-based approaches by significant margins, with results comparable to existing ANN and RNN-based methods. Unlike ANN-only networks, the hybrid setup allows us to implement the SNN blocks on digital neuromorphic hardware to investigate the feasibility of our approach. Extensive ablation studies and implementation on neuromorphic hardware confirm the effectiveness of our proposed modules and architectural choices. Our hybrid SNN-ANN architectures pave the way for ANN-like performance at a drastically reduced parameter, latency, and power budget.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2403.10173","src/content/posts/2025-06-23-efficient-event-based-object-detection-a-hybrid-neural-network-with-spatial-and-temporal-attention.md","eb3ea6519f1db5a3",{"html":25,"metadata":27166},{"headings":27167,"localImagePaths":27168,"remoteImagePaths":27169,"frontmatter":27170,"imagePaths":27171},[],[],[],{"title":27159,"description":27160,"summary":27160,"pubDate":25202,"source":25191,"url":27162,"thumbnail":25193},[],"2025-06-23-efficient-event-based-object-detection-a-hybrid-neural-network-with-spatial-and-temporal-attention.md","2025-06-23-efficient-mixture-of-expert-for-video-based-driver-state-and-physiological-multi-task-estimation-in-conditional-autonomous-driving",{"id":27173,"data":27175,"filePath":27180,"digest":27181,"rendered":27182,"legacyId":27189},{"title":27176,"description":27177,"summary":27177,"pubDate":27178,"source":25191,"url":27179,"thumbnail":25193},"Efficient Mixture-of-Expert for Video-based Driver State and Physiological Multi-task Estimation in Conditional Autonomous Driving","arXiv:2410.21086v2 Announce Type: replace-cross Abstract: Road safety remains a critical challenge worldwide, with approximately 1.35 million fatalities annually attributed to traffic accidents, often due to human errors. As we advance towards higher levels of vehicle automation, challenges still exist, as driving with automation can cognitively over-demand drivers if they engage in non-driving-related tasks (NDRTs), or lead to drowsiness if driving was the sole task. This calls for the urgent need for an effective Driver Monitoring System (DMS) that can evaluate cognitive load and drowsiness in SAE Level-2/3 autonomous driving contexts. In this study, we propose a novel multi-task DMS, termed VDMoE, which leverages RGB video input to monitor driver states non-invasively. By utilizing key facial features to minimize computational load and integrating remote Photoplethysmography (rPPG) for physiological insights, our approach enhances detection accuracy while maintaining efficiency. Additionally, we optimize the Mixture-of-Experts (MoE) framework to accommodate multi-modal inputs and improve performance across different tasks. A novel prior-inclusive regularization method is introduced to align model outputs with statistical priors, thus accelerating convergence and mitigating overfitting risks. We validate our method with the creation of a new dataset (MCDD), which comprises RGB video and physiological indicators from 42 participants, and two public datasets. Our findings demonstrate the effectiveness of VDMoE in monitoring driver states, contributing to safer autonomous driving systems. The code and data will be released.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2410.21086","src/content/posts/2025-06-23-efficient-mixture-of-expert-for-video-based-driver-state-and-physiological-multi-task-estimation-in-conditional-autonomous-driving.md","a3da6d45a4cf0672",{"html":25,"metadata":27183},{"headings":27184,"localImagePaths":27185,"remoteImagePaths":27186,"frontmatter":27187,"imagePaths":27188},[],[],[],{"title":27176,"description":27177,"summary":27177,"pubDate":25202,"source":25191,"url":27179,"thumbnail":25193},[],"2025-06-23-efficient-mixture-of-expert-for-video-based-driver-state-and-physiological-multi-task-estimation-in-conditional-autonomous-driving.md","2025-06-23-efficient-retail-video-annotation-a-robust-key-frame-generation-approach-for-product-and-customer-interaction-analysis",{"id":27190,"data":27192,"filePath":27197,"digest":27198,"rendered":27199,"legacyId":27206},{"title":27193,"description":27194,"summary":27194,"pubDate":27195,"source":25191,"url":27196,"thumbnail":25193},"Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis","arXiv:2506.14854v2 Announce Type: replace-cross Abstract: Accurate video annotation plays a vital role in modern retail applications, including customer behavior analysis, product interaction detection, and in-store activity recognition. However, conventional annotation methods heavily rely on time-consuming manual labeling by human annotators, introducing non-robust frame selection and increasing operational costs. To address these challenges in the retail domain, we propose a deep learning-based approach that automates key-frame identification in retail videos and provides automatic annotations of products and customers. Our method leverages deep neural networks to learn discriminative features by embedding video frames and incorporating object detection-based techniques tailored for retail environments. Experimental results showcase the superiority of our approach over traditional methods, achieving accuracy comparable to human annotator labeling while enhancing the overall efficiency of retail video annotation. Remarkably, our approach leads to an average of 2 times cost savings in video annotation. By allowing human annotators to verify/adjust less than 5% of detected frames in the video dataset, while automating the annotation process for the remaining frames without reducing annotation quality, retailers can significantly reduce operational costs. The automation of key-frame detection enables substantial time and effort savings in retail video labeling tasks, proving highly valuable for diverse retail applications such as shopper journey analysis, product interaction detection, and in-store security monitoring.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.14854","src/content/posts/2025-06-23-efficient-retail-video-annotation-a-robust-key-frame-generation-approach-for-product-and-customer-interaction-analysis.md","b8b746b37627c0d9",{"html":25,"metadata":27200},{"headings":27201,"localImagePaths":27202,"remoteImagePaths":27203,"frontmatter":27204,"imagePaths":27205},[],[],[],{"title":27193,"description":27194,"summary":27194,"pubDate":25202,"source":25191,"url":27196,"thumbnail":25193},[],"2025-06-23-efficient-retail-video-annotation-a-robust-key-frame-generation-approach-for-product-and-customer-interaction-analysis.md","2025-06-23-elevating-styled-mahjong-agents-with-learning-from-demonstration",{"id":27207,"data":27209,"filePath":27214,"digest":27215,"rendered":27216,"legacyId":27223},{"title":27210,"description":27211,"summary":27211,"pubDate":27212,"source":25191,"url":27213,"thumbnail":25193},"Elevating Styled Mahjong Agents with Learning from Demonstration","arXiv:2506.16995v1 Announce Type: new Abstract: A wide variety of bots in games enriches the gameplay experience and enhances replayability. Recent advancements in game artificial intelligence have predominantly focused on improving the proficiency of bots. Nevertheless, developing highly competent bots with a wide range of distinct play styles remains a relatively under-explored area. We select the Mahjong game environment as a case study. The high degree of randomness inherent in the Mahjong game and the prevalence of out-of-distribution states lead to suboptimal performance of existing offline learning and Learning-from-Demonstration (LfD) algorithms. In this paper, we leverage the gameplay histories of existing Mahjong agents and put forward a novel LfD algorithm that necessitates only minimal modifications to the Proximal Policy Optimization algorithm. The comprehensive empirical results illustrate that our proposed method not only significantly enhances the proficiency of the agents but also effectively preserves their unique play styles.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16995","src/content/posts/2025-06-23-elevating-styled-mahjong-agents-with-learning-from-demonstration.md","b50cd0f9612d86da",{"html":25,"metadata":27217},{"headings":27218,"localImagePaths":27219,"remoteImagePaths":27220,"frontmatter":27221,"imagePaths":27222},[],[],[],{"title":27210,"description":27211,"summary":27211,"pubDate":25202,"source":25191,"url":27213,"thumbnail":25193},[],"2025-06-23-elevating-styled-mahjong-agents-with-learning-from-demonstration.md","2025-06-23-efficient-transformations-in-deep-learning-convolutional-neural-networks",{"id":27224,"data":27226,"filePath":27231,"digest":27232,"rendered":27233,"legacyId":27240},{"title":27227,"description":27228,"summary":27228,"pubDate":27229,"source":25191,"url":27230,"thumbnail":25193},"Efficient Transformations in Deep Learning Convolutional Neural Networks","arXiv:2506.16418v1 Announce Type: cross Abstract: This study investigates the integration of signal processing transformations -- Fast Fourier Transform (FFT), Walsh-Hadamard Transform (WHT), and Discrete Cosine Transform (DCT) -- within the ResNet50 convolutional neural network (CNN) model for image classification. The primary objective is to assess the trade-offs between computational efficiency, energy consumption, and classification accuracy during training and inference. Using the CIFAR-100 dataset (100 classes, 60,000 images), experiments demonstrated that incorporating WHT significantly reduced energy consumption while improving accuracy. Specifically, a baseline ResNet50 model achieved a testing accuracy of 66%, consuming an average of 25,606 kJ per model. In contrast, a modified ResNet50 incorporating WHT in the early convolutional layers achieved 74% accuracy, and an enhanced version with WHT applied to both early and late layers achieved 79% accuracy, with an average energy consumption of only 39 kJ per model. These results demonstrate the potential of WHT as a highly efficient and effective approach for energy-constrained CNN applications.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16418","src/content/posts/2025-06-23-efficient-transformations-in-deep-learning-convolutional-neural-networks.md","876360bcba05cd47",{"html":25,"metadata":27234},{"headings":27235,"localImagePaths":27236,"remoteImagePaths":27237,"frontmatter":27238,"imagePaths":27239},[],[],[],{"title":27227,"description":27228,"summary":27228,"pubDate":25202,"source":25191,"url":27230,"thumbnail":25193},[],"2025-06-23-efficient-transformations-in-deep-learning-convolutional-neural-networks.md","2025-06-23-embodied-web-agents-bridging-physical-digital-realms-for-integrated-agent-intelligence",{"id":27241,"data":27243,"filePath":27248,"digest":27249,"rendered":27250,"legacyId":27257},{"title":27244,"description":27245,"summary":27245,"pubDate":27246,"source":25191,"url":27247,"thumbnail":25193},"Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence","arXiv:2506.15677v2 Announce Type: replace Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15677","src/content/posts/2025-06-23-embodied-web-agents-bridging-physical-digital-realms-for-integrated-agent-intelligence.md","228434835baacf7f",{"html":25,"metadata":27251},{"headings":27252,"localImagePaths":27253,"remoteImagePaths":27254,"frontmatter":27255,"imagePaths":27256},[],[],[],{"title":27244,"description":27245,"summary":27245,"pubDate":25202,"source":25191,"url":27247,"thumbnail":25193},[],"2025-06-23-embodied-web-agents-bridging-physical-digital-realms-for-integrated-agent-intelligence.md","2025-06-23-energy-based-transfer-for-reinforcement-learning",{"id":27258,"data":27260,"filePath":27265,"digest":27266,"rendered":27267,"legacyId":27274},{"title":27261,"description":27262,"summary":27262,"pubDate":27263,"source":25191,"url":27264,"thumbnail":25193},"Energy-Based Transfer for Reinforcement Learning","arXiv:2506.16590v1 Announce Type: cross Abstract: Reinforcement learning algorithms often suffer from poor sample efficiency, making them challenging to apply in multi-task or continual learning settings. Efficiency can be improved by transferring knowledge from a previously trained teacher policy to guide exploration in new but related tasks. However, if the new task sufficiently differs from the teacher's training task, the transferred guidance may be sub-optimal and bias exploration toward low-reward behaviors. We propose an energy-based transfer learning method that uses out-of-distribution detection to selectively issue guidance, enabling the teacher to intervene only in states within its training distribution. We theoretically show that energy scores reflect the teacher's state-visitation density and empirically demonstrate improved sample efficiency and performance across both single-task and multi-task settings.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16590","src/content/posts/2025-06-23-energy-based-transfer-for-reinforcement-learning.md","999c750475f95b3c",{"html":25,"metadata":27268},{"headings":27269,"localImagePaths":27270,"remoteImagePaths":27271,"frontmatter":27272,"imagePaths":27273},[],[],[],{"title":27261,"description":27262,"summary":27262,"pubDate":25202,"source":25191,"url":27264,"thumbnail":25193},[],"2025-06-23-energy-based-transfer-for-reinforcement-learning.md","2025-06-23-enhancing-step-by-step-and-verifiable-medical-reasoning-in-mllms",{"id":27275,"data":27277,"filePath":27282,"digest":27283,"rendered":27284,"legacyId":27291},{"title":27278,"description":27279,"summary":27279,"pubDate":27280,"source":25191,"url":27281,"thumbnail":25193},"Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs","arXiv:2506.16962v1 Announce Type: cross Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16962","src/content/posts/2025-06-23-enhancing-step-by-step-and-verifiable-medical-reasoning-in-mllms.md","64811612b04ca86c",{"html":25,"metadata":27285},{"headings":27286,"localImagePaths":27287,"remoteImagePaths":27288,"frontmatter":27289,"imagePaths":27290},[],[],[],{"title":27278,"description":27279,"summary":27279,"pubDate":25202,"source":25191,"url":27281,"thumbnail":25193},[],"2025-06-23-enhancing-step-by-step-and-verifiable-medical-reasoning-in-mllms.md","2025-06-23-enhancing-mathematical-reasoning-in-large-language-models-with-self-consistency-based-hallucination-detection",{"id":27292,"data":27294,"filePath":27299,"digest":27300,"rendered":27301,"legacyId":27308},{"title":27295,"description":27296,"summary":27296,"pubDate":27297,"source":25191,"url":27298,"thumbnail":25193},"Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection","arXiv:2504.09440v3 Announce Type: replace Abstract: Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2504.09440","src/content/posts/2025-06-23-enhancing-mathematical-reasoning-in-large-language-models-with-self-consistency-based-hallucination-detection.md","53faa3254c2dadf1",{"html":25,"metadata":27302},{"headings":27303,"localImagePaths":27304,"remoteImagePaths":27305,"frontmatter":27306,"imagePaths":27307},[],[],[],{"title":27295,"description":27296,"summary":27296,"pubDate":25202,"source":25191,"url":27298,"thumbnail":25193},[],"2025-06-23-enhancing-mathematical-reasoning-in-large-language-models-with-self-consistency-based-hallucination-detection.md","2025-06-23-essential-web-v10-24t-tokens-of-organized-web-data",{"id":27309,"data":27311,"filePath":27316,"digest":27317,"rendered":27318,"legacyId":27325},{"title":27312,"description":27313,"summary":27313,"pubDate":27314,"source":25191,"url":27315,"thumbnail":25193},"Essential-Web v1.0: 24T tokens of organized web data","arXiv:2506.14111v2 Announce Type: replace-cross Abstract: Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.14111","src/content/posts/2025-06-23-essential-web-v10-24t-tokens-of-organized-web-data.md","dab8998ede0e902c",{"html":25,"metadata":27319},{"headings":27320,"localImagePaths":27321,"remoteImagePaths":27322,"frontmatter":27323,"imagePaths":27324},[],[],[],{"title":27312,"description":27313,"summary":27313,"pubDate":25202,"source":25191,"url":27315,"thumbnail":25193},[],"2025-06-23-essential-web-v10-24t-tokens-of-organized-web-data.md","2025-06-23-evolm-in-search-of-lost-language-model-training-dynamics",{"id":27326,"data":27328,"filePath":27333,"digest":27334,"rendered":27335,"legacyId":27342},{"title":27329,"description":27330,"summary":27330,"pubDate":27331,"source":25191,"url":27332,"thumbnail":25193},"EvoLM: In Search of Lost Language Model Training Dynamics","arXiv:2506.16029v1 Announce Type: cross Abstract: Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16029","src/content/posts/2025-06-23-evolm-in-search-of-lost-language-model-training-dynamics.md","808471abbc8e75b0",{"html":25,"metadata":27336},{"headings":27337,"localImagePaths":27338,"remoteImagePaths":27339,"frontmatter":27340,"imagePaths":27341},[],[],[],{"title":27329,"description":27330,"summary":27330,"pubDate":25202,"source":25191,"url":27332,"thumbnail":25193},[],"2025-06-23-evolm-in-search-of-lost-language-model-training-dynamics.md","2025-06-23-every-rollout-counts-optimal-resource-allocation-for-efficient-test-time-scaling",{"id":27343,"data":27345,"filePath":27350,"digest":27351,"rendered":27352,"legacyId":27359},{"title":27346,"description":27347,"summary":27347,"pubDate":27348,"source":25191,"url":27349,"thumbnail":25193},"Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling","arXiv:2506.15707v1 Announce Type: cross Abstract: Test-Time Scaling (TTS) improves the performance of Large Language Models (LLMs) by using additional inference-time computation to explore multiple reasoning paths through search. Yet how to allocate a fixed rollout budget most effectively during search remains underexplored, often resulting in inefficient use of compute at test time. To bridge this gap, we formulate test-time search as a resource allocation problem and derive the optimal allocation strategy that maximizes the probability of obtaining a correct solution under a fixed rollout budget. Within this formulation, we reveal a core limitation of existing search methods: solution-level allocation tends to favor reasoning directions with more candidates, leading to theoretically suboptimal and inefficient use of compute. To address this, we propose Direction-Oriented Resource Allocation (DORA), a provably optimal method that mitigates this bias by decoupling direction quality from candidate count and allocating resources at the direction level. To demonstrate DORA's effectiveness, we conduct extensive experiments on challenging mathematical reasoning benchmarks including MATH500, AIME2024, and AIME2025. The empirical results show that DORA consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art accuracy. We hope our findings contribute to a broader understanding of optimal TTS for LLMs.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15707","src/content/posts/2025-06-23-every-rollout-counts-optimal-resource-allocation-for-efficient-test-time-scaling.md","0b352d4c58624069",{"html":25,"metadata":27353},{"headings":27354,"localImagePaths":27355,"remoteImagePaths":27356,"frontmatter":27357,"imagePaths":27358},[],[],[],{"title":27346,"description":27347,"summary":27347,"pubDate":25202,"source":25191,"url":27349,"thumbnail":25193},[],"2025-06-23-every-rollout-counts-optimal-resource-allocation-for-efficient-test-time-scaling.md","2025-06-23-explainable-rule-application-via-structured-prompting-a-neural-symbolic-approach",{"id":27360,"data":27362,"filePath":27367,"digest":27368,"rendered":27369,"legacyId":27376},{"title":27363,"description":27364,"summary":27364,"pubDate":27365,"source":25191,"url":27366,"thumbnail":25193},"Explainable Rule Application via Structured Prompting: A Neural-Symbolic Approach","arXiv:2506.16335v1 Announce Type: new Abstract: Large Language Models (LLMs) excel in complex reasoning tasks but struggle with consistent rule application, exception handling, and explainability, particularly in domains like legal analysis that require both natural language understanding and precise logical inference. This paper introduces a structured prompting framework that decomposes reasoning into three verifiable steps: entity identification, property extraction, and symbolic rule application. By integrating neural and symbolic approaches, our method leverages LLMs' interpretive flexibility while ensuring logical consistency through formal verification. The framework externalizes task definitions, enabling domain experts to refine logical structures without altering the architecture. Evaluated on the LegalBench hearsay determination task, our approach significantly outperformed baselines, with OpenAI o-family models showing substantial improvements - o1 achieving an F1 score of 0.929 and o3-mini reaching 0.867 using structured decomposition with complementary predicates, compared to their few-shot baselines of 0.714 and 0.74 respectively. This hybrid neural-symbolic system offers a promising pathway for transparent and consistent rule-based reasoning, suggesting potential for explainable AI applications in structured legal reasoning tasks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16335","src/content/posts/2025-06-23-explainable-rule-application-via-structured-prompting-a-neural-symbolic-approach.md","39ff69c7a3043f4e",{"html":25,"metadata":27370},{"headings":27371,"localImagePaths":27372,"remoteImagePaths":27373,"frontmatter":27374,"imagePaths":27375},[],[],[],{"title":27363,"description":27364,"summary":27364,"pubDate":25202,"source":25191,"url":27366,"thumbnail":25193},[],"2025-06-23-explainable-rule-application-via-structured-prompting-a-neural-symbolic-approach.md","2025-06-23-exploring-big-five-personality-and-ai-capability-effects-in-llm-simulated-negotiation-dialogues",{"id":27377,"data":27379,"filePath":27384,"digest":27385,"rendered":27386,"legacyId":27393},{"title":27380,"description":27381,"summary":27381,"pubDate":27382,"source":25191,"url":27383,"thumbnail":25193},"Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues","arXiv:2506.15928v1 Announce Type: new Abstract: This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts, addressing the need for AI agents that can adapt to diverse human operators and stakeholders. Using Sotopia as a simulation testbed, we present two experiments that systematically evaluated how personality traits and AI agent characteristics influence LLM-simulated social negotiation outcomes--a capability essential for a variety of applications involving cross-team coordination and civil-military interactions. Experiment 1 employs causal discovery methods to measure how personality traits impact price bargaining negotiations, through which we found that Agreeableness and Extraversion significantly affect believability, goal achievement, and knowledge acquisition outcomes. Sociocognitive lexical measures extracted from team communications detected fine-grained differences in agents' empathic communication, moral foundations, and opinion patterns, providing actionable insights for agentic AI systems that must operate reliably in high-stakes operational scenarios. Experiment 2 evaluates human-AI job negotiations by manipulating both simulated human personality and AI system characteristics, specifically transparency, competence, adaptability, demonstrating how AI agent trustworthiness impact mission effectiveness. These findings establish a repeatable evaluation methodology for experimenting with AI agent reliability across diverse operator personalities and human-agent team dynamics, directly supporting operational requirements for reliable AI systems. Our work advances the evaluation of agentic AI workflows by moving beyond standard performance metrics to incorporate social dynamics essential for mission success in complex operations.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15928","src/content/posts/2025-06-23-exploring-big-five-personality-and-ai-capability-effects-in-llm-simulated-negotiation-dialogues.md","537f7bb028dfce76",{"html":25,"metadata":27387},{"headings":27388,"localImagePaths":27389,"remoteImagePaths":27390,"frontmatter":27391,"imagePaths":27392},[],[],[],{"title":27380,"description":27381,"summary":27381,"pubDate":25202,"source":25191,"url":27383,"thumbnail":25193},[],"2025-06-23-exploring-big-five-personality-and-ai-capability-effects-in-llm-simulated-negotiation-dialogues.md","2025-06-23-facial-landmark-visualization-and-emotion-recognition-through-neural-networks",{"id":27394,"data":27396,"filePath":27401,"digest":27402,"rendered":27403,"legacyId":27410},{"title":27397,"description":27398,"summary":27398,"pubDate":27399,"source":25191,"url":27400,"thumbnail":25193},"Facial Landmark Visualization and Emotion Recognition Through Neural Networks","arXiv:2506.17191v1 Announce Type: cross Abstract: Emotion recognition from facial images is a crucial task in human-computer interaction, enabling machines to learn human emotions through facial expressions. Previous studies have shown that facial images can be used to train deep learning models; however, most of these studies do not include a through dataset analysis. Visualizing facial landmarks can be challenging when extracting meaningful dataset insights; to address this issue, we propose facial landmark box plots, a visualization technique designed to identify outliers in facial datasets. Additionally, we compare two sets of facial landmark features: (i) the landmarks' absolute positions and (ii) their displacements from a neutral expression to the peak of an emotional expression. Our results indicate that a neural network achieves better performance than a random forest classifier.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17191","src/content/posts/2025-06-23-facial-landmark-visualization-and-emotion-recognition-through-neural-networks.md","71232ac00109408c",{"html":25,"metadata":27404},{"headings":27405,"localImagePaths":27406,"remoteImagePaths":27407,"frontmatter":27408,"imagePaths":27409},[],[],[],{"title":27397,"description":27398,"summary":27398,"pubDate":25202,"source":25191,"url":27400,"thumbnail":25193},[],"2025-06-23-facial-landmark-visualization-and-emotion-recognition-through-neural-networks.md","2025-06-23-falcon-feedback-driven-adaptive-longshort-term-memory-reinforced-coding-optimization-system",{"id":27411,"data":27413,"filePath":27418,"digest":27419,"rendered":27420,"legacyId":27427},{"title":27414,"description":27415,"summary":27415,"pubDate":27416,"source":25191,"url":27417,"thumbnail":25193},"FALCON: Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization system","arXiv:2410.21349v5 Announce Type: replace-cross Abstract: Recently, large language models (LLMs) have achieved significant progress in automated code generation. Despite their strong instruction-following capabilities, these models frequently struggled to align with user intent in coding scenarios. In particular, they were hampered by datasets that lacked diversity and failed to address specialized tasks or edge cases. Furthermore, challenges in supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) led to failures in generating precise, human-intent-aligned code. To tackle these challenges and improve the code generation performance for automated programming systems, we propose Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization (i.e., FALCON). FALCON is structured into two hierarchical levels. From the global level, long-term memory improves code quality by retaining and applying learned knowledge. At the local level, short-term memory allows for the incorporation of immediate feedback from compilers and AI systems. Additionally, we introduce meta-reinforcement learning with feedback rewards to solve the global-local bi-level optimization problem and enhance the model's adaptability across diverse code generation tasks. Extensive experiments demonstrate that our technique achieves state-of-the-art performance, leading other reinforcement learning methods by more than 4.5 percentage points on the MBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The open-sourced code is publicly available at https://github.com/titurte/FALCON.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2410.21349","src/content/posts/2025-06-23-falcon-feedback-driven-adaptive-longshort-term-memory-reinforced-coding-optimization-system.md","da6d8ed5c091db6f",{"html":25,"metadata":27421},{"headings":27422,"localImagePaths":27423,"remoteImagePaths":27424,"frontmatter":27425,"imagePaths":27426},[],[],[],{"title":27414,"description":27415,"summary":27415,"pubDate":25202,"source":25191,"url":27417,"thumbnail":25193},[],"2025-06-23-falcon-feedback-driven-adaptive-longshort-term-memory-reinforced-coding-optimization-system.md","2025-06-23-fast-and-stable-diffusion-planning-through-variational-adaptive-weighting",{"id":27428,"data":27430,"filePath":27435,"digest":27436,"rendered":27437,"legacyId":27444},{"title":27431,"description":27432,"summary":27432,"pubDate":27433,"source":25191,"url":27434,"thumbnail":25193},"Fast and Stable Diffusion Planning through Variational Adaptive Weighting","arXiv:2506.16688v1 Announce Type: cross Abstract: Diffusion models have recently shown promise in offline RL. However, these methods often suffer from high training costs and slow convergence, particularly when using transformer-based denoising backbones. While several optimization strategies have been proposed -- such as modified noise schedules, auxiliary prediction targets, and adaptive loss weighting -- challenges remain in achieving stable and efficient training. In particular, existing loss weighting functions typically rely on neural network approximators, which can be ineffective in early training phases due to limited generalization capacity of MLPs when exposed to sparse feedback in the early training stages. In this work, we derive a variationally optimal uncertainty-aware weighting function and introduce a closed-form polynomial approximation method for its online estimation under the flow-based generative modeling framework. We integrate our method into a diffusion planning pipeline and evaluate it on standard offline RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our method achieves competitive performance with up to 10 times fewer training steps, highlighting its practical effectiveness.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16688","src/content/posts/2025-06-23-fast-and-stable-diffusion-planning-through-variational-adaptive-weighting.md","5c6c0d9bb12fe711",{"html":25,"metadata":27438},{"headings":27439,"localImagePaths":27440,"remoteImagePaths":27441,"frontmatter":27442,"imagePaths":27443},[],[],[],{"title":27431,"description":27432,"summary":27432,"pubDate":25202,"source":25191,"url":27434,"thumbnail":25193},[],"2025-06-23-fast-and-stable-diffusion-planning-through-variational-adaptive-weighting.md","2025-06-23-fdllm-a-dedicated-detector-for-black-box-llms-fingerprinting",{"id":27445,"data":27447,"filePath":27452,"digest":27453,"rendered":27454,"legacyId":27461},{"title":27448,"description":27449,"summary":27449,"pubDate":27450,"source":25191,"url":27451,"thumbnail":25193},"FDLLM: A Dedicated Detector for Black-Box LLMs Fingerprinting","arXiv:2501.16029v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) are rapidly transforming the landscape of digital content creation. However, the prevalent black-box Application Programming Interface (API) access to many LLMs introduces significant challenges in accountability, governance, and security. LLM fingerprinting, which aims to identify the source model by analyzing statistical and stylistic features of generated text, offers a potential solution. Current progress in this area is hindered by a lack of dedicated datasets and the need for efficient, practical methods that are robust against adversarial manipulations. To address these challenges, we introduce FD-Dataset, a comprehensive bilingual fingerprinting benchmark comprising 90,000 text samples from 20 famous proprietary and open-source LLMs. Furthermore, we present FDLLM, a novel fingerprinting method that leverages parameter-efficient Low-Rank Adaptation (LoRA) to fine-tune a foundation model. This approach enables LoRA to extract deep, persistent features that characterize each source LLM. Through our analysis, we find that LoRA adaptation promotes the aggregation of outputs from the same LLM in representation space while enhancing the separation between different LLMs. This mechanism explains why LoRA proves particularly effective for LLM fingerprinting. Extensive empirical evaluations on FD-Dataset demonstrate FDLLM's superiority, achieving a Macro F1 score 22.1% higher than the strongest baseline. FDLLM also exhibits strong generalization to newly released models, achieving an average accuracy of 95% on unseen models. Notably, FDLLM remains consistently robust under various adversarial attacks, including polishing, translation, and synonym substitution. Experimental results show that FDLLM reduces the average attack success rate from 49.2% (LM-D) to 23.9%.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2501.16029","src/content/posts/2025-06-23-fdllm-a-dedicated-detector-for-black-box-llms-fingerprinting.md","9ad0dc592f2360f5",{"html":25,"metadata":27455},{"headings":27456,"localImagePaths":27457,"remoteImagePaths":27458,"frontmatter":27459,"imagePaths":27460},[],[],[],{"title":27448,"description":27449,"summary":27449,"pubDate":25202,"source":25191,"url":27451,"thumbnail":25193},[],"2025-06-23-fdllm-a-dedicated-detector-for-black-box-llms-fingerprinting.md","2025-06-23-federated-incomplete-multi-view-clustering-with-globally-fused-graph-guidance",{"id":27462,"data":27464,"filePath":27469,"digest":27470,"rendered":27471,"legacyId":27478},{"title":27465,"description":27466,"summary":27466,"pubDate":27467,"source":25191,"url":27468,"thumbnail":25193},"Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance","arXiv:2506.15703v1 Announce Type: cross Abstract: Federated multi-view clustering has been proposed to mine the valuable information within multi-view data distributed across different devices and has achieved impressive results while preserving the privacy. Despite great progress, most federated multi-view clustering methods only used global pseudo-labels to guide the downstream clustering process and failed to exploit the global information when extracting features. In addition, missing data problem in federated multi-view clustering task is less explored. To address these problems, we propose a novel Federated Incomplete Multi-view Clustering method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a dual-head graph convolutional encoder at each client to extract two kinds of underlying features containing global and view-specific information. Subsequently, under the guidance of the fused graph, the two underlying features are fused into high-level features, based on which clustering is conducted under the supervision of pseudo-labeling. Finally, the high-level features are uploaded to the server to refine the graph fusion and pseudo-labeling computation. Extensive experimental results demonstrate the effectiveness and superiority of FIMCFG. Our code is publicly available at https://github.com/PaddiHunter/FIMCFG.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15703","src/content/posts/2025-06-23-federated-incomplete-multi-view-clustering-with-globally-fused-graph-guidance.md","99b96ed70c3eb755",{"html":25,"metadata":27472},{"headings":27473,"localImagePaths":27474,"remoteImagePaths":27475,"frontmatter":27476,"imagePaths":27477},[],[],[],{"title":27465,"description":27466,"summary":27466,"pubDate":25202,"source":25191,"url":27468,"thumbnail":25193},[],"2025-06-23-federated-incomplete-multi-view-clustering-with-globally-fused-graph-guidance.md","2025-06-23-federated-learning-for-mri-based-brainage-a-multicenter-study-on-post-stroke-functional-outcome-prediction",{"id":27479,"data":27481,"filePath":27486,"digest":27487,"rendered":27488,"legacyId":27495},{"title":27482,"description":27483,"summary":27483,"pubDate":27484,"source":25191,"url":27485,"thumbnail":25193},"Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction","arXiv:2506.15626v2 Announce Type: replace-cross Abstract: $textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a neuroimaging biomarker reflecting brain health. However, training robust BrainAGE models requires large datasets, often restricted by privacy concerns. This study evaluates the performance of federated learning (FL) for BrainAGE estimation in ischemic stroke patients treated with mechanical thrombectomy, and investigates its association with clinical phenotypes and functional outcomes. $textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients across 16 hospital centers. We implemented standard machine learning and deep learning models for BrainAGE estimates under three data management strategies: centralized learning (pooled data), FL (local training at each site), and single-site learning. We reported prediction errors and examined associations between BrainAGE and vascular risk factors (e.g., diabetes mellitus, hypertension, smoking), as well as functional outcomes at three months post-stroke. Logistic regression evaluated BrainAGE's predictive value for these outcomes, adjusting for age, sex, vascular risk factors, stroke severity, time between MRI and arterial puncture, prior intravenous thrombolysis, and recanalisation outcome. $textbf{Results:}$ While centralized learning yielded the most accurate predictions, FL consistently outperformed single-site models. BrainAGE was significantly higher in patients with diabetes mellitus across all models. Comparisons between patients with good and poor functional outcomes, and multivariate predictions of these outcomes showed the significance of the association between BrainAGE and post-stroke recovery. $textbf{Conclusion:}$ FL enables accurate age predictions without data centralization. The strong association between BrainAGE, vascular risk factors, and post-stroke recovery highlights its potential for prognostic modeling in stroke care.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15626","src/content/posts/2025-06-23-federated-learning-for-mri-based-brainage-a-multicenter-study-on-post-stroke-functional-outcome-prediction.md","a7b65cbffd231a18",{"html":25,"metadata":27489},{"headings":27490,"localImagePaths":27491,"remoteImagePaths":27492,"frontmatter":27493,"imagePaths":27494},[],[],[],{"title":27482,"description":27483,"summary":27483,"pubDate":25202,"source":25191,"url":27485,"thumbnail":25193},[],"2025-06-23-federated-learning-for-mri-based-brainage-a-multicenter-study-on-post-stroke-functional-outcome-prediction.md","2025-06-23-finance-language-model-evaluation-flame",{"id":27496,"data":27498,"filePath":27503,"digest":27504,"rendered":27505,"legacyId":27512},{"title":27499,"description":27500,"summary":27500,"pubDate":27501,"source":25191,"url":27502,"thumbnail":25193},"Finance Language Model Evaluation (FLaME)","arXiv:2506.15846v1 Announce Type: cross Abstract: Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs' performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15846","src/content/posts/2025-06-23-finance-language-model-evaluation-flame.md","2caa7b4380b69c76",{"html":25,"metadata":27506},{"headings":27507,"localImagePaths":27508,"remoteImagePaths":27509,"frontmatter":27510,"imagePaths":27511},[],[],[],{"title":27499,"description":27500,"summary":27500,"pubDate":25202,"source":25191,"url":27502,"thumbnail":25193},[],"2025-06-23-finance-language-model-evaluation-flame.md","2025-06-23-flame-towards-federated-fine-tuning-large-language-models-through-adaptive-smoe",{"id":27513,"data":27515,"filePath":27520,"digest":27521,"rendered":27522,"legacyId":27529},{"title":27516,"description":27517,"summary":27517,"pubDate":27518,"source":25191,"url":27519,"thumbnail":25193},"FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE","arXiv:2506.16600v1 Announce Type: cross Abstract: Existing resource-adaptive LoRA federated fine-tuning methods enable clients to fine-tune models using compressed versions of global LoRA matrices, in order to accommodate various compute resources across clients. This compression requirement will lead to suboptimal performance due to information loss. To address this, we propose FLAME, a novel federated learning framework based on the Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches, FLAME retains full (uncompressed) global LoRA matrices and achieves client-side adaptability by varying the number of activated experts per client. However, incorporating SMoE into federated learning introduces unique challenges, specifically, the mismatch in output magnitude from partial expert activation and the imbalance in expert training quality across clients. FLAME tackles these challenges through a lightweight rescaling mechanism and an activation-aware aggregation scheme. Empirical results across diverse computational settings demonstrate that FLAME consistently outperforms existing methods, providing a robust and effective solution for resource-adaptive federated learning.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16600","src/content/posts/2025-06-23-flame-towards-federated-fine-tuning-large-language-models-through-adaptive-smoe.md","3383fce546c4a21f",{"html":25,"metadata":27523},{"headings":27524,"localImagePaths":27525,"remoteImagePaths":27526,"frontmatter":27527,"imagePaths":27528},[],[],[],{"title":27516,"description":27517,"summary":27517,"pubDate":25202,"source":25191,"url":27519,"thumbnail":25193},[],"2025-06-23-flame-towards-federated-fine-tuning-large-language-models-through-adaptive-smoe.md","2025-06-23-flow-based-non-stationary-temporal-regime-causal-structure-learning",{"id":27530,"data":27532,"filePath":27537,"digest":27538,"rendered":27539,"legacyId":27546},{"title":27533,"description":27534,"summary":27534,"pubDate":27535,"source":25191,"url":27536,"thumbnail":25193},"Flow-Based Non-stationary Temporal Regime Causal Structure Learning","arXiv:2506.17065v1 Announce Type: cross Abstract: Understanding causal relationships in multivariate time series is crucial in many scenarios, such as those dealing with financial or neurological data. Many such time series exhibit multiple regimes, i.e., consecutive temporal segments with a priori unknown boundaries, with each regime having its own causal structure. Inferring causal dependencies and regime shifts is critical for analyzing the underlying processes. However, causal structure learning in this setting is challenging due to (1) non stationarity, i.e., each regime can have its own causal graph and mixing function, and (2) complex noise distributions, which may be non Gaussian or heteroscedastic. Existing causal discovery approaches cannot address these challenges, since generally assume stationarity or Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified framework for causal discovery that handles non stationary processes along with non Gaussian and heteroscedastic noises. FANTOM simultaneously infers the number of regimes and their corresponding indices and learns each regime's Directed Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm that maximizes the evidence lower bound of the data log likelihood. On the theoretical side, we prove, under mild assumptions, that temporal heteroscedastic causal models, introduced in FANTOM's formulation, are identifiable in both stationary and non stationary settings. In addition, extensive experiments on synthetic and real data show that FANTOM outperforms existing methods.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17065","src/content/posts/2025-06-23-flow-based-non-stationary-temporal-regime-causal-structure-learning.md","b244b7195f32a0f4",{"html":25,"metadata":27540},{"headings":27541,"localImagePaths":27542,"remoteImagePaths":27543,"frontmatter":27544,"imagePaths":27545},[],[],[],{"title":27533,"description":27534,"summary":27534,"pubDate":25202,"source":25191,"url":27536,"thumbnail":25193},[],"2025-06-23-flow-based-non-stationary-temporal-regime-causal-structure-learning.md","2025-06-23-formal-control-for-uncertain-systems-via-contract-based-probabilistic-surrogates-extended-version",{"id":27547,"data":27549,"filePath":27554,"digest":27555,"rendered":27556,"legacyId":27563},{"title":27550,"description":27551,"summary":27551,"pubDate":27552,"source":25191,"url":27553,"thumbnail":25193},"Formal Control for Uncertain Systems via Contract-Based Probabilistic Surrogates (Extended Version)","arXiv:2506.16971v1 Announce Type: cross Abstract: The requirement for identifying accurate system representations has not only been a challenge to fulfill, but it has compromised the scalability of formal methods, as the resulting models are often too complex for effective decision making with formal correctness and performance guarantees. Focusing on probabilistic simulation relations and surrogate models of stochastic systems, we propose an approach that significantly enhances the scalability and practical applicability of such simulation relations by eliminating the need to compute error bounds directly. As a result, we provide an abstraction-based technique that scales effectively to higher dimensions while addressing complex nonlinear agent-environment interactions with infinite-horizon temporal logic guarantees amidst uncertainty. Our approach trades scalability for conservatism favorably, as demonstrated on a complex high-dimensional vehicle intersection case study.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16971","src/content/posts/2025-06-23-formal-control-for-uncertain-systems-via-contract-based-probabilistic-surrogates-extended-version.md","337f1ece43d6d9c0",{"html":25,"metadata":27557},{"headings":27558,"localImagePaths":27559,"remoteImagePaths":27560,"frontmatter":27561,"imagePaths":27562},[],[],[],{"title":27550,"description":27551,"summary":27551,"pubDate":25202,"source":25191,"url":27553,"thumbnail":25193},[],"2025-06-23-formal-control-for-uncertain-systems-via-contract-based-probabilistic-surrogates-extended-version.md","2025-06-23-fractional-reasoning-via-latent-steering-vectors-improves-inference-time-compute",{"id":27564,"data":27566,"filePath":27571,"digest":27572,"rendered":27573,"legacyId":27580},{"title":27567,"description":27568,"summary":27568,"pubDate":27569,"source":25191,"url":27570,"thumbnail":25193},"Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute","arXiv:2506.15882v1 Announce Type: cross Abstract: Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15882","src/content/posts/2025-06-23-fractional-reasoning-via-latent-steering-vectors-improves-inference-time-compute.md","9f94156812b9484a",{"html":25,"metadata":27574},{"headings":27575,"localImagePaths":27576,"remoteImagePaths":27577,"frontmatter":27578,"imagePaths":27579},[],[],[],{"title":27567,"description":27568,"summary":27568,"pubDate":25202,"source":25191,"url":27570,"thumbnail":25193},[],"2025-06-23-fractional-reasoning-via-latent-steering-vectors-improves-inference-time-compute.md","2025-06-23-frida-to-the-rescue-analyzing-synthetic-data-effectiveness-in-object-based-common-sense-reasoning-for-disaster-response",{"id":27581,"data":27583,"filePath":27588,"digest":27589,"rendered":27590,"legacyId":27597},{"title":27584,"description":27585,"summary":27585,"pubDate":27586,"source":25191,"url":27587,"thumbnail":25193},"FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response","arXiv:2502.18452v2 Announce Type: replace-cross Abstract: During Human Robot Interactions in disaster relief scenarios, Large Language Models (LLMs) have the potential for substantial physical reasoning to assist in mission objectives. However, these capabilities are often found only in larger models, which are frequently not reasonable to deploy on robotic systems. To meet our problem space requirements, we introduce a dataset and pipeline to create Field Reasoning and Instruction Decoding Agent (FRIDA) models. In our pipeline, domain experts and linguists combine their knowledge to make high-quality few-shot prompts used to generate synthetic data for fine-tuning. We hand-curate datasets for this few-shot prompting and for evaluation to improve LLM reasoning on both general and disaster-specific objects. We concurrently run an ablation study to understand which kinds of synthetic data most affect performance. We fine-tune several small instruction-tuned models and find that ablated FRIDA models only trained on objects' physical state and function data outperformed both the FRIDA models trained on all synthetic data and the base models in our customized evaluation. We demonstrate that the FRIDA pipeline is capable of instilling physical common sense with minimal data.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2502.18452","src/content/posts/2025-06-23-frida-to-the-rescue-analyzing-synthetic-data-effectiveness-in-object-based-common-sense-reasoning-for-disaster-response.md","11675deff2d8b3fc",{"html":25,"metadata":27591},{"headings":27592,"localImagePaths":27593,"remoteImagePaths":27594,"frontmatter":27595,"imagePaths":27596},[],[],[],{"title":27584,"description":27585,"summary":27585,"pubDate":25202,"source":25191,"url":27587,"thumbnail":25193},[],"2025-06-23-frida-to-the-rescue-analyzing-synthetic-data-effectiveness-in-object-based-common-sense-reasoning-for-disaster-response.md","2025-06-23-from-general-to-targeted-rewards-surpassing-gpt-4-in-open-ended-long-context-generation",{"id":27598,"data":27600,"filePath":27605,"digest":27606,"rendered":27607,"legacyId":27614},{"title":27601,"description":27602,"summary":27602,"pubDate":27603,"source":25191,"url":27604,"thumbnail":25193},"From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation","arXiv:2506.16024v1 Announce Type: cross Abstract: Current research on long-form context in Large Language Models (LLMs) primarily focuses on the understanding of long-contexts, the Open-ended Long Text Generation (Open-LTG) remains insufficiently explored. Training a long-context generation model requires curation of gold standard reference data, which is typically nonexistent for informative Open-LTG tasks. However, previous methods only utilize general assessments as reward signals, which limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative reinforcement learning (RL) based framework, which includes a dataset and a reward signal computation method. Firstly, ProxyReward Dataset generation is accomplished through simple prompts that enables the model to create automatically, obviating extensive labeled data or significant manual effort. Secondly, ProxyReward Signal offers a targeted evaluation of information comprehensiveness and accuracy for specific questions. The experimental results indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can significantly enhance performance by 20% on the Open-LTG task when training widely used open-source models, while also surpassing the LLM-as-a-Judge approach. Our work presents effective methods to enhance the ability of LLMs to address complex open-ended questions posed by human.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16024","src/content/posts/2025-06-23-from-general-to-targeted-rewards-surpassing-gpt-4-in-open-ended-long-context-generation.md","57913570df7261af",{"html":25,"metadata":27608},{"headings":27609,"localImagePaths":27610,"remoteImagePaths":27611,"frontmatter":27612,"imagePaths":27613},[],[],[],{"title":27601,"description":27602,"summary":27602,"pubDate":25202,"source":25191,"url":27604,"thumbnail":25193},[],"2025-06-23-from-general-to-targeted-rewards-surpassing-gpt-4-in-open-ended-long-context-generation.md","2025-06-23-from-concepts-to-components-concept-agnostic-attention-module-discovery-in-transformers",{"id":27615,"data":27617,"filePath":27622,"digest":27623,"rendered":27624,"legacyId":27631},{"title":27618,"description":27619,"summary":27619,"pubDate":27620,"source":25191,"url":27621,"thumbnail":25193},"From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers","arXiv:2506.17052v1 Announce Type: cross Abstract: Transformers have achieved state-of-the-art performance across language and vision tasks. This success drives the imperative to interpret their internal mechanisms with the dual goals of enhancing performance and improving behavioral control. Attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components. Current attribution research primarily studies multi-layer perceptron neurons and addresses relatively simple concepts such as factual associations (e.g., Paris is located in France). This focus tends to overlook the impact of the attention mechanism and lacks a unified approach for analyzing more complex concepts. To fill these gaps, we introduce Scalable Attention Module Discovery (SAMD), a concept-agnostic method for mapping arbitrary, complex concepts to specific attention heads of general transformer models. We accomplish this by representing each concept as a vector, calculating its cosine similarity with each attention head, and selecting the TopK-scoring heads to construct the concept-associated attention module. We then propose Scalar Attention Module Intervention (SAMI), a simple strategy to diminish or amplify the effects of a concept by adjusting the attention module using only a single scalar parameter. Empirically, we demonstrate SAMD on concepts of varying complexity, and visualize the locations of their corresponding modules. Our results demonstrate that module locations remain stable before and after LLM post-training, and confirm prior work on the mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on HarmBench (+72.7%) by diminishing 'safety' and improve performance on the GSM8K benchmark (+1.6%) by amplifying 'reasoning'. Lastly, we highlight the domain-agnostic nature of our approach by suppressing the image classification accuracy of vision transformers on ImageNet.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17052","src/content/posts/2025-06-23-from-concepts-to-components-concept-agnostic-attention-module-discovery-in-transformers.md","0eea24950151d457",{"html":25,"metadata":27625},{"headings":27626,"localImagePaths":27627,"remoteImagePaths":27628,"frontmatter":27629,"imagePaths":27630},[],[],[],{"title":27618,"description":27619,"summary":27619,"pubDate":25202,"source":25191,"url":27621,"thumbnail":25193},[],"2025-06-23-from-concepts-to-components-concept-agnostic-attention-module-discovery-in-transformers.md","2025-06-23-from-llm-anation-to-llm-orchestrator-coordinating-small-models-for-data-labeling",{"id":27632,"data":27634,"filePath":27639,"digest":27640,"rendered":27641,"legacyId":27648},{"title":27635,"description":27636,"summary":27636,"pubDate":27637,"source":25191,"url":27638,"thumbnail":25193},"From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling","arXiv:2506.16393v1 Announce Type: cross Abstract: Although the annotation paradigm based on Large Language Models (LLMs) has made significant breakthroughs in recent years, its actual deployment still has two core bottlenecks: first, the cost of calling commercial APIs in large-scale annotation is very expensive; second, in scenarios that require fine-grained semantic understanding, such as sentiment classification and toxicity classification, the annotation accuracy of LLMs is even lower than that of Small Language Models (SLMs) dedicated to this field. To address these problems, we propose a new paradigm of multi-model cooperative annotation and design a fully automatic annotation framework AutoAnnotator based on this. Specifically, AutoAnnotator consists of two layers. The upper-level meta-controller layer uses the generation and reasoning capabilities of LLMs to select SLMs for annotation, automatically generate annotation code and verify difficult samples; the lower-level task-specialist layer consists of multiple SLMs that perform annotation through multi-model voting. In addition, we use the difficult samples obtained by the secondary review of the meta-controller layer as the reinforcement learning set and fine-tune the SLMs in stages through a continual learning strategy, thereby improving the generalization of SLMs. Extensive experiments show that AutoAnnotator outperforms existing open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings. Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to directly annotating with GPT-3.5-turbo, while still improving the accuracy by 6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16393","src/content/posts/2025-06-23-from-llm-anation-to-llm-orchestrator-coordinating-small-models-for-data-labeling.md","fc90bcdbcad6d705",{"html":25,"metadata":27642},{"headings":27643,"localImagePaths":27644,"remoteImagePaths":27645,"frontmatter":27646,"imagePaths":27647},[],[],[],{"title":27635,"description":27636,"summary":27636,"pubDate":25202,"source":25191,"url":27638,"thumbnail":25193},[],"2025-06-23-from-llm-anation-to-llm-orchestrator-coordinating-small-models-for-data-labeling.md","2025-06-23-from-rag-to-memory-non-parametric-continual-learning-for-large-language-models",{"id":27649,"data":27651,"filePath":27656,"digest":27657,"rendered":27658,"legacyId":27665},{"title":27652,"description":27653,"summary":27653,"pubDate":27654,"source":25191,"url":27655,"thumbnail":25193},"From RAG to Memory: Non-Parametric Continual Learning for Large Language Models","arXiv:2502.14802v2 Announce Type: replace-cross Abstract: Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2502.14802","src/content/posts/2025-06-23-from-rag-to-memory-non-parametric-continual-learning-for-large-language-models.md","12dcab05b4edf85f",{"html":25,"metadata":27659},{"headings":27660,"localImagePaths":27661,"remoteImagePaths":27662,"frontmatter":27663,"imagePaths":27664},[],[],[],{"title":27652,"description":27653,"summary":27653,"pubDate":25202,"source":25191,"url":27655,"thumbnail":25193},[],"2025-06-23-from-rag-to-memory-non-parametric-continual-learning-for-large-language-models.md","2025-06-23-from-prompts-to-constructs-a-dual-validity-framework-for-llm-research-in-psychology",{"id":27666,"data":27668,"filePath":27673,"digest":27674,"rendered":27675,"legacyId":27682},{"title":27669,"description":27670,"summary":27670,"pubDate":27671,"source":25191,"url":27672,"thumbnail":25193},"From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology","arXiv:2506.16697v1 Announce Type: cross Abstract: Large language models (LLMs) are rapidly being adopted across psychology, serving as research tools, experimental subjects, human simulators, and computational models of cognition. However, the application of human measurement tools to these systems can produce contradictory results, raising concerns that many findings are measurement phantoms--statistical artifacts rather than genuine psychological phenomena. In this Perspective, we argue that building a robust science of AI psychology requires integrating two of our field's foundational pillars: the principles of reliable measurement and the standards for sound causal inference. We present a dual-validity framework to guide this integration, which clarifies how the evidence needed to support a claim scales with its scientific ambition. Using an LLM to classify text may require only basic accuracy checks, whereas claiming it can simulate anxiety demands a far more rigorous validation process. Current practice systematically fails to meet these requirements, often treating statistical pattern matching as evidence of psychological phenomena. The same model output--endorsing 'I am anxious'--requires different validation strategies depending on whether researchers claim to measure, characterize, simulate, or model psychological constructs. Moving forward requires developing computational analogues of psychological constructs and establishing clear, scalable standards of evidence rather than the uncritical application of human measurement tools.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16697","src/content/posts/2025-06-23-from-prompts-to-constructs-a-dual-validity-framework-for-llm-research-in-psychology.md","58ee4830e8d94566",{"html":25,"metadata":27676},{"headings":27677,"localImagePaths":27678,"remoteImagePaths":27679,"frontmatter":27680,"imagePaths":27681},[],[],[],{"title":27669,"description":27670,"summary":27670,"pubDate":25202,"source":25191,"url":27672,"thumbnail":25193},[],"2025-06-23-from-prompts-to-constructs-a-dual-validity-framework-for-llm-research-in-psychology.md","2025-06-23-from-semantic-to-instance-a-semi-self-supervised-learning-approach",{"id":27683,"data":27685,"filePath":27690,"digest":27691,"rendered":27692,"legacyId":27699},{"title":27686,"description":27687,"summary":27687,"pubDate":27688,"source":25191,"url":27689,"thumbnail":25193},"From Semantic To Instance: A Semi-Self-Supervised Learning Approach","arXiv:2506.16563v1 Announce Type: cross Abstract: Instance segmentation is essential for applications such as automated monitoring of plant health, growth, and yield. However, extensive effort is required to create large-scale datasets with pixel-level annotations of each object instance for developing instance segmentation models that restrict the use of deep learning in these areas. This challenge is more significant in images with densely packed, self-occluded objects, which are common in agriculture. To address this challenge, we propose a semi-self-supervised learning approach that requires minimal manual annotation to develop a high-performing instance segmentation model. We design GLMask, an image-mask representation for the model to focus on shape, texture, and pattern while minimizing its dependence on color features. We develop a pipeline to generate semantic segmentation and then transform it into instance-level segmentation. The proposed approach substantially outperforms the conventional instance segmentation models, establishing a state-of-the-art wheat head instance segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed methodology on the general-purpose Microsoft COCO dataset, achieving a significant performance improvement of over 12.6% mAP@50. This highlights that the utility of our proposed approach extends beyond precision agriculture and applies to other domains, specifically those with similar data characteristics.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16563","src/content/posts/2025-06-23-from-semantic-to-instance-a-semi-self-supervised-learning-approach.md","f5d7b0288f75b416",{"html":25,"metadata":27693},{"headings":27694,"localImagePaths":27695,"remoteImagePaths":27696,"frontmatter":27697,"imagePaths":27698},[],[],[],{"title":27686,"description":27687,"summary":27687,"pubDate":25202,"source":25191,"url":27689,"thumbnail":25193},[],"2025-06-23-from-semantic-to-instance-a-semi-self-supervised-learning-approach.md","2025-06-23-from-teacher-to-student-tracking-memorization-through-model-distillation",{"id":27700,"data":27702,"filePath":27707,"digest":27708,"rendered":27709,"legacyId":27716},{"title":27703,"description":27704,"summary":27704,"pubDate":27705,"source":25191,"url":27706,"thumbnail":25193},"From Teacher to Student: Tracking Memorization Through Model Distillation","arXiv:2506.16170v1 Announce Type: cross Abstract: Large language models (LLMs) are known to memorize parts of their training data, raising important concerns around privacy and security. While previous research has focused on studying memorization in pre-trained models, much less is known about how knowledge distillation (KD) affects memorization.In this study, we explore how different KD methods influence the memorization of fine-tuned task data when a large teacher model is distilled into smaller student variants.This study demonstrates that distilling a larger teacher model, fine-tuned on a dataset, into a smaller variant not only lowers computational costs and model size but also significantly reduces the memorization risks compared to standard fine-tuning approaches.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16170","src/content/posts/2025-06-23-from-teacher-to-student-tracking-memorization-through-model-distillation.md","36ccf9ca9a863670",{"html":25,"metadata":27710},{"headings":27711,"localImagePaths":27712,"remoteImagePaths":27713,"frontmatter":27714,"imagePaths":27715},[],[],[],{"title":27703,"description":27704,"summary":27704,"pubDate":25202,"source":25191,"url":27706,"thumbnail":25193},[],"2025-06-23-from-teacher-to-student-tracking-memorization-through-model-distillation.md","2025-06-23-generalizable-agent-modeling-for-agent-collaboration-competition-adaptation-with-multi-retrieval-and-dynamic-generation",{"id":27717,"data":27719,"filePath":27724,"digest":27725,"rendered":27726,"legacyId":27733},{"title":27720,"description":27721,"summary":27721,"pubDate":27722,"source":25191,"url":27723,"thumbnail":25193},"Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation","arXiv:2506.16718v1 Announce Type: cross Abstract: Adapting a single agent to a new multi-agent system brings challenges, necessitating adjustments across various tasks, environments, and interactions with unknown teammates and opponents. Addressing this challenge is highly complex, and researchers have proposed two simplified scenarios, Multi-agent reinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on these foundations, we propose a more comprehensive setting, Agent Collaborative-Competitive Adaptation (ACCA), which evaluates an agent to generalize across diverse scenarios, tasks, and interactions with both unfamiliar opponents and teammates. In ACCA, agents adjust to task and environmental changes, collaborate with unseen teammates, and compete against unknown opponents. We introduce a new modeling approach, Multi-Retrieval and Dynamic Generation (MRDG), that effectively models both teammates and opponents using their behavioral trajectories. This method incorporates a positional encoder for varying team sizes and a hypernetwork module to boost agents' learning and adaptive capabilities. Additionally, a viewpoint alignment module harmonizes the observational perspectives of retrieved teammates and opponents with the learning agent. Extensive tests in benchmark scenarios like SMAC, Overcooked-AI, and Melting Pot show that MRDG significantly improves robust collaboration and competition with unseen teammates and opponents, surpassing established baselines. Our code is available at: https://github.com/vcis-wangchenxu/MRDG.git",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16718","src/content/posts/2025-06-23-generalizable-agent-modeling-for-agent-collaboration-competition-adaptation-with-multi-retrieval-and-dynamic-generation.md","6f092169e0aa821b",{"html":25,"metadata":27727},{"headings":27728,"localImagePaths":27729,"remoteImagePaths":27730,"frontmatter":27731,"imagePaths":27732},[],[],[],{"title":27720,"description":27721,"summary":27721,"pubDate":25202,"source":25191,"url":27723,"thumbnail":25193},[],"2025-06-23-generalizable-agent-modeling-for-agent-collaboration-competition-adaptation-with-multi-retrieval-and-dynamic-generation.md","2025-06-23-generalisation-bounds-of-zero-shot-economic-forecasting-using-time-series-foundation-models",{"id":27734,"data":27736,"filePath":27741,"digest":27742,"rendered":27743,"legacyId":27750},{"title":27737,"description":27738,"summary":27738,"pubDate":27739,"source":25191,"url":27740,"thumbnail":25193},"Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models","arXiv:2506.15705v1 Announce Type: cross Abstract: This study investigates zero-shot forecasting capabilities of Time Series Foundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to forecasting economic indicators under univariate conditions, bypassing the need for train bespoke econometric models using and extensive training datasets. Our experiments were conducted on a case study dataset, without additional customisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos, TimeGPT and Moirai) under data-scarce conditions and structural breaks. Our results demonstrate that appropriately engineered TSFMs can internalise rich economic dynamics, accommodate regime shifts, and deliver well-behaved uncertainty estimates out of the box, while matching state-of-the-art multivariate models on this domain. Our findings suggest that, without any fine-tuning, TSFMs can match or exceed classical models during stable economic conditions. However, they are vulnerable to degradation in performances during periods of rapid shocks. The findings offer guidance to practitioners on when zero-shot deployments are viable for macroeconomic monitoring and strategic planning.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15705","src/content/posts/2025-06-23-generalisation-bounds-of-zero-shot-economic-forecasting-using-time-series-foundation-models.md","146c52662a482a42",{"html":25,"metadata":27744},{"headings":27745,"localImagePaths":27746,"remoteImagePaths":27747,"frontmatter":27748,"imagePaths":27749},[],[],[],{"title":27737,"description":27738,"summary":27738,"pubDate":25202,"source":25191,"url":27740,"thumbnail":25193},[],"2025-06-23-generalisation-bounds-of-zero-shot-economic-forecasting-using-time-series-foundation-models.md","2025-06-23-geoguess-multimodal-reasoning-based-on-hierarchy-of-visual-information-in-street-view",{"id":27751,"data":27753,"filePath":27758,"digest":27759,"rendered":27760,"legacyId":27767},{"title":27754,"description":27755,"summary":27755,"pubDate":27756,"source":25191,"url":27757,"thumbnail":25193},"GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View","arXiv:2506.16633v1 Announce Type: cross Abstract: Multimodal reasoning is a process of understanding, integrating and inferring information across different data modalities. It has recently attracted surging academic attention as a benchmark for Artificial Intelligence (AI). Although there are various tasks for evaluating multimodal reasoning ability, they still have limitations. Lack of reasoning on hierarchical visual clues at different levels of granularity, e.g., local details and global context, is of little discussion, despite its frequent involvement in real scenarios. To bridge the gap, we introduce a novel and challenging task for multimodal reasoning, namely GeoGuess. Given a street view image, the task is to identify its location and provide a detailed explanation. A system that succeeds in GeoGuess should be able to detect tiny visual clues, perceive the broader landscape, and associate with vast geographic knowledge. Therefore, GeoGuess would require the ability to reason between hierarchical visual information and geographic knowledge. In this work, we establish a benchmark for GeoGuess by introducing a specially curated dataset GeoExplain which consists of panoramas-geocoordinates-explanation tuples. Additionally, we present a multimodal and multilevel reasoning method, namely SightSense which can make prediction and generate comprehensive explanation based on hierarchy of visual information and external knowledge. Our analysis and experiments demonstrate their outstanding performance in GeoGuess.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16633","src/content/posts/2025-06-23-geoguess-multimodal-reasoning-based-on-hierarchy-of-visual-information-in-street-view.md","71797777a0c5be75",{"html":25,"metadata":27761},{"headings":27762,"localImagePaths":27763,"remoteImagePaths":27764,"frontmatter":27765,"imagePaths":27766},[],[],[],{"title":27754,"description":27755,"summary":27755,"pubDate":25202,"source":25191,"url":27757,"thumbnail":25193},[],"2025-06-23-geoguess-multimodal-reasoning-based-on-hierarchy-of-visual-information-in-street-view.md","2025-06-23-geometric-learning-in-black-box-optimization-a-gnn-framework-for-algorithm-performance-prediction",{"id":27768,"data":27770,"filePath":27775,"digest":27776,"rendered":27777,"legacyId":27784},{"title":27771,"description":27772,"summary":27772,"pubDate":27773,"source":25191,"url":27774,"thumbnail":25193},"Geometric Learning in Black-Box Optimization: A GNN Framework for Algorithm Performance Prediction","arXiv:2506.16144v1 Announce Type: new Abstract: Automated algorithm performance prediction in numerical blackbox optimization often relies on problem characterizations, such as exploratory landscape analysis features. These features are typically used as inputs to machine learning models and are represented in a tabular format. However, such approaches often overlook algorithm configurations, a key factor influencing performance. The relationships between algorithm operators, parameters, problem characteristics, and performance outcomes form a complex structure best represented as a graph. This work explores the use of heterogeneous graph data structures and graph neural networks to predict the performance of optimization algorithms by capturing the complex dependencies between problems, algorithm configurations, and performance outcomes. We focus on two modular frameworks, modCMA-ES and modDE, which decompose two widely used derivative-free optimization algorithms: the covariance matrix adaptation evolution strategy (CMA-ES) and differential evolution (DE). We evaluate 324 modCMA-ES and 576 modDE variants on 24 BBOB problems across six runtime budgets and two problem dimensions. Achieving up to 36.6% improvement in MSE over traditional tabular-based methods, this work highlights the potential of geometric learning in black-box optimization.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16144","src/content/posts/2025-06-23-geometric-learning-in-black-box-optimization-a-gnn-framework-for-algorithm-performance-prediction.md","6fd0930f09c9ae3f",{"html":25,"metadata":27778},{"headings":27779,"localImagePaths":27780,"remoteImagePaths":27781,"frontmatter":27782,"imagePaths":27783},[],[],[],{"title":27771,"description":27772,"summary":27772,"pubDate":25202,"source":25191,"url":27774,"thumbnail":25193},[],"2025-06-23-geometric-learning-in-black-box-optimization-a-gnn-framework-for-algorithm-performance-prediction.md","2025-06-23-gflowgr-fine-tuning-generative-recommendation-frameworks-with-generative-flow-networks",{"id":27785,"data":27787,"filePath":27792,"digest":27793,"rendered":27794,"legacyId":27801},{"title":27788,"description":27789,"summary":27789,"pubDate":27790,"source":25191,"url":27791,"thumbnail":25193},"GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks","arXiv:2506.16114v1 Announce Type: cross Abstract: Generative recommendations (GR), which usually include item tokenizers and generative Large Language Models (LLMs), have demonstrated remarkable success across a wide range of scenarios. The majority of existing research efforts primarily concentrate on developing powerful item tokenizers or advancing LLM decoding strategies to attain superior performance. However, the critical fine-tuning step in GR frameworks, which is essential for adapting LLMs to recommendation data, remains largely unexplored. Current approaches predominantly rely on either the next-token prediction loss of supervised fine-tuning (SFT) or recommendationspecific direct preference optimization (DPO) strategies. Both methods ignore the exploration of possible positive unobserved samples, which is commonly referred to as the exposure bias problem. To mitigate this problem, this paper treats the GR as a multi-step generation task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The proposed framework integrates collaborative knowledge from traditional recommender systems to create an adaptive trajectory sampler and a comprehensive reward model. Leveraging the diverse generation property of GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR emerges as a promising approach to mitigate the exposure bias problem. Extensive empirical results on two real-world datasets and with two different GR backbones highlight the effectiveness and robustness of GFlowGR.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16114","src/content/posts/2025-06-23-gflowgr-fine-tuning-generative-recommendation-frameworks-with-generative-flow-networks.md","9001574cee11f58c",{"html":25,"metadata":27795},{"headings":27796,"localImagePaths":27797,"remoteImagePaths":27798,"frontmatter":27799,"imagePaths":27800},[],[],[],{"title":27788,"description":27789,"summary":27789,"pubDate":25202,"source":25191,"url":27791,"thumbnail":25193},[],"2025-06-23-gflowgr-fine-tuning-generative-recommendation-frameworks-with-generative-flow-networks.md","2025-06-23-graph-diffusion-that-can-insert-and-delete",{"id":27802,"data":27804,"filePath":27809,"digest":27810,"rendered":27811,"legacyId":27818},{"title":27805,"description":27806,"summary":27806,"pubDate":27807,"source":25191,"url":27808,"thumbnail":25193},"Graph Diffusion that can Insert and Delete","arXiv:2506.15725v1 Announce Type: cross Abstract: Generative models of graphs based on discrete Denoising Diffusion Probabilistic Models (DDPMs) offer a principled approach to molecular generation by systematically removing structural noise through iterative atom and bond adjustments. However, existing formulations are fundamentally limited by their inability to adapt the graph size (that is, the number of atoms) during the diffusion process, severely restricting their effectiveness in conditional generation scenarios such as property-driven molecular design, where the targeted property often correlates with the molecular size. In this paper, we reformulate the noising and denoising processes to support monotonic insertion and deletion of nodes. The resulting model, which we call GrIDDD, dynamically grows or shrinks the chemical graph during generation. GrIDDD matches or exceeds the performance of existing graph diffusion models on molecular property targeting despite being trained on a more difficult problem. Furthermore, when applied to molecular optimization, GrIDDD exhibits competitive performance compared to specialized optimization models. This work paves the way for size-adaptive molecular generation with graph diffusion.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15725","src/content/posts/2025-06-23-graph-diffusion-that-can-insert-and-delete.md","b255716c48b3a019",{"html":25,"metadata":27812},{"headings":27813,"localImagePaths":27814,"remoteImagePaths":27815,"frontmatter":27816,"imagePaths":27817},[],[],[],{"title":27805,"description":27806,"summary":27806,"pubDate":25202,"source":25191,"url":27808,"thumbnail":25193},[],"2025-06-23-graph-diffusion-that-can-insert-and-delete.md","2025-06-23-graphics4science-computer-graphics-for-scientific-impacts",{"id":27819,"data":27821,"filePath":27826,"digest":27827,"rendered":27828,"legacyId":27835},{"title":27822,"description":27823,"summary":27823,"pubDate":27824,"source":25191,"url":27825,"thumbnail":25193},"Graphics4Science: Computer Graphics for Scientific Impacts","arXiv:2506.15786v1 Announce Type: cross Abstract: Computer graphics, often associated with films, games, and visual effects, has long been a powerful tool for addressing scientific challenges--from its origins in 3D visualization for medical imaging to its role in modern computational modeling and simulation. This course explores the deep and evolving relationship between computer graphics and science, highlighting past achievements, ongoing contributions, and open questions that remain. We show how core methods, such as geometric reasoning and physical modeling, provide inductive biases that help address challenges in both fields, especially in data-scarce settings. To that end, we aim to reframe graphics as a modeling language for science by bridging vocabulary gaps between the two communities. Designed for both newcomers and experts, Graphics4Science invites the graphics community to engage with science, tackle high-impact problems where graphics expertise can make a difference, and contribute to the future of scientific discovery. Additional details are available on the course website: https://graphics4science.github.io",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15786","src/content/posts/2025-06-23-graphics4science-computer-graphics-for-scientific-impacts.md","67f71f251521d220",{"html":25,"metadata":27829},{"headings":27830,"localImagePaths":27831,"remoteImagePaths":27832,"frontmatter":27833,"imagePaths":27834},[],[],[],{"title":27822,"description":27823,"summary":27823,"pubDate":25202,"source":25191,"url":27825,"thumbnail":25193},[],"2025-06-23-graphics4science-computer-graphics-for-scientific-impacts.md","2025-06-23-graphrag-bench-challenging-domain-specific-reasoning-for-evaluating-graph-retrieval-augmented-generation",{"id":27836,"data":27838,"filePath":27843,"digest":27844,"rendered":27845,"legacyId":27852},{"title":27839,"description":27840,"summary":27840,"pubDate":27841,"source":25191,"url":27842,"thumbnail":25193},"GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation","arXiv:2506.02404v3 Announce Type: replace-cross Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: ((i)) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. ((ii)) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. ((iii)) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.02404","src/content/posts/2025-06-23-graphrag-bench-challenging-domain-specific-reasoning-for-evaluating-graph-retrieval-augmented-generation.md","8f7ad6293508ee4a",{"html":25,"metadata":27846},{"headings":27847,"localImagePaths":27848,"remoteImagePaths":27849,"frontmatter":27850,"imagePaths":27851},[],[],[],{"title":27839,"description":27840,"summary":27840,"pubDate":25202,"source":25191,"url":27842,"thumbnail":25193},[],"2025-06-23-graphrag-bench-challenging-domain-specific-reasoning-for-evaluating-graph-retrieval-augmented-generation.md","2025-06-23-grounding-language-models-with-semantic-digital-twins-for-robotic-planning",{"id":27853,"data":27855,"filePath":27860,"digest":27861,"rendered":27862,"legacyId":27869},{"title":27856,"description":27857,"summary":27857,"pubDate":27858,"source":25191,"url":27859,"thumbnail":25193},"Grounding Language Models with Semantic Digital Twins for Robotic Planning","arXiv:2506.16493v1 Announce Type: cross Abstract: We introduce a novel framework that integrates Semantic Digital Twins (SDTs) with Large Language Models (LLMs) to enable adaptive and goal-driven robotic task execution in dynamic environments. The system decomposes natural language instructions into structured action triplets, which are grounded in contextual environmental data provided by the SDT. This semantic grounding allows the robot to interpret object affordances and interaction rules, enabling action planning and real-time adaptability. In case of execution failures, the LLM utilizes error feedback and SDT insights to generate recovery strategies and iteratively revise the action plan. We evaluate our approach using tasks from the ALFRED benchmark, demonstrating robust performance across various household scenarios. The proposed framework effectively combines high-level reasoning with semantic environment understanding, achieving reliable task completion in the face of uncertainty and failure.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16493","src/content/posts/2025-06-23-grounding-language-models-with-semantic-digital-twins-for-robotic-planning.md","c11585f7bcb5b819",{"html":25,"metadata":27863},{"headings":27864,"localImagePaths":27865,"remoteImagePaths":27866,"frontmatter":27867,"imagePaths":27868},[],[],[],{"title":27856,"description":27857,"summary":27857,"pubDate":25202,"source":25191,"url":27859,"thumbnail":25193},[],"2025-06-23-grounding-language-models-with-semantic-digital-twins-for-robotic-planning.md","2025-06-23-grpo-care-consistency-aware-reinforcement-learning-for-multimodal-reasoning",{"id":27870,"data":27872,"filePath":27877,"digest":27878,"rendered":27879,"legacyId":27886},{"title":27873,"description":27874,"summary":27874,"pubDate":27875,"source":25191,"url":27876,"thumbnail":25193},"GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning","arXiv:2506.16141v1 Announce Type: cross Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO, have advanced Chain-of-Thought reasoning in large language models (LLMs), yet their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack of rigorous evaluation for MLLM post-training methods, we introduce SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced perception and reasoning. It offers a large training set and evaluates generalization across three escalating challenges: in-distribution, cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1, we find that standard GRPO, while improving answer accuracy, often reduces logical coherence between reasoning steps and answers, with only a 57.9% consistency rate. This stems from reward signals focusing solely on final answers, encouraging shortcuts, and strict KL penalties limiting exploration.To address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing both answer correctness and reasoning coherence without explicit supervision. GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer correctness, and (2) an adaptive consistency bonus, computed by comparing the model's reasoning-to-answer likelihood (via a slowly-evolving reference model) against group peers.This dual mechanism amplifies rewards for reasoning paths that are both correct and logically consistent. Replacing KL penalties with this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1, achieving a 6.7% performance gain on the hardest evaluation level and a 24.5% improvement in consistency. It also shows strong transferability, improving model performance across diverse video understanding benchmarks. Our work contributes a systematically designed benchmark and a generalizable post-training framework, advancing the development of more interpretable and robust MLLMs.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16141","src/content/posts/2025-06-23-grpo-care-consistency-aware-reinforcement-learning-for-multimodal-reasoning.md","bb97fee97e78383b",{"html":25,"metadata":27880},{"headings":27881,"localImagePaths":27882,"remoteImagePaths":27883,"frontmatter":27884,"imagePaths":27885},[],[],[],{"title":27873,"description":27874,"summary":27874,"pubDate":25202,"source":25191,"url":27876,"thumbnail":25193},[],"2025-06-23-grpo-care-consistency-aware-reinforcement-learning-for-multimodal-reasoning.md","2025-06-23-guaranteed-prediction-sets-for-functional-surrogate-models",{"id":27887,"data":27889,"filePath":27894,"digest":27895,"rendered":27896,"legacyId":27903},{"title":27890,"description":27891,"summary":27891,"pubDate":27892,"source":25191,"url":27893,"thumbnail":25193},"Guaranteed prediction sets for functional surrogate models","arXiv:2501.18426v2 Announce Type: replace-cross Abstract: We propose a method for obtaining statistically guaranteed prediction sets for functional machine learning methods: surrogate models which map between function spaces, motivated by the need to build reliable PDE emulators. The method constructs nested prediction sets on a low-dimensional representation (an SVD) of the surrogate model's error, and then maps these sets to the prediction space using set-propagation techniques. This results in prediction sets for functional surrogate models with conformal prediction coverage guarantees. We use zonotopes as basis of the set construction, which allow an exact linear propagation and are closed under Cartesian products, making them well-suited to this high-dimensional problem. The method is model agnostic and can thus be applied to complex Sci-ML models, including Neural Operators, but also in simpler settings. We also introduce a technique to capture the truncation error of the SVD, preserving the guarantees of the method.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2501.18426","src/content/posts/2025-06-23-guaranteed-prediction-sets-for-functional-surrogate-models.md","767f39ab779beba9",{"html":25,"metadata":27897},{"headings":27898,"localImagePaths":27899,"remoteImagePaths":27900,"frontmatter":27901,"imagePaths":27902},[],[],[],{"title":27890,"description":27891,"summary":27891,"pubDate":25202,"source":25191,"url":27893,"thumbnail":25193},[],"2025-06-23-guaranteed-prediction-sets-for-functional-surrogate-models.md","2025-06-23-guided-absolutegrad-magnitude-of-gradients-matters-to-explanations-localization-and-saliency",{"id":27904,"data":27906,"filePath":27911,"digest":27912,"rendered":27913,"legacyId":27920},{"title":27907,"description":27908,"summary":27908,"pubDate":27909,"source":25191,"url":27910,"thumbnail":25193},"Guided AbsoluteGrad: Magnitude of Gradients Matters to Explanation's Localization and Saliency","arXiv:2404.15564v2 Announce Type: replace-cross Abstract: This paper proposes a new gradient-based XAI method called Guided AbsoluteGrad for saliency map explanations. We utilize both positive and negative gradient magnitudes and employ gradient variance to distinguish the important areas for noise deduction. We also introduce a novel evaluation metric named ReCover And Predict (RCAP), which considers the Localization and Visual Noise Level objectives of the explanations. We propose two propositions for these two objectives and prove the necessity of evaluating them. We evaluate Guided AbsoluteGrad with seven gradient-based XAI methods using the RCAP metric and other SOTA metrics in three case studies: (1) ImageNet dataset with ResNet50 model; (2) International Skin Imaging Collaboration (ISIC) dataset with EfficientNet model; (3) the Places365 dataset with DenseNet161 model. Our method surpasses other gradient-based approaches, showcasing the quality of enhanced saliency map explanations through gradient magnitude.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2404.15564","src/content/posts/2025-06-23-guided-absolutegrad-magnitude-of-gradients-matters-to-explanations-localization-and-saliency.md","898d7b42c5ed222c",{"html":25,"metadata":27914},{"headings":27915,"localImagePaths":27916,"remoteImagePaths":27917,"frontmatter":27918,"imagePaths":27919},[],[],[],{"title":27907,"description":27908,"summary":27908,"pubDate":25202,"source":25191,"url":27910,"thumbnail":25193},[],"2025-06-23-guided-absolutegrad-magnitude-of-gradients-matters-to-explanations-localization-and-saliency.md","2025-06-23-heterogeneous-modal-unsupervised-domain-adaptation-via-latent-space-bridging",{"id":27921,"data":27923,"filePath":27928,"digest":27929,"rendered":27930,"legacyId":27937},{"title":27924,"description":27925,"summary":27925,"pubDate":27926,"source":25191,"url":27927,"thumbnail":25193},"Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging","arXiv:2506.15971v1 Announce Type: cross Abstract: Unsupervised domain adaptation (UDA) methods effectively bridge domain gaps but become struggled when the source and target domains belong to entirely distinct modalities. To address this limitation, we propose a novel setting called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA), which enables knowledge transfer between completely different modalities by leveraging a bridge domain containing unlabeled samples from both modalities. To learn under the HMUDA setting, we propose Latent Space Bridging (LSB), a specialized framework designed for the semantic segmentation task. Specifically, LSB utilizes a dual-branch architecture, incorporating a feature consistency loss to align representations across modalities and a domain alignment loss to reduce discrepancies between class centroids across domains. Extensive experiments conducted on six benchmark datasets demonstrate that LSB achieves state-of-the-art performance.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15971","src/content/posts/2025-06-23-heterogeneous-modal-unsupervised-domain-adaptation-via-latent-space-bridging.md","c122be1a0df1efc2",{"html":25,"metadata":27931},{"headings":27932,"localImagePaths":27933,"remoteImagePaths":27934,"frontmatter":27935,"imagePaths":27936},[],[],[],{"title":27924,"description":27925,"summary":27925,"pubDate":25202,"source":25191,"url":27927,"thumbnail":25193},[],"2025-06-23-heterogeneous-modal-unsupervised-domain-adaptation-via-latent-space-bridging.md","2025-06-23-hierarchical-and-modular-network-on-non-prehensile-manipulation-in-general-environments",{"id":27938,"data":27940,"filePath":27945,"digest":27946,"rendered":27947,"legacyId":27954},{"title":27941,"description":27942,"summary":27942,"pubDate":27943,"source":25191,"url":27944,"thumbnail":25193},"Hierarchical and Modular Network on Non-prehensile Manipulation in General Environments","arXiv:2502.20843v2 Announce Type: replace-cross Abstract: For robots to operate in general environments like households, they must be able to perform non-prehensile manipulation actions such as toppling and rolling to manipulate ungraspable objects. However, prior works on non-prehensile manipulation cannot yet generalize across environments with diverse geometries. The main challenge lies in adapting to varying environmental constraints: within a cabinet, the robot must avoid walls and ceilings; to lift objects to the top of a step, the robot must account for the step's pose and extent. While deep reinforcement learning (RL) has demonstrated impressive success in non-prehensile manipulation, accounting for such variability presents a challenge for the generalist policy, as it must learn diverse strategies for each new combination of constraints. To address this, we propose a modular and reconfigurable architecture that adaptively reconfigures network modules based on task requirements. To capture the geometric variability in environments, we extend the contact-based object representation (CORN) to environment geometries, and propose a procedural algorithm for generating diverse environments to train our agent. Taken together, the resulting policy can zero-shot transfer to novel real-world environments and objects despite training entirely within a simulator. We additionally release a simulation-based benchmark featuring nine digital twins of real-world scenes with 353 objects to facilitate non-prehensile manipulation research in realistic domains.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2502.20843","src/content/posts/2025-06-23-hierarchical-and-modular-network-on-non-prehensile-manipulation-in-general-environments.md","e86310e6cae5d5be",{"html":25,"metadata":27948},{"headings":27949,"localImagePaths":27950,"remoteImagePaths":27951,"frontmatter":27952,"imagePaths":27953},[],[],[],{"title":27941,"description":27942,"summary":27942,"pubDate":25202,"source":25191,"url":27944,"thumbnail":25193},[],"2025-06-23-hierarchical-and-modular-network-on-non-prehensile-manipulation-in-general-environments.md","2025-06-23-human-like-forgetting-curves-in-deep-neural-networks",{"id":27955,"data":27957,"filePath":27962,"digest":27963,"rendered":27964,"legacyId":27971},{"title":27958,"description":27959,"summary":27959,"pubDate":27960,"source":25191,"url":27961,"thumbnail":25193},"Human-like Forgetting Curves in Deep Neural Networks","arXiv:2506.12034v2 Announce Type: replace-cross Abstract: This study bridges cognitive science and neural network design by examining whether artificial models exhibit human-like forgetting curves. Drawing upon Ebbinghaus' seminal work on memory decay and principles of spaced repetition, we propose a quantitative framework to measure information retention in neural networks. Our approach computes the recall probability by evaluating the similarity between a network's current hidden state and previously stored prototype representations. This retention metric facilitates the scheduling of review sessions, thereby mitigating catastrophic forgetting during deployment and enhancing training efficiency by prompting targeted reviews. Our experiments with Multi-Layer Perceptrons reveal human-like forgetting curves, with knowledge becoming increasingly robust through scheduled reviews. This alignment between neural network forgetting curves and established human memory models identifies neural networks as an architecture that naturally emulates human memory decay and can inform state-of-the-art continual learning algorithms.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.12034","src/content/posts/2025-06-23-human-like-forgetting-curves-in-deep-neural-networks.md","f16916c02390c661",{"html":25,"metadata":27965},{"headings":27966,"localImagePaths":27967,"remoteImagePaths":27968,"frontmatter":27969,"imagePaths":27970},[],[],[],{"title":27958,"description":27959,"summary":27959,"pubDate":25202,"source":25191,"url":27961,"thumbnail":25193},[],"2025-06-23-human-like-forgetting-curves-in-deep-neural-networks.md","2025-06-23-history-augmented-vision-language-models-for-frontier-based-zero-shot-object-navigation",{"id":27972,"data":27974,"filePath":27979,"digest":27980,"rendered":27981,"legacyId":27988},{"title":27975,"description":27976,"summary":27976,"pubDate":27977,"source":25191,"url":27978,"thumbnail":25193},"History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation","arXiv:2506.16623v1 Announce Type: cross Abstract: Object Goal Navigation (ObjectNav) challenges robots to find objects in unseen environments, demanding sophisticated reasoning. While Vision-Language Models (VLMs) show potential, current ObjectNav methods often employ them superficially, primarily using vision-language embeddings for object-scene similarity checks rather than leveraging deeper reasoning. This limits contextual understanding and leads to practical issues like repetitive navigation behaviors. This paper introduces a novel zero-shot ObjectNav framework that pioneers the use of dynamic, history-aware prompting to more deeply integrate VLM reasoning into frontier-based exploration. Our core innovation lies in providing the VLM with action history context, enabling it to generate semantic guidance scores for navigation actions while actively avoiding decision loops. We also introduce a VLM-assisted waypoint generation mechanism for refining the final approach to detected objects. Evaluated on the HM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and 24.8% Success weighted by Path Length (SPL). These results are comparable to state-of-the-art zero-shot methods, demonstrating the significant potential of our history-augmented VLM prompting strategy for more robust and context-aware robotic navigation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16623","src/content/posts/2025-06-23-history-augmented-vision-language-models-for-frontier-based-zero-shot-object-navigation.md","0c964b9e7d228baf",{"html":25,"metadata":27982},{"headings":27983,"localImagePaths":27984,"remoteImagePaths":27985,"frontmatter":27986,"imagePaths":27987},[],[],[],{"title":27975,"description":27976,"summary":27976,"pubDate":25202,"source":25191,"url":27978,"thumbnail":25193},[],"2025-06-23-history-augmented-vision-language-models-for-frontier-based-zero-shot-object-navigation.md","2025-06-23-human2locoman-learning-versatile-quadrupedal-manipulation-with-human-pretraining",{"id":27989,"data":27991,"filePath":27996,"digest":27997,"rendered":27998,"legacyId":28005},{"title":27992,"description":27993,"summary":27993,"pubDate":27994,"source":25191,"url":27995,"thumbnail":25193},"Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining","arXiv:2506.16475v1 Announce Type: cross Abstract: Quadrupedal robots have demonstrated impressive locomotion capabilities in complex environments, but equipping them with autonomous versatile manipulation skills in a scalable way remains a significant challenge. In this work, we introduce a cross-embodiment imitation learning system for quadrupedal manipulation, leveraging data collected from both humans and LocoMan, a quadruped equipped with multiple manipulation modes. Specifically, we develop a teleoperation and data collection pipeline, which unifies and modularizes the observation and action spaces of the human and the robot. To effectively leverage the collected data, we propose an efficient modularized architecture that supports co-training and pretraining on structured modality-aligned data across different embodiments. Additionally, we construct the first manipulation dataset for the LocoMan robot, covering various household tasks in both unimanual and bimanual modes, supplemented by a corresponding human dataset. We validate our system on six real-world manipulation tasks, where it achieves an average success rate improvement of 41.9% overall and 79.7% under out-of-distribution (OOD) settings compared to the baseline. Pretraining with human data contributes a 38.6% success rate improvement overall and 82.7% under OOD settings, enabling consistently better performance with only half the amount of robot data. Our code, hardware, and data are open-sourced at: https://human2bots.github.io.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16475","src/content/posts/2025-06-23-human2locoman-learning-versatile-quadrupedal-manipulation-with-human-pretraining.md","9cdd46bb9158b4a1",{"html":25,"metadata":27999},{"headings":28000,"localImagePaths":28001,"remoteImagePaths":28002,"frontmatter":28003,"imagePaths":28004},[],[],[],{"title":27992,"description":27993,"summary":27993,"pubDate":25202,"source":25191,"url":27995,"thumbnail":25193},[],"2025-06-23-human2locoman-learning-versatile-quadrupedal-manipulation-with-human-pretraining.md","2025-06-23-how-to-train-your-text-to-image-model-evaluating-design-choices-for-synthetic-training-captions",{"id":28006,"data":28008,"filePath":28013,"digest":28014,"rendered":28015,"legacyId":28022},{"title":28009,"description":28010,"summary":28010,"pubDate":28011,"source":25191,"url":28012,"thumbnail":25193},"How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions","arXiv:2506.16679v1 Announce Type: cross Abstract: Training data is at the core of any successful text-to-image models. The quality and descriptiveness of image text are crucial to a model's performance. Given the noisiness and inconsistency in web-scraped datasets, recent works shifted towards synthetic training captions. While this setup is generally believed to produce more capable models, current literature does not provide any insights into its design choices. This study closes this gap by systematically investigating how different synthetic captioning strategies impact the downstream performance of text-to-image models. Our experiments demonstrate that dense, high-quality captions enhance text alignment but may introduce trade-offs in output aesthetics and diversity. Conversely, captions of randomized lengths yield balanced improvements across aesthetics and alignment without compromising sample diversity. We also demonstrate that varying caption distributions introduce significant shifts in the output bias of a trained model. Our findings underscore the importance of caption design in achieving optimal model performance and provide practical insights for more effective training data strategies in text-to-image generation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16679","src/content/posts/2025-06-23-how-to-train-your-text-to-image-model-evaluating-design-choices-for-synthetic-training-captions.md","b2154a506d343f50",{"html":25,"metadata":28016},{"headings":28017,"localImagePaths":28018,"remoteImagePaths":28019,"frontmatter":28020,"imagePaths":28021},[],[],[],{"title":28009,"description":28010,"summary":28010,"pubDate":25202,"source":25191,"url":28012,"thumbnail":25193},[],"2025-06-23-how-to-train-your-text-to-image-model-evaluating-design-choices-for-synthetic-training-captions.md","2025-06-23-hybrid-attention-network-for-accurate-breast-tumor-segmentation-in-ultrasound-images",{"id":28023,"data":28025,"filePath":28030,"digest":28031,"rendered":28032,"legacyId":28039},{"title":28026,"description":28027,"summary":28027,"pubDate":28028,"source":25191,"url":28029,"thumbnail":25193},"Hybrid Attention Network for Accurate Breast Tumor Segmentation in Ultrasound Images","arXiv:2506.16592v1 Announce Type: cross Abstract: Breast ultrasound imaging is a valuable tool for early breast cancer detection, but automated tumor segmentation is challenging due to inherent noise, variations in scale of lesions, and fuzzy boundaries. To address these challenges, we propose a novel hybrid attention-based network for lesion segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in the encoder part for robust feature extraction with a multi-branch attention-enhanced decoder tailored for breast ultrasound images. The bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE), and Scaled Dot-Product Attention (SDPA) to learn global context, spatial relationships, and relative positional features. The Spatial Feature Enhancement Block (SFEB) is embedded at skip connections to refine and enhance spatial features, enabling the network to focus more effectively on tumor regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap metrics, enhancing robustness to class imbalance and irregular tumor shapes. Experiments on public datasets demonstrate that our method outperforms existing approaches, highlighting its potential to assist radiologists in early and accurate breast cancer diagnosis.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16592","src/content/posts/2025-06-23-hybrid-attention-network-for-accurate-breast-tumor-segmentation-in-ultrasound-images.md","b7eb375c59e6047c",{"html":25,"metadata":28033},{"headings":28034,"localImagePaths":28035,"remoteImagePaths":28036,"frontmatter":28037,"imagePaths":28038},[],[],[],{"title":28026,"description":28027,"summary":28027,"pubDate":25202,"source":25191,"url":28029,"thumbnail":25193},[],"2025-06-23-hybrid-attention-network-for-accurate-breast-tumor-segmentation-in-ultrasound-images.md","2025-06-23-hunyuan3d-25-towards-high-fidelity-3d-assets-generation-with-ultimate-details",{"id":28040,"data":28042,"filePath":28047,"digest":28048,"rendered":28049,"legacyId":28056},{"title":28043,"description":28044,"summary":28044,"pubDate":28045,"source":25191,"url":28046,"thumbnail":25193},"Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details","arXiv:2506.16504v1 Announce Type: cross Abstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16504","src/content/posts/2025-06-23-hunyuan3d-25-towards-high-fidelity-3d-assets-generation-with-ultimate-details.md","cbb253dd6c10c237",{"html":25,"metadata":28050},{"headings":28051,"localImagePaths":28052,"remoteImagePaths":28053,"frontmatter":28054,"imagePaths":28055},[],[],[],{"title":28043,"description":28044,"summary":28044,"pubDate":25202,"source":25191,"url":28046,"thumbnail":25193},[],"2025-06-23-hunyuan3d-25-towards-high-fidelity-3d-assets-generation-with-ultimate-details.md","2025-06-23-identifiability-of-deep-polynomial-neural-networks",{"id":28057,"data":28059,"filePath":28064,"digest":28065,"rendered":28066,"legacyId":28073},{"title":28060,"description":28061,"summary":28061,"pubDate":28062,"source":25191,"url":28063,"thumbnail":25193},"Identifiability of Deep Polynomial Neural Networks","arXiv:2506.17093v1 Announce Type: cross Abstract: Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric structure. However, their identifiability -- a key property for ensuring interpretability -- remains poorly understood. In this work, we present a comprehensive analysis of the identifiability of deep PNNs, including architectures with and without bias terms. Our results reveal an intricate interplay between activation degrees and layer widths in achieving identifiability. As special cases, we show that architectures with non-increasing layer widths are generically identifiable under mild conditions, while encoder-decoder networks are identifiable when the decoder widths do not grow too rapidly. Our proofs are constructive and center on a connection between deep PNNs and low-rank tensor decompositions, and Kruskal-type uniqueness theorems. This yields both generic conditions determined by the architecture, and effective conditions that depend on the network's parameters. We also settle an open conjecture on the expected dimension of PNN's neurovarieties, and provide new bounds on the activation degrees required for it to reach its maximum.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17093","src/content/posts/2025-06-23-identifiability-of-deep-polynomial-neural-networks.md","f77f4fa10c103179",{"html":25,"metadata":28067},{"headings":28068,"localImagePaths":28069,"remoteImagePaths":28070,"frontmatter":28071,"imagePaths":28072},[],[],[],{"title":28060,"description":28061,"summary":28061,"pubDate":25202,"source":25191,"url":28063,"thumbnail":25193},[],"2025-06-23-identifiability-of-deep-polynomial-neural-networks.md","2025-06-23-ignition-phase-standard-training-for-fast-adversarial-robustness",{"id":28074,"data":28076,"filePath":28081,"digest":28082,"rendered":28083,"legacyId":28090},{"title":28077,"description":28078,"summary":28078,"pubDate":28079,"source":25191,"url":28080,"thumbnail":25193},"Ignition Phase : Standard Training for Fast Adversarial Robustness","arXiv:2506.15685v1 Announce Type: cross Abstract: Adversarial Training (AT) is a cornerstone defense, but many variants overlook foundational feature representations by primarily focusing on stronger attack generation. We introduce Adversarial Evolution Training (AET), a simple yet powerful framework that strategically prepends an Empirical Risk Minimization (ERM) phase to conventional AT. We hypothesize this initial ERM phase cultivates a favorable feature manifold, enabling more efficient and effective robustness acquisition. Empirically, AET achieves comparable or superior robustness more rapidly, improves clean accuracy, and cuts training costs by 8-25%. Its effectiveness is shown across multiple datasets, architectures, and when augmenting established AT methods. Our findings underscore the impact of feature pre-conditioning via standard training for developing more efficient, principled robust defenses. Code is available in the supplementary material.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15685","src/content/posts/2025-06-23-ignition-phase-standard-training-for-fast-adversarial-robustness.md","4950d5dfee6f2148",{"html":25,"metadata":28084},{"headings":28085,"localImagePaths":28086,"remoteImagePaths":28087,"frontmatter":28088,"imagePaths":28089},[],[],[],{"title":28077,"description":28078,"summary":28078,"pubDate":25202,"source":25191,"url":28080,"thumbnail":25193},[],"2025-06-23-ignition-phase-standard-training-for-fast-adversarial-robustness.md","2025-06-23-improved-exploration-in-gflownets-via-enhanced-epistemic-neural-networks",{"id":28091,"data":28093,"filePath":28098,"digest":28099,"rendered":28100,"legacyId":28107},{"title":28094,"description":28095,"summary":28095,"pubDate":28096,"source":25191,"url":28097,"thumbnail":25193},"Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks","arXiv:2506.16313v1 Announce Type: cross Abstract: Efficiently identifying the right trajectories for training remains an open problem in GFlowNets. To address this, it is essential to prioritize exploration in regions of the state space where the reward distribution has not been sufficiently learned. This calls for uncertainty-driven exploration, in other words, the agent should be aware of what it does not know. This attribute can be measured by joint predictions, which are particularly important for combinatorial and sequential decision problems. In this research, we integrate epistemic neural networks (ENN) with the conventional architecture of GFlowNets to enable more efficient joint predictions and better uncertainty quantification, thereby improving exploration and the identification of optimal trajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the baseline method in GFlownets and evaluated in grid environments and structured sequence generation in various settings, demonstrating both its efficacy and efficiency.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16313","src/content/posts/2025-06-23-improved-exploration-in-gflownets-via-enhanced-epistemic-neural-networks.md","aa7f5768d5625bf5",{"html":25,"metadata":28101},{"headings":28102,"localImagePaths":28103,"remoteImagePaths":28104,"frontmatter":28105,"imagePaths":28106},[],[],[],{"title":28094,"description":28095,"summary":28095,"pubDate":25202,"source":25191,"url":28097,"thumbnail":25193},[],"2025-06-23-improved-exploration-in-gflownets-via-enhanced-epistemic-neural-networks.md","2025-06-23-improved-intelligibility-of-dysarthric-speech-using-conditional-flow-matching",{"id":28108,"data":28110,"filePath":28115,"digest":28116,"rendered":28117,"legacyId":28124},{"title":28111,"description":28112,"summary":28112,"pubDate":28113,"source":25191,"url":28114,"thumbnail":25193},"Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching","arXiv:2506.16127v1 Announce Type: cross Abstract: Dysarthria is a neurological disorder that significantly impairs speech intelligibility, often rendering affected individuals unable to communicate effectively. This necessitates the development of robust dysarthric-to-regular speech conversion techniques. In this work, we investigate the utility and limitations of self-supervised learning (SSL) features and their quantized representations as an alternative to mel-spectrograms for speech generation. Additionally, we explore methods to mitigate speaker variability by generating clean speech in a single-speaker voice using features extracted from WavLM. To this end, we propose a fully non-autoregressive approach that leverages Conditional Flow Matching (CFM) with Diffusion Transformers to learn a direct mapping from dysarthric to clean speech. Our findings highlight the effectiveness of discrete acoustic units in improving intelligibility while achieving faster convergence compared to traditional mel-spectrogram-based approaches.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16127","src/content/posts/2025-06-23-improved-intelligibility-of-dysarthric-speech-using-conditional-flow-matching.md","8da1fb67d2bb6980",{"html":25,"metadata":28118},{"headings":28119,"localImagePaths":28120,"remoteImagePaths":28121,"frontmatter":28122,"imagePaths":28123},[],[],[],{"title":28111,"description":28112,"summary":28112,"pubDate":25202,"source":25191,"url":28114,"thumbnail":25193},[],"2025-06-23-improved-intelligibility-of-dysarthric-speech-using-conditional-flow-matching.md","2025-06-23-incentivizing-high-quality-participation-from-federated-learning-agents",{"id":28125,"data":28127,"filePath":28132,"digest":28133,"rendered":28134,"legacyId":28141},{"title":28128,"description":28129,"summary":28129,"pubDate":28130,"source":25191,"url":28131,"thumbnail":25193},"Incentivizing High-quality Participation From Federated Learning Agents","arXiv:2506.16731v1 Announce Type: new Abstract: Federated learning (FL) provides a promising paradigm for facilitating collaboration between multiple clients that jointly learn a global model without directly sharing their local data. However, existing research suffers from two caveats: 1) From the perspective of agents, voluntary and unselfish participation is often assumed. But self-interested agents may opt out of the system or provide low-quality contributions without proper incentives; 2) From the mechanism designer's perspective, the aggregated models can be unsatisfactory as the existing game-theoretical federated learning approach for data collection ignores the potential heterogeneous effort caused by contributed data. To alleviate above challenges, we propose an incentive-aware framework for agent participation that considers data heterogeneity to accelerate the convergence process. Specifically, we first introduce the notion of Wasserstein distance to explicitly illustrate the heterogeneous effort and reformulate the existing upper bound of convergence. To induce truthful reporting from agents, we analyze and measure the generalization error gap of any two agents by leveraging the peer prediction mechanism to develop score functions. We further present a two-stage Stackelberg game model that formalizes the process and examines the existence of equilibrium. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed mechanism.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16731","src/content/posts/2025-06-23-incentivizing-high-quality-participation-from-federated-learning-agents.md","535d1a2b28d73bfc",{"html":25,"metadata":28135},{"headings":28136,"localImagePaths":28137,"remoteImagePaths":28138,"frontmatter":28139,"imagePaths":28140},[],[],[],{"title":28128,"description":28129,"summary":28129,"pubDate":25202,"source":25191,"url":28131,"thumbnail":25193},[],"2025-06-23-incentivizing-high-quality-participation-from-federated-learning-agents.md","2025-06-23-incivility-and-rigidity-the-risks-of-fine-tuning-llms-for-political-argumentation",{"id":28142,"data":28144,"filePath":28149,"digest":28150,"rendered":28151,"legacyId":28158},{"title":28145,"description":28146,"summary":28146,"pubDate":28147,"source":25191,"url":28148,"thumbnail":25193},"Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political Argumentation","arXiv:2411.16813v3 Announce Type: replace-cross Abstract: The incivility prevalent on platforms like Twitter (now X) and Reddit poses a challenge for developing AI systems that can support productive and rhetorically sound political argumentation. In this study, we report experiments with GPT-3.5 Turbo, fine-tuned on two contrasting datasets of political discussions: high-variance, high-incivility Twitter replies to U.S. Congress, and low-variance, low-incivility posts from Reddit's r/ChangeMyView. We systematically evaluate how these data sources and prompting strategies shape the rhetorical framing and deliberative quality of model-generated arguments. Our results show that Reddit-finetuned models produce safer but rhetorically rigid arguments, while cross-platform fine-tuning amplifies toxicity. Prompting reduces specific toxic behaviors, such as personal attacks, but fails to fully mitigate the influence of high-incivility training data. We introduce and validate a rhetorical evaluation rubric and provide practical guidelines for deploying LLMs in content authoring, moderation, and deliberation support.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2411.16813","src/content/posts/2025-06-23-incivility-and-rigidity-the-risks-of-fine-tuning-llms-for-political-argumentation.md","9ef9944c3f65098f",{"html":25,"metadata":28152},{"headings":28153,"localImagePaths":28154,"remoteImagePaths":28155,"frontmatter":28156,"imagePaths":28157},[],[],[],{"title":28145,"description":28146,"summary":28146,"pubDate":25202,"source":25191,"url":28148,"thumbnail":25193},[],"2025-06-23-incivility-and-rigidity-the-risks-of-fine-tuning-llms-for-political-argumentation.md","2025-06-23-info-coevolution-an-efficient-framework-for-data-model-coevolution",{"id":28159,"data":28161,"filePath":28166,"digest":28167,"rendered":28168,"legacyId":28175},{"title":28162,"description":28163,"summary":28163,"pubDate":28164,"source":25191,"url":28165,"thumbnail":25193},"Info-Coevolution: An Efficient Framework for Data Model Coevolution","arXiv:2506.08070v2 Announce Type: replace-cross Abstract: Machine learning relies heavily on data, yet the continuous growth of real-world data poses challenges for efficient dataset construction and training. A fundamental yet unsolved question is: given our current model and data, does a new data (sample/batch) need annotation/learning? Conventional approaches retain all available data, leading to non-optimal data and training efficiency. Active learning aims to reduce data redundancy by selecting a subset of samples to annotate, while it increases pipeline complexity and introduces bias. In this work, we propose Info-Coevolution, a novel framework that efficiently enables models and data to coevolve through online selective annotation with no bias. Leveraging task-specific models (and open-source models), it selectively annotates and integrates online and web data to improve datasets efficiently. For real-world datasets like ImageNet-1K, Info-Coevolution reduces annotation and training costs by 32% without performance loss. It is able to automatically give the saving ratio without tuning the ratio. It can further reduce the annotation ratio to 50% with semi-supervised learning. We also explore retrieval-based dataset enhancement using unlabeled open-source data. Code is available at https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.08070","src/content/posts/2025-06-23-info-coevolution-an-efficient-framework-for-data-model-coevolution.md","b810170875292b3f",{"html":25,"metadata":28169},{"headings":28170,"localImagePaths":28171,"remoteImagePaths":28172,"frontmatter":28173,"imagePaths":28174},[],[],[],{"title":28162,"description":28163,"summary":28163,"pubDate":25202,"source":25191,"url":28165,"thumbnail":25193},[],"2025-06-23-info-coevolution-an-efficient-framework-for-data-model-coevolution.md","2025-06-23-informaticaがaiエージェントに本気-日本に向け挑戦はリスクだが待っていては追い付けない",{"id":28176,"data":28178,"filePath":28184,"digest":28185,"rendered":28186,"legacyId":28194},{"title":28179,"description":28180,"summary":28180,"pubDate":28181,"source":24944,"url":28182,"thumbnail":28183},"InformaticaがAIエージェントに本気　日本に向け「挑戦はリスク、だが待っていては追い付けない」","米Informaticaは年次イベント「Informatica World 2025」で、AIエージェントに関連する取り組みの大幅強化を発表した。日本に向けては、失敗を恐れず挑戦するよう訴えかけている。",["Date","2025-06-23T01:00:00.000Z"],"https://www.itmedia.co.jp/enterprise/articles/2506/23/news033.html","https://image.itmedia.co.jp/enterprise/articles/2506/23/cover_news033.jpg","src/content/posts/2025-06-23-informaticaがaiエージェントに本気-日本に向け挑戦はリスクだが待っていては追い付けない.md","7959c827bb8076fa",{"html":25,"metadata":28187},{"headings":28188,"localImagePaths":28189,"remoteImagePaths":28190,"frontmatter":28191,"imagePaths":28193},[],[],[],{"title":28179,"description":28180,"summary":28180,"pubDate":28192,"source":24944,"url":28182,"thumbnail":28183},"Mon, 23 Jun 2025 10:00:00 +0900",[],"2025-06-23-informaticaがaiエージェントに本気-日本に向け挑戦はリスクだが待っていては追い付けない.md","2025-06-23-infrastructure-for-ai-agents",{"id":28195,"data":28197,"filePath":28202,"digest":28203,"rendered":28204,"legacyId":28211},{"title":28198,"description":28199,"summary":28199,"pubDate":28200,"source":25191,"url":28201,"thumbnail":25193},"Infrastructure for AI Agents","arXiv:2501.10114v3 Announce Type: replace Abstract: AI agents plan and execute interactions in open-ended environments. For example, OpenAI's Operator can use a web browser to do product comparisons and buy online goods. Much research on making agents useful and safe focuses on directly modifying their behaviour, such as by training them to follow user instructions. Direct behavioural modifications are useful, but do not fully address how heterogeneous agents will interact with each other and other actors. Rather, we will need external protocols and systems to shape such interactions. For instance, agents will need more efficient protocols to communicate with each other and form agreements. Attributing an agent's actions to a particular human or other legal entity can help to establish trust, and also disincentivize misuse. Given this motivation, we propose the concept of textbf{agent infrastructure}: technical systems and shared protocols external to agents that are designed to mediate and influence their interactions with and impacts on their environments. Just as the Internet relies on protocols like HTTPS, our work argues that agent infrastructure will be similarly indispensable to ecosystems of agents. We identify three functions for agent infrastructure: 1) attributing actions, properties, and other information to specific agents, their users, or other actors; 2) shaping agents' interactions; and 3) detecting and remedying harmful actions from agents. We provide an incomplete catalog of research directions for such functions. For each direction, we include analysis of use cases, infrastructure adoption, relationships to existing (internet) infrastructure, limitations, and open questions. Making progress on agent infrastructure can prepare society for the adoption of more advanced agents.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2501.10114","src/content/posts/2025-06-23-infrastructure-for-ai-agents.md","7ce50198e7445e69",{"html":25,"metadata":28205},{"headings":28206,"localImagePaths":28207,"remoteImagePaths":28208,"frontmatter":28209,"imagePaths":28210},[],[],[],{"title":28198,"description":28199,"summary":28199,"pubDate":25202,"source":25191,"url":28201,"thumbnail":25193},[],"2025-06-23-infrastructure-for-ai-agents.md","2025-06-23-instituto-de-telecomunicaccoes-at-iwslt-2025-aligning-small-scale-speech-and-language-models-for-speech-to-text-learning",{"id":28212,"data":28214,"filePath":28219,"digest":28220,"rendered":28221,"legacyId":28228},{"title":28215,"description":28216,"summary":28216,"pubDate":28217,"source":25191,"url":28218,"thumbnail":25193},"Instituto de Telecomunicac{c}~oes at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning","arXiv:2506.17019v1 Announce Type: cross Abstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on Instruction Following Speech Processing. We submit results for the Short Track, i.e., speech recognition, translation, and spoken question answering. Our model is a unified speech-to-text model that integrates a pre-trained continuous speech encoder and text decoder through a first phase of modality alignment and a second phase of instruction fine-tuning. Crucially, we focus on using small-scale language model backbones (\u003C 2B) and restrict to high-quality, CC-BY data along with synthetic data generation to supplement existing resources.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17019","src/content/posts/2025-06-23-instituto-de-telecomunicaccoes-at-iwslt-2025-aligning-small-scale-speech-and-language-models-for-speech-to-text-learning.md","dc76add6251c7b62",{"html":25,"metadata":28222},{"headings":28223,"localImagePaths":28224,"remoteImagePaths":28225,"frontmatter":28226,"imagePaths":28227},[],[],[],{"title":28215,"description":28216,"summary":28216,"pubDate":25202,"source":25191,"url":28218,"thumbnail":25193},[],"2025-06-23-instituto-de-telecomunicaccoes-at-iwslt-2025-aligning-small-scale-speech-and-language-models-for-speech-to-text-learning.md","2025-06-23-infomaticaがaiエージェントに本気-日本に向け挑戦はリスクだが待っていては追い付けない",{"id":28229,"data":28231,"filePath":28234,"digest":28235,"rendered":28236,"legacyId":28243},{"title":28232,"description":28180,"summary":28180,"pubDate":28233,"source":24944,"url":28182,"thumbnail":28183},"InfomaticaがAIエージェントに本気　日本に向け「挑戦はリスク、だが待っていては追い付けない」",["Date","2025-06-23T01:00:00.000Z"],"src/content/posts/2025-06-23-infomaticaがaiエージェントに本気-日本に向け挑戦はリスクだが待っていては追い付けない.md","f1cbf08dcf9f51cd",{"html":25,"metadata":28237},{"headings":28238,"localImagePaths":28239,"remoteImagePaths":28240,"frontmatter":28241,"imagePaths":28242},[],[],[],{"title":28232,"description":28180,"summary":28180,"pubDate":28192,"source":24944,"url":28182,"thumbnail":28183},[],"2025-06-23-infomaticaがaiエージェントに本気-日本に向け挑戦はリスクだが待っていては追い付けない.md","2025-06-23-interpretable-low-dimensional-modeling-of-spatiotemporal-agent-states-for-decision-making-in-football-tactics",{"id":28244,"data":28246,"filePath":28251,"digest":28252,"rendered":28253,"legacyId":28260},{"title":28247,"description":28248,"summary":28248,"pubDate":28249,"source":25191,"url":28250,"thumbnail":25193},"Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics","arXiv:2506.16696v1 Announce Type: new Abstract: Understanding football tactics is crucial for managers and analysts. Previous research has proposed models based on spatial and kinematic equations, but these are computationally expensive. Also, Reinforcement learning approaches use player positions and velocities but lack interpretability and require large datasets. Rule-based models align with expert knowledge but have not fully considered all players' states. This study explores whether low-dimensional, rule-based models using spatiotemporal data can effectively capture football tactics. Our approach defines interpretable state variables for both the ball-holder and potential pass receivers, based on criteria that explore options like passing. Through discussions with a manager, we identified key variables representing the game state. We then used StatsBomb event data and SkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost model to predict pass success. The analysis revealed that the distance between the player and the ball, as well as the player's space score, were key factors in determining successful passes. Our interpretable low-dimensional modeling facilitates tactical analysis through the use of intuitive variables and provides practical value as a tool to support decision-making in football.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16696","src/content/posts/2025-06-23-interpretable-low-dimensional-modeling-of-spatiotemporal-agent-states-for-decision-making-in-football-tactics.md","41fb8040dab7028a",{"html":25,"metadata":28254},{"headings":28255,"localImagePaths":28256,"remoteImagePaths":28257,"frontmatter":28258,"imagePaths":28259},[],[],[],{"title":28247,"description":28248,"summary":28248,"pubDate":25202,"source":25191,"url":28250,"thumbnail":25193},[],"2025-06-23-interpretable-low-dimensional-modeling-of-spatiotemporal-agent-states-for-decision-making-in-football-tactics.md","2025-06-23-is-bench-evaluating-interactive-safety-of-vlm-driven-embodied-agents-in-daily-household-tasks",{"id":28261,"data":28263,"filePath":28268,"digest":28269,"rendered":28270,"legacyId":28277},{"title":28264,"description":28265,"summary":28265,"pubDate":28266,"source":25191,"url":28267,"thumbnail":25193},"IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks","arXiv:2506.16402v1 Announce Type: new Abstract: Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16402","src/content/posts/2025-06-23-is-bench-evaluating-interactive-safety-of-vlm-driven-embodied-agents-in-daily-household-tasks.md","4631b340dab4affd",{"html":25,"metadata":28271},{"headings":28272,"localImagePaths":28273,"remoteImagePaths":28274,"frontmatter":28275,"imagePaths":28276},[],[],[],{"title":28264,"description":28265,"summary":28265,"pubDate":25202,"source":25191,"url":28267,"thumbnail":25193},[],"2025-06-23-is-bench-evaluating-interactive-safety-of-vlm-driven-embodied-agents-in-daily-household-tasks.md","2025-06-23-jethics-japanese-ethics-understanding-evaluation-dataset",{"id":28278,"data":28280,"filePath":28285,"digest":28286,"rendered":28287,"legacyId":28294},{"title":28281,"description":28282,"summary":28282,"pubDate":28283,"source":25191,"url":28284,"thumbnail":25193},"JETHICS: Japanese Ethics Understanding Evaluation Dataset","arXiv:2506.16187v1 Announce Type: cross Abstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics understanding of AI models. JETHICS contains 78K examples and is built by following the construction methods of the existing English ETHICS dataset. It includes four categories based normative theories and concepts from ethics and political philosophy; and one representing commonsense morality. Our evaluation experiments on non-proprietary large language models (LLMs) and on GPT-4o reveal that even GPT-4o achieves only an average score of about 0.7, while the best-performing Japanese LLM attains around 0.5, indicating a relatively large room for improvement in current LLMs.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16187","src/content/posts/2025-06-23-jethics-japanese-ethics-understanding-evaluation-dataset.md","f35fbb48df8b0306",{"html":25,"metadata":28288},{"headings":28289,"localImagePaths":28290,"remoteImagePaths":28291,"frontmatter":28292,"imagePaths":28293},[],[],[],{"title":28281,"description":28282,"summary":28282,"pubDate":25202,"source":25191,"url":28284,"thumbnail":25193},[],"2025-06-23-jethics-japanese-ethics-understanding-evaluation-dataset.md","2025-06-23-joint-tensor-train-parameterization-for-efficient-and-expressive-low-rank-adaptation",{"id":28295,"data":28297,"filePath":28302,"digest":28303,"rendered":28304,"legacyId":28311},{"title":28298,"description":28299,"summary":28299,"pubDate":28300,"source":25191,"url":28301,"thumbnail":25193},"Joint Tensor-Train Parameterization for Efficient and Expressive Low-Rank Adaptation","arXiv:2506.16456v1 Announce Type: cross Abstract: Low-Rank Adaptation (LoRA) is widely recognized for its parameter-efficient fine-tuning of large-scale neural models. However, standard LoRA independently optimizes low-rank matrices, which inherently limits its expressivity and generalization capabilities. While classical tensor-train (TT) decomposition can be separately employed on individual LoRA matrices, this work demonstrates that the classical TT-based approach neither significantly improves parameter efficiency nor achieves substantial performance gains. This paper proposes TensorGuide, a novel tensor-train-guided adaptation framework to overcome these limitations. TensorGuide generates two correlated low-rank LoRA matrices through a unified TT structure driven by controlled Gaussian noise. The resulting joint TT representation inherently provides structured, low-rank adaptations, significantly enhancing expressivity, generalization, and parameter efficiency without increasing the number of trainable parameters. Theoretically, we justify these improvements through neural tangent kernel analyses, demonstrating superior optimization dynamics and enhanced generalization. Extensive experiments on quantum dot classification and GPT-2 fine-tuning benchmarks demonstrate that TensorGuide-based LoRA consistently outperforms standard LoRA and TT-LoRA, achieving improved accuracy and scalability with fewer parameters.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16456","src/content/posts/2025-06-23-joint-tensor-train-parameterization-for-efficient-and-expressive-low-rank-adaptation.md","1686839c12a4c0bc",{"html":25,"metadata":28305},{"headings":28306,"localImagePaths":28307,"remoteImagePaths":28308,"frontmatter":28309,"imagePaths":28310},[],[],[],{"title":28298,"description":28299,"summary":28299,"pubDate":25202,"source":25191,"url":28301,"thumbnail":25193},[],"2025-06-23-joint-tensor-train-parameterization-for-efficient-and-expressive-low-rank-adaptation.md","2025-06-23-kg-fgnn-knowledge-guided-gnn-foundation-model-for-fertilisation-oriented-soil-ghg-flux-prediction",{"id":28312,"data":28314,"filePath":28319,"digest":28320,"rendered":28321,"legacyId":28328},{"title":28315,"description":28316,"summary":28316,"pubDate":28317,"source":25191,"url":28318,"thumbnail":25193},"KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction","arXiv:2506.15896v1 Announce Type: cross Abstract: Precision soil greenhouse gas (GHG) flux prediction is essential in agricultural systems for assessing environmental impacts, developing emission mitigation strategies and promoting sustainable agriculture. Due to the lack of advanced sensor and network technologies on majority of farms, there are challenges in obtaining comprehensive and diverse agricultural data. As a result, the scarcity of agricultural data seriously obstructs the application of machine learning approaches in precision soil GHG flux prediction. This research proposes a knowledge-guided graph neural network framework that addresses the above challenges by integrating knowledge embedded in an agricultural process-based model and graph neural network techniques. Specifically, we utilise the agricultural process-based model to simulate and generate multi-dimensional agricultural datasets for 47 countries that cover a wide range of agricultural variables. To extract key agricultural features and integrate correlations among agricultural features in the prediction process, we propose a machine learning framework that integrates the autoencoder and multi-target multi-graph based graph neural networks, which utilises the autoencoder to selectively extract significant agricultural features from the agricultural process-based model simulation data and the graph neural network to integrate correlations among agricultural features for accurately predict fertilisation-oriented soil GHG fluxes. Comprehensive experiments were conducted with both the agricultural simulation dataset and real-world agricultural dataset to evaluate the proposed approach in comparison with well-known baseline and state-of-the-art regression methods. The results demonstrate that our proposed approach provides superior accuracy and stability in fertilisation-oriented soil GHG prediction.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15896","src/content/posts/2025-06-23-kg-fgnn-knowledge-guided-gnn-foundation-model-for-fertilisation-oriented-soil-ghg-flux-prediction.md","5b0b031f663701c3",{"html":25,"metadata":28322},{"headings":28323,"localImagePaths":28324,"remoteImagePaths":28325,"frontmatter":28326,"imagePaths":28327},[],[],[],{"title":28315,"description":28316,"summary":28316,"pubDate":25202,"source":25191,"url":28318,"thumbnail":25193},[],"2025-06-23-kg-fgnn-knowledge-guided-gnn-foundation-model-for-fertilisation-oriented-soil-ghg-flux-prediction.md","2025-06-23-kvcache-cache-in-the-wild-characterizing-and-optimizing-kvcache-cache-at-a-large-cloud-provider",{"id":28329,"data":28331,"filePath":28336,"digest":28337,"rendered":28338,"legacyId":28345},{"title":28332,"description":28333,"summary":28333,"pubDate":28334,"source":25191,"url":28335,"thumbnail":25193},"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider","arXiv:2506.02634v3 Announce Type: replace-cross Abstract: Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.02634","src/content/posts/2025-06-23-kvcache-cache-in-the-wild-characterizing-and-optimizing-kvcache-cache-at-a-large-cloud-provider.md","62b5f9e7b368ae05",{"html":25,"metadata":28339},{"headings":28340,"localImagePaths":28341,"remoteImagePaths":28342,"frontmatter":28343,"imagePaths":28344},[],[],[],{"title":28332,"description":28333,"summary":28333,"pubDate":25202,"source":25191,"url":28335,"thumbnail":25193},[],"2025-06-23-kvcache-cache-in-the-wild-characterizing-and-optimizing-kvcache-cache-at-a-large-cloud-provider.md","2025-06-23-laecips-large-vision-model-assisted-adaptive-edge-cloud-collaboration-for-iot-based-embodied-intelligence-system",{"id":28346,"data":28348,"filePath":28353,"digest":28354,"rendered":28355,"legacyId":28362},{"title":28349,"description":28350,"summary":28350,"pubDate":28351,"source":25191,"url":28352,"thumbnail":25193},"LAECIPS: Large Vision Model Assisted Adaptive Edge-Cloud Collaboration for IoT-based Embodied Intelligence System","arXiv:2404.10498v2 Announce Type: replace Abstract: Embodied intelligence (EI) enables manufacturing systems to flexibly perceive, reason, adapt, and operate within dynamic shop floor environments. In smart manufacturing, a representative EI scenario is robotic visual inspection, where industrial robots must accurately inspect components on rapidly changing, heterogeneous production lines. This task requires both high inference accuracy especially for uncommon defects and low latency to match production speeds, despite evolving lighting, part geometries, and surface conditions. To meet these needs, we propose LAECIPS, a large vision model-assisted adaptive edge-cloud collaboration framework for IoT-based embodied intelligence systems. LAECIPS decouples large vision models in the cloud from lightweight models on the edge, enabling plug-and-play model adaptation and continual learning. Through a hard input mining-based inference strategy, LAECIPS routes complex and uncertain inspection cases to the cloud while handling routine tasks at the edge, achieving both high accuracy and low latency. Experiments conducted on a real-world robotic semantic segmentation system for visual inspection demonstrate significant improvements in accuracy, processing latency, and communication overhead compared to state-of-the-art methods. LAECIPS provides a practical and scalable foundation for embodied intelligence in smart manufacturing, especially in adaptive robotic inspection and quality control scenarios.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2404.10498","src/content/posts/2025-06-23-laecips-large-vision-model-assisted-adaptive-edge-cloud-collaboration-for-iot-based-embodied-intelligence-system.md","9b96f733e2319ad2",{"html":25,"metadata":28356},{"headings":28357,"localImagePaths":28358,"remoteImagePaths":28359,"frontmatter":28360,"imagePaths":28361},[],[],[],{"title":28349,"description":28350,"summary":28350,"pubDate":25202,"source":25191,"url":28352,"thumbnail":25193},[],"2025-06-23-laecips-large-vision-model-assisted-adaptive-edge-cloud-collaboration-for-iot-based-embodied-intelligence-system.md","2025-06-23-language-bottleneck-models-a-framework-for-interpretable-knowledge-tracing-and-beyond",{"id":28363,"data":28365,"filePath":28370,"digest":28371,"rendered":28372,"legacyId":28379},{"title":28366,"description":28367,"summary":28367,"pubDate":28368,"source":25191,"url":28369,"thumbnail":25193},"Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond","arXiv:2506.16982v1 Announce Type: cross Abstract: Accurately assessing student knowledge is critical for effective education, yet traditional Knowledge Tracing (KT) methods rely on opaque latent embeddings, limiting interpretability. Even LLM-based approaches generate direct predictions or summaries that may hallucinate without any accuracy guarantees. We recast KT as an inverse problem: learning the minimum natural-language summary that makes past answers explainable and future answers predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM that writes an interpretable knowledge summary and a frozen decoder LLM that must reconstruct and predict student responses using only that summary text. By constraining all predictive information to pass through a short natural-language bottleneck, LBMs ensure that the summary contains accurate information while remaining human-interpretable. Experiments on synthetic arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the accuracy of state-of-the-art KT and direct LLM methods while requiring orders-of-magnitude fewer student trajectories. We demonstrate that training the encoder with group-relative policy optimization, using downstream decoding accuracy as a reward signal, effectively improves summary quality.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16982","src/content/posts/2025-06-23-language-bottleneck-models-a-framework-for-interpretable-knowledge-tracing-and-beyond.md","ad388c9a0f9474e5",{"html":25,"metadata":28373},{"headings":28374,"localImagePaths":28375,"remoteImagePaths":28376,"frontmatter":28377,"imagePaths":28378},[],[],[],{"title":28366,"description":28367,"summary":28367,"pubDate":25202,"source":25191,"url":28369,"thumbnail":25193},[],"2025-06-23-language-bottleneck-models-a-framework-for-interpretable-knowledge-tracing-and-beyond.md","2025-06-23-language-informed-synthesis-of-rational-agent-models-for-grounded-theory-of-mind-reasoning-on-the-fly",{"id":28380,"data":28382,"filePath":28387,"digest":28388,"rendered":28389,"legacyId":28396},{"title":28383,"description":28384,"summary":28384,"pubDate":28385,"source":25191,"url":28386,"thumbnail":25193},"Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly","arXiv:2506.16755v1 Announce Type: cross Abstract: Drawing real world social inferences usually requires taking into account information from multiple modalities. Language is a particularly powerful source of information in social settings, especially in novel situations where language can provide both abstract information about the environment dynamics and concrete specifics about an agent that cannot be easily visually observed. In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a framework for drawing context-specific social inferences that integrate linguistic and visual inputs. LIRAS frames multimodal social reasoning as a process of constructing structured but situation-specific agent and environment representations - leveraging multimodal language models to parse language and visual inputs into unified symbolic representations, over which a Bayesian inverse planning engine can be run to produce granular probabilistic judgments. On a range of existing and new social reasoning tasks derived from cognitive science experiments, we find that our model (instantiated with a comparatively lightweight VLM) outperforms ablations and state-of-the-art models in capturing human judgments across all domains.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16755","src/content/posts/2025-06-23-language-informed-synthesis-of-rational-agent-models-for-grounded-theory-of-mind-reasoning-on-the-fly.md","e067bace86bd4d93",{"html":25,"metadata":28390},{"headings":28391,"localImagePaths":28392,"remoteImagePaths":28393,"frontmatter":28394,"imagePaths":28395},[],[],[],{"title":28383,"description":28384,"summary":28384,"pubDate":25202,"source":25191,"url":28386,"thumbnail":25193},[],"2025-06-23-language-informed-synthesis-of-rational-agent-models-for-grounded-theory-of-mind-reasoning-on-the-fly.md","2025-06-23-language-models-can-perform-single-utterance-self-correction-of-perturbed-reasoning",{"id":28397,"data":28399,"filePath":28404,"digest":28405,"rendered":28406,"legacyId":28413},{"title":28400,"description":28401,"summary":28401,"pubDate":28402,"source":25191,"url":28403,"thumbnail":25193},"Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning","arXiv:2506.15894v1 Announce Type: cross Abstract: Large Language Models (LLMs) have demonstrated impressive mathematical reasoning capabilities, yet their performance remains brittle to minor variations in problem description and prompting strategy. Furthermore, reasoning is vulnerable to sampling-induced errors which autoregressive models must primarily address using self-correction via additionally-generated tokens. To better understand self-correction capabilities of recent models, we conduct experiments measuring models' ability to self-correct synthetic perturbations introduced into their Chain of Thought (CoT) reasoning. We observe robust single-utterance intrinsic self-correction behavior across a range of open-weight models and datasets, ranging from subtle, implicit corrections to explicit acknowledgments and corrections of errors. Our findings suggest that LLMs, including those not finetuned for long CoT, may possess stronger intrinsic self-correction capabilities than commonly shown in the literature. The presence of this ability suggests that recent 'reasoning' model work involves amplification of traits already meaningfully present in models.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15894","src/content/posts/2025-06-23-language-models-can-perform-single-utterance-self-correction-of-perturbed-reasoning.md","f997cfad729d37b9",{"html":25,"metadata":28407},{"headings":28408,"localImagePaths":28409,"remoteImagePaths":28410,"frontmatter":28411,"imagePaths":28412},[],[],[],{"title":28400,"description":28401,"summary":28401,"pubDate":25202,"source":25191,"url":28403,"thumbnail":25193},[],"2025-06-23-language-models-can-perform-single-utterance-self-correction-of-perturbed-reasoning.md","2025-06-23-large-language-models-are-near-optimal-decision-makers-with-a-non-human-learning-behavior",{"id":28414,"data":28416,"filePath":28421,"digest":28422,"rendered":28423,"legacyId":28430},{"title":28417,"description":28418,"summary":28418,"pubDate":28419,"source":25191,"url":28420,"thumbnail":25193},"Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior","arXiv:2506.16163v1 Announce Type: new Abstract: Human decision-making belongs to the foundation of our society and civilization, but we are on the verge of a future where much of it will be delegated to artificial intelligence. The arrival of Large Language Models (LLMs) has transformed the nature and scope of AI-supported decision-making; however, the process by which they learn to make decisions, compared to humans, remains poorly understood. In this study, we examined the decision-making behavior of five leading LLMs across three core dimensions of real-world decision-making: uncertainty, risk, and set-shifting. Using three well-established experimental psychology tasks designed to probe these dimensions, we benchmarked LLMs against 360 newly recruited human participants. Across all tasks, LLMs often outperformed humans, approaching near-optimal performance. Moreover, the processes underlying their decisions diverged fundamentally from those of humans. On the one hand, our finding demonstrates the ability of LLMs to manage uncertainty, calibrate risk, and adapt to changes. On the other hand, this disparity highlights the risks of relying on them as substitutes for human judgment, calling for further inquiry.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16163","src/content/posts/2025-06-23-large-language-models-are-near-optimal-decision-makers-with-a-non-human-learning-behavior.md","3569b027ac595c83",{"html":25,"metadata":28424},{"headings":28425,"localImagePaths":28426,"remoteImagePaths":28427,"frontmatter":28428,"imagePaths":28429},[],[],[],{"title":28417,"description":28418,"summary":28418,"pubDate":25202,"source":25191,"url":28420,"thumbnail":25193},[],"2025-06-23-large-language-models-are-near-optimal-decision-makers-with-a-non-human-learning-behavior.md","2025-06-23-large-language-models-as-psychological-simulators-a-methodological-guide",{"id":28431,"data":28433,"filePath":28438,"digest":28439,"rendered":28440,"legacyId":28447},{"title":28434,"description":28435,"summary":28435,"pubDate":28436,"source":25191,"url":28437,"thumbnail":25193},"Large Language Models as Psychological Simulators: A Methodological Guide","arXiv:2506.16702v1 Announce Type: cross Abstract: Large language models (LLMs) offer emerging opportunities for psychological and behavioral research, but methodological guidance is lacking. This article provides a framework for using LLMs as psychological simulators across two primary applications: simulating roles and personas to explore diverse contexts, and serving as computational models to investigate cognitive processes. For simulation, we present methods for developing psychologically grounded personas that move beyond demographic categories, with strategies for validation against human data and use cases ranging from studying inaccessible populations to prototyping research instruments. For cognitive modeling, we synthesize emerging approaches for probing internal representations, methodological advances in causal interventions, and strategies for relating model behavior to human cognition. We address overarching challenges including prompt sensitivity, temporal limitations from training data cutoffs, and ethical considerations that extend beyond traditional human subjects review. Throughout, we emphasize the need for transparency about model capabilities and constraints. Together, this framework integrates emerging empirical evidence about LLM performance--including systematic biases, cultural limitations, and prompt brittleness--to help researchers wrangle these challenges and leverage the unique capabilities of LLMs in psychological research.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16702","src/content/posts/2025-06-23-large-language-models-as-psychological-simulators-a-methodological-guide.md","fb630a68a2e80383",{"html":25,"metadata":28441},{"headings":28442,"localImagePaths":28443,"remoteImagePaths":28444,"frontmatter":28445,"imagePaths":28446},[],[],[],{"title":28434,"description":28435,"summary":28435,"pubDate":25202,"source":25191,"url":28437,"thumbnail":25193},[],"2025-06-23-large-language-models-as-psychological-simulators-a-methodological-guide.md","2025-06-23-lars-latent-reasoning-skills-for-chain-of-thought-reasoning",{"id":28448,"data":28450,"filePath":28455,"digest":28456,"rendered":28457,"legacyId":28464},{"title":28451,"description":28452,"summary":28452,"pubDate":28453,"source":25191,"url":28454,"thumbnail":25193},"LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning","arXiv:2312.04684v4 Announce Type: replace-cross Abstract: Chain-of-thought (CoT) prompting is a popular in-context learning (ICL) approach for large language models (LLMs), especially when tackling complex reasoning tasks. Traditional ICL approaches construct prompts using examples that contain questions similar to the input question. However, CoT prompting, which includes crucial intermediate reasoning steps (rationales) within its examples, necessitates selecting examples based on these rationales rather than the questions themselves. Existing methods require human experts or pre-trained LLMs to describe the skill, a high-level abstraction of rationales, to guide the selection. These methods, however, are often costly and difficult to scale. Instead, this paper introduces a new approach named Latent Reasoning Skills (LaRS) that employs unsupervised learning to create a latent space representation of rationales, with a latent variable called a reasoning skill. Concurrently, LaRS learns a reasoning policy to determine the required reasoning skill for a given question. Then the ICL examples are selected by aligning the reasoning skills between past examples and the question. This approach is theoretically grounded and compute-efficient, eliminating the need for auxiliary LLM inference or manual prompt design. Empirical results demonstrate that LaRS consistently outperforms SOTA skill-based selection methods, processing example banks four times faster, reducing LLM inferences during the selection stage by half, and showing greater robustness to sub-optimal example banks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2312.04684","src/content/posts/2025-06-23-lars-latent-reasoning-skills-for-chain-of-thought-reasoning.md","ce5e526e42e238e9",{"html":25,"metadata":28458},{"headings":28459,"localImagePaths":28460,"remoteImagePaths":28461,"frontmatter":28462,"imagePaths":28463},[],[],[],{"title":28451,"description":28452,"summary":28452,"pubDate":25202,"source":25191,"url":28454,"thumbnail":25193},[],"2025-06-23-lars-latent-reasoning-skills-for-chain-of-thought-reasoning.md","2025-06-23-latent-concept-disentanglement-in-transformer-based-language-models",{"id":28465,"data":28467,"filePath":28472,"digest":28473,"rendered":28474,"legacyId":28481},{"title":28468,"description":28469,"summary":28469,"pubDate":28470,"source":25191,"url":28471,"thumbnail":25193},"Latent Concept Disentanglement in Transformer-based Language Models","arXiv:2506.16975v1 Announce Type: cross Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a new task, they seem to grasp not only the goal of the task but also core, latent concepts in the demonstration examples. This begs the question of whether transformers represent latent structures as part of their computation or whether they take shortcuts to solve the problem. Prior mechanistic work on ICL does not address this question because it does not sufficiently examine the relationship between the learned representation and the latent concept, and the considered problem settings often involve only single-step reasoning. In this work, we examine how transformers disentangle and use latent concepts. We show that in 2-hop reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. In tasks parameterized by a continuous latent concept, we find low-dimensional subspaces in the representation space where the geometry mimics the underlying parameterization. Together, these results refine our understanding of ICL and the representation of transformers, and they provide evidence for highly localized structures in the model that disentangle latent concepts in ICL tasks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16975","src/content/posts/2025-06-23-latent-concept-disentanglement-in-transformer-based-language-models.md","f48d85c933ce3f7d",{"html":25,"metadata":28475},{"headings":28476,"localImagePaths":28477,"remoteImagePaths":28478,"frontmatter":28479,"imagePaths":28480},[],[],[],{"title":28468,"description":28469,"summary":28469,"pubDate":25202,"source":25191,"url":28471,"thumbnail":25193},[],"2025-06-23-latent-concept-disentanglement-in-transformer-based-language-models.md","2025-06-23-latent-noise-injection-for-private-and-statistically-aligned-synthetic-data-generation",{"id":28482,"data":28484,"filePath":28489,"digest":28490,"rendered":28491,"legacyId":28498},{"title":28485,"description":28486,"summary":28486,"pubDate":28487,"source":25191,"url":28488,"thumbnail":25193},"Latent Noise Injection for Private and Statistically Aligned Synthetic Data Generation","arXiv:2506.16636v1 Announce Type: cross Abstract: Synthetic Data Generation has become essential for scalable, privacy-preserving statistical analysis. While standard approaches based on generative models, such as Normalizing Flows, have been widely used, they often suffer from slow convergence in high-dimensional settings, frequently converging more slowly than the canonical $1/sqrt{n}$ rate when approximating the true data distribution. To overcome these limitations, we propose a Latent Noise Injection method using Masked Autoregressive Flows (MAF). Instead of directly sampling from the trained model, our method perturbs each data point in the latent space and maps it back to the data domain. This construction preserves a one to one correspondence between observed and synthetic data, enabling synthetic outputs that closely reflect the underlying distribution, particularly in challenging high-dimensional regimes where traditional sampling struggles. Our procedure satisfies local $(epsilon, delta)$-differential privacy and introduces a single perturbation parameter to control the privacy-utility trade-off. Although estimators based on individual synthetic datasets may converge slowly, we show both theoretically and empirically that aggregating across $K$ studies in a meta analysis framework restores classical efficiency and yields consistent, reliable inference. We demonstrate that with a well-calibrated perturbation parameter, Latent Noise Injection achieves strong statistical alignment with the original data and robustness against membership inference attacks. These results position our method as a compelling alternative to conventional flow-based sampling for synthetic data sharing in decentralized and privacy-sensitive domains, such as biomedical research.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16636","src/content/posts/2025-06-23-latent-noise-injection-for-private-and-statistically-aligned-synthetic-data-generation.md","2f4e684256aba554",{"html":25,"metadata":28492},{"headings":28493,"localImagePaths":28494,"remoteImagePaths":28495,"frontmatter":28496,"imagePaths":28497},[],[],[],{"title":28485,"description":28486,"summary":28486,"pubDate":25202,"source":25191,"url":28488,"thumbnail":25193},[],"2025-06-23-latent-noise-injection-for-private-and-statistically-aligned-synthetic-data-generation.md","2025-06-23-learn-from-the-past-fast-sparse-indexing-for-large-language-model-decoding",{"id":28499,"data":28501,"filePath":28506,"digest":28507,"rendered":28508,"legacyId":28515},{"title":28502,"description":28503,"summary":28503,"pubDate":28504,"source":25191,"url":28505,"thumbnail":25193},"Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding","arXiv:2506.15704v1 Announce Type: cross Abstract: As large language models (LLMs) continue to support increasingly longer contexts, the memory demand for key-value (KV) caches during decoding grows rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe bandwidth. Sparse attention mechanisms alleviate this issue by computing attention weights only for selected key-value pairs. However, their indexing computation typically requires traversing all key vectors, resulting in significant computational and data transfer overhead. To reduce the cost of index retrieval, existing methods often treat each decoding step as an independent process, failing to exploit the temporal correlations embedded in historical decoding information. To this end, we propose LFPS(Learn From the Past for Sparse Indexing), an acceleration method that dynamically constructs sparse indexing candidates based on historical attention patterns. LFPS captures two prevalent trends in decoder attention -vertical patterns (attending to fixed positions) and slash patterns (attending to relative positions) -and incorporates a positional expansion strategy to effectively predict the Top-k indices for the current step. We validate LFPS on challenging long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as the base model. Experimental results show that LFPS achieves up to 22.8$times$ speedup over full attention and 9.6$times$ speedup over exact Top-k retrieval on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively, while preserving generation accuracy. These results demonstrate that LFPS offers a practical and efficient solution for decoding optimization in long-context LLM inference.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15704","src/content/posts/2025-06-23-learn-from-the-past-fast-sparse-indexing-for-large-language-model-decoding.md","1a5dde7979c64dd7",{"html":25,"metadata":28509},{"headings":28510,"localImagePaths":28511,"remoteImagePaths":28512,"frontmatter":28513,"imagePaths":28514},[],[],[],{"title":28502,"description":28503,"summary":28503,"pubDate":25202,"source":25191,"url":28505,"thumbnail":25193},[],"2025-06-23-learn-from-the-past-fast-sparse-indexing-for-large-language-model-decoding.md","2025-06-23-learnalign-reasoning-data-selection-for-reinforcement-learning-in-large-language-models-based-on-improved-gradient-alignment",{"id":28516,"data":28518,"filePath":28523,"digest":28524,"rendered":28525,"legacyId":28532},{"title":28519,"description":28520,"summary":28520,"pubDate":28521,"source":25191,"url":28522,"thumbnail":25193},"LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large Language Models Based on Improved Gradient Alignment","arXiv:2506.11480v2 Announce Type: replace-cross Abstract: Reinforcement learning (RL) has become a key technique for enhancing LLMs' reasoning abilities, yet its data inefficiency remains a major bottleneck. To address this critical yet challenging issue, we present a novel gradient-alignment-based method, named LearnAlign, which intelligently selects the learnable and representative training reasoning data for RL post-training. To overcome the issue of response-length bias in gradient norms, we introduce the data learnability based on the success rate, which can indicate the learning potential of each data point. Experiments across three mathematical reasoning benchmarks demonstrate that our method significantly reduces training data requirements while achieving minor performance degradation or even improving performance compared to full-data training. For example, it reduces data requirements by up to 1,000 data points with better performance (77.53%) than that on the full dataset on GSM8K benchmark (77.04%). Furthermore, we show its effectiveness in the staged RL setting. This work provides valuable insights into data-efficient RL post-training and establishes a foundation for future research in optimizing reasoning data selection. To facilitate future work, we will release code.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.11480","src/content/posts/2025-06-23-learnalign-reasoning-data-selection-for-reinforcement-learning-in-large-language-models-based-on-improved-gradient-alignment.md","15d845b15f59b84a",{"html":25,"metadata":28526},{"headings":28527,"localImagePaths":28528,"remoteImagePaths":28529,"frontmatter":28530,"imagePaths":28531},[],[],[],{"title":28519,"description":28520,"summary":28520,"pubDate":25202,"source":25191,"url":28522,"thumbnail":25193},[],"2025-06-23-learnalign-reasoning-data-selection-for-reinforcement-learning-in-large-language-models-based-on-improved-gradient-alignment.md","2025-06-23-learning-dexterous-object-handover",{"id":28533,"data":28535,"filePath":28540,"digest":28541,"rendered":28542,"legacyId":28549},{"title":28536,"description":28537,"summary":28537,"pubDate":28538,"source":25191,"url":28539,"thumbnail":25193},"Learning Dexterous Object Handover","arXiv:2506.16822v1 Announce Type: cross Abstract: Object handover is an important skill that we use daily when interacting with other humans. To deploy robots in collaborative setting, like houses, being able to receive and handing over objects safely and efficiently becomes a crucial skill. In this work, we demonstrate the use of Reinforcement Learning (RL) for dexterous object handover between two multi-finger hands. Key to this task is the use of a novel reward function based on dual quaternions to minimize the rotation distance, which outperforms other rotation representations such as Euler and rotation matrices. The robustness of the trained policy is experimentally evaluated by testing w.r.t. objects that are not included in the training distribution, and perturbations during the handover process. The results demonstrate that the trained policy successfully perform this task, achieving a total success rate of 94% in the best-case scenario after 100 experiments, thereby showing the robustness of our policy with novel objects. In addition, the best-case performance of the policy decreases by only 13.8% when the other robot moves during the handover, proving that our policy is also robust to this type of perturbation, which is common in real-world object handovers.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16822","src/content/posts/2025-06-23-learning-dexterous-object-handover.md","48556081bee8f688",{"html":25,"metadata":28543},{"headings":28544,"localImagePaths":28545,"remoteImagePaths":28546,"frontmatter":28547,"imagePaths":28548},[],[],[],{"title":28536,"description":28537,"summary":28537,"pubDate":25202,"source":25191,"url":28539,"thumbnail":25193},[],"2025-06-23-learning-dexterous-object-handover.md","2025-06-23-learning-dynamics-in-continual-pre-training-for-large-language-models",{"id":28550,"data":28552,"filePath":28557,"digest":28558,"rendered":28559,"legacyId":28566},{"title":28553,"description":28554,"summary":28554,"pubDate":28555,"source":25191,"url":28556,"thumbnail":25193},"Learning Dynamics in Continual Pre-Training for Large Language Models","arXiv:2505.07796v2 Announce Type: replace-cross Abstract: Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.07796","src/content/posts/2025-06-23-learning-dynamics-in-continual-pre-training-for-large-language-models.md","0c52a30c8c018f27",{"html":25,"metadata":28560},{"headings":28561,"localImagePaths":28562,"remoteImagePaths":28563,"frontmatter":28564,"imagePaths":28565},[],[],[],{"title":28553,"description":28554,"summary":28554,"pubDate":25202,"source":25191,"url":28556,"thumbnail":25193},[],"2025-06-23-learning-dynamics-in-continual-pre-training-for-large-language-models.md","2025-06-23-learning-from-m-tuple-dominant-positive-and-unlabeled-data",{"id":28567,"data":28569,"filePath":28574,"digest":28575,"rendered":28576,"legacyId":28583},{"title":28570,"description":28571,"summary":28571,"pubDate":28572,"source":25191,"url":28573,"thumbnail":25193},"Learning from M-Tuple Dominant Positive and Unlabeled Data","arXiv:2506.15686v1 Announce Type: cross Abstract: Label Proportion Learning (LLP) addresses the classification problem where multiple instances are grouped into bags and each bag contains information about the proportion of each class. However, in practical applications, obtaining precise supervisory information regarding the proportion of instances in a specific class is challenging. To better align with real-world application scenarios and effectively leverage the proportional constraints of instances within tuples, this paper proposes a generalized learning framework emph{MDPU}. Specifically, we first mathematically model the distribution of instances within tuples of arbitrary size, under the constraint that the number of positive instances is no less than that of negative instances. Then we derive an unbiased risk estimator that satisfies risk consistency based on the empirical risk minimization (ERM) method. To mitigate the inevitable overfitting issue during training, a risk correction method is introduced, leading to the development of a corrected risk estimator. The generalization error bounds of the unbiased risk estimator theoretically demonstrate the consistency of the proposed method. Extensive experiments on multiple datasets and comparisons with other relevant baseline methods comprehensively validate the effectiveness of the proposed learning framework.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15686","src/content/posts/2025-06-23-learning-from-m-tuple-dominant-positive-and-unlabeled-data.md","e70e62cdc1c2329d",{"html":25,"metadata":28577},{"headings":28578,"localImagePaths":28579,"remoteImagePaths":28580,"frontmatter":28581,"imagePaths":28582},[],[],[],{"title":28570,"description":28571,"summary":28571,"pubDate":25202,"source":25191,"url":28573,"thumbnail":25193},[],"2025-06-23-learning-from-m-tuple-dominant-positive-and-unlabeled-data.md","2025-06-23-learning-from-other-domains-to-advance-ai-evaluation-and-testing",{"id":28584,"data":28586,"filePath":28591,"digest":28592,"rendered":28593,"legacyId":28601},{"title":28587,"description":28588,"summary":28588,"pubDate":28589,"source":23065,"url":28590,"thumbnail":23067},"Learning from other domains to advance AI evaluation and testing","\u003Cp>As generative AI becomes more capable and widely deployed, familiar questions from the governance of other transformative technologies have resurfaced. Which opportunities, capabilities, risks, and impacts should be evaluated? Who should conduct evaluations, and at what stages of the technology lifecycle? What tests or measurements should be used? And how can we know if the [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://www.microsoft.com/en-us/research/blog/learning-from-other-domains-to-advance-ai-evaluation-and-testing/'>Learning from other domains to advance AI evaluation and testing\u003C/a> appeared first on \u003Ca href='https://www.microsoft.com/en-us/research'>Microsoft Research\u003C/a>.\u003C/p>",["Date","2025-06-23T16:35:06.000Z"],"https://www.microsoft.com/en-us/research/blog/learning-from-other-domains-to-advance-ai-evaluation-and-testing/","src/content/posts/2025-06-23-learning-from-other-domains-to-advance-ai-evaluation-and-testing.md","4a21d4f13478f505",{"html":25,"metadata":28594},{"headings":28595,"localImagePaths":28596,"remoteImagePaths":28597,"frontmatter":28598,"imagePaths":28600},[],[],[],{"title":28587,"description":28588,"summary":28588,"pubDate":28599,"source":23065,"url":28590,"thumbnail":23067},"Mon, 23 Jun 2025 16:35:06 +0000",[],"2025-06-23-learning-from-other-domains-to-advance-ai-evaluation-and-testing.md","2025-06-23-learning-multi-branch-cooperation-for-enhanced-click-through-rate-prediction-at-taobao",{"id":28602,"data":28604,"filePath":28609,"digest":28610,"rendered":28611,"legacyId":28618},{"title":28605,"description":28606,"summary":28606,"pubDate":28607,"source":25191,"url":28608,"thumbnail":25193},"Learning Multi-Branch Cooperation for Enhanced Click-Through Rate Prediction at Taobao","arXiv:2411.13057v2 Announce Type: replace-cross Abstract: Existing click-through rate (CTR) prediction works have studied the role of feature interaction through a variety of techniques. Each interaction technique exhibits its own strength, and solely using one type usually constrains the model's capability to capture the complex feature relationships, especially for industrial data with enormous input feature fields. Recent research shows that effective CTR models often combine an MLP network with a dedicated feature interaction network in a two-parallel structure. However, the interplay and cooperative dynamics between different streams or branches remain under-researched. In this work, we introduce a novel Multi-Branch Cooperation Network (MBCnet) which enables multiple branch networks to collaborate with each other for better complex feature interaction modeling. Specifically, MBCnet consists of three branches: the Extensible Feature Grouping and Crossing (EFGC) branch that promotes the model's memorization ability of specific feature fields, the low rank Cross Net branch and Deep branch to enhance explicit and implicit feature crossing for improved generalization. Among these branches, a novel cooperation scheme is proposed based on two principles: Branch co-teaching and moderate differentiation. Branch co-teaching encourages well-learned branches to support poorly-learned ones on specific training samples. Moderate differentiation advocates branches to maintain a reasonable level of difference in their feature representations on the same inputs. This cooperation strategy improves learning through mutual knowledge sharing and boosts the discovery of diverse feature interactions across branches. Experiments on large-scale industrial datasets and online A/B test at Taobao app demonstrate MBCnet's superior performance, delivering a 0.09 point increase in CTR, 1.49% growth in deals, and 1.62% rise in GMV. Core codes are available online.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2411.13057","src/content/posts/2025-06-23-learning-multi-branch-cooperation-for-enhanced-click-through-rate-prediction-at-taobao.md","06c65678ca97f281",{"html":25,"metadata":28612},{"headings":28613,"localImagePaths":28614,"remoteImagePaths":28615,"frontmatter":28616,"imagePaths":28617},[],[],[],{"title":28605,"description":28606,"summary":28606,"pubDate":25202,"source":25191,"url":28608,"thumbnail":25193},[],"2025-06-23-learning-multi-branch-cooperation-for-enhanced-click-through-rate-prediction-at-taobao.md","2025-06-23-learning-multi-scale-spatial-frequency-features-for-image-denoising",{"id":28619,"data":28621,"filePath":28626,"digest":28627,"rendered":28628,"legacyId":28635},{"title":28622,"description":28623,"summary":28623,"pubDate":28624,"source":25191,"url":28625,"thumbnail":25193},"Learning Multi-scale Spatial-frequency Features for Image Denoising","arXiv:2506.16307v1 Announce Type: cross Abstract: Recent advancements in multi-scale architectures have demonstrated exceptional performance in image denoising tasks. However, existing architectures mainly depends on a fixed single-input single-output Unet architecture, ignoring the multi-scale representations of pixel level. In addition, previous methods treat the frequency domain uniformly, ignoring the different characteristics of high-frequency and low-frequency noise. In this paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for image denoising. We use image pyramid inputs to restore noise-free results from low-resolution images. In order to realize the interaction of high-frequency and low-frequency information, we design an adaptive spatial-frequency learning unit (ASFU), where a learnable mask is used to separate the information into high-frequency and low-frequency components. In the skip connections, we design a global feature fusion block to enhance the features at different scales. Extensive experiments on both synthetic and real noisy image datasets verify the effectiveness of MADNet compared with current state-of-the-art denoising approaches.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16307","src/content/posts/2025-06-23-learning-multi-scale-spatial-frequency-features-for-image-denoising.md","57e2a03e88718674",{"html":25,"metadata":28629},{"headings":28630,"localImagePaths":28631,"remoteImagePaths":28632,"frontmatter":28633,"imagePaths":28634},[],[],[],{"title":28622,"description":28623,"summary":28623,"pubDate":25202,"source":25191,"url":28625,"thumbnail":25193},[],"2025-06-23-learning-multi-scale-spatial-frequency-features-for-image-denoising.md","2025-06-23-learning-to-route-llms-with-confidence-tokens",{"id":28636,"data":28638,"filePath":28643,"digest":28644,"rendered":28645,"legacyId":28652},{"title":28639,"description":28640,"summary":28640,"pubDate":28641,"source":25191,"url":28642,"thumbnail":25193},"Learning to Route LLMs with Confidence Tokens","arXiv:2410.13284v3 Announce Type: replace-cross Abstract: Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-Reflection with Error-based Feedback (Self-REF), a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2410.13284","src/content/posts/2025-06-23-learning-to-route-llms-with-confidence-tokens.md","2ac4d5f4d8d87b6f",{"html":25,"metadata":28646},{"headings":28647,"localImagePaths":28648,"remoteImagePaths":28649,"frontmatter":28650,"imagePaths":28651},[],[],[],{"title":28639,"description":28640,"summary":28640,"pubDate":25202,"source":25191,"url":28642,"thumbnail":25193},[],"2025-06-23-learning-to-route-llms-with-confidence-tokens.md","2025-06-23-lego-puzzles-how-good-are-mllms-at-multi-step-spatial-reasoning",{"id":28653,"data":28655,"filePath":28660,"digest":28661,"rendered":28662,"legacyId":28669},{"title":28656,"description":28657,"summary":28657,"pubDate":28658,"source":25191,"url":28659,"thumbnail":25193},"LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?","arXiv:2503.19990v3 Announce Type: replace Abstract: Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of 20 state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90% accuracy. Furthermore, based on LEGO-Puzzles, we design generation tasks to investigate whether MLLMs can transfer their spatial understanding and reasoning abilities to image generation. Our experiments show that only GPT-4o and Gemini-2.0-Flash exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2503.19990","src/content/posts/2025-06-23-lego-puzzles-how-good-are-mllms-at-multi-step-spatial-reasoning.md","28f2ac6751071d7c",{"html":25,"metadata":28663},{"headings":28664,"localImagePaths":28665,"remoteImagePaths":28666,"frontmatter":28667,"imagePaths":28668},[],[],[],{"title":28656,"description":28657,"summary":28657,"pubDate":25202,"source":25191,"url":28659,"thumbnail":25193},[],"2025-06-23-lego-puzzles-how-good-are-mllms-at-multi-step-spatial-reasoning.md","2025-06-23-leveraging-influence-functions-for-resampling-data-in-physics-informed-neural-networks",{"id":28670,"data":28672,"filePath":28677,"digest":28678,"rendered":28679,"legacyId":28686},{"title":28673,"description":28674,"summary":28674,"pubDate":28675,"source":25191,"url":28676,"thumbnail":25193},"Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks","arXiv:2506.16443v1 Announce Type: cross Abstract: Physics-informed neural networks (PINNs) offer a powerful approach to solving partial differential equations (PDEs), which are ubiquitous in the quantitative sciences. Applied to both forward and inverse problems across various scientific domains, PINNs have recently emerged as a valuable tool in the field of scientific machine learning. A key aspect of their training is that the data -- spatio-temporal points sampled from the PDE's input domain -- are readily available. Influence functions, a tool from the field of explainable AI (XAI), approximate the effect of individual training points on the model, enhancing interpretability. In the present work, we explore the application of influence function-based sampling approaches for the training data. Our results indicate that such targeted resampling based on data attribution methods has the potential to enhance prediction accuracy in physics-informed neural networks, demonstrating a practical application of an XAI method in PINN training.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16443","src/content/posts/2025-06-23-leveraging-influence-functions-for-resampling-data-in-physics-informed-neural-networks.md","8f4f717b0ac1b52b",{"html":25,"metadata":28680},{"headings":28681,"localImagePaths":28682,"remoteImagePaths":28683,"frontmatter":28684,"imagePaths":28685},[],[],[],{"title":28673,"description":28674,"summary":28674,"pubDate":25202,"source":25191,"url":28676,"thumbnail":25193},[],"2025-06-23-leveraging-influence-functions-for-resampling-data-in-physics-informed-neural-networks.md","2025-06-23-leverb-humanoid-whole-body-control-with-latent-vision-language-instruction",{"id":28687,"data":28689,"filePath":28694,"digest":28695,"rendered":28696,"legacyId":28703},{"title":28690,"description":28691,"summary":28691,"pubDate":28692,"source":25191,"url":28693,"thumbnail":25193},"LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction","arXiv:2506.13751v2 Announce Type: replace-cross Abstract: Vision-language-action (VLA) models have demonstrated strong semantic understanding and zero-shot generalization, yet most existing systems assume an accurate low-level controller with hand-crafted action 'vocabulary' such as end-effector pose or root velocity. This assumption confines prior work to quasi-static tasks and precludes the agile, whole-body behaviors required by humanoid whole-body control (WBC) tasks. To capture this gap in the literature, we start by introducing the first sim-to-real-ready, vision-language, closed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10 categories. We then propose LeVERB: Latent Vision-Language-Encoded Robot Behavior, a hierarchical latent instruction-following framework for humanoid vision-language WBC, the first of its kind. At the top level, a vision-language policy learns a latent action vocabulary from synthetically rendered kinematic demonstrations; at the low level, a reinforcement-learned WBC policy consumes these latent verbs to generate dynamics-level commands. In our benchmark, LeVERB can zero-shot attain a 80% success rate on simple visual navigation tasks, and 58.5% success rate overall, outperforming naive hierarchical whole-body VLA implementation by 7.8 times.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.13751","src/content/posts/2025-06-23-leverb-humanoid-whole-body-control-with-latent-vision-language-instruction.md","62ce24083f73e8c3",{"html":25,"metadata":28697},{"headings":28698,"localImagePaths":28699,"remoteImagePaths":28700,"frontmatter":28701,"imagePaths":28702},[],[],[],{"title":28690,"description":28691,"summary":28691,"pubDate":25202,"source":25191,"url":28693,"thumbnail":25193},[],"2025-06-23-leverb-humanoid-whole-body-control-with-latent-vision-language-instruction.md","2025-06-23-linear-time-primitives-for-algorithm-development-in-graphical-causal-inference",{"id":28704,"data":28706,"filePath":28711,"digest":28712,"rendered":28713,"legacyId":28720},{"title":28707,"description":28708,"summary":28708,"pubDate":28709,"source":25191,"url":28710,"thumbnail":25193},"Linear-Time Primitives for Algorithm Development in Graphical Causal Inference","arXiv:2506.15758v1 Announce Type: new Abstract: We introduce CIfly, a framework for efficient algorithmic primitives in graphical causal inference that isolates reachability as a reusable core operation. It builds on the insight that many causal reasoning tasks can be reduced to reachability in purpose-built state-space graphs that can be constructed on the fly during traversal. We formalize a rule table schema for specifying such algorithms and prove they run in linear time. We establish CIfly as a more efficient alternative to the common primitives moralization and latent projection, which we show are computationally equivalent to Boolean matrix multiplication. Our open-source Rust implementation parses rule table text files and runs the specified CIfly algorithms providing high-performance execution accessible from Python and R. We demonstrate CIfly's utility by re-implementing a range of established causal inference tasks within the framework and by developing new algorithms for instrumental variables. These contributions position CIfly as a flexible and scalable backbone for graphical causal inference, guiding algorithm development and enabling easy and efficient deployment.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15758","src/content/posts/2025-06-23-linear-time-primitives-for-algorithm-development-in-graphical-causal-inference.md","8192761c6cb4061d",{"html":25,"metadata":28714},{"headings":28715,"localImagePaths":28716,"remoteImagePaths":28717,"frontmatter":28718,"imagePaths":28719},[],[],[],{"title":28707,"description":28708,"summary":28708,"pubDate":25202,"source":25191,"url":28710,"thumbnail":25193},[],"2025-06-23-linear-time-primitives-for-algorithm-development-in-graphical-causal-inference.md","2025-06-23-llm-based-bot-broadens-the-range-of-arguments-in-online-discussions-even-when-transparently-disclosed-as-ai",{"id":28721,"data":28723,"filePath":28728,"digest":28729,"rendered":28730,"legacyId":28737},{"title":28724,"description":28725,"summary":28725,"pubDate":28726,"source":25191,"url":28727,"thumbnail":25193},"LLM-Based Bot Broadens the Range of Arguments in Online Discussions, Even When Transparently Disclosed as AI","arXiv:2506.17073v1 Announce Type: cross Abstract: A wide range of participation is essential for democracy, as it helps prevent the dominance of extreme views, erosion of legitimacy, and political polarization. However, engagement in online political discussions often features a limited spectrum of views due to high levels of self-selection and the tendency of online platforms to facilitate exchanges primarily among like-minded individuals. This study examines whether an LLM-based bot can widen the scope of perspectives expressed by participants in online discussions through two pre-registered randomized experiments conducted in a chatroom. We evaluate the impact of a bot that actively monitors discussions, identifies missing arguments, and introduces them into the conversation. The results indicate that our bot significantly expands the range of arguments, as measured by both objective and subjective metrics. Furthermore, disclosure of the bot as AI does not significantly alter these effects. These findings suggest that LLM-based moderation tools can positively influence online political discourse.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17073","src/content/posts/2025-06-23-llm-based-bot-broadens-the-range-of-arguments-in-online-discussions-even-when-transparently-disclosed-as-ai.md","38c388b482417692",{"html":25,"metadata":28731},{"headings":28732,"localImagePaths":28733,"remoteImagePaths":28734,"frontmatter":28735,"imagePaths":28736},[],[],[],{"title":28724,"description":28725,"summary":28725,"pubDate":25202,"source":25191,"url":28727,"thumbnail":25193},[],"2025-06-23-llm-based-bot-broadens-the-range-of-arguments-in-online-discussions-even-when-transparently-disclosed-as-ai.md","2025-06-23-llm-guided-indoor-navigation-with-multimodal-map-understanding",{"id":28738,"data":28740,"filePath":28745,"digest":28746,"rendered":28747,"legacyId":28754},{"title":28741,"description":28742,"summary":28742,"pubDate":28743,"source":25191,"url":28744,"thumbnail":25193},"LLM-Guided Indoor Navigation with Multimodal Map Understanding","arXiv:2503.11702v4 Announce Type: replace Abstract: Indoor navigation presents unique challenges due to complex layouts and the unavailability of GNSS signals. Existing solutions often struggle with contextual adaptation, and typically require dedicated hardware. In this work, we explore the potential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural, context-aware navigation instructions from indoor map images. We design and evaluate test cases across different real-world environments, analyzing the effectiveness of LLMs in interpreting spatial layouts, handling user constraints, and planning efficient routes. Our findings demonstrate the potential of LLMs for supporting personalized indoor navigation, with an average of 86.59% correct indications and a maximum of 97.14%. The proposed system achieves high accuracy and reasoning performance. These results have key implications for AI-driven navigation and assistive technologies.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2503.11702","src/content/posts/2025-06-23-llm-guided-indoor-navigation-with-multimodal-map-understanding.md","9757c8114ef5cee1",{"html":25,"metadata":28748},{"headings":28749,"localImagePaths":28750,"remoteImagePaths":28751,"frontmatter":28752,"imagePaths":28753},[],[],[],{"title":28741,"description":28742,"summary":28742,"pubDate":25202,"source":25191,"url":28744,"thumbnail":25193},[],"2025-06-23-llm-guided-indoor-navigation-with-multimodal-map-understanding.md","2025-06-23-llm-web-dynamics-tracing-model-collapse-in-a-network-of-llms",{"id":28755,"data":28757,"filePath":28762,"digest":28763,"rendered":28764,"legacyId":28771},{"title":28758,"description":28759,"summary":28759,"pubDate":28760,"source":25191,"url":28761,"thumbnail":25193},"LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs","arXiv:2506.15690v1 Announce Type: cross Abstract: The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15690","src/content/posts/2025-06-23-llm-web-dynamics-tracing-model-collapse-in-a-network-of-llms.md","5b14d7a9deda9c69",{"html":25,"metadata":28765},{"headings":28766,"localImagePaths":28767,"remoteImagePaths":28768,"frontmatter":28769,"imagePaths":28770},[],[],[],{"title":28758,"description":28759,"summary":28759,"pubDate":25202,"source":25191,"url":28761,"thumbnail":25193},[],"2025-06-23-llm-web-dynamics-tracing-model-collapse-in-a-network-of-llms.md","2025-06-23-linearithmic-clean-up-for-vector-symbolic-key-value-memory-with-kroneker-rotation-products",{"id":28772,"data":28774,"filePath":28779,"digest":28780,"rendered":28781,"legacyId":28788},{"title":28775,"description":28776,"summary":28776,"pubDate":28777,"source":25191,"url":28778,"thumbnail":25193},"Linearithmic Clean-up for Vector-Symbolic Key-Value Memory with Kroneker Rotation Products","arXiv:2506.15793v1 Announce Type: cross Abstract: A computational bottleneck in current Vector-Symbolic Architectures (VSAs) is the ``clean-up'' step, which decodes the noisy vectors retrieved from the architecture. Clean-up typically compares noisy vectors against a ``codebook'' of prototype vectors, incurring computational complexity that is quadratic or similar. We present a new codebook representation that supports efficient clean-up, based on Kroneker products of rotation-like matrices. The resulting clean-up time complexity is linearithmic, i.e. $mathcal{O}(N,text{log},N)$, where $N$ is the vector dimension and also the number of vectors in the codebook. Clean-up space complexity is $mathcal{O}(N)$. Furthermore, the codebook is not stored explicitly in computer memory: It can be represented in $mathcal{O}(text{log},N)$ space, and individual vectors in the codebook can be materialized in $mathcal{O}(N)$ time and space. At the same time, asymptotic memory capacity remains comparable to standard approaches. Computer experiments confirm these results, demonstrating several orders of magnitude more scalability than baseline VSA techniques.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15793","src/content/posts/2025-06-23-linearithmic-clean-up-for-vector-symbolic-key-value-memory-with-kroneker-rotation-products.md","1bbcfb1a269e7fe0",{"html":25,"metadata":28782},{"headings":28783,"localImagePaths":28784,"remoteImagePaths":28785,"frontmatter":28786,"imagePaths":28787},[],[],[],{"title":28775,"description":28776,"summary":28776,"pubDate":25202,"source":25191,"url":28778,"thumbnail":25193},[],"2025-06-23-linearithmic-clean-up-for-vector-symbolic-key-value-memory-with-kroneker-rotation-products.md","2025-06-23-llms-and-stack-overflow-discussions-reliability-impact-and-challenges",{"id":28789,"data":28791,"filePath":28796,"digest":28797,"rendered":28798,"legacyId":28805},{"title":28792,"description":28793,"summary":28793,"pubDate":28794,"source":25191,"url":28795,"thumbnail":25193},"LLMs and Stack Overflow Discussions: Reliability, Impact, and Challenges","arXiv:2402.08801v2 Announce Type: replace-cross Abstract: Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the premier platform for developers queries on programming and software development. Demonstrating an ability to generate instant, human-like responses to technical questions, ChatGPT has ignited debates within the developer community about the evolving role of human-driven platforms in the age of generative AI. Two months after ChatGPT release, Meta released its answer with its own Large Language Model (LLM) called LLaMA: the race was on. We conducted an empirical study analyzing questions from Stack Overflow and using these LLMs to address them. This way, we aim to (i) quantify the reliability of LLMs answers and their potential to replace Stack Overflow in the long term; (ii) identify and understand why LLMs fail; (iii) measure users activity evolution with Stack Overflow over time; and (iv) compare LLMs together. Our empirical results are unequivocal: ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains, while a significant decline in user posting activity has been observed. Furthermore, we also discuss the impact of our findings regarding the usage and development of new LLMs and provide guidelines for future challenges faced by users and researchers.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2402.08801","src/content/posts/2025-06-23-llms-and-stack-overflow-discussions-reliability-impact-and-challenges.md","01dd5f35287bd93d",{"html":25,"metadata":28799},{"headings":28800,"localImagePaths":28801,"remoteImagePaths":28802,"frontmatter":28803,"imagePaths":28804},[],[],[],{"title":28792,"description":28793,"summary":28793,"pubDate":25202,"source":25191,"url":28795,"thumbnail":25193},[],"2025-06-23-llms-and-stack-overflow-discussions-reliability-impact-and-challenges.md","2025-06-23-llms-factor-in-unrelated-information-when-recommending-medical-treatments",{"id":28806,"data":28808,"filePath":28814,"digest":28815,"rendered":28816,"legacyId":28823},{"title":28809,"description":28810,"summary":28810,"pubDate":28811,"source":21340,"url":28812,"thumbnail":28813},"LLMs factor in unrelated information when recommending medical treatments","Researchers find nonclinical information in patient messages — like typos, extra white space, and colorful language — reduces the accuracy of an AI model.",["Date","2025-06-23T04:00:00.000Z"],"https://news.mit.edu/2025/llms-factor-unrelated-information-when-recommending-medical-treatments-0623","https://news.mit.edu/sites/default/files/images/202506/MIT_Medium-Message-01-press.jpg","src/content/posts/2025-06-23-llms-factor-in-unrelated-information-when-recommending-medical-treatments.md","d082fb97bc06d53f",{"html":25,"metadata":28817},{"headings":28818,"localImagePaths":28819,"remoteImagePaths":28820,"frontmatter":28821,"imagePaths":28822},[],[],[],{"title":28809,"description":28810,"summary":28810,"pubDate":25202,"source":21340,"url":28812,"thumbnail":28813},[],"2025-06-23-llms-factor-in-unrelated-information-when-recommending-medical-treatments.md","2025-06-23-llms-in-coding-and-their-impact-on-the-commercial-software-engineering-landscape",{"id":28824,"data":28826,"filePath":28831,"digest":28832,"rendered":28833,"legacyId":28840},{"title":28827,"description":28828,"summary":28828,"pubDate":28829,"source":25191,"url":28830,"thumbnail":25193},"LLMs in Coding and their Impact on the Commercial Software Engineering Landscape","arXiv:2506.16653v1 Announce Type: cross Abstract: Large-language-model coding tools are now mainstream in software engineering. But as these same tools move human effort up the development stack, they present fresh dangers: 10% of real prompts leak private data, 42% of generated snippets hide security flaws, and the models can even ``agree'' with wrong ideas, a trait called sycophancy. We argue that firms must tag and review every AI-generated line of code, keep prompts and outputs inside private or on-premises deployments, obey emerging safety regulations, and add tests that catch sycophantic answers -- so they can gain speed without losing security and accuracy.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16653","src/content/posts/2025-06-23-llms-in-coding-and-their-impact-on-the-commercial-software-engineering-landscape.md","c461c862d8a1f898",{"html":25,"metadata":28834},{"headings":28835,"localImagePaths":28836,"remoteImagePaths":28837,"frontmatter":28838,"imagePaths":28839},[],[],[],{"title":28827,"description":28828,"summary":28828,"pubDate":25202,"source":25191,"url":28830,"thumbnail":25193},[],"2025-06-23-llms-in-coding-and-their-impact-on-the-commercial-software-engineering-landscape.md","2025-06-23-llms-in-disease-diagnosis-a-comparative-study-of-deepseek-r1-and-o3-mini-across-chronic-health-conditions",{"id":28841,"data":28843,"filePath":28848,"digest":28849,"rendered":28850,"legacyId":28857},{"title":28844,"description":28845,"summary":28845,"pubDate":28846,"source":25191,"url":28847,"thumbnail":25193},"LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions","arXiv:2503.10486v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) are revolutionizing medical diagnostics by enhancing both disease classification and clinical decision-making. In this study, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek R1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We assessed their predictive accuracy at both the disease and category levels, as well as the reliability of their confidence scores. DeepSeek R1 achieved a disease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3 Mini, which attained 72% and 75% respectively. Notably, DeepSeek R1 demonstrated exceptional performance in Mental Health, Neurological Disorders, and Oncology, where it reached 100% accuracy, while O3 Mini excelled in Autoimmune Disease classification with 100% accuracy. Both models, however, struggled with Respiratory Disease classification, recording accuracies of only 40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of confidence scores revealed that DeepSeek R1 provided high-confidence predictions in 92% of cases, compared to 68% for O3 Mini. Ethical considerations regarding bias, model interpretability, and data privacy are also discussed to ensure the responsible integration of LLMs into clinical practice. Overall, our findings offer valuable insights into the strengths and limitations of LLM-based diagnostic systems and provide a roadmap for future enhancements in AI-driven healthcare.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2503.10486","src/content/posts/2025-06-23-llms-in-disease-diagnosis-a-comparative-study-of-deepseek-r1-and-o3-mini-across-chronic-health-conditions.md","d8c204226c4df3e9",{"html":25,"metadata":28851},{"headings":28852,"localImagePaths":28853,"remoteImagePaths":28854,"frontmatter":28855,"imagePaths":28856},[],[],[],{"title":28844,"description":28845,"summary":28845,"pubDate":25202,"source":25191,"url":28847,"thumbnail":25193},[],"2025-06-23-llms-in-disease-diagnosis-a-comparative-study-of-deepseek-r1-and-o3-mini-across-chronic-health-conditions.md","2025-06-23-lm-spt-lm-aligned-semantic-distillation-for-speech-tokenization",{"id":28858,"data":28860,"filePath":28865,"digest":28866,"rendered":28867,"legacyId":28874},{"title":28861,"description":28862,"summary":28862,"pubDate":28863,"source":25191,"url":28864,"thumbnail":25193},"LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization","arXiv:2506.16738v1 Announce Type: cross Abstract: With the rapid progress of speech language models (SLMs), discrete speech tokens have emerged as a core interface between speech and text, enabling unified modeling across modalities. Recent speech tokenization approaches aim to isolate semantic information from low-level acoustics to better align with language models. In particular, previous methods use SSL teachers such as HuBERT to extract semantic representations, which are then distilled into a semantic quantizer to suppress acoustic redundancy as well as capture content-related latent structures. However, they still produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques, such as rigid average pooling across frames, can distort or dilute the semantic structure required for effective LM alignment. To address this, we propose LM-SPT, a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, we reconstruct speech solely from semantic tokens and minimize the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder. This indirect yet data-driven supervision enables the tokenizer to learn discrete units that are more semantically aligned with language models. LM-SPT further incorporates architectural improvements to the encoder and decoder for speech tokenization, and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz. Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines, and that SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16738","src/content/posts/2025-06-23-lm-spt-lm-aligned-semantic-distillation-for-speech-tokenization.md","d64363f82b77fe6f",{"html":25,"metadata":28868},{"headings":28869,"localImagePaths":28870,"remoteImagePaths":28871,"frontmatter":28872,"imagePaths":28873},[],[],[],{"title":28861,"description":28862,"summary":28862,"pubDate":25202,"source":25191,"url":28864,"thumbnail":25193},[],"2025-06-23-lm-spt-lm-aligned-semantic-distillation-for-speech-tokenization.md","2025-06-23-llms-struggle-to-perform-counterfactual-reasoning-with-parametric-knowledge",{"id":28875,"data":28877,"filePath":28882,"digest":28883,"rendered":28884,"legacyId":28891},{"title":28878,"description":28879,"summary":28879,"pubDate":28880,"source":25191,"url":28881,"thumbnail":25193},"LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge","arXiv:2506.15732v1 Announce Type: new Abstract: Large Language Models have been shown to contain extensive world knowledge in their parameters, enabling impressive performance on many knowledge intensive tasks. However, when deployed in novel settings, LLMs often encounter situations where they must integrate parametric knowledge with new or unfamiliar information. In this work, we explore whether LLMs can combine knowledge in-context with their parametric knowledge through the lens of counterfactual reasoning. Through synthetic and real experiments in multi-hop reasoning problems, we show that LLMs generally struggle with counterfactual reasoning, often resorting to exclusively using their parametric knowledge. Moreover, we show that simple post-hoc finetuning can struggle to instill counterfactual reasoning ability -- often leading to degradation in stored parametric knowledge. Ultimately, our work reveals important limitations of current LLM's abilities to re-purpose parametric knowledge in novel settings.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15732","src/content/posts/2025-06-23-llms-struggle-to-perform-counterfactual-reasoning-with-parametric-knowledge.md","f85983c2f135b65c",{"html":25,"metadata":28885},{"headings":28886,"localImagePaths":28887,"remoteImagePaths":28888,"frontmatter":28889,"imagePaths":28890},[],[],[],{"title":28878,"description":28879,"summary":28879,"pubDate":25202,"source":25191,"url":28881,"thumbnail":25193},[],"2025-06-23-llms-struggle-to-perform-counterfactual-reasoning-with-parametric-knowledge.md","2025-06-23-logprober-disentangling-confidence-from-contamination-in-llm-responses",{"id":28892,"data":28894,"filePath":28899,"digest":28900,"rendered":28901,"legacyId":28908},{"title":28895,"description":28896,"summary":28896,"pubDate":28897,"source":25191,"url":28898,"thumbnail":25193},"LogProber: Disentangling confidence from contamination in LLM responses","arXiv:2408.14352v3 Announce Type: replace-cross Abstract: In machine learning, contamination refers to situations where testing data leak into the training set. The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on gargantuan, and generally opaque, corpora of text scraped from the world wide web. Developing tools to detect contamination is therefore crucial to be able to fairly and properly track the evolution of the performance of LLMs. To date, only a few recent studies have attempted to address the issue of quantifying and detecting contamination in short text sequences, such as those commonly found in benchmarks. However, these methods have limitations that can sometimes render them impractical. In the present paper, we introduce LogProber, a novel, efficient algorithm that we show to be able to detect contamination in a black box setting that tries to tackle some of these drawbacks by focusing on the familiarity with the question rather than the answer. Here, we explore the properties of the proposed method in comparison with concurrent approaches, identify its advantages and limitations, and illustrate how different forms of contamination can go undetected depending on the design of the detection algorithm.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2408.14352","src/content/posts/2025-06-23-logprober-disentangling-confidence-from-contamination-in-llm-responses.md","1ab6d24b84bb6079",{"html":25,"metadata":28902},{"headings":28903,"localImagePaths":28904,"remoteImagePaths":28905,"frontmatter":28906,"imagePaths":28907},[],[],[],{"title":28895,"description":28896,"summary":28896,"pubDate":25202,"source":25191,"url":28898,"thumbnail":25193},[],"2025-06-23-logprober-disentangling-confidence-from-contamination-in-llm-responses.md","2025-06-23-long-context-generalization-with-sparse-attention",{"id":28909,"data":28911,"filePath":28916,"digest":28917,"rendered":28918,"legacyId":28925},{"title":28912,"description":28913,"summary":28913,"pubDate":28914,"source":25191,"url":28915,"thumbnail":25193},"Long-Context Generalization with Sparse Attention","arXiv:2506.16640v1 Announce Type: cross Abstract: Transformer-based architectures traditionally employ softmax to compute attention weights, which produces dense distributions over all tokens in a sequence. While effective in many settings, this density has been shown to be detrimental for tasks that demand precise focus on fixed-size patterns: as sequence length increases, non-informative tokens accumulate attention probability mass, leading to dispersion and representational collapse. We show in this paper that sparse attention mechanisms using $alpha$-entmax can avoid these issues, due to their ability to assign exact zeros to irrelevant tokens. Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows $alpha$-entmax with a learnable temperature parameter, allowing the attention distribution to interpolate between sparse (pattern-focused) and dense (softmax-like) regimes. Finally, we show that the ability to locate and generalize fixed-size patterns can be further improved through a careful design of position encodings, which impacts both dense and sparse attention methods. By integrating ASEntmax into standard transformer layers alongside proper positional encodings, we show that our models greatly outperform softmax, scalable softmax, and fixed-temperature $alpha$-entmax baselines on long-context generalization.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16640","src/content/posts/2025-06-23-long-context-generalization-with-sparse-attention.md","7b0d740c360e80a7",{"html":25,"metadata":28919},{"headings":28920,"localImagePaths":28921,"remoteImagePaths":28922,"frontmatter":28923,"imagePaths":28924},[],[],[],{"title":28912,"description":28913,"summary":28913,"pubDate":25202,"source":25191,"url":28915,"thumbnail":25193},[],"2025-06-23-long-context-generalization-with-sparse-attention.md","2025-06-23-long-term-traffic-simulation-with-interleaved-autoregressive-motion-and-scenario-generation",{"id":28926,"data":28928,"filePath":28933,"digest":28934,"rendered":28935,"legacyId":28942},{"title":28929,"description":28930,"summary":28930,"pubDate":28931,"source":25191,"url":28932,"thumbnail":25193},"Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation","arXiv:2506.17213v1 Announce Type: cross Abstract: An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17213","src/content/posts/2025-06-23-long-term-traffic-simulation-with-interleaved-autoregressive-motion-and-scenario-generation.md","5dccf65ec2eff49b",{"html":25,"metadata":28936},{"headings":28937,"localImagePaths":28938,"remoteImagePaths":28939,"frontmatter":28940,"imagePaths":28941},[],[],[],{"title":28929,"description":28930,"summary":28930,"pubDate":25202,"source":25191,"url":28932,"thumbnail":25193},[],"2025-06-23-long-term-traffic-simulation-with-interleaved-autoregressive-motion-and-scenario-generation.md","2025-06-23-loupe-a-generalizable-and-adaptive-framework-for-image-forgery-detection",{"id":28943,"data":28945,"filePath":28950,"digest":28951,"rendered":28952,"legacyId":28959},{"title":28946,"description":28947,"summary":28947,"pubDate":28948,"source":25191,"url":28949,"thumbnail":25193},"Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection","arXiv:2506.16819v1 Announce Type: cross Abstract: The proliferation of generative models has raised serious concerns about visual content forgery. Existing deepfake detection methods primarily target either image-level classification or pixel-wise localization. While some achieve high accuracy, they often suffer from limited generalization across manipulation types or rely on complex architectures. In this paper, we propose Loupe, a lightweight yet effective framework for joint deepfake detection and localization. Loupe integrates a patch-aware classifier and a segmentation module with conditional queries, allowing simultaneous global authenticity classification and fine-grained mask prediction. To enhance robustness against distribution shifts of test set, Loupe introduces a pseudo-label-guided test-time adaptation mechanism by leveraging patch-level predictions to supervise the segmentation head. Extensive experiments on the DDL dataset demonstrate that Loupe achieves state-of-the-art performance, securing the first place in the IJCAI 2025 Deepfake Detection and Localization Challenge with an overall score of 0.846. Our results validate the effectiveness of the proposed patch-level fusion and conditional query design in improving both classification accuracy and spatial localization under diverse forgery patterns. The code is available at https://github.com/Kamichanw/Loupe.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16819","src/content/posts/2025-06-23-loupe-a-generalizable-and-adaptive-framework-for-image-forgery-detection.md","0f5c218d9ceda526",{"html":25,"metadata":28953},{"headings":28954,"localImagePaths":28955,"remoteImagePaths":28956,"frontmatter":28957,"imagePaths":28958},[],[],[],{"title":28946,"description":28947,"summary":28947,"pubDate":25202,"source":25191,"url":28949,"thumbnail":25193},[],"2025-06-23-loupe-a-generalizable-and-adaptive-framework-for-image-forgery-detection.md","2025-06-23-lscd-lomb-scargle-conditioned-diffusion-for-time-series-imputation",{"id":28960,"data":28962,"filePath":28967,"digest":28968,"rendered":28969,"legacyId":28976},{"title":28963,"description":28964,"summary":28964,"pubDate":28965,"source":25191,"url":28966,"thumbnail":25193},"LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation","arXiv:2506.17039v1 Announce Type: cross Abstract: Time series with missing or irregularly sampled data are a persistent challenge in machine learning. Many methods operate on the frequency-domain, relying on the Fast Fourier Transform (FFT) which assumes uniform sampling, therefore requiring prior interpolation that can distort the spectra. To address this limitation, we introduce a differentiable Lomb--Scargle layer that enables a reliable computation of the power spectrum of irregularly sampled data. We integrate this layer into a novel score-based diffusion model (LSCD) for time series imputation conditioned on the entire signal spectrum. Experiments on synthetic and real-world benchmarks demonstrate that our method recovers missing data more accurately than purely time-domain baselines, while simultaneously producing consistent frequency estimates. Crucially, our method can be easily integrated into learning frameworks, enabling broader adoption of spectral guidance in machine learning approaches involving incomplete or irregular data.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17039","src/content/posts/2025-06-23-lscd-lomb-scargle-conditioned-diffusion-for-time-series-imputation.md","406f6c842c243b30",{"html":25,"metadata":28970},{"headings":28971,"localImagePaths":28972,"remoteImagePaths":28973,"frontmatter":28974,"imagePaths":28975},[],[],[],{"title":28963,"description":28964,"summary":28964,"pubDate":25202,"source":25191,"url":28966,"thumbnail":25193},[],"2025-06-23-lscd-lomb-scargle-conditioned-diffusion-for-time-series-imputation.md","2025-06-23-machine-learning-methods-for-small-data-and-upstream-bioprocessing-applications-a-comprehensive-review",{"id":28977,"data":28979,"filePath":28984,"digest":28985,"rendered":28986,"legacyId":28993},{"title":28980,"description":28981,"summary":28981,"pubDate":28982,"source":25191,"url":28983,"thumbnail":25193},"Machine Learning Methods for Small Data and Upstream Bioprocessing Applications: A Comprehensive Review","arXiv:2506.12322v2 Announce Type: replace-cross Abstract: Data is crucial for machine learning (ML) applications, yet acquiring large datasets can be costly and time-consuming, especially in complex, resource-intensive fields like biopharmaceuticals. A key process in this industry is upstream bioprocessing, where living cells are cultivated and optimised to produce therapeutic proteins and biologics. The intricate nature of these processes, combined with high resource demands, often limits data collection, resulting in smaller datasets. This comprehensive review explores ML methods designed to address the challenges posed by small data and classifies them into a taxonomy to guide practical applications. Furthermore, each method in the taxonomy was thoroughly analysed, with a detailed discussion of its core concepts and an evaluation of its effectiveness in tackling small data challenges, as demonstrated by application results in the upstream bioprocessing and other related domains. By analysing how these methods tackle small data challenges from different perspectives, this review provides actionable insights, identifies current research gaps, and offers guidance for leveraging ML in data-constrained environments.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.12322","src/content/posts/2025-06-23-machine-learning-methods-for-small-data-and-upstream-bioprocessing-applications-a-comprehensive-review.md","fb0f68b285a6b7c0",{"html":25,"metadata":28987},{"headings":28988,"localImagePaths":28989,"remoteImagePaths":28990,"frontmatter":28991,"imagePaths":28992},[],[],[],{"title":28980,"description":28981,"summary":28981,"pubDate":25202,"source":25191,"url":28983,"thumbnail":25193},[],"2025-06-23-machine-learning-methods-for-small-data-and-upstream-bioprocessing-applications-a-comprehensive-review.md","2025-06-23-machine-mental-imagery-empower-multimodal-reasoning-with-latent-visual-tokens",{"id":28994,"data":28996,"filePath":29001,"digest":29002,"rendered":29003,"legacyId":29010},{"title":28997,"description":28998,"summary":28998,"pubDate":28999,"source":25191,"url":29000,"thumbnail":25193},"Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens","arXiv:2506.17218v1 Announce Type: cross Abstract: Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery-the internal construction and manipulation of visual cues-we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to ``think visually'', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17218","src/content/posts/2025-06-23-machine-mental-imagery-empower-multimodal-reasoning-with-latent-visual-tokens.md","0d7f17f348b1b85d",{"html":25,"metadata":29004},{"headings":29005,"localImagePaths":29006,"remoteImagePaths":29007,"frontmatter":29008,"imagePaths":29009},[],[],[],{"title":28997,"description":28998,"summary":28998,"pubDate":25202,"source":25191,"url":29000,"thumbnail":25193},[],"2025-06-23-machine-mental-imagery-empower-multimodal-reasoning-with-latent-visual-tokens.md","2025-06-23-madakv-adaptive-modality-perception-kv-cache-eviction-for-efficient-multimodal-long-context-inference",{"id":29011,"data":29013,"filePath":29018,"digest":29019,"rendered":29020,"legacyId":29027},{"title":29014,"description":29015,"summary":29015,"pubDate":29016,"source":25191,"url":29017,"thumbnail":25193},"MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference","arXiv:2506.15724v1 Announce Type: cross Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache eviction strategy designed to enhance the efficiency of multimodal large language models (MLLMs) in long-context inference. In multimodal scenarios, attention heads exhibit varying preferences for different modalities, resulting in significant disparities in modality importance across attention heads. Traditional KV cache eviction methods, which are tailored for unimodal settings, fail to capture modality-specific information, thereby yielding suboptimal performance. MadaKV addresses these challenges through two key components: modality preference adaptation and hierarchical compression compensation. By dynamically sensing modality information within attention heads and adaptively retaining critical tokens, MadaKV achieves substantial reductions in KV cache memory footprint and model inference decoding latency (1.3 to 1.5 times improvement) while maintaining high accuracy across various multimodal long-context tasks. Extensive experiments on representative MLLMs and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to existing KV cache eviction methods.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15724","src/content/posts/2025-06-23-madakv-adaptive-modality-perception-kv-cache-eviction-for-efficient-multimodal-long-context-inference.md","309aa7989721ca92",{"html":25,"metadata":29021},{"headings":29022,"localImagePaths":29023,"remoteImagePaths":29024,"frontmatter":29025,"imagePaths":29026},[],[],[],{"title":29014,"description":29015,"summary":29015,"pubDate":25202,"source":25191,"url":29017,"thumbnail":25193},[],"2025-06-23-madakv-adaptive-modality-perception-kv-cache-eviction-for-efficient-multimodal-long-context-inference.md","2025-06-23-mapper-multimodal-prior-guided-parameter-efficient-tuning-for-referring-expression-comprehension",{"id":29028,"data":29030,"filePath":29035,"digest":29036,"rendered":29037,"legacyId":29044},{"title":29031,"description":29032,"summary":29032,"pubDate":29033,"source":25191,"url":29034,"thumbnail":25193},"MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension","arXiv:2409.13609v4 Announce Type: replace-cross Abstract: Referring Expression Comprehension (REC), which aims to ground a local visual region via natural language, is a task that heavily relies on multimodal alignment. Most existing methods utilize powerful pre-trained models to transfer visual/linguistic knowledge by full fine-tuning. However, full fine-tuning the entire backbone not only breaks the rich prior knowledge embedded in the pre-training, but also incurs significant computational costs. Motivated by the recent emergence of Parameter-Efficient Transfer Learning (PETL) methods, we aim to solve the REC task in an effective and efficient manner. Directly applying these PETL methods to the REC task is inappropriate, as they lack the specific-domain abilities for precise local visual perception and visual-language alignment. Therefore, we propose a novel framework of Multimodal Prior-guided Parameter Efficient Tuning, namely MaPPER. Specifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned prior, and Local Convolution Adapters to extract precise local semantics for better visual perception. Moreover, the Prior-Guided Text module is proposed to further utilize the prior for facilitating the cross-modal alignment. Experimental results on three widely-used benchmarks demonstrate that MaPPER achieves the best accuracy compared to the full fine-tuning and other PETL methods with only 1.41% tunable backbone parameters. Our code is available at https://github.com/liuting20/MaPPER.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2409.13609","src/content/posts/2025-06-23-mapper-multimodal-prior-guided-parameter-efficient-tuning-for-referring-expression-comprehension.md","a9c8039d9f86a059",{"html":25,"metadata":29038},{"headings":29039,"localImagePaths":29040,"remoteImagePaths":29041,"frontmatter":29042,"imagePaths":29043},[],[],[],{"title":29031,"description":29032,"summary":29032,"pubDate":25202,"source":25191,"url":29034,"thumbnail":25193},[],"2025-06-23-mapper-multimodal-prior-guided-parameter-efficient-tuning-for-referring-expression-comprehension.md","2025-06-23-mask-pinns-regulating-feature-distributions-in-physics-informed-neural-networks",{"id":29045,"data":29047,"filePath":29052,"digest":29053,"rendered":29054,"legacyId":29061},{"title":29048,"description":29049,"summary":29049,"pubDate":29050,"source":25191,"url":29051,"thumbnail":25193},"Mask-PINNs: Regulating Feature Distributions in Physics-Informed Neural Networks","arXiv:2505.06331v2 Announce Type: replace-cross Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework for solving partial differential equations (PDEs) by embedding physical laws directly into the loss function. However, effective training of PINNs remains challenging due to internal covariate shift, which destabilizes feature distributions and impairs model expressiveness. While normalization techniques like Batch Normalization and Layer Normalization are standard remedies in deep learning, they disrupt the pointwise input-output mappings critical to preserving the physical consistency in PINNs. In this work, we introduce Mask-PINNs, a novel architecture that regulates internal feature distributions through a smooth, learnable mask function applied pointwise across hidden layers. Unlike conventional normalization methods, the proposed mask function preserves the deterministic nature of input-output relationships while suppressing activation drift and saturation. Theoretically, we demonstrate that Mask-PINNs control feature spread near initialization by attenuating gradient variance growth through a tailored modulation mechanism. Empirically, we validate the method on multiple PDE benchmarks across diverse activation functions. Our results show consistent improvements in prediction accuracy, convergence stability, and robustness, with relative L2 errors reduced by up to two orders of magnitude over baseline models. Furthermore, we demonstrate that Mask-PINNs enable the effective use of wider networks, overcoming a key limitation in existing PINN frameworks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.06331","src/content/posts/2025-06-23-mask-pinns-regulating-feature-distributions-in-physics-informed-neural-networks.md","054b62286191cb38",{"html":25,"metadata":29055},{"headings":29056,"localImagePaths":29057,"remoteImagePaths":29058,"frontmatter":29059,"imagePaths":29060},[],[],[],{"title":29048,"description":29049,"summary":29049,"pubDate":25202,"source":25191,"url":29051,"thumbnail":25193},[],"2025-06-23-mask-pinns-regulating-feature-distributions-in-physics-informed-neural-networks.md","2025-06-23-mathematical-proof-as-a-litmus-test-revealing-failure-modes-of-advanced-large-reasoning-models",{"id":29062,"data":29064,"filePath":29069,"digest":29070,"rendered":29071,"legacyId":29078},{"title":29065,"description":29066,"summary":29066,"pubDate":29067,"source":25191,"url":29068,"thumbnail":25193},"Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models","arXiv:2506.17114v1 Announce Type: new Abstract: Large reasoning models (e.g., R1, o3) have demonstrated remarkable mathematical problem-solving abilities. However, the high reported accuracy of these advanced models on popular datasets, reliance on purely numerical evaluation and potential benchmark leakage, often masks their true reasoning shortcomings. To address this, we propose leveraging the inherent rigor and methodological complexity of mathematical proofs as a diagnostic tool to expose these hidden failures. Specifically, we introduce the RFMDataset (Reveal Failure Modes), a collection of 200 diverse mathematical proof problems, and thoroughly evaluate advanced models' performance on it. Our in-depth analysis of their failures uncovers 10 fine-grained error types, which shows fundamental limitations in current large reasoning models: 1) large reasoning models grapple profoundly with mathematical proofs, with some generating entirely correct proofs for less than 20% of problems and failing even on basic ones; 2) models exhibit a diverse spectrum of reasoning failures, prominently demonstrating the lack of guarantees for the correctness and rigor of single-step reasoning; and 3) models show hallucination and incompleteness during the reasoning process. Our findings reveal that models' self-reflection is insufficient to resolve the current logical dilemmas, necessitating formalized and fine-grained logical training.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17114","src/content/posts/2025-06-23-mathematical-proof-as-a-litmus-test-revealing-failure-modes-of-advanced-large-reasoning-models.md","4b3d77ca79a2231b",{"html":25,"metadata":29072},{"headings":29073,"localImagePaths":29074,"remoteImagePaths":29075,"frontmatter":29076,"imagePaths":29077},[],[],[],{"title":29065,"description":29066,"summary":29066,"pubDate":25202,"source":25191,"url":29068,"thumbnail":25193},[],"2025-06-23-mathematical-proof-as-a-litmus-test-revealing-failure-modes-of-advanced-large-reasoning-models.md","2025-06-23-mdpo-multi-granularity-direct-preference-optimization-for-mathematical-reasoning",{"id":29079,"data":29081,"filePath":29086,"digest":29087,"rendered":29088,"legacyId":29095},{"title":29082,"description":29083,"summary":29083,"pubDate":29084,"source":25191,"url":29085,"thumbnail":25193},"MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning","arXiv:2506.15706v1 Announce Type: cross Abstract: Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) as it requires ensuring the correctness of each reasoning step. Researchers have been strengthening the mathematical reasoning abilities of LLMs through supervised fine-tuning, but due to the inability to suppress incorrect outputs, illusions can easily arise. Recently, Direct Preference Optimization (DPO) has been widely adopted for aligning human intent by using preference data to prevent LLMs from generating incorrect outputs. However, it has shown limited benefits in long-chain mathematical reasoning, mainly because DPO struggles to effectively capture the differences between accepted and rejected answers from preferences in long-chain data. The inconsistency between DPO training and LLMs' generation metrics also affects the effectiveness of suppressing incorrect outputs. We propose the Multi-Granularity Direct Preference Optimization (MDPO) method, optimizing the mathematical reasoning of LLMs at three granularities: Solution2Solution, Inference2Inference, and Step2Step. Solution2Solution focuses on the correctness of entire long-chain reasoning; Inference2Inference concentrates on logical reasoning between steps; Step2Step corrects computational errors in steps, enhancing the computational capabilities of LLMs. Additionally, we unify the training objectives of the three granularities to align with the generation metrics. We conducted experiments on the open-source models Qwen2 and Llama3, achieving improvements of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset, outperforming DPO and other DPO variant methods. Furthermore, we also provide a pipeline for constructing MDPO training data that is simple and does not require manual annotation costs.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15706","src/content/posts/2025-06-23-mdpo-multi-granularity-direct-preference-optimization-for-mathematical-reasoning.md","08d7211e850185b6",{"html":25,"metadata":29089},{"headings":29090,"localImagePaths":29091,"remoteImagePaths":29092,"frontmatter":29093,"imagePaths":29094},[],[],[],{"title":29082,"description":29083,"summary":29083,"pubDate":25202,"source":25191,"url":29085,"thumbnail":25193},[],"2025-06-23-mdpo-multi-granularity-direct-preference-optimization-for-mathematical-reasoning.md","2025-06-23-mawiflow-benchmark-realistic-flow-based-evaluation-for-network-intrusion-detection",{"id":29096,"data":29098,"filePath":29103,"digest":29104,"rendered":29105,"legacyId":29112},{"title":29099,"description":29100,"summary":29100,"pubDate":29101,"source":25191,"url":29102,"thumbnail":25193},"MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection","arXiv:2506.17041v1 Announce Type: cross Abstract: Benchmark datasets for network intrusion detection commonly rely on synthetically generated traffic, which fails to reflect the statistical variability and temporal drift encountered in operational environments. This paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1 dataset, designed to enable realistic and reproducible evaluation of anomaly detection methods. A reproducible preprocessing pipeline is presented that transforms raw packet captures into flow representations conforming to the CICFlowMeter format, while preserving MAWILab's original anomaly labels. The resulting datasets comprise temporally distinct samples from January 2011, 2016, and 2021, drawn from trans-Pacific backbone traffic. To establish reference baselines, traditional machine learning methods, including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical results demonstrate that tree-based classifiers perform well on temporally static data but experience significant performance degradation over time. In contrast, the CNN-BiLSTM model maintains better performance, thus showing improved generalization. These findings underscore the limitations of synthetic benchmarks and static models, and motivate the adoption of realistic datasets with explicit temporal structure. All datasets, pipeline code, and model implementations are made publicly available to foster transparency and reproducibility.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17041","src/content/posts/2025-06-23-mawiflow-benchmark-realistic-flow-based-evaluation-for-network-intrusion-detection.md","66fd3e719bbd8944",{"html":25,"metadata":29106},{"headings":29107,"localImagePaths":29108,"remoteImagePaths":29109,"frontmatter":29110,"imagePaths":29111},[],[],[],{"title":29099,"description":29100,"summary":29100,"pubDate":25202,"source":25191,"url":29102,"thumbnail":25193},[],"2025-06-23-mawiflow-benchmark-realistic-flow-based-evaluation-for-network-intrusion-detection.md","2025-06-23-measuring-a-sufficient-world-model-in-llms-a-variance-decomposition-framework",{"id":29113,"data":29115,"filePath":29120,"digest":29121,"rendered":29122,"legacyId":29129},{"title":29116,"description":29117,"summary":29117,"pubDate":29118,"source":25191,"url":29119,"thumbnail":25193},"Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework","arXiv:2506.16584v1 Announce Type: cross Abstract: Understanding whether large language models (LLMs) possess a world model-a structured understanding of the world that supports generalization beyond surface-level patterns-is central to assessing their reliability, especially in high-stakes applications. We propose a formal framework for evaluating whether an LLM exhibits a sufficiently robust world model, defined as producing consistent outputs across semantically equivalent prompts while distinguishing between prompts that express different intents. We introduce a new evaluation approach to measure this that decomposes model response variability into three components: variability due to user purpose, user articulation, and model instability. An LLM with a strong world model should attribute most of the variability in its responses to changes in foundational purpose rather than superficial changes in articulation. This approach allows us to quantify how much of a model's behavior is semantically grounded rather than driven by model instability or alternative wording. We apply this framework to evaluate LLMs across diverse domains. Our results show how larger models attribute a greater share of output variability to changes in user purpose, indicating a more robust world model. This improvement is not uniform, however: larger models do not consistently outperform smaller ones across all domains, and their advantage in robustness is often modest. These findings highlight the importance of moving beyond accuracy-based benchmarks toward semantic diagnostics that more directly assess the structure and stability of a model's internal understanding of the world.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16584","src/content/posts/2025-06-23-measuring-a-sufficient-world-model-in-llms-a-variance-decomposition-framework.md","0ae6b82779abd7ce",{"html":25,"metadata":29123},{"headings":29124,"localImagePaths":29125,"remoteImagePaths":29126,"frontmatter":29127,"imagePaths":29128},[],[],[],{"title":29116,"description":29117,"summary":29117,"pubDate":25202,"source":25191,"url":29119,"thumbnail":25193},[],"2025-06-23-measuring-a-sufficient-world-model-in-llms-a-variance-decomposition-framework.md","2025-06-23-med-u1-incentivizing-unified-medical-reasoning-in-llms-via-large-scale-reinforcement-learning",{"id":29130,"data":29132,"filePath":29137,"digest":29138,"rendered":29139,"legacyId":29146},{"title":29133,"description":29134,"summary":29134,"pubDate":29135,"source":25191,"url":29136,"thumbnail":25193},"Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning","arXiv:2506.12307v2 Announce Type: replace-cross Abstract: Medical Question-Answering (QA) encompasses a broad spectrum of tasks, including multiple choice questions (MCQ), open-ended text generation, and complex computational reasoning. Despite this variety, a unified framework for delivering high-quality medical QA has yet to emerge. Although recent progress in reasoning-augmented large language models (LLMs) has shown promise, their ability to achieve comprehensive medical understanding is still largely unexplored. In this paper, we present Med-U1, a unified framework for robust reasoning across medical QA tasks with diverse output formats, ranging from MCQs to complex generation and computation tasks. Med-U1 employs pure large-scale reinforcement learning with mixed rule-based binary reward functions, incorporating a length penalty to manage output verbosity. With multi-objective reward optimization, Med-U1 directs LLMs to produce concise and verifiable reasoning chains. Empirical results reveal that Med-U1 significantly improves performance across multiple challenging Med-QA benchmarks, surpassing even larger specialized and proprietary models. Furthermore, Med-U1 demonstrates robust generalization to out-of-distribution (OOD) tasks. Extensive analysis presents insights into training strategies, reasoning chain length control, and reward design for medical LLMs. Our code is available here.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.12307","src/content/posts/2025-06-23-med-u1-incentivizing-unified-medical-reasoning-in-llms-via-large-scale-reinforcement-learning.md","18f607a6dcc8335e",{"html":25,"metadata":29140},{"headings":29141,"localImagePaths":29142,"remoteImagePaths":29143,"frontmatter":29144,"imagePaths":29145},[],[],[],{"title":29133,"description":29134,"summary":29134,"pubDate":25202,"source":25191,"url":29136,"thumbnail":25193},[],"2025-06-23-med-u1-incentivizing-unified-medical-reasoning-in-llms-via-large-scale-reinforcement-learning.md","2025-06-23-medi-metadata-guided-diffusion-models-for-mitigating-biases-in-tumor-classification",{"id":29147,"data":29149,"filePath":29154,"digest":29155,"rendered":29156,"legacyId":29163},{"title":29150,"description":29151,"summary":29151,"pubDate":29152,"source":25191,"url":29153,"thumbnail":25193},"MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification","arXiv:2506.17140v1 Announce Type: cross Abstract: Deep learning models have made significant advances in histological prediction tasks in recent years. However, for adaptation in clinical practice, their lack of robustness to varying conditions such as staining, scanner, hospital, and demographics is still a limiting factor: if trained on overrepresented subpopulations, models regularly struggle with less frequent patterns, leading to shortcut learning and biased predictions. Large-scale foundation models have not fully eliminated this issue. Therefore, we propose a novel approach explicitly modeling such metadata into a Metadata-guided generative Diffusion model framework (MeDi). MeDi allows for a targeted augmentation of underrepresented subpopulations with synthetic data, which balances limited training data and mitigates biases in downstream models. We experimentally show that MeDi generates high-quality histopathology images for unseen subpopulations in TCGA, boosts the overall fidelity of the generated images, and enables improvements in performance for downstream classifiers on datasets with subpopulation shifts. Our work is a proof-of-concept towards better mitigating data biases with generative models.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17140","src/content/posts/2025-06-23-medi-metadata-guided-diffusion-models-for-mitigating-biases-in-tumor-classification.md","4faa54f29ab7c924",{"html":25,"metadata":29157},{"headings":29158,"localImagePaths":29159,"remoteImagePaths":29160,"frontmatter":29161,"imagePaths":29162},[],[],[],{"title":29150,"description":29151,"summary":29151,"pubDate":25202,"source":25191,"url":29153,"thumbnail":25193},[],"2025-06-23-medi-metadata-guided-diffusion-models-for-mitigating-biases-in-tumor-classification.md","2025-06-23-mem1-learning-to-synergize-memory-and-reasoning-for-efficient-long-horizon-agents",{"id":29164,"data":29166,"filePath":29171,"digest":29172,"rendered":29173,"legacyId":29180},{"title":29167,"description":29168,"summary":29168,"pubDate":29169,"source":25191,"url":29170,"thumbnail":25193},"MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents","arXiv:2506.15841v1 Announce Type: cross Abstract: Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15841","src/content/posts/2025-06-23-mem1-learning-to-synergize-memory-and-reasoning-for-efficient-long-horizon-agents.md","3b3078e6b93fb943",{"html":25,"metadata":29174},{"headings":29175,"localImagePaths":29176,"remoteImagePaths":29177,"frontmatter":29178,"imagePaths":29179},[],[],[],{"title":29167,"description":29168,"summary":29168,"pubDate":25202,"source":25191,"url":29170,"thumbnail":25193},[],"2025-06-23-mem1-learning-to-synergize-memory-and-reasoning-for-efficient-long-horizon-agents.md","2025-06-23-metapath-based-hyperbolic-contrastive-learning-for-heterogeneous-graph-embedding",{"id":29181,"data":29183,"filePath":29188,"digest":29189,"rendered":29190,"legacyId":29197},{"title":29184,"description":29185,"summary":29185,"pubDate":29186,"source":25191,"url":29187,"thumbnail":25193},"Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding","arXiv:2506.16754v1 Announce Type: cross Abstract: The hyperbolic space, characterized by a constant negative curvature and exponentially expanding space, aligns well with the structural properties of heterogeneous graphs. However, although heterogeneous graphs inherently possess diverse power-law structures, most hyperbolic heterogeneous graph embedding models rely on a single hyperbolic space. This approach may fail to effectively capture the diverse power-law structures within heterogeneous graphs. To address this limitation, we propose a Metapath-based Hyperbolic Contrastive Learning framework (MHCL), which uses multiple hyperbolic spaces to capture diverse complex structures within heterogeneous graphs. Specifically, by learning each hyperbolic space to describe the distribution of complex structures corresponding to each metapath, it is possible to capture semantic information effectively. Since metapath embeddings represent distinct semantic information, preserving their discriminability is important when aggregating them to obtain node representations. Therefore, we use a contrastive learning approach to optimize MHCL and improve the discriminability of metapath embeddings. In particular, our contrastive learning method minimizes the distance between embeddings of the same metapath and maximizes the distance between those of different metapaths in hyperbolic space, thereby improving the separability of metapath embeddings with distinct semantic information. We conduct comprehensive experiments to evaluate the effectiveness of MHCL. The experimental results demonstrate that MHCL outperforms state-of-the-art baselines in various graph machine learning tasks, effectively capturing the complex structures of heterogeneous graphs.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16754","src/content/posts/2025-06-23-metapath-based-hyperbolic-contrastive-learning-for-heterogeneous-graph-embedding.md","ea45e2affb92fdcd",{"html":25,"metadata":29191},{"headings":29192,"localImagePaths":29193,"remoteImagePaths":29194,"frontmatter":29195,"imagePaths":29196},[],[],[],{"title":29184,"description":29185,"summary":29185,"pubDate":25202,"source":25191,"url":29187,"thumbnail":25193},[],"2025-06-23-metapath-based-hyperbolic-contrastive-learning-for-heterogeneous-graph-embedding.md","2025-06-23-mexa-towards-general-multimodal-reasoning-with-dynamic-multi-expert-aggregation",{"id":29198,"data":29200,"filePath":29205,"digest":29206,"rendered":29207,"legacyId":29214},{"title":29201,"description":29202,"summary":29202,"pubDate":29203,"source":25191,"url":29204,"thumbnail":25193},"MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation","arXiv:2506.17113v1 Announce Type: cross Abstract: Combining pre-trained expert models offers substantial potential for scalable multimodal reasoning, but building a unified framework remains challenging due to the increasing diversity of input modalities and task complexity. For instance, medical diagnosis requires precise reasoning over structured clinical tables, while financial forecasting depends on interpreting plot-based data to make informed predictions. To tackle this challenge, we introduce MEXA, a training-free framework that performs modality- and task-aware aggregation of multiple expert models to enable effective multimodal reasoning across diverse and distinct domains. MEXA dynamically selects expert models based on the input modality and the task-specific reasoning demands (i.e., skills). Each expert model, specialized in a modality task pair, generates interpretable textual reasoning outputs. MEXA then aggregates and reasons over these outputs using a Large Reasoning Model (LRM) to produce the final answer. This modular design allows flexible and transparent multimodal reasoning across diverse domains without additional training overhead. We extensively evaluate our approach on diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D Understanding, and Medical QA. MEXA consistently delivers performance improvements over strong multimodal baselines, highlighting the effectiveness and broad applicability of our expert-driven selection and aggregation in diverse multimodal reasoning tasks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17113","src/content/posts/2025-06-23-mexa-towards-general-multimodal-reasoning-with-dynamic-multi-expert-aggregation.md","7f378d7e73d6f4fe",{"html":25,"metadata":29208},{"headings":29209,"localImagePaths":29210,"remoteImagePaths":29211,"frontmatter":29212,"imagePaths":29213},[],[],[],{"title":29201,"description":29202,"summary":29202,"pubDate":25202,"source":25191,"url":29204,"thumbnail":25193},[],"2025-06-23-mexa-towards-general-multimodal-reasoning-with-dynamic-multi-expert-aggregation.md","2025-06-23-minifinetuning-low-data-generation-domain-adaptation-through-corrective-self-distillation",{"id":29215,"data":29217,"filePath":29222,"digest":29223,"rendered":29224,"legacyId":29231},{"title":29218,"description":29219,"summary":29219,"pubDate":29220,"source":25191,"url":29221,"thumbnail":25193},"Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation","arXiv:2506.15702v1 Announce Type: cross Abstract: Finetuning language models for a new domain inevitably leads to the deterioration of their general performance. This becomes more pronounced the more limited the finetuning data resource. We introduce minifinetuning (MFT), a method for language model domain adaptation that considerably reduces the effects of overfitting-induced degeneralization in low-data settings and which does so in the absence of any pre-training data for replay. MFT demonstrates 2-10x more favourable specialization-to-degeneralization ratios than standard finetuning across a wide range of models and domains and exhibits an intrinsic robustness to overfitting when data in the new domain is scarce and down to as little as 500 samples. Employing corrective self-distillation that is individualized on the sample level, MFT outperforms parameter-efficient finetuning methods, demonstrates replay-like degeneralization mitigation properties, and is composable with either for a combined effect.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15702","src/content/posts/2025-06-23-minifinetuning-low-data-generation-domain-adaptation-through-corrective-self-distillation.md","71089b073161fb8d",{"html":25,"metadata":29225},{"headings":29226,"localImagePaths":29227,"remoteImagePaths":29228,"frontmatter":29229,"imagePaths":29230},[],[],[],{"title":29218,"description":29219,"summary":29219,"pubDate":25202,"source":25191,"url":29221,"thumbnail":25193},[],"2025-06-23-minifinetuning-low-data-generation-domain-adaptation-through-corrective-self-distillation.md","2025-06-23-mist-jailbreaking-black-box-large-language-models-via-iterative-semantic-tuning",{"id":29232,"data":29234,"filePath":29239,"digest":29240,"rendered":29241,"legacyId":29248},{"title":29235,"description":29236,"summary":29236,"pubDate":29237,"source":25191,"url":29238,"thumbnail":25193},"MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning","arXiv:2506.16792v1 Announce Type: cross Abstract: Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks--methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version--order-determining optimization. Extensive experiments across two open-source models and four closed-source models demonstrate that MIST achieves competitive attack success rates and attack transferability compared with other state-of-the-art white-box and black-box jailbreak methods. Additionally, we conduct experiments on computational efficiency to validate the practical viability of MIST.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16792","src/content/posts/2025-06-23-mist-jailbreaking-black-box-large-language-models-via-iterative-semantic-tuning.md","693ef115b91795d1",{"html":25,"metadata":29242},{"headings":29243,"localImagePaths":29244,"remoteImagePaths":29245,"frontmatter":29246,"imagePaths":29247},[],[],[],{"title":29235,"description":29236,"summary":29236,"pubDate":25202,"source":25191,"url":29238,"thumbnail":25193},[],"2025-06-23-mist-jailbreaking-black-box-large-language-models-via-iterative-semantic-tuning.md","2025-06-23-ml-master-towards-ai-for-ai-via-integration-of-exploration-and-reasoning",{"id":29249,"data":29251,"filePath":29256,"digest":29257,"rendered":29258,"legacyId":29265},{"title":29252,"description":29253,"summary":29253,"pubDate":29254,"source":25191,"url":29255,"thumbnail":25193},"ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning","arXiv:2506.16499v1 Announce Type: new Abstract: As AI capabilities advance toward and potentially beyond human-level performance, a natural transition emerges where AI-driven development becomes more efficient than human-centric approaches. A promising pathway toward this transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate and optimize the design, training, and deployment of AI systems themselves. While LLM-based agents have shown the potential to realize AI4AI, they are often unable to fully leverage the experience accumulated by agents during the exploration of solutions in the reasoning process, leading to inefficiencies and suboptimal performance. To address this limitation, we propose ML-Master, a novel AI4AI agent that seamlessly integrates exploration and reasoning by employing a selectively scoped memory mechanism. This approach allows ML-Master to efficiently combine diverse insights from parallel solution trajectories with analytical reasoning, guiding further exploration without overwhelming the agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it achieves a 29.3% average medal rate, significantly surpassing existing methods, particularly in medium-complexity tasks, while accomplishing this superior performance within a strict 12-hour time constraint-half the 24-hour limit used by previous baselines. These results demonstrate ML-Master's potential as a powerful tool for advancing AI4AI.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16499","src/content/posts/2025-06-23-ml-master-towards-ai-for-ai-via-integration-of-exploration-and-reasoning.md","87c92313cc98e6d8",{"html":25,"metadata":29259},{"headings":29260,"localImagePaths":29261,"remoteImagePaths":29262,"frontmatter":29263,"imagePaths":29264},[],[],[],{"title":29252,"description":29253,"summary":29253,"pubDate":25202,"source":25191,"url":29255,"thumbnail":25193},[],"2025-06-23-ml-master-towards-ai-for-ai-via-integration-of-exploration-and-reasoning.md","2025-06-23-modeling-public-perceptions-of-science-in-media",{"id":29266,"data":29268,"filePath":29273,"digest":29274,"rendered":29275,"legacyId":29282},{"title":29269,"description":29270,"summary":29270,"pubDate":29271,"source":25191,"url":29272,"thumbnail":25193},"Modeling Public Perceptions of Science in Media","arXiv:2506.16622v1 Announce Type: cross Abstract: Effectively engaging the public with science is vital for fostering trust and understanding in our scientific community. Yet, with an ever-growing volume of information, science communicators struggle to anticipate how audiences will perceive and interact with scientific news. In this paper, we introduce a computational framework that models public perception across twelve dimensions, such as newsworthiness, importance, and surprisingness. Using this framework, we create a large-scale science news perception dataset with 10,489 annotations from 2,101 participants from diverse US and UK populations, providing valuable insights into public responses to scientific information across domains. We further develop NLP models that predict public perception scores with a strong performance. Leveraging the dataset and model, we examine public perception of science from two perspectives: (1) Perception as an outcome: What factors affect the public perception of scientific information? (2) Perception as a predictor: Can we use the estimated perceptions to predict public engagement with science? We find that individuals' frequency of science news consumption is the driver of perception, whereas demographic factors exert minimal influence. More importantly, through a large-scale analysis and carefully designed natural experiment on Reddit, we demonstrate that the estimated public perception of scientific information has direct connections with the final engagement pattern. Posts with more positive perception scores receive significantly more comments and upvotes, which is consistent across different scientific information and for the same science, but are framed differently. Overall, this research underscores the importance of nuanced perception modeling in science communication, offering new pathways to predict public interest and engagement with scientific content.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16622","src/content/posts/2025-06-23-modeling-public-perceptions-of-science-in-media.md","71f26fa7aa627d56",{"html":25,"metadata":29276},{"headings":29277,"localImagePaths":29278,"remoteImagePaths":29279,"frontmatter":29280,"imagePaths":29281},[],[],[],{"title":29269,"description":29270,"summary":29270,"pubDate":25202,"source":25191,"url":29272,"thumbnail":25193},[],"2025-06-23-modeling-public-perceptions-of-science-in-media.md","2025-06-23-moirexnet-adaptive-multi-scale-demoireing-with-linear-attention-test-time-training-and-truncated-flow-matching-prior",{"id":29283,"data":29285,"filePath":29290,"digest":29291,"rendered":29292,"legacyId":29299},{"title":29286,"description":29287,"summary":29287,"pubDate":29288,"source":25191,"url":29289,"thumbnail":25193},"Moir'eXNet: Adaptive Multi-Scale Demoir'eing with Linear Attention Test-Time Training and Truncated Flow Matching Prior","arXiv:2506.15929v1 Announce Type: cross Abstract: This paper introduces a novel framework for image and video demoir'eing by integrating Maximum A Posteriori (MAP) estimation with advanced deep learning techniques. Demoir'eing addresses inherently nonlinear degradation processes, which pose significant challenges for existing methods. Traditional supervised learning approaches either fail to remove moir'e patterns completely or produce overly smooth results. This stems from constrained model capacity and scarce training data, which inadequately represent the clean image distribution and hinder accurate reconstruction of ground-truth images. While generative models excel in image restoration for linear degradations, they struggle with nonlinear cases such as demoir'eing and often introduce artifacts. To address these limitations, we propose a hybrid MAP-based framework that integrates two complementary components. The first is a supervised learning model enhanced with efficient linear attention Test-Time Training (TTT) modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir'eing. The second is a Truncated Flow Matching Prior (TFMP) that further refines the outputs by aligning them with the clean image distribution, effectively restoring high-frequency details and suppressing artifacts. These two components combine the computational efficiency of linear attention with the refinement abilities of generative models, resulting in improved restoration performance.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15929","src/content/posts/2025-06-23-moirexnet-adaptive-multi-scale-demoireing-with-linear-attention-test-time-training-and-truncated-flow-matching-prior.md","cf84e308d9719524",{"html":25,"metadata":29293},{"headings":29294,"localImagePaths":29295,"remoteImagePaths":29296,"frontmatter":29297,"imagePaths":29298},[],[],[],{"title":29286,"description":29287,"summary":29287,"pubDate":25202,"source":25191,"url":29289,"thumbnail":25193},[],"2025-06-23-moirexnet-adaptive-multi-scale-demoireing-with-linear-attention-test-time-training-and-truncated-flow-matching-prior.md","2025-06-23-monetv2-enhanced-motion-network-for-freehand-3d-ultrasound-reconstruction",{"id":29300,"data":29302,"filePath":29307,"digest":29308,"rendered":29309,"legacyId":29316},{"title":29303,"description":29304,"summary":29304,"pubDate":29305,"source":25191,"url":29306,"thumbnail":25193},"MoNetV2: Enhanced Motion Network for Freehand 3D Ultrasound Reconstruction","arXiv:2506.15835v1 Announce Type: cross Abstract: Three-dimensional (3D) ultrasound (US) aims to provide sonographers with the spatial relationships of anatomical structures, playing a crucial role in clinical diagnosis. Recently, deep-learning-based freehand 3D US has made significant advancements. It reconstructs volumes by estimating transformations between images without external tracking. However, image-only reconstruction poses difficulties in reducing cumulative drift and further improving reconstruction accuracy, particularly in scenarios involving complex motion trajectories. In this context, we propose an enhanced motion network (MoNetV2) to enhance the accuracy and generalizability of reconstruction under diverse scanning velocities and tactics. First, we propose a sensor-based temporal and multi-branch structure that fuses image and motion information from a velocity perspective to improve image-only reconstruction accuracy. Second, we devise an online multi-level consistency constraint that exploits the inherent consistency of scans to handle various scanning velocities and tactics. This constraint exploits both scan-level velocity consistency, path-level appearance consistency, and patch-level motion consistency to supervise inter-frame transformation estimation. Third, we distill an online multi-modal self-supervised strategy that leverages the correlation between network estimation and motion information to further reduce cumulative errors. Extensive experiments clearly demonstrate that MoNetV2 surpasses existing methods in both reconstruction quality and generalizability performance across three large datasets.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15835","src/content/posts/2025-06-23-monetv2-enhanced-motion-network-for-freehand-3d-ultrasound-reconstruction.md","a426398f719a7f94",{"html":25,"metadata":29310},{"headings":29311,"localImagePaths":29312,"remoteImagePaths":29313,"frontmatter":29314,"imagePaths":29315},[],[],[],{"title":29303,"description":29304,"summary":29304,"pubDate":25202,"source":25191,"url":29306,"thumbnail":25193},[],"2025-06-23-monetv2-enhanced-motion-network-for-freehand-3d-ultrasound-reconstruction.md","2025-06-23-monosowa-scalable-monocular-3d-object-detector-without-human-annotations",{"id":29317,"data":29319,"filePath":29324,"digest":29325,"rendered":29326,"legacyId":29333},{"title":29320,"description":29321,"summary":29321,"pubDate":29322,"source":25191,"url":29323,"thumbnail":25193},"MonoSOWA: Scalable monocular 3D Object detector Without human Annotations","arXiv:2501.09481v3 Announce Type: replace-cross Abstract: Inferring object 3D position and orientation from a single RGB camera is a foundational task in computer vision with many important applications. Traditionally, 3D object detection methods are trained in a fully-supervised setup, requiring LiDAR and vast amounts of human annotations, which are laborious, costly, and do not scale well with the ever-increasing amounts of data being captured. We present a novel method to train a 3D object detector from a single RGB camera without domain-specific human annotations, making orders of magnitude more data available for training. The method uses newly proposed Local Object Motion Model to disentangle object movement source between subsequent frames, is approximately 700 times faster than previous work and compensates camera focal length differences to aggregate multiple datasets. The method is evaluated on three public datasets, where despite using no human labels, it outperforms prior work by a significant margin. It also shows its versatility as a pre-training tool for fully-supervised training and shows that combining pseudo-labels from multiple datasets can achieve comparable accuracy to using human labels from a single dataset. The source code and model are available at https://github.com/jskvrna/MonoSOWA.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2501.09481","src/content/posts/2025-06-23-monosowa-scalable-monocular-3d-object-detector-without-human-annotations.md","181ac039a1eb3e79",{"html":25,"metadata":29327},{"headings":29328,"localImagePaths":29329,"remoteImagePaths":29330,"frontmatter":29331,"imagePaths":29332},[],[],[],{"title":29320,"description":29321,"summary":29321,"pubDate":25202,"source":25191,"url":29323,"thumbnail":25193},[],"2025-06-23-monosowa-scalable-monocular-3d-object-detector-without-human-annotations.md","2025-06-23-mor-better-handling-diverse-queries-with-a-mixture-of-sparse-dense-and-human-retrievers",{"id":29334,"data":29336,"filePath":29341,"digest":29342,"rendered":29343,"legacyId":29350},{"title":29337,"description":29338,"summary":29338,"pubDate":29339,"source":25191,"url":29340,"thumbnail":25193},"MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers","arXiv:2506.15862v1 Announce Type: cross Abstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness hinges on which retrievers we use and how. Different retrievers offer distinct, often complementary signals: BM25 captures lexical matches; dense retrievers, semantic similarity. Yet in practice, we typically fix a single retriever based on heuristics, which fails to generalize across diverse information needs. Can we dynamically select and integrate multiple retrievers for each individual query, without the need for manual selection? In our work, we validate this intuition with quantitative analysis and introduce mixture of retrievers: a zero-shot, weighted combination of heterogeneous retrievers. Extensive experiments show that such mixtures are effective and efficient: Despite totaling just 0.8B parameters, this mixture outperforms every individual retriever and even larger 7B models by +10.8% and +3.9% on average, respectively. Further analysis also shows that this mixture framework can help incorporate specialized non-oracle human information sources as retrievers to achieve good collaboration, with a 58.9% relative performance improvement over simulated humans alone.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15862","src/content/posts/2025-06-23-mor-better-handling-diverse-queries-with-a-mixture-of-sparse-dense-and-human-retrievers.md","f469053d12d87c54",{"html":25,"metadata":29344},{"headings":29345,"localImagePaths":29346,"remoteImagePaths":29347,"frontmatter":29348,"imagePaths":29349},[],[],[],{"title":29337,"description":29338,"summary":29338,"pubDate":25202,"source":25191,"url":29340,"thumbnail":25193},[],"2025-06-23-mor-better-handling-diverse-queries-with-a-mixture-of-sparse-dense-and-human-retrievers.md","2025-06-23-more-thinking-less-seeing-assessing-amplified-hallucination-in-multimodal-reasoning-models",{"id":29351,"data":29353,"filePath":29358,"digest":29359,"rendered":29360,"legacyId":29367},{"title":29354,"description":29355,"summary":29355,"pubDate":29356,"source":25191,"url":29357,"thumbnail":25193},"More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models","arXiv:2505.21523v3 Announce Type: replace-cross Abstract: Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.21523","src/content/posts/2025-06-23-more-thinking-less-seeing-assessing-amplified-hallucination-in-multimodal-reasoning-models.md","eb1a213b07c8ccb1",{"html":25,"metadata":29361},{"headings":29362,"localImagePaths":29363,"remoteImagePaths":29364,"frontmatter":29365,"imagePaths":29366},[],[],[],{"title":29354,"description":29355,"summary":29355,"pubDate":25202,"source":25191,"url":29357,"thumbnail":25193},[],"2025-06-23-more-thinking-less-seeing-assessing-amplified-hallucination-in-multimodal-reasoning-models.md","2025-06-23-multi-preference-optimization-generalizing-dpo-via-set-level-contrasts",{"id":29368,"data":29370,"filePath":29375,"digest":29376,"rendered":29377,"legacyId":29384},{"title":29371,"description":29372,"summary":29372,"pubDate":29373,"source":25191,"url":29374,"thumbnail":25193},"Multi-Preference Optimization: Generalizing DPO via Set-Level Contrasts","arXiv:2412.04628v4 Announce Type: replace-cross Abstract: Direct Preference Optimization (DPO) has become a popular approach for aligning language models using pairwise preferences. However, in practical post-training pipelines, on-policy generation typically yields multiple candidate responses per prompt, which are scored by a reward model to guide learning. In this setting, we propose $textbf{Multi-Preference Optimization (MPO)}$, a generalization of DPO that optimizes over entire sets of responses by extending the Bradley-Terry model to groupwise comparisons between chosen and rejected sets. To further enhance learning, MPO employs deviation-based weighting, which emphasizes outlier responses that deviate most from the mean reward, effectively inducing a self-paced curriculum. We theoretically prove that MPO reduces alignment bias at a rate of $mathcal{O}left(frac{1}{sqrt{n}}right)$ with respect to the number of responses per query. Empirically, MPO achieves state-of-the-art performance on the UltraFeedback benchmark and yields up to $sim 17.5%$ improvement over the state-of-the-art baseline in length-controlled win rate on AlpacaEval2, establishing a new baseline for preference-based alignment",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2412.04628","src/content/posts/2025-06-23-multi-preference-optimization-generalizing-dpo-via-set-level-contrasts.md","07810d4a52c62135",{"html":25,"metadata":29378},{"headings":29379,"localImagePaths":29380,"remoteImagePaths":29381,"frontmatter":29382,"imagePaths":29383},[],[],[],{"title":29371,"description":29372,"summary":29372,"pubDate":25202,"source":25191,"url":29374,"thumbnail":25193},[],"2025-06-23-multi-preference-optimization-generalizing-dpo-via-set-level-contrasts.md","2025-06-23-multimodal-fused-learning-for-solving-the-generalized-traveling-salesman-problem-in-robotic-task-planning",{"id":29385,"data":29387,"filePath":29392,"digest":29393,"rendered":29394,"legacyId":29401},{"title":29388,"description":29389,"summary":29389,"pubDate":29390,"source":25191,"url":29391,"thumbnail":25193},"Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning","arXiv:2506.16931v1 Announce Type: new Abstract: Effective and efficient task planning is essential for mobile robots, especially in applications like warehouse retrieval and environmental monitoring. These tasks often involve selecting one location from each of several target clusters, forming a Generalized Traveling Salesman Problem (GTSP) that remains challenging to solve both accurately and efficiently. To address this, we propose a Multimodal Fused Learning (MMFL) framework that leverages both graph and image-based representations to capture complementary aspects of the problem, and learns a policy capable of generating high-quality task planning schemes in real time. Specifically, we first introduce a coordinate-based image builder that transforms GTSP instances into spatially informative representations. We then design an adaptive resolution scaling strategy to enhance adaptability across different problem scales, and develop a multimodal fusion module with dedicated bottlenecks that enables effective integration of geometric and spatial features. Extensive experiments show that our MMFL approach significantly outperforms state-of-the-art methods across various GTSP instances while maintaining the computational efficiency required for real-time robotic applications. Physical robot tests further validate its practical effectiveness in real-world scenarios.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16931","src/content/posts/2025-06-23-multimodal-fused-learning-for-solving-the-generalized-traveling-salesman-problem-in-robotic-task-planning.md","a4d3a095d8d77bd4",{"html":25,"metadata":29395},{"headings":29396,"localImagePaths":29397,"remoteImagePaths":29398,"frontmatter":29399,"imagePaths":29400},[],[],[],{"title":29388,"description":29389,"summary":29389,"pubDate":25202,"source":25191,"url":29391,"thumbnail":25193},[],"2025-06-23-multimodal-fused-learning-for-solving-the-generalized-traveling-salesman-problem-in-robotic-task-planning.md","2025-06-23-nature-language-model-deciphering-the-language-of-nature-for-scientific-discovery",{"id":29402,"data":29404,"filePath":29409,"digest":29410,"rendered":29411,"legacyId":29418},{"title":29405,"description":29406,"summary":29406,"pubDate":29407,"source":25191,"url":29408,"thumbnail":25193},"Nature Language Model: Deciphering the Language of Nature for Scientific Discovery","arXiv:2502.07527v3 Announce Type: replace Abstract: Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, RNA and even cells. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the 'language of nature', we introduce Nature Language Model (NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) top performance across different domains, matching or surpassing state-of-the-art specialist models. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2502.07527","src/content/posts/2025-06-23-nature-language-model-deciphering-the-language-of-nature-for-scientific-discovery.md","2f1348a4dccfac89",{"html":25,"metadata":29412},{"headings":29413,"localImagePaths":29414,"remoteImagePaths":29415,"frontmatter":29416,"imagePaths":29417},[],[],[],{"title":29405,"description":29406,"summary":29406,"pubDate":25202,"source":25191,"url":29408,"thumbnail":25193},[],"2025-06-23-nature-language-model-deciphering-the-language-of-nature-for-scientific-discovery.md","2025-06-23-nepaligpt-a-generative-language-model-for-the-nepali-language",{"id":29419,"data":29421,"filePath":29426,"digest":29427,"rendered":29428,"legacyId":29435},{"title":29422,"description":29423,"summary":29423,"pubDate":29424,"source":25191,"url":29425,"thumbnail":25193},"NepaliGPT: A Generative Language Model for the Nepali Language","arXiv:2506.16399v1 Announce Type: cross Abstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge popularity in recent days and thousands of variants of LLMs have been released. However, there is no generative language model for the Nepali language, due to which other downstream tasks, including fine-tuning, have not been explored yet. To fill this research gap in the Nepali NLP space, this research proposes textit{NepaliGPT}, a generative large language model tailored specifically for the Nepali language. This research introduces an advanced corpus for the Nepali language collected from several sources, called the Devanagari Corpus. Likewise, the research introduces the first NepaliGPT benchmark dataset comprised of 4,296 question-answer pairs in the Nepali language. The proposed LLM NepaliGPT achieves the following metrics in text generation: Perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25%, and causal consistency of 85.41%.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16399","src/content/posts/2025-06-23-nepaligpt-a-generative-language-model-for-the-nepali-language.md","7e03070143b79cfe",{"html":25,"metadata":29429},{"headings":29430,"localImagePaths":29431,"remoteImagePaths":29432,"frontmatter":29433,"imagePaths":29434},[],[],[],{"title":29422,"description":29423,"summary":29423,"pubDate":25202,"source":25191,"url":29425,"thumbnail":25193},[],"2025-06-23-nepaligpt-a-generative-language-model-for-the-nepali-language.md","2025-06-23-network-sparsity-unlocks-the-scaling-potential-of-deep-reinforcement-learning",{"id":29436,"data":29438,"filePath":29443,"digest":29444,"rendered":29445,"legacyId":29452},{"title":29439,"description":29440,"summary":29440,"pubDate":29441,"source":25191,"url":29442,"thumbnail":25193},"Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning","arXiv:2506.17204v1 Announce Type: cross Abstract: Effectively scaling up deep reinforcement learning models has proven notoriously difficult due to network pathologies during training, motivating various targeted interventions such as periodic reset and architectural advances such as layer normalization. Instead of pursuing more complex modifications, we show that introducing static network sparsity alone can unlock further scaling potential beyond their dense counterparts with state-of-the-art architectures. This is achieved through simple one-shot random pruning, where a predetermined percentage of network weights are randomly removed once before training. Our analysis reveals that, in contrast to naively scaling up dense DRL networks, such sparse networks achieve both higher parameter efficiency for network expressivity and stronger resistance to optimization challenges like plasticity loss and gradient interference. We further extend our evaluation to visual and streaming RL scenarios, demonstrating the consistent benefits of network sparsity.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17204","src/content/posts/2025-06-23-network-sparsity-unlocks-the-scaling-potential-of-deep-reinforcement-learning.md","eb05b74422a78b48",{"html":25,"metadata":29446},{"headings":29447,"localImagePaths":29448,"remoteImagePaths":29449,"frontmatter":29450,"imagePaths":29451},[],[],[],{"title":29439,"description":29440,"summary":29440,"pubDate":25202,"source":25191,"url":29442,"thumbnail":25193},[],"2025-06-23-network-sparsity-unlocks-the-scaling-potential-of-deep-reinforcement-learning.md","2025-06-23-neuronseek-on-stability-and-expressivity-of-task-driven-neurons",{"id":29453,"data":29455,"filePath":29460,"digest":29461,"rendered":29462,"legacyId":29469},{"title":29456,"description":29457,"summary":29457,"pubDate":29458,"source":25191,"url":29459,"thumbnail":25193},"NeuronSeek: On Stability and Expressivity of Task-driven Neurons","arXiv:2506.15715v1 Announce Type: cross Abstract: Drawing inspiration from our human brain that designs different neurons for different tasks, recent advances in deep learning have explored modifying a network's neurons to develop so-called task-driven neurons. Prototyping task-driven neurons (referred to as NeuronSeek) employs symbolic regression (SR) to discover the optimal neuron formulation and construct a network from these optimized neurons. Along this direction, this work replaces symbolic regression with tensor decomposition (TD) to discover optimal neuronal formulations, offering enhanced stability and faster convergence. Furthermore, we establish theoretical guarantees that modifying the aggregation functions with common activation functions can empower a network with a fixed number of parameters to approximate any continuous function with an arbitrarily small error, providing a rigorous mathematical foundation for the NeuronSeek framework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD framework not only achieves superior stability, but also is competitive relative to the state-of-the-art models across diverse benchmarks. The code is available at https://github.com/HanyuPei22/NeuronSeek.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15715","src/content/posts/2025-06-23-neuronseek-on-stability-and-expressivity-of-task-driven-neurons.md","fdc94fbfc3c14ad6",{"html":25,"metadata":29463},{"headings":29464,"localImagePaths":29465,"remoteImagePaths":29466,"frontmatter":29467,"imagePaths":29468},[],[],[],{"title":29456,"description":29457,"summary":29457,"pubDate":25202,"source":25191,"url":29459,"thumbnail":25193},[],"2025-06-23-neuronseek-on-stability-and-expressivity-of-task-driven-neurons.md","2025-06-23-next-token-prediction-should-be-ambiguity-sensitive-a-meta-learning-perspective",{"id":29470,"data":29472,"filePath":29477,"digest":29478,"rendered":29479,"legacyId":29486},{"title":29473,"description":29474,"summary":29474,"pubDate":29475,"source":25191,"url":29476,"thumbnail":25193},"Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective","arXiv:2506.16288v1 Announce Type: cross Abstract: The rapid adaptation ability of auto-regressive foundation models is often attributed to the diversity of their pre-training data. This is because, from a Bayesian standpoint, minimizing prediction error in such settings requires integrating over all plausible latent hypotheses consistent with observations. While this behavior is desirable in principle, it often proves too ambitious in practice: under high ambiguity, the number of plausible latent alternatives makes Bayes-optimal prediction computationally intractable. Cognitive science has long recognized this limitation, suggesting that under such conditions, heuristics or information-seeking strategies are preferable to exhaustive inference. Translating this insight to next-token prediction, we hypothesize that low- and high-ambiguity predictions pose different computational demands, making ambiguity-agnostic next-token prediction a detrimental inductive bias. To test this, we introduce MetaHMM, a synthetic sequence meta-learning benchmark with rich compositional structure and a tractable Bayesian oracle. We show that Transformers indeed struggle with high-ambiguity predictions across model sizes. Motivated by cognitive theories, we propose a method to convert pre-trained models into Monte Carlo predictors that decouple task inference from token prediction. Preliminary results show substantial gains in ambiguous contexts through improved capacity allocation and test-time scalable inference, though challenges remain.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16288","src/content/posts/2025-06-23-next-token-prediction-should-be-ambiguity-sensitive-a-meta-learning-perspective.md","28c90d1869a0809a",{"html":25,"metadata":29480},{"headings":29481,"localImagePaths":29482,"remoteImagePaths":29483,"frontmatter":29484,"imagePaths":29485},[],[],[],{"title":29473,"description":29474,"summary":29474,"pubDate":25202,"source":25191,"url":29476,"thumbnail":25193},[],"2025-06-23-next-token-prediction-should-be-ambiguity-sensitive-a-meta-learning-perspective.md","2025-06-23-no-free-lunch-rethinking-internal-feedback-for-llm-reasoning",{"id":29487,"data":29489,"filePath":29494,"digest":29495,"rendered":29496,"legacyId":29503},{"title":29490,"description":29491,"summary":29491,"pubDate":29492,"source":25191,"url":29493,"thumbnail":25193},"No Free Lunch: Rethinking Internal Feedback for LLM Reasoning","arXiv:2506.17219v1 Announce Type: cross Abstract: Reinforcement learning has emerged as a powerful paradigm for post-training large language models (LLMs) to improve reasoning. Approaches like Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) have shown strong results, but they require extensive external supervision. We investigate an alternative class of methods, Reinforcement Learning from Internal Feedback (RLIF), which relies solely on intrinsic model-derived signals instead of external rewards. In particular, we leverage unsupervised reward proxies such as token-level entropy, trajectory-level entropy, and self-certainty. Our theoretical analysis shows these internal objectives are partially equivalent, and we empirically evaluate various RLIF strategies on challenging math reasoning benchmarks. Experimental results demonstrate that RLIF can boost the reasoning performance of base LLMs at the beginning phase of the training, matching or surpassing RLVR techniques on these tasks. However, when training progresses, performance degrades even below the model before training. Moreover, we find that RLIF yields little improvement for instruction-tuned models, indicating diminishing returns of intrinsic feedback once an LLM is already instruction-tuned. We further analyze this limitation by mixing model weights and explain the reason of RLIF's training behaviors, providing practical guidelines for integrating internal feedback signals into LLM training. We hope our analysis of internal feedback will inform more principled and effective strategies for LLM post-training.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17219","src/content/posts/2025-06-23-no-free-lunch-rethinking-internal-feedback-for-llm-reasoning.md","b91b22e21ece9d74",{"html":25,"metadata":29497},{"headings":29498,"localImagePaths":29499,"remoteImagePaths":29500,"frontmatter":29501,"imagePaths":29502},[],[],[],{"title":29490,"description":29491,"summary":29491,"pubDate":25202,"source":25191,"url":29493,"thumbnail":25193},[],"2025-06-23-no-free-lunch-rethinking-internal-feedback-for-llm-reasoning.md","2025-06-23-ntt-comも参入-nttグループはaiエージェント事業に総力を結集できるか",{"id":29504,"data":29506,"filePath":29512,"digest":29513,"rendered":29514,"legacyId":29522},{"title":29507,"description":29508,"summary":29508,"pubDate":29509,"source":24944,"url":29510,"thumbnail":29511},"NTT Comも参入　NTTグループはAIエージェント事業に総力を結集できるか？","“百花繚乱”のAIエージェント市場において、NTT Comが業務、業界別のソリューションを打ち出した。NTTデータも既にAIエージェント市場に参入しているが、NTTグループとしてこの分野に総力を結集する態勢になり得るのだろうか。",["Date","2025-06-23T07:00:00.000Z"],"https://www.itmedia.co.jp/enterprise/articles/2506/23/news064.html","https://image.itmedia.co.jp/enterprise/articles/2506/23/cover_news064.jpg","src/content/posts/2025-06-23-ntt-comも参入-nttグループはaiエージェント事業に総力を結集できるか.md","74bd010a47a9a355",{"html":25,"metadata":29515},{"headings":29516,"localImagePaths":29517,"remoteImagePaths":29518,"frontmatter":29519,"imagePaths":29521},[],[],[],{"title":29507,"description":29508,"summary":29508,"pubDate":29520,"source":24944,"url":29510,"thumbnail":29511},"Mon, 23 Jun 2025 16:00:00 +0900",[],"2025-06-23-ntt-comも参入-nttグループはaiエージェント事業に総力を結集できるか.md","2025-06-23-oagents-an-empirical-study-of-building-effective-agents",{"id":29523,"data":29525,"filePath":29530,"digest":29531,"rendered":29532,"legacyId":29539},{"title":29526,"description":29527,"summary":29527,"pubDate":29528,"source":25191,"url":29529,"thumbnail":25193},"OAgents: An Empirical Study of Building Effective Agents","arXiv:2506.15741v1 Announce Type: new Abstract: Recently, Agentic AI has become an increasingly popular research field. However, we argue that current agent research practices lack standardization and scientific rigor, making it hard to conduct fair comparisons among methods. As a result, it is still unclear how different design choices in agent frameworks affect effectiveness, and measuring their progress remains challenging. In this work, we conduct a systematic empirical study on GAIA benchmark and BrowseComp to examine the impact of popular design choices in key agent components in a fair and rigorous manner. We find that the lack of a standard evaluation protocol makes previous works, even open-sourced ones, non-reproducible, with significant variance between random runs. Therefore, we introduce a more robust evaluation protocol to stabilize comparisons. Our study reveals which components and designs are crucial for effective agents, while others are redundant, despite seeming logical. Based on our findings, we build and open-source OAgents, a new foundation agent framework that achieves state-of-the-art performance among open-source projects. OAgents offers a modular design for various agent components, promoting future research in Agentic AI.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15741","src/content/posts/2025-06-23-oagents-an-empirical-study-of-building-effective-agents.md","1bff5620415367e5",{"html":25,"metadata":29533},{"headings":29534,"localImagePaths":29535,"remoteImagePaths":29536,"frontmatter":29537,"imagePaths":29538},[],[],[],{"title":29526,"description":29527,"summary":29527,"pubDate":25202,"source":25191,"url":29529,"thumbnail":25193},[],"2025-06-23-oagents-an-empirical-study-of-building-effective-agents.md","2025-06-23-off-policy-actor-critic-for-adversarial-observation-robustness-virtual-alternative-training-via-symmetric-policy-evaluation",{"id":29540,"data":29542,"filePath":29547,"digest":29548,"rendered":29549,"legacyId":29556},{"title":29543,"description":29544,"summary":29544,"pubDate":29545,"source":25191,"url":29546,"thumbnail":25193},"Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation","arXiv:2506.16753v1 Announce Type: cross Abstract: Recently, robust reinforcement learning (RL) methods designed to handle adversarial input observations have received significant attention, motivated by RL's inherent vulnerabilities. While existing approaches have demonstrated reasonable success, addressing worst-case scenarios over long time horizons requires both minimizing the agent's cumulative rewards for adversaries and training agents to counteract them through alternating learning. However, this process introduces mutual dependencies between the agent and the adversary, making interactions with the environment inefficient and hindering the development of off-policy methods. In this work, we propose a novel off-policy method that eliminates the need for additional environmental interactions by reformulating adversarial learning as a soft-constrained optimization problem. Our approach is theoretically supported by the symmetric property of policy evaluation between the agent and the adversary. The implementation is available at https://github.com/nakanakakosuke/VALT_SAC.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16753","src/content/posts/2025-06-23-off-policy-actor-critic-for-adversarial-observation-robustness-virtual-alternative-training-via-symmetric-policy-evaluation.md","5bbdb1bf55cbd9b7",{"html":25,"metadata":29550},{"headings":29551,"localImagePaths":29552,"remoteImagePaths":29553,"frontmatter":29554,"imagePaths":29555},[],[],[],{"title":29543,"description":29544,"summary":29544,"pubDate":25202,"source":25191,"url":29546,"thumbnail":25193},[],"2025-06-23-off-policy-actor-critic-for-adversarial-observation-robustness-virtual-alternative-training-via-symmetric-policy-evaluation.md","2025-06-23-on-path-to-multimodal-historical-reasoning-histbench-and-histagent",{"id":29557,"data":29559,"filePath":29564,"digest":29565,"rendered":29566,"legacyId":29573},{"title":29560,"description":29561,"summary":29561,"pubDate":29562,"source":25191,"url":29563,"thumbnail":25193},"On Path to Multimodal Historical Reasoning: HistBench and HistAgent","arXiv:2505.20246v3 Announce Type: replace Abstract: Recent advances in large language models (LLMs) have led to remarkable progress across domains, yet their capabilities in the humanities, particularly history, remain underexplored. Historical reasoning poses unique challenges for AI, involving multimodal source interpretation, temporal inference, and cross-linguistic analysis. While general-purpose agents perform well on many existing benchmarks, they lack the domain-specific expertise required to engage with historical materials and questions. To address this gap, we introduce HistBench, a new benchmark of 414 high-quality questions designed to evaluate AI's capacity for historical reasoning and authored by more than 40 expert contributors. The tasks span a wide range of historical problems-from factual retrieval based on primary sources to interpretive analysis of manuscripts and images, to interdisciplinary challenges involving archaeology, linguistics, or cultural history. Furthermore, the benchmark dataset spans 29 ancient and modern languages and covers a wide range of historical periods and world regions. Finding the poor performance of LLMs and other agents on HistBench, we further present HistAgent, a history-specific agent equipped with carefully designed tools for OCR, translation, archival search, and image understanding in History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of 27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online search and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%) and Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These results highlight the limitations of existing LLMs and generalist agents and demonstrate the advantages of HistAgent for historical reasoning.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.20246","src/content/posts/2025-06-23-on-path-to-multimodal-historical-reasoning-histbench-and-histagent.md","eab683823056dbd3",{"html":25,"metadata":29567},{"headings":29568,"localImagePaths":29569,"remoteImagePaths":29570,"frontmatter":29571,"imagePaths":29572},[],[],[],{"title":29560,"description":29561,"summary":29561,"pubDate":25202,"source":25191,"url":29563,"thumbnail":25193},[],"2025-06-23-on-path-to-multimodal-historical-reasoning-histbench-and-histagent.md","2025-06-23-on-the-limits-of-language-generation-trade-offs-between-hallucination-and-mode-collapse",{"id":29574,"data":29576,"filePath":29581,"digest":29582,"rendered":29583,"legacyId":29590},{"title":29577,"description":29578,"summary":29578,"pubDate":29579,"source":25191,"url":29580,"thumbnail":25193},"On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse","arXiv:2411.09642v2 Announce Type: replace-cross Abstract: Specifying all desirable properties of a language model is challenging, but certain requirements seem essential. Given samples from an unknown language, the trained model should produce valid strings not seen in training and be expressive enough to capture the language's full richness. Otherwise, outputting invalid strings constitutes 'hallucination,' and failing to capture the full range leads to 'mode collapse.' We ask if a language model can meet both requirements. We investigate this within a statistical language generation setting building on Gold and Angluin. Here, the model receives random samples from a distribution over an unknown language K, which belongs to a possibly infinite collection of languages. The goal is to generate unseen strings from K. We say the model generates from K with consistency and breadth if, as training size increases, its output converges to all unseen strings in K. Kleinberg and Mullainathan [KM24] asked if consistency and breadth in language generation are possible. We answer this negatively: for a large class of language models, including next-token prediction models, this is impossible for most collections of candidate languages. This contrasts with [KM24]'s result, showing consistent generation without breadth is possible for any countable collection of languages. Our finding highlights that generation with breadth fundamentally differs from generation without breadth. As a byproduct, we establish near-tight bounds on the number of samples needed for generation with or without breadth. Finally, our results offer hope: consistent generation with breadth is achievable for any countable collection of languages when negative examples (strings outside K) are available alongside positive ones. This suggests that post-training feedback, which encodes negative examples, can be crucial in reducing hallucinations while limiting mode collapse.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2411.09642","src/content/posts/2025-06-23-on-the-limits-of-language-generation-trade-offs-between-hallucination-and-mode-collapse.md","a754925de2952dfe",{"html":25,"metadata":29584},{"headings":29585,"localImagePaths":29586,"remoteImagePaths":29587,"frontmatter":29588,"imagePaths":29589},[],[],[],{"title":29577,"description":29578,"summary":29578,"pubDate":25202,"source":25191,"url":29580,"thumbnail":25193},[],"2025-06-23-on-the-limits-of-language-generation-trade-offs-between-hallucination-and-mode-collapse.md","2025-06-23-on-training-test-misalignment-in-unsupervised-combinatorial-optimization-observation-empirical-exploration-and-analysis",{"id":29591,"data":29593,"filePath":29598,"digest":29599,"rendered":29600,"legacyId":29607},{"title":29594,"description":29595,"summary":29595,"pubDate":29596,"source":25191,"url":29597,"thumbnail":25193},"On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis","arXiv:2506.16732v1 Announce Type: cross Abstract: In unsupervised combinatorial optimization (UCO), during training, one aims to have continuous decisions that are promising in a probabilistic sense for each training instance, which enables end-to-end training on initially discrete and non-differentiable problems. At the test time, for each test instance, starting from continuous decisions, derandomization is typically applied to obtain the final deterministic decisions. Researchers have developed more and more powerful test-time derandomization schemes to enhance the empirical performance and the theoretical guarantee of UCO methods. However, we notice a misalignment between training and testing in the existing UCO methods. Consequently, lower training losses do not necessarily entail better post-derandomization performance, even for the training instances without any data distribution shift. Empirically, we indeed observe such undesirable cases. We explore a preliminary idea to better align training and testing in UCO by including a differentiable version of derandomization into training. Our empirical exploration shows that such an idea indeed improves training-test alignment, but also introduces nontrivial challenges into training.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16732","src/content/posts/2025-06-23-on-training-test-misalignment-in-unsupervised-combinatorial-optimization-observation-empirical-exploration-and-analysis.md","2454ad24fcb543c3",{"html":25,"metadata":29601},{"headings":29602,"localImagePaths":29603,"remoteImagePaths":29604,"frontmatter":29605,"imagePaths":29606},[],[],[],{"title":29594,"description":29595,"summary":29595,"pubDate":25202,"source":25191,"url":29597,"thumbnail":25193},[],"2025-06-23-on-training-test-misalignment-in-unsupervised-combinatorial-optimization-observation-empirical-exploration-and-analysis.md","2025-06-23-on-using-ai-for-eeg-based-bci-applications-problems-current-challenges-and-future-trends",{"id":29608,"data":29610,"filePath":29615,"digest":29616,"rendered":29617,"legacyId":29624},{"title":29611,"description":29612,"summary":29612,"pubDate":29613,"source":25191,"url":29614,"thumbnail":25193},"On using AI for EEG-based BCI applications: problems, current challenges and future trends","arXiv:2506.16168v1 Announce Type: cross Abstract: Imagine unlocking the power of the mind to communicate, create, and even interact with the world around us. Recent breakthroughs in Artificial Intelligence (AI), especially in how machines 'see' and 'understand' language, are now fueling exciting progress in decoding brain signals from scalp electroencephalography (EEG). Prima facie, this opens the door to revolutionary brain-computer interfaces (BCIs) designed for real life, moving beyond traditional uses to envision Brain-to-Speech, Brain-to-Image, and even a Brain-to-Internet of Things (BCIoT). However, the journey is not as straightforward as it was for Computer Vision (CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based BCIs, particularly in building powerful foundational models, presents unique and intricate hurdles that could affect their reliability. Here, we unfold a guided exploration of this dynamic and rapidly evolving research area. Rather than barely outlining a map of current endeavors and results, the goal is to provide a principled navigation of this hot and cutting-edge research landscape. We consider the basic paradigms that emerge from a causal perspective and the attendant challenges presented to AI-based models. Looking ahead, we then discuss promising research avenues that could overcome today's technological, methodological, and ethical limitations. Our aim is to lay out a clear roadmap for creating truly practical and effective EEG-based BCI solutions that can thrive in everyday environments.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16168","src/content/posts/2025-06-23-on-using-ai-for-eeg-based-bci-applications-problems-current-challenges-and-future-trends.md","e7d28e0da3a94bab",{"html":25,"metadata":29618},{"headings":29619,"localImagePaths":29620,"remoteImagePaths":29621,"frontmatter":29622,"imagePaths":29623},[],[],[],{"title":29611,"description":29612,"summary":29612,"pubDate":25202,"source":25191,"url":29614,"thumbnail":25193},[],"2025-06-23-on-using-ai-for-eeg-based-bci-applications-problems-current-challenges-and-future-trends.md","2025-06-23-one-sample-is-enough-to-make-conformal-prediction-robust",{"id":29625,"data":29627,"filePath":29632,"digest":29633,"rendered":29634,"legacyId":29641},{"title":29628,"description":29629,"summary":29629,"pubDate":29630,"source":25191,"url":29631,"thumbnail":25193},"One Sample is Enough to Make Conformal Prediction Robust","arXiv:2506.16553v1 Announce Type: cross Abstract: Given any model, conformal prediction (CP) returns prediction sets guaranteed to include the true label with high adjustable probability. Robust CP (RCP) extends this to inputs with worst-case noise. A well-established approach is to use randomized smoothing for RCP since it is applicable to any black-box model and provides smaller sets compared to deterministic methods. However, current smoothing-based RCP requires many model forward passes per each input which is computationally expensive. We show that conformal prediction attains some robustness even with a forward pass on a single randomly perturbed input. Using any binary certificate we propose a single sample robust CP (RCP1). Our approach returns robust sets with smaller average set size compared to SOTA methods which use many (e.g. around 100) passes per input. Our key insight is to certify the conformal prediction procedure itself rather than individual scores. Our approach is agnostic to the setup (classification and regression). We further extend our approach to smoothing-based robust conformal risk control.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16553","src/content/posts/2025-06-23-one-sample-is-enough-to-make-conformal-prediction-robust.md","ef886d6cc2148fbe",{"html":25,"metadata":29635},{"headings":29636,"localImagePaths":29637,"remoteImagePaths":29638,"frontmatter":29639,"imagePaths":29640},[],[],[],{"title":29628,"description":29629,"summary":29629,"pubDate":25202,"source":25191,"url":29631,"thumbnail":25193},[],"2025-06-23-one-sample-is-enough-to-make-conformal-prediction-robust.md","2025-06-23-one-step-diffusion-for-detail-rich-and-temporally-consistent-video-super-resolution",{"id":29642,"data":29644,"filePath":29649,"digest":29650,"rendered":29651,"legacyId":29658},{"title":29645,"description":29646,"summary":29646,"pubDate":29647,"source":25191,"url":29648,"thumbnail":25193},"One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution","arXiv:2506.15591v2 Announce Type: replace-cross Abstract: It is a challenging problem to reproduce rich spatial details while maintaining temporal consistency in real-world video super-resolution (Real-VSR), especially when we leverage pre-trained generative models such as stable diffusion (SD) for realistic details synthesis. Existing SD-based Real-VSR methods often compromise spatial details for temporal coherence, resulting in suboptimal visual quality. We argue that the key lies in how to effectively extract the degradation-robust temporal consistency priors from the low-quality (LQ) input video and enhance the video details while maintaining the extracted consistency priors. To achieve this, we propose a Dual LoRA Learning (DLoRAL) paradigm to train an effective SD-based one-step diffusion model, achieving realistic frame details and temporal consistency simultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module to aggregate complementary information across frames, and train a Consistency-LoRA (C-LoRA) to learn robust temporal representations from degraded inputs. After consistency learning, we fix the CFR and C-LoRA modules and train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with the temporal space defined by C-LoRA to keep temporal coherence. The two phases alternate iteratively for optimization, collaboratively delivering consistent and detail-rich outputs. During inference, the two LoRA branches are merged into the SD model, allowing efficient and high-quality video restoration in a single diffusion step. Experiments show that DLoRAL achieves strong performance in both accuracy and speed. Code and models are available at https://github.com/yjsunnn/DLoRAL.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15591","src/content/posts/2025-06-23-one-step-diffusion-for-detail-rich-and-temporally-consistent-video-super-resolution.md","5f22ea95e5d9c5f4",{"html":25,"metadata":29652},{"headings":29653,"localImagePaths":29654,"remoteImagePaths":29655,"frontmatter":29656,"imagePaths":29657},[],[],[],{"title":29645,"description":29646,"summary":29646,"pubDate":25202,"source":25191,"url":29648,"thumbnail":25193},[],"2025-06-23-one-step-diffusion-for-detail-rich-and-temporally-consistent-video-super-resolution.md","2025-06-23-open-set-graph-anomaly-detection-via-normal-structure-regularisation",{"id":29659,"data":29661,"filePath":29666,"digest":29667,"rendered":29668,"legacyId":29675},{"title":29662,"description":29663,"summary":29663,"pubDate":29664,"source":25191,"url":29665,"thumbnail":25193},"Open-Set Graph Anomaly Detection via Normal Structure Regularisation","arXiv:2311.06835v5 Announce Type: replace-cross Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to train a detection model using a small number of normal and anomaly nodes (referred to as seen anomalies) to detect both seen anomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the training anomalies). Those labelled training data provide crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current supervised GAD methods tend to over-emphasise fitting the seen anomalies, leading to many errors of detecting the unseen anomalies as normal nodes. Further, existing open-set AD models were introduced to handle Euclidean data, failing to effectively capture discriminative features from graph structure and node attributes for GAD. In this work, we propose a novel open-set GAD approach, namely normal structure regularisation (NSReg), to achieve generalised detection ability to unseen anomalies, while maintaining its effectiveness on detecting seen anomalies. The key idea in NSReg is to introduce a regularisation term that enforces the learning of compact, semantically-rich representations of normal nodes based on their structural relations to other nodes. When being optimised with supervised anomaly detection losses, the regularisation term helps incorporate strong normality into the modelling, and thus, it effectively avoids over-fitting the seen anomalies and learns a better normality decision boundary, largely reducing the false negatives of detecting unseen anomalies as normal. Extensive empirical results on seven real-world datasets show that NSReg significantly outperforms state-of-the-art competing methods by at least 14% AUC-ROC on the unseen anomaly classes and by 10% AUC-ROC on all anomaly classes.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2311.06835","src/content/posts/2025-06-23-open-set-graph-anomaly-detection-via-normal-structure-regularisation.md","2ed70f305084ec97",{"html":25,"metadata":29669},{"headings":29670,"localImagePaths":29671,"remoteImagePaths":29672,"frontmatter":29673,"imagePaths":29674},[],[],[],{"title":29662,"description":29663,"summary":29663,"pubDate":25202,"source":25191,"url":29665,"thumbnail":25193},[],"2025-06-23-open-set-graph-anomaly-detection-via-normal-structure-regularisation.md","2025-06-23-optimizing-moe-routers-design-implementation-and-evaluation-in-transformer-models",{"id":29676,"data":29678,"filePath":29683,"digest":29684,"rendered":29685,"legacyId":29692},{"title":29679,"description":29680,"summary":29680,"pubDate":29681,"source":25191,"url":29682,"thumbnail":25193},"Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models","arXiv:2506.16419v1 Announce Type: cross Abstract: Mixture of Experts (MoE) architectures increase large language model scalability, yet their performance depends on the router module that moves tokens to specialized experts. Bad routing can load imbalance and reduced accuracy. This project designed and implemented different router architectures within Transformer models to fix these limitations. We experimented with six distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP), Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference latency, routing entropy, and expert utilization patterns. Our evaluations showed distinct trade-offs: Linear routers offer speed, while MLP and Attention routers provide greater expressiveness. The MLP-Hadamard router shows a unique capability for structured, sparse routing. We successfully replaced and fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This work provides a comparative analysis of MoE router designs and offers insights into optimizing their performance for efficient and effective large-scale model deployment.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16419","src/content/posts/2025-06-23-optimizing-moe-routers-design-implementation-and-evaluation-in-transformer-models.md","b049d45de52864e5",{"html":25,"metadata":29686},{"headings":29687,"localImagePaths":29688,"remoteImagePaths":29689,"frontmatter":29690,"imagePaths":29691},[],[],[],{"title":29679,"description":29680,"summary":29680,"pubDate":25202,"source":25191,"url":29682,"thumbnail":25193},[],"2025-06-23-optimizing-moe-routers-design-implementation-and-evaluation-in-transformer-models.md","2025-06-23-optimizing-sensory-neurons-nonlinear-attention-mechanisms-for-accelerated-convergence-in-permutation-invariant-neural-networks-for-reinforcement-learning",{"id":29693,"data":29695,"filePath":29700,"digest":29701,"rendered":29702,"legacyId":29709},{"title":29696,"description":29697,"summary":29697,"pubDate":29698,"source":25191,"url":29699,"thumbnail":25193},"Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning","arXiv:2506.00691v3 Announce Type: replace-cross Abstract: Training reinforcement learning (RL) agents often requires significant computational resources and prolonged training durations. To address this challenge, we build upon prior work that introduced a neural architecture with permutation-invariant sensory processing. We propose a modified attention mechanism that applies a non-linear transformation to the key vectors (K), producing enriched representations (K') through a custom mapping function. This Nonlinear Attention (NLA) mechanism enhances the representational capacity of the attention layer, enabling the agent to learn more expressive feature interactions. As a result, our model achieves significantly faster convergence and improved training efficiency, while maintaining performance on par with the baseline. These results highlight the potential of nonlinear attention mechanisms to accelerate reinforcement learning without sacrificing effectiveness.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.00691","src/content/posts/2025-06-23-optimizing-sensory-neurons-nonlinear-attention-mechanisms-for-accelerated-convergence-in-permutation-invariant-neural-networks-for-reinforcement-learning.md","34f2fe190b525c50",{"html":25,"metadata":29703},{"headings":29704,"localImagePaths":29705,"remoteImagePaths":29706,"frontmatter":29707,"imagePaths":29708},[],[],[],{"title":29696,"description":29697,"summary":29697,"pubDate":25202,"source":25191,"url":29699,"thumbnail":25193},[],"2025-06-23-optimizing-sensory-neurons-nonlinear-attention-mechanisms-for-accelerated-convergence-in-permutation-invariant-neural-networks-for-reinforcement-learning.md","2025-06-23-osworld-human-benchmarking-the-efficiency-of-computer-use-agents",{"id":29710,"data":29712,"filePath":29717,"digest":29718,"rendered":29719,"legacyId":29726},{"title":29713,"description":29714,"summary":29714,"pubDate":29715,"source":25191,"url":29716,"thumbnail":25193},"OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents","arXiv:2506.16042v1 Announce Type: new Abstract: Generative AI is being leveraged to solve a variety of computer-use tasks involving desktop applications. State-of-the-art systems have focused solely on improving accuracy on leading benchmarks. However, these systems are practically unusable due to extremely high end-to-end latency (e.g., tens of minutes) for tasks that typically take humans just a few minutes to complete. To understand the cause behind this and to guide future developments of computer agents, we conduct the first study on the temporal performance of computer-use agents on OSWorld, the flagship benchmark in computer-use AI. We find that large model calls for planning and reflection account for the majority of the overall latency, and as an agent uses more steps to complete a task, each successive step can take 3x longer than steps at the beginning of a task. We then construct OSWorld-Human, a manually annotated version of the original OSWorld dataset that contains a human-determined trajectory for each task. We evaluate 16 agents on their efficiency using OSWorld-Human and found that even the highest-scoring agents on OSWorld take 1.4-2.7x more steps than necessary.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16042","src/content/posts/2025-06-23-osworld-human-benchmarking-the-efficiency-of-computer-use-agents.md","7a4cfbf5a2173bcc",{"html":25,"metadata":29720},{"headings":29721,"localImagePaths":29722,"remoteImagePaths":29723,"frontmatter":29724,"imagePaths":29725},[],[],[],{"title":29713,"description":29714,"summary":29714,"pubDate":25202,"source":25191,"url":29716,"thumbnail":25193},[],"2025-06-23-osworld-human-benchmarking-the-efficiency-of-computer-use-agents.md","2025-06-23-parkformer-a-transformer-based-parking-policy-with-goal-embedding-and-pedestrian-aware-control",{"id":29727,"data":29729,"filePath":29734,"digest":29735,"rendered":29736,"legacyId":29743},{"title":29730,"description":29731,"summary":29731,"pubDate":29732,"source":25191,"url":29733,"thumbnail":25193},"ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control","arXiv:2506.16856v1 Announce Type: cross Abstract: Autonomous parking plays a vital role in intelligent vehicle systems, particularly in constrained urban environments where high-precision control is required. While traditional rule-based parking systems struggle with environmental uncertainties and lack adaptability in crowded or dynamic scenes, human drivers demonstrate the ability to park intuitively without explicit modeling. Inspired by this observation, we propose a Transformer-based end-to-end framework for autonomous parking that learns from expert demonstrations. The network takes as input surround-view camera images, goal-point representations, ego vehicle motion, and pedestrian trajectories. It outputs discrete control sequences including throttle, braking, steering, and gear selection. A novel cross-attention module integrates BEV features with target points, and a GRU-based pedestrian predictor enhances safety by modeling dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both vertical and parallel parking scenarios. Experiments show our model achieves a high success rate of 96.57%, with average positional and orientation errors of 0.21 meters and 0.41 degrees, respectively. The ablation studies further demonstrate the effectiveness of key modules such as pedestrian prediction and goal-point attention fusion. The code and dataset will be released at: https://github.com/little-snail-f/ParkFormer.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16856","src/content/posts/2025-06-23-parkformer-a-transformer-based-parking-policy-with-goal-embedding-and-pedestrian-aware-control.md","f1c173b842b8c366",{"html":25,"metadata":29737},{"headings":29738,"localImagePaths":29739,"remoteImagePaths":29740,"frontmatter":29741,"imagePaths":29742},[],[],[],{"title":29730,"description":29731,"summary":29731,"pubDate":25202,"source":25191,"url":29733,"thumbnail":25193},[],"2025-06-23-parkformer-a-transformer-based-parking-policy-with-goal-embedding-and-pedestrian-aware-control.md","2025-06-23-part2gs-part-aware-modeling-of-articulated-objects-using-3d-gaussian-splatting",{"id":29744,"data":29746,"filePath":29751,"digest":29752,"rendered":29753,"legacyId":29760},{"title":29747,"description":29748,"summary":29748,"pubDate":29749,"source":25191,"url":29750,"thumbnail":25193},"Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting","arXiv:2506.17212v1 Announce Type: cross Abstract: Articulated objects are common in the real world, yet modeling their structure and motion remains a challenging task for 3D reconstruction methods. In this work, we introduce Part$^{2}$GS, a novel framework for modeling articulated digital twins of multi-part objects with high-fidelity geometry and physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D Gaussian representation that encodes articulated components with learnable attributes, enabling structured, disentangled transformations that preserve high-fidelity geometry. To ensure physically consistent motion, we propose a motion-aware canonical representation guided by physics-based constraints, including contact enforcement, velocity consistency, and vector-field alignment. Furthermore, we introduce a field of repel points to prevent part collisions and maintain stable articulation paths, significantly improving motion coherence over baselines. Extensive evaluations on both synthetic and real-world datasets show that Part$^{2}$GS consistently outperforms state-of-the-art methods by up to 10$times$ in Chamfer Distance for movable parts.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17212","src/content/posts/2025-06-23-part2gs-part-aware-modeling-of-articulated-objects-using-3d-gaussian-splatting.md","6e2f96ca3b17fab6",{"html":25,"metadata":29754},{"headings":29755,"localImagePaths":29756,"remoteImagePaths":29757,"frontmatter":29758,"imagePaths":29759},[],[],[],{"title":29747,"description":29748,"summary":29748,"pubDate":25202,"source":25191,"url":29750,"thumbnail":25193},[],"2025-06-23-part2gs-part-aware-modeling-of-articulated-objects-using-3d-gaussian-splatting.md","2025-06-23-perceptionlm-open-access-data-and-models-for-detailed-visual-understanding",{"id":29761,"data":29763,"filePath":29768,"digest":29769,"rendered":29770,"legacyId":29777},{"title":29764,"description":29765,"summary":29765,"pubDate":29766,"source":25191,"url":29767,"thumbnail":25193},"PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding","arXiv:2504.13180v2 Announce Type: replace-cross Abstract: Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLM-VideoBench, a suite for evaluating challenging video understanding tasks focusing on the ability to reason about 'what', 'where', 'when', and 'how' of a video. We make our work fully reproducible by providing data, training recipes, code & models. https://github.com/facebookresearch/perception_models",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2504.13180","src/content/posts/2025-06-23-perceptionlm-open-access-data-and-models-for-detailed-visual-understanding.md","4b940a250743e630",{"html":25,"metadata":29771},{"headings":29772,"localImagePaths":29773,"remoteImagePaths":29774,"frontmatter":29775,"imagePaths":29776},[],[],[],{"title":29764,"description":29765,"summary":29765,"pubDate":25202,"source":25191,"url":29767,"thumbnail":25193},[],"2025-06-23-perceptionlm-open-access-data-and-models-for-detailed-visual-understanding.md","2025-06-23-planning-of-heuristics-strategic-planning-on-large-language-models-with-monte-carlo-tree-search-for-automating-heuristic-optimization",{"id":29778,"data":29780,"filePath":29785,"digest":29786,"rendered":29787,"legacyId":29794},{"title":29781,"description":29782,"summary":29782,"pubDate":29783,"source":25191,"url":29784,"thumbnail":25193},"Planning of Heuristics: Strategic Planning on Large Language Models with Monte Carlo Tree Search for Automating Heuristic Optimization","arXiv:2502.11422v3 Announce Type: replace Abstract: Heuristics have achieved great success in solving combinatorial optimization problems~(COPs). However, heuristics designed by humans require too much domain knowledge and testing time. Since Large Language Models~(LLMs) possess strong capabilities to understand and generate content with a knowledge base that covers various domains, they offer potential ways to automatically optimize heuristics. To this end, we propose Planning of Heuristics~(PoH), an optimization method that integrates LLM self-reflection with Monte Carlo Tree Search, a well-known planning algorithm. PoH iteratively refines generated heuristics by evaluating their performance and providing improvement suggestions. Our method enables to iteratively evaluate the generated heuristics~(states) and improve them based on the improvement suggestions~(actions) and evaluation results~(rewards), by effectively simulating future states to search for paths with higher rewards. In this paper, we apply PoH to solve the Traveling Salesman Problem and the Flow Shop Scheduling Problem. The experimental results show that PoH outperforms hand-crafted heuristics and other Automatic Heuristic Design methods based on LLMs, and achieves the state-of-the-art performance in automating heuristic optimization with LLMs to solve tested COPs, especially with large sizes.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2502.11422","src/content/posts/2025-06-23-planning-of-heuristics-strategic-planning-on-large-language-models-with-monte-carlo-tree-search-for-automating-heuristic-optimization.md","32a125d94f121014",{"html":25,"metadata":29788},{"headings":29789,"localImagePaths":29790,"remoteImagePaths":29791,"frontmatter":29792,"imagePaths":29793},[],[],[],{"title":29781,"description":29782,"summary":29782,"pubDate":25202,"source":25191,"url":29784,"thumbnail":25193},[],"2025-06-23-planning-of-heuristics-strategic-planning-on-large-language-models-with-monte-carlo-tree-search-for-automating-heuristic-optimization.md","2025-06-23-plantbert-an-open-source-language-model-for-plant-science",{"id":29795,"data":29797,"filePath":29802,"digest":29803,"rendered":29804,"legacyId":29811},{"title":29798,"description":29799,"summary":29799,"pubDate":29800,"source":25191,"url":29801,"thumbnail":25193},"PlantBert: An Open Source Language Model for Plant Science","arXiv:2506.08897v2 Announce Type: replace-cross Abstract: The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantBert, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantBert is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantBert to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantBert exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields. By providing a scalable and reproducible framework for high-resolution entity recognition, PlantBert bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.08897","src/content/posts/2025-06-23-plantbert-an-open-source-language-model-for-plant-science.md","8ea6b41935fd0e04",{"html":25,"metadata":29805},{"headings":29806,"localImagePaths":29807,"remoteImagePaths":29808,"frontmatter":29809,"imagePaths":29810},[],[],[],{"title":29798,"description":29799,"summary":29799,"pubDate":25202,"source":25191,"url":29801,"thumbnail":25193},[],"2025-06-23-plantbert-an-open-source-language-model-for-plant-science.md","2025-06-23-pncspower-norm-cosine-similarity-for-diverse-client-selection-in-federated-learning",{"id":29812,"data":29814,"filePath":29819,"digest":29820,"rendered":29821,"legacyId":29828},{"title":29815,"description":29816,"summary":29816,"pubDate":29817,"source":25191,"url":29818,"thumbnail":25193},"PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning","arXiv:2506.15923v1 Announce Type: cross Abstract: Federated Learning (FL) has emerged as a powerful paradigm for leveraging diverse datasets from multiple sources while preserving data privacy by avoiding centralized storage. However, many existing approaches fail to account for the intricate gradient correlations between remote clients, a limitation that becomes especially problematic in data heterogeneity scenarios. In this work, we propose a novel FL framework utilizing Power-Norm Cosine Similarity (PNCS) to improve client selection for model aggregation. By capturing higher-order gradient moments, PNCS addresses non-IID data challenges, enhancing convergence speed and accuracy. Additionally, we introduce a simple algorithm ensuring diverse client selection through a selection history queue. Experiments with a VGG16 model across varied data partitions demonstrate consistent improvements over state-of-the-art methods.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15923","src/content/posts/2025-06-23-pncspower-norm-cosine-similarity-for-diverse-client-selection-in-federated-learning.md","1516a32facbf6bef",{"html":25,"metadata":29822},{"headings":29823,"localImagePaths":29824,"remoteImagePaths":29825,"frontmatter":29826,"imagePaths":29827},[],[],[],{"title":29815,"description":29816,"summary":29816,"pubDate":25202,"source":25191,"url":29818,"thumbnail":25193},[],"2025-06-23-pncspower-norm-cosine-similarity-for-diverse-client-selection-in-federated-learning.md","2025-06-23-pov-learning-individual-alignment-of-multimodal-models-using-human-perception",{"id":29829,"data":29831,"filePath":29836,"digest":29837,"rendered":29838,"legacyId":29845},{"title":29832,"description":29833,"summary":29833,"pubDate":29834,"source":25191,"url":29835,"thumbnail":25193},"POV Learning: Individual Alignment of Multimodal Models using Human Perception","arXiv:2405.04443v2 Announce Type: replace Abstract: Aligning machine learning systems with human expectations is mostly attempted by training with manually vetted human behavioral samples, typically explicit feedback. This is done on a population level since the context that is capturing the subjective Point-Of-View (POV) of a concrete person in a specific situational context is not retained in the data. However, we argue that alignment on an individual level can boost the subjective predictive performance for the individual user interacting with the system considerably. Since perception differs for each person, the same situation is observed differently. Consequently, the basis for decision making and the subsequent reasoning processes and observable reactions differ. We hypothesize that individual perception patterns can be used for improving the alignment on an individual level. We test this, by integrating perception information into machine learning systems and measuring their predictive performance wrt.~individual subjective assessments. For our empirical study, we collect a novel data set of multimodal stimuli and corresponding eye tracking sequences for the novel task of Perception-Guided Crossmodal Entailment and tackle it with our Perception-Guided Multimodal Transformer. Our findings suggest that exploiting individual perception signals for the machine learning of subjective human assessments provides a valuable cue for individual alignment. It does not only improve the overall predictive performance from the point-of-view of the individual user but might also contribute to steering AI systems towards every person's individual expectations and values.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2405.04443","src/content/posts/2025-06-23-pov-learning-individual-alignment-of-multimodal-models-using-human-perception.md","ca16c12d7c2011e7",{"html":25,"metadata":29839},{"headings":29840,"localImagePaths":29841,"remoteImagePaths":29842,"frontmatter":29843,"imagePaths":29844},[],[],[],{"title":29832,"description":29833,"summary":29833,"pubDate":25202,"source":25191,"url":29835,"thumbnail":25193},[],"2025-06-23-pov-learning-individual-alignment-of-multimodal-models-using-human-perception.md","2025-06-23-pqcad-dm-progressive-quantization-and-calibration-assisted-distillation-for-extremely-efficient-diffusion-model",{"id":29846,"data":29848,"filePath":29853,"digest":29854,"rendered":29855,"legacyId":29862},{"title":29849,"description":29850,"summary":29850,"pubDate":29851,"source":25191,"url":29852,"thumbnail":25193},"PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model","arXiv:2506.16776v1 Announce Type: cross Abstract: Diffusion models excel in image generation but are computational and resource-intensive due to their reliance on iterative Markov chain processes, leading to error accumulation and limiting the effectiveness of naive compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid compression framework combining Progressive Quantization (PQ) and Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs a two-stage quantization with adaptive bit-width transitions guided by a momentum-based mechanism, reducing excessive weight perturbations in low-precision. CAD leverages full-precision calibration datasets during distillation, enabling the student to match full-precision performance even with a quantized teacher. As a result, PQCAD-DM achieves a balance between computational efficiency and generative quality, halving inference time while maintaining competitive performance. Extensive experiments validate PQCAD-DM's superior generative capabilities and efficiency across diverse datasets, outperforming fixed-bit quantization methods.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16776","src/content/posts/2025-06-23-pqcad-dm-progressive-quantization-and-calibration-assisted-distillation-for-extremely-efficient-diffusion-model.md","81f0f0990f5d53fb",{"html":25,"metadata":29856},{"headings":29857,"localImagePaths":29858,"remoteImagePaths":29859,"frontmatter":29860,"imagePaths":29861},[],[],[],{"title":29849,"description":29850,"summary":29850,"pubDate":25202,"source":25191,"url":29852,"thumbnail":25193},[],"2025-06-23-pqcad-dm-progressive-quantization-and-calibration-assisted-distillation-for-extremely-efficient-diffusion-model.md","2025-06-23-pr-attack-coordinated-prompt-rag-attacks-on-retrieval-augmented-generation-in-large-language-models-via-bilevel-optimization",{"id":29863,"data":29865,"filePath":29870,"digest":29871,"rendered":29872,"legacyId":29879},{"title":29866,"description":29867,"summary":29867,"pubDate":29868,"source":25191,"url":29869,"thumbnail":25193},"PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization","arXiv:2504.07717v3 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of applications, e.g., medical question-answering, mathematical sciences, and code generation. However, they also exhibit inherent limitations, such as outdated knowledge and susceptibility to hallucinations. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these issues, but it also introduces new vulnerabilities. Recent efforts have focused on the security of RAG-based LLMs, yet existing attack methods face three critical challenges: (1) their effectiveness declines sharply when only a limited number of poisoned texts can be injected into the knowledge database, (2) they lack sufficient stealth, as the attacks are often detectable by anomaly detection systems, which compromises their effectiveness, and (3) they rely on heuristic approaches to generate poisoned texts, lacking formal optimization frameworks and theoretic guarantees, which limits their effectiveness and applicability. To address these issues, we propose coordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack that introduces a small number of poisoned texts into the knowledge database while embedding a backdoor trigger within the prompt. When activated, the trigger causes the LLM to generate pre-designed responses to targeted queries, while maintaining normal behavior in other contexts. This ensures both high effectiveness and stealth. We formulate the attack generation process as a bilevel optimization problem leveraging a principled optimization framework to develop optimal poisoned texts and triggers. Extensive experiments across diverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving a high attack success rate even with a limited number of poisoned texts and significantly improved stealth compared to existing methods.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2504.07717","src/content/posts/2025-06-23-pr-attack-coordinated-prompt-rag-attacks-on-retrieval-augmented-generation-in-large-language-models-via-bilevel-optimization.md","5828a19da60f3763",{"html":25,"metadata":29873},{"headings":29874,"localImagePaths":29875,"remoteImagePaths":29876,"frontmatter":29877,"imagePaths":29878},[],[],[],{"title":29866,"description":29867,"summary":29867,"pubDate":25202,"source":25191,"url":29869,"thumbnail":25193},[],"2025-06-23-pr-attack-coordinated-prompt-rag-attacks-on-retrieval-augmented-generation-in-large-language-models-via-bilevel-optimization.md","2025-06-23-preference-driven-multi-objective-combinatorial-optimization-with-conditional-computation",{"id":29880,"data":29882,"filePath":29887,"digest":29888,"rendered":29889,"legacyId":29896},{"title":29883,"description":29884,"summary":29884,"pubDate":29885,"source":25191,"url":29886,"thumbnail":25193},"Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation","arXiv:2506.08898v2 Announce Type: replace Abstract: Recent deep reinforcement learning methods have achieved remarkable success in solving multi-objective combinatorial optimization problems (MOCOPs) by decomposing them into multiple subproblems, each associated with a specific weight vector. However, these methods typically treat all subproblems equally and solve them using a single model, hindering the effective exploration of the solution space and thus leading to suboptimal performance. To overcome the limitation, we propose POCCO, a novel plug-and-play framework that enables adaptive selection of model structures for subproblems, which are subsequently optimized based on preference signals rather than explicit reward values. Specifically, we design a conditional computation block that routes subproblems to specialized neural architectures. Moreover, we propose a preference-driven optimization algorithm that learns pairwise preferences between winning and losing solutions. We evaluate the efficacy and versatility of POCCO by applying it to two state-of-the-art neural methods for MOCOPs. Experimental results across four classic MOCOP benchmarks demonstrate its significant superiority and strong generalization.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.08898","src/content/posts/2025-06-23-preference-driven-multi-objective-combinatorial-optimization-with-conditional-computation.md","4ba6b2908c7feca5",{"html":25,"metadata":29890},{"headings":29891,"localImagePaths":29892,"remoteImagePaths":29893,"frontmatter":29894,"imagePaths":29895},[],[],[],{"title":29883,"description":29884,"summary":29884,"pubDate":25202,"source":25191,"url":29886,"thumbnail":25193},[],"2025-06-23-preference-driven-multi-objective-combinatorial-optimization-with-conditional-computation.md","2025-06-23-prison-unmasking-the-criminal-potential-of-large-language-models",{"id":29897,"data":29899,"filePath":29904,"digest":29905,"rendered":29906,"legacyId":29913},{"title":29900,"description":29901,"summary":29901,"pubDate":29902,"source":25191,"url":29903,"thumbnail":25193},"PRISON: Unmasking the Criminal Potential of Large Language Models","arXiv:2506.16150v1 Announce Type: cross Abstract: As large language models (LLMs) advance, concerns about their misconduct in complex social contexts intensify. Existing research overlooked the systematic understanding and assessment of their criminal capability in realistic interactions. We propose a unified framework PRISON, to quantify LLMs' criminal potential across five dimensions: False Statements, Frame-Up, Psychological Manipulation, Emotional Disguise, and Moral Disengagement. Using structured crime scenarios adapted from classic films, we evaluate both criminal potential and anti-crime ability of LLMs via role-play. Results show that state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as proposing misleading statements or evasion tactics, even without explicit instructions. Moreover, when placed in a detective role, models recognize deceptive behavior with only 41% accuracy on average, revealing a striking mismatch between conducting and detecting criminal behavior. These findings underscore the urgent need for adversarial robustness, behavioral alignment, and safety mechanisms before broader LLM deployment.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16150","src/content/posts/2025-06-23-prison-unmasking-the-criminal-potential-of-large-language-models.md","029ad1946d899d6a",{"html":25,"metadata":29907},{"headings":29908,"localImagePaths":29909,"remoteImagePaths":29910,"frontmatter":29911,"imagePaths":29912},[],[],[],{"title":29900,"description":29901,"summary":29901,"pubDate":25202,"source":25191,"url":29903,"thumbnail":25193},[],"2025-06-23-prison-unmasking-the-criminal-potential-of-large-language-models.md","2025-06-23-probing-the-robustness-of-large-language-models-safety-to-latent-perturbations",{"id":29914,"data":29916,"filePath":29921,"digest":29922,"rendered":29923,"legacyId":29930},{"title":29917,"description":29918,"summary":29918,"pubDate":29919,"source":25191,"url":29920,"thumbnail":25193},"Probing the Robustness of Large Language Models Safety to Latent Perturbations","arXiv:2506.16078v1 Announce Type: cross Abstract: Safety alignment is a key requirement for building reliable Artificial General Intelligence. Despite significant advances in safety alignment, we observe that minor latent shifts can still trigger unsafe responses in aligned models. We argue that this stems from the shallow nature of existing alignment methods, which focus on surface-level refusal behaviors without sufficiently altering internal representations. Consequently, small shifts in hidden activations can re-trigger harmful behaviors embedded in the latent space. To explore the robustness of safety alignment to latent perturbations, we introduce a probing method that measures the Negative Log-Likelihood of the original response generated by the model. This probe quantifies local sensitivity in the latent space, serving as a diagnostic tool for identifying vulnerable directions. Based on this signal, we construct effective jailbreak trajectories, giving rise to the Activation Steering Attack (ASA). More importantly, these insights offer a principled foundation for improving alignment robustness. To this end, we introduce Layer-wise Adversarial Patch Training~(LAPT), a fine-tuning strategy that inject controlled perturbations into hidden representations during training. Experimental results highlight that LAPT strengthen alignment robustness without compromising general capabilities. Our findings reveal fundamental flaws in current alignment paradigms and call for representation-level training strategies that move beyond surface-level behavior supervision. Codes and results are available at https://github.com/Carol-gutianle/LatentSafety.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16078","src/content/posts/2025-06-23-probing-the-robustness-of-large-language-models-safety-to-latent-perturbations.md","5b749e2b18a4ef69",{"html":25,"metadata":29924},{"headings":29925,"localImagePaths":29926,"remoteImagePaths":29927,"frontmatter":29928,"imagePaths":29929},[],[],[],{"title":29917,"description":29918,"summary":29918,"pubDate":25202,"source":25191,"url":29920,"thumbnail":25193},[],"2025-06-23-probing-the-robustness-of-large-language-models-safety-to-latent-perturbations.md","2025-06-23-progressive-inference-time-annealing-of-diffusion-models-for-sampling-from-boltzmann-densities",{"id":29931,"data":29933,"filePath":29938,"digest":29939,"rendered":29940,"legacyId":29947},{"title":29934,"description":29935,"summary":29935,"pubDate":29936,"source":25191,"url":29937,"thumbnail":25193},"Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities","arXiv:2506.16471v1 Announce Type: cross Abstract: Sampling efficiently from a target unnormalized probability density remains a core challenge, with relevance across countless high-impact scientific applications. A promising approach towards this challenge is the design of amortized samplers that borrow key ideas, such as probability path design, from state-of-the-art generative diffusion models. However, all existing diffusion-based samplers remain unable to draw samples from distributions at the scale of even simple molecular systems. In this paper, we propose Progressive Inference-Time Annealing (PITA), a novel framework to learn diffusion-based samplers that combines two complementary interpolation techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion smoothing. PITA trains a sequence of diffusion models from high to low temperatures by sequentially training each model at progressively higher temperatures, leveraging engineered easy access to samples of the temperature-annealed target density. In the subsequent step, PITA enables simulating the trained diffusion model to procure training samples at a lower temperature for the next diffusion model through inference-time annealing using a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA enables, for the first time, equilibrium sampling of N-body particle systems, Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically lower energy function evaluations. Code available at: https://github.com/taraak/pita",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16471","src/content/posts/2025-06-23-progressive-inference-time-annealing-of-diffusion-models-for-sampling-from-boltzmann-densities.md","9c300aa3729e3f46",{"html":25,"metadata":29941},{"headings":29942,"localImagePaths":29943,"remoteImagePaths":29944,"frontmatter":29945,"imagePaths":29946},[],[],[],{"title":29934,"description":29935,"summary":29935,"pubDate":25202,"source":25191,"url":29937,"thumbnail":25193},[],"2025-06-23-progressive-inference-time-annealing-of-diffusion-models-for-sampling-from-boltzmann-densities.md","2025-06-23-promptdsi-prompt-based-rehearsal-free-instance-wise-incremental-learning-for-document-retrieval",{"id":29948,"data":29950,"filePath":29955,"digest":29956,"rendered":29957,"legacyId":29964},{"title":29951,"description":29952,"summary":29952,"pubDate":29953,"source":25191,"url":29954,"thumbnail":25193},"PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval","arXiv:2406.12593v3 Announce Type: replace-cross Abstract: Differentiable Search Index (DSI) utilizes pre-trained language models to perform indexing and document retrieval via end-to-end learning without relying on external indexes. However, DSI requires full re-training to index new documents, causing significant computational inefficiencies. Continual learning (CL) offers a solution by enabling the model to incrementally update without full re-training. Existing CL solutions in document retrieval rely on memory buffers or generative models for rehearsal, which is infeasible when accessing previous training data is restricted due to privacy concerns. To this end, we introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach for document retrieval. PromptDSI follows the Prompt-based Continual Learning (PCL) framework, using learnable prompts to efficiently index new documents without accessing previous documents or queries. To improve retrieval latency, we remove the initial forward pass of PCL, which otherwise greatly increases training and inference time, with a negligible trade-off in performance. Additionally, we introduce a novel topic-aware prompt pool that employs neural topic embeddings as fixed keys, eliminating the instability of prompt key optimization while maintaining competitive performance with existing PCL prompt pools. In a challenging rehearsal-free continual learning setup, we demonstrate that PromptDSI variants outperform rehearsal-based baselines, match the strong cache-based baseline in mitigating forgetting, and significantly improving retrieval performance on new corpora.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2406.12593","src/content/posts/2025-06-23-promptdsi-prompt-based-rehearsal-free-instance-wise-incremental-learning-for-document-retrieval.md","1b497370367083cc",{"html":25,"metadata":29958},{"headings":29959,"localImagePaths":29960,"remoteImagePaths":29961,"frontmatter":29962,"imagePaths":29963},[],[],[],{"title":29951,"description":29952,"summary":29952,"pubDate":25202,"source":25191,"url":29954,"thumbnail":25193},[],"2025-06-23-promptdsi-prompt-based-rehearsal-free-instance-wise-incremental-learning-for-document-retrieval.md","2025-06-23-proportional-sensitivity-in-generative-adversarial-network-gan-augmented-brain-tumor-classification-using-convolutional-neural-network",{"id":29965,"data":29967,"filePath":29972,"digest":29973,"rendered":29974,"legacyId":29981},{"title":29968,"description":29969,"summary":29969,"pubDate":29970,"source":25191,"url":29971,"thumbnail":25193},"Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network","arXiv:2506.17165v1 Announce Type: cross Abstract: Generative Adversarial Networks (GAN) have shown potential in expanding limited medical imaging datasets. This study explores how different ratios of GAN-generated and real brain tumor MRI images impact the performance of a CNN in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic images which were mixed with real ones at various ratios to train a custom CNN. The CNN was then evaluated on a separate real-world test set. Our results indicate that the model maintains high sensitivity and precision in tumor classification, even when trained predominantly on synthetic data. When only a small portion of GAN data was added, such as 900 real images and 100 GAN images, the model achieved excellent performance, with test accuracy reaching 95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the proportion of GAN images increased further, performance gradually declined. This study suggests that while GANs are useful for augmenting limited datasets especially when real data is scarce, too much synthetic data can introduce artifacts that affect the model's ability to generalize to real world cases.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17165","src/content/posts/2025-06-23-proportional-sensitivity-in-generative-adversarial-network-gan-augmented-brain-tumor-classification-using-convolutional-neural-network.md","41ebcfc48799380c",{"html":25,"metadata":29975},{"headings":29976,"localImagePaths":29977,"remoteImagePaths":29978,"frontmatter":29979,"imagePaths":29980},[],[],[],{"title":29968,"description":29969,"summary":29969,"pubDate":25202,"source":25191,"url":29971,"thumbnail":25193},[],"2025-06-23-proportional-sensitivity-in-generative-adversarial-network-gan-augmented-brain-tumor-classification-using-convolutional-neural-network.md","2025-06-23-qg-sms-enhancing-test-item-analysis-via-student-modeling-and-simulation",{"id":29982,"data":29984,"filePath":29989,"digest":29990,"rendered":29991,"legacyId":29998},{"title":29985,"description":29986,"summary":29986,"pubDate":29987,"source":25191,"url":29988,"thumbnail":25193},"QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation","arXiv:2503.05888v2 Announce Type: replace-cross Abstract: While the Question Generation (QG) task has been increasingly adopted in educational assessments, its evaluation remains limited by approaches that lack a clear connection to the educational values of test items. In this work, we introduce test item analysis, a method frequently used by educators to assess test question quality, into QG evaluation. Specifically, we construct pairs of candidate questions that differ in quality across dimensions such as topic coverage, item difficulty, item discrimination, and distractor efficiency. We then examine whether existing QG evaluation approaches can effectively distinguish these differences. Our findings reveal significant shortcomings in these approaches with respect to accurately assessing test item quality in relation to student performance. To address this gap, we propose a novel QG evaluation framework, QG-SMS, which leverages Large Language Model for Student Modeling and Simulation to perform test item analysis. As demonstrated in our extensive experiments and human evaluation study, the additional perspectives introduced by the simulated student profiles lead to a more effective and robust assessment of test items.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2503.05888","src/content/posts/2025-06-23-qg-sms-enhancing-test-item-analysis-via-student-modeling-and-simulation.md","bbe021f2f603a69f",{"html":25,"metadata":29992},{"headings":29993,"localImagePaths":29994,"remoteImagePaths":29995,"frontmatter":29996,"imagePaths":29997},[],[],[],{"title":29985,"description":29986,"summary":29986,"pubDate":25202,"source":25191,"url":29988,"thumbnail":25193},[],"2025-06-23-qg-sms-enhancing-test-item-analysis-via-student-modeling-and-simulation.md","2025-06-23-quantum-artificial-intelligence-for-secure-autonomous-vehicle-navigation-an-architectural-proposal",{"id":29999,"data":30001,"filePath":30006,"digest":30007,"rendered":30008,"legacyId":30015},{"title":30002,"description":30003,"summary":30003,"pubDate":30004,"source":25191,"url":30005,"thumbnail":25193},"Quantum Artificial Intelligence for Secure Autonomous Vehicle Navigation: An Architectural Proposal","arXiv:2506.16000v1 Announce Type: cross Abstract: Navigation is a very crucial aspect of autonomous vehicle ecosystem which heavily relies on collecting and processing large amounts of data in various states and taking a confident and safe decision to define the next vehicle maneuver. In this paper, we propose a novel architecture based on Quantum Artificial Intelligence by enabling quantum and AI at various levels of navigation decision making and communication process in Autonomous vehicles : Quantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum reinforcement learning for navigation policy optimization and finally post-quantum cryptographic protocols for secure communication. Quantum neural networks uses quantum amplitude encoding to fuse data from various sensors like LiDAR, radar, camera, GPS and weather etc., This approach gives a unified quantum state representation between heterogeneous sensor modalities. Nav-Q module processes the fused quantum states through variational quantum circuits to learn optimal navigation policies under swift dynamic and complex conditions. Finally, post quantum cryptographic protocols are used to secure communication channels for both within vehicle communication and V2X (Vehicle to Everything) communications and thus secures the autonomous vehicle communication from both classical and quantum security threats. Thus, the proposed framework addresses fundamental challenges in autonomous vehicles navigation by providing quantum performance and future proof security. Index Terms Quantum Computing, Autonomous Vehicles, Sensor Fusion",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16000","src/content/posts/2025-06-23-quantum-artificial-intelligence-for-secure-autonomous-vehicle-navigation-an-architectural-proposal.md","713ccdadbb2a7ddf",{"html":25,"metadata":30009},{"headings":30010,"localImagePaths":30011,"remoteImagePaths":30012,"frontmatter":30013,"imagePaths":30014},[],[],[],{"title":30002,"description":30003,"summary":30003,"pubDate":25202,"source":25191,"url":30005,"thumbnail":25193},[],"2025-06-23-quantum-artificial-intelligence-for-secure-autonomous-vehicle-navigation-an-architectural-proposal.md","2025-06-23-quantifying-artificial-intelligence-through-algorithmic-generalization",{"id":30016,"data":30018,"filePath":30023,"digest":30024,"rendered":30025,"legacyId":30032},{"title":30019,"description":30020,"summary":30020,"pubDate":30021,"source":25191,"url":30022,"thumbnail":25193},"Quantifying artificial intelligence through algorithmic generalization","arXiv:2411.05943v2 Announce Type: replace Abstract: The rapid development of artificial intelligence (AI) systems has created an urgent need for their scientific quantification. While their fluency across a variety of domains is impressive, AI systems fall short on tests requiring algorithmic reasoning -- a glaring limitation given the necessity for interpretable and reliable technology. Despite a surge of reasoning benchmarks emerging from the academic community, no theoretical framework exists to quantify algorithmic reasoning in AI systems. Here, we adopt a framework from computational complexity theory to quantify algorithmic generalization using algebraic expressions: algebraic circuit complexity. Algebraic circuit complexity theory -- the study of algebraic expressions as circuit models -- is a natural framework to study the complexity of algorithmic computation. Algebraic circuit complexity enables the study of generalization by defining benchmarks in terms of the computational requirements to solve a problem. Moreover, algebraic circuits are generic mathematical objects; an arbitrarily large number of samples can be generated for a specified circuit, making it an ideal experimental sandbox for the data-hungry models that are used today. In this Perspective, we adopt tools from algebraic circuit complexity, apply them to formalize a science of algorithmic generalization, and address key challenges for its successful application to AI science.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2411.05943","src/content/posts/2025-06-23-quantifying-artificial-intelligence-through-algorithmic-generalization.md","aede0e5c46321da4",{"html":25,"metadata":30026},{"headings":30027,"localImagePaths":30028,"remoteImagePaths":30029,"frontmatter":30030,"imagePaths":30031},[],[],[],{"title":30019,"description":30020,"summary":30020,"pubDate":25202,"source":25191,"url":30022,"thumbnail":25193},[],"2025-06-23-quantifying-artificial-intelligence-through-algorithmic-generalization.md","2025-06-23-rapflow-tts-rapid-and-high-fidelity-text-to-speech-with-improved-consistency-flow-matching",{"id":30033,"data":30035,"filePath":30040,"digest":30041,"rendered":30042,"legacyId":30049},{"title":30036,"description":30037,"summary":30037,"pubDate":30038,"source":25191,"url":30039,"thumbnail":25193},"RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching","arXiv:2506.16741v1 Announce Type: cross Abstract: We introduce RapFlow-TTS, a rapid and high-fidelity TTS acoustic model that leverages velocity consistency constraints in flow matching (FM) training. Although ordinary differential equation (ODE)-based TTS generation achieves natural-quality speech, it typically requires a large number of generation steps, resulting in a trade-off between quality and inference speed. To address this challenge, RapFlow-TTS enforces consistency in the velocity field along the FM-straightened ODE trajectory, enabling consistent synthetic quality with fewer generation steps. Additionally, we introduce techniques such as time interval scheduling and adversarial learning to further enhance the quality of the few-step synthesis. Experimental results show that RapFlow-TTS achieves high-fidelity speech synthesis with a 5- and 10-fold reduction in synthesis steps than the conventional FM- and score-based approaches, respectively.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16741","src/content/posts/2025-06-23-rapflow-tts-rapid-and-high-fidelity-text-to-speech-with-improved-consistency-flow-matching.md","26bfa04d2aa55519",{"html":25,"metadata":30043},{"headings":30044,"localImagePaths":30045,"remoteImagePaths":30046,"frontmatter":30047,"imagePaths":30048},[],[],[],{"title":30036,"description":30037,"summary":30037,"pubDate":25202,"source":25191,"url":30039,"thumbnail":25193},[],"2025-06-23-rapflow-tts-rapid-and-high-fidelity-text-to-speech-with-improved-consistency-flow-matching.md","2025-06-23-rapid-and-continuous-trust-evaluation-for-effective-task-collaboration-through-siamese-model",{"id":30050,"data":30052,"filePath":30057,"digest":30058,"rendered":30059,"legacyId":30066},{"title":30053,"description":30054,"summary":30054,"pubDate":30055,"source":25191,"url":30056,"thumbnail":25193},"Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model","arXiv:2506.17128v1 Announce Type: cross Abstract: Trust is emerging as an effective tool to ensure the successful completion of collaborative tasks within collaborative systems. However, rapidly and continuously evaluating the trustworthiness of collaborators during task execution is a significant challenge due to distributed devices, complex operational environments, and dynamically changing resources. To tackle this challenge, this paper proposes a Siamese-enabled rapid and continuous trust evaluation framework (SRCTE) to facilitate effective task collaboration. First, the communication and computing resource attributes of the collaborator in a trusted state, along with historical collaboration data, are collected and represented using an attributed control flow graph (ACFG) that captures trust-related semantic information and serves as a reference for comparison with data collected during task execution. At each time slot of task execution, the collaborator's communication and computing resource attributes, as well as task completion effectiveness, are collected in real time and represented with an ACFG to convey their trust-related semantic information. A Siamese model, consisting of two shared-parameter Structure2vec networks, is then employed to learn the deep semantics of each pair of ACFGs and generate their embeddings. Finally, the similarity between the embeddings of each pair of ACFGs is calculated to determine the collaborator's trust value at each time slot. A real system is built using two Dell EMC 5200 servers and a Google Pixel 8 to test the effectiveness of the proposed SRCTE framework. Experimental results demonstrate that SRCTE converges rapidly with only a small amount of data and achieves a high anomaly trust detection rate compared to the baseline algorithm.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17128","src/content/posts/2025-06-23-rapid-and-continuous-trust-evaluation-for-effective-task-collaboration-through-siamese-model.md","e128eb3c8d3bd502",{"html":25,"metadata":30060},{"headings":30061,"localImagePaths":30062,"remoteImagePaths":30063,"frontmatter":30064,"imagePaths":30065},[],[],[],{"title":30053,"description":30054,"summary":30054,"pubDate":25202,"source":25191,"url":30056,"thumbnail":25193},[],"2025-06-23-rapid-and-continuous-trust-evaluation-for-effective-task-collaboration-through-siamese-model.md","2025-06-23-rast-reasoning-activation-in-llms-via-small-model-transfer",{"id":30067,"data":30069,"filePath":30074,"digest":30075,"rendered":30076,"legacyId":30083},{"title":30070,"description":30071,"summary":30071,"pubDate":30072,"source":25191,"url":30073,"thumbnail":25193},"RAST: Reasoning Activation in LLMs via Small-model Transfer","arXiv:2506.15710v1 Announce Type: cross Abstract: Reinforcement learning (RL) has become a powerful approach for improving the reasoning capabilities of large language models (LLMs), as evidenced by recent successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale remains intimidatingly resource-intensive, requiring multiple model copies and extensive GPU workloads. On the other hand, while being powerful, recent studies suggest that RL does not fundamentally endow models with new knowledge; rather, it primarily reshapes the model's output distribution to activate reasoning capabilities latent in the base model. Building on this insight, we hypothesize that the changes in output probabilities induced by RL are largely model-size invariant, opening the door to a more efficient paradigm: training a small model with RL and transferring its induced probability shifts to larger base models. To verify our hypothesis, we conduct a token-level analysis of decoding trajectories and find high alignment in RL-induced output distributions across model scales, validating our hypothesis. Motivated by this, we propose RAST, a simple yet effective method that transfers reasoning behaviors by injecting RL-induced probability adjustments from a small RL-trained model into larger models. Experiments across multiple mathematical reasoning benchmarks show that RAST substantially and consistently enhances the reasoning capabilities of base models while requiring significantly lower GPU memory than direct RL training, sometimes even yielding better performance than the RL-trained counterparts. Our findings offer new insights into the nature of RL-driven reasoning and practical strategies for scaling its benefits without incurring its full computational cost. The project page of RAST is available at https://ozyyshr.github.io/RAST/.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15710","src/content/posts/2025-06-23-rast-reasoning-activation-in-llms-via-small-model-transfer.md","aa7d8d70c1c1f104",{"html":25,"metadata":30077},{"headings":30078,"localImagePaths":30079,"remoteImagePaths":30080,"frontmatter":30081,"imagePaths":30082},[],[],[],{"title":30070,"description":30071,"summary":30071,"pubDate":25202,"source":25191,"url":30073,"thumbnail":25193},[],"2025-06-23-rast-reasoning-activation-in-llms-via-small-model-transfer.md","2025-06-23-real-time-black-box-optimization-for-dynamic-discrete-environments-using-embedded-ising-machines",{"id":30084,"data":30086,"filePath":30091,"digest":30092,"rendered":30093,"legacyId":30100},{"title":30087,"description":30088,"summary":30088,"pubDate":30089,"source":25191,"url":30090,"thumbnail":25193},"Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines","arXiv:2506.16924v1 Announce Type: new Abstract: Many real-time systems require the optimization of discrete variables. Black-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms perform optimization by repeatedly taking actions and observing the corresponding instant rewards without any prior knowledge. Recently, a BBO method using an Ising machine has been proposed to find the best action that is represented by a combination of discrete values and maximizes the instant reward in static environments. In contrast, dynamic environments, where real-time systems operate, necessitate MAB algorithms that maximize the average reward over multiple trials. However, due to the enormous number of actions resulting from the combinatorial nature of discrete optimization, conventional MAB algorithms cannot effectively optimize dynamic, discrete environments. Here, we show a heuristic MAB method for dynamic, discrete environments by extending the BBO method, in which an Ising machine effectively explores the actions while considering interactions between variables and changes in dynamic environments. We demonstrate the dynamic adaptability of the proposed method in a wireless communication system with moving users.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16924","src/content/posts/2025-06-23-real-time-black-box-optimization-for-dynamic-discrete-environments-using-embedded-ising-machines.md","1ff982ed5f35282b",{"html":25,"metadata":30094},{"headings":30095,"localImagePaths":30096,"remoteImagePaths":30097,"frontmatter":30098,"imagePaths":30099},[],[],[],{"title":30087,"description":30088,"summary":30088,"pubDate":25202,"source":25191,"url":30090,"thumbnail":25193},[],"2025-06-23-real-time-black-box-optimization-for-dynamic-discrete-environments-using-embedded-ising-machines.md","2025-06-23-reasongrm-enhancing-generative-reward-models-through-large-reasoning-models",{"id":30101,"data":30103,"filePath":30108,"digest":30109,"rendered":30110,"legacyId":30117},{"title":30104,"description":30105,"summary":30105,"pubDate":30106,"source":25191,"url":30107,"thumbnail":25193},"ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models","arXiv:2506.16712v1 Announce Type: cross Abstract: Generative Reward Models (GRMs) provide greater flexibility than scalar reward models in capturing human preferences, but their effectiveness is limited by poor reasoning capabilities. This often results in incomplete or overly speculative reasoning paths, leading to hallucinations or missing key information in complex tasks. We address this challenge with ReasonGRM, a three-stage generative reward modeling framework. In the first stage, Zero-RL is used to generate concise, outcome-directed reasoning paths that reduce the likelihood of critical omissions. In the second stage, we introduce a novel evaluation metric, $R^star$, which scores reasoning paths based on their generation likelihood. This favors paths that reach correct answers with minimal exploration, helping to reduce hallucination-prone data during training. In the final stage, the model is further refined through reinforcement learning on challenging examples to enhance its preference discrimination capabilities. Experiments on three public benchmarks show that ReasonGRM achieves competitive or state-of-the-art performance, outperforming previous best GRMs by 1.8% on average and surpassing proprietary models such as GPT-4o by up to 5.6%. These results demonstrate the effectiveness of reasoning-aware training and highlight the importance of high-quality rationale selection for reliable preference modeling.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16712","src/content/posts/2025-06-23-reasongrm-enhancing-generative-reward-models-through-large-reasoning-models.md","46bd5832fab99757",{"html":25,"metadata":30111},{"headings":30112,"localImagePaths":30113,"remoteImagePaths":30114,"frontmatter":30115,"imagePaths":30116},[],[],[],{"title":30104,"description":30105,"summary":30105,"pubDate":25202,"source":25191,"url":30107,"thumbnail":25193},[],"2025-06-23-reasongrm-enhancing-generative-reward-models-through-large-reasoning-models.md","2025-06-23-recbayes-recurrent-bayesian-ad-hoc-teamwork-in-large-partially-observable-domains",{"id":30118,"data":30120,"filePath":30125,"digest":30126,"rendered":30127,"legacyId":30134},{"title":30121,"description":30122,"summary":30122,"pubDate":30123,"source":25191,"url":30124,"thumbnail":25193},"RecBayes: Recurrent Bayesian Ad Hoc Teamwork in Large Partially Observable Domains","arXiv:2506.15756v1 Announce Type: cross Abstract: This paper proposes RecBayes, a novel approach for ad hoc teamwork under partial observability, a setting where agents are deployed on-the-fly to environments where pre-existing teams operate, that never requires, at any stage, access to the states of the environment or the actions of its teammates. We show that by relying on a recurrent Bayesian classifier trained using past experiences, an ad hoc agent is effectively able to identify known teams and tasks being performed from observations alone. Unlike recent approaches such as PO-GPL (Gu et al., 2021) and FEAT (Rahman et al., 2023), that require at some stage fully observable states of the environment, actions of teammates, or both, or approaches such as ATPO (Ribeiro et al., 2023) that require the environments to be small enough to be tabularly modelled (Ribeiro et al., 2023), in their work up to 4.8K states and 1.7K observations, we show RecBayes is both able to handle arbitrarily large spaces while never relying on either states and teammates' actions. Our results in benchmark domains from the multi-agent systems literature, adapted for partial observability and scaled up to 1M states and 2^125 observations, show that RecBayes is effective at identifying known teams and tasks being performed from partial observations alone, and as a result, is able to assist the teams in solving the tasks effectively.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15756","src/content/posts/2025-06-23-recbayes-recurrent-bayesian-ad-hoc-teamwork-in-large-partially-observable-domains.md","b53f67eb89d9a528",{"html":25,"metadata":30128},{"headings":30129,"localImagePaths":30130,"remoteImagePaths":30131,"frontmatter":30132,"imagePaths":30133},[],[],[],{"title":30121,"description":30122,"summary":30122,"pubDate":25202,"source":25191,"url":30124,"thumbnail":25193},[],"2025-06-23-recbayes-recurrent-bayesian-ad-hoc-teamwork-in-large-partially-observable-domains.md","2025-06-23-refined-causal-graph-structure-learning-via-curvature-for-brain-disease-classification",{"id":30135,"data":30137,"filePath":30142,"digest":30143,"rendered":30144,"legacyId":30151},{"title":30138,"description":30139,"summary":30139,"pubDate":30140,"source":25191,"url":30141,"thumbnail":25193},"Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification","arXiv:2506.15708v1 Announce Type: cross Abstract: Graph neural networks (GNNs) have been developed to model the relationship between regions of interest (ROIs) in brains and have shown significant improvement in detecting brain diseases. However, most of these frameworks do not consider the intrinsic relationship of causality factor between brain ROIs, which is arguably more essential to observe cause and effect interaction between signals rather than typical correlation values. We propose a novel framework called CGB (Causal Graphs for Brains) for brain disease classification/detection, which models refined brain networks based on the causal discovery method, transfer entropy, and geometric curvature strategy. CGB unveils causal relationships between ROIs that bring vital information to enhance brain disease classification performance. Furthermore, CGB also performs a graph rewiring through a geometric curvature strategy to refine the generated causal graph to become more expressive and reduce potential information bottlenecks when GNNs model it. Our extensive experiments show that CGB outperforms state-of-the-art methods in classification tasks on brain disease datasets, as measured by average F1 scores.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15708","src/content/posts/2025-06-23-refined-causal-graph-structure-learning-via-curvature-for-brain-disease-classification.md","e5baf140f17e5657",{"html":25,"metadata":30145},{"headings":30146,"localImagePaths":30147,"remoteImagePaths":30148,"frontmatter":30149,"imagePaths":30150},[],[],[],{"title":30138,"description":30139,"summary":30139,"pubDate":25202,"source":25191,"url":30141,"thumbnail":25193},[],"2025-06-23-refined-causal-graph-structure-learning-via-curvature-for-brain-disease-classification.md","2025-06-23-refining-music-sample-identification-with-a-self-supervised-graph-neural-network",{"id":30152,"data":30154,"filePath":30159,"digest":30160,"rendered":30161,"legacyId":30168},{"title":30155,"description":30156,"summary":30156,"pubDate":30157,"source":25191,"url":30158,"thumbnail":25193},"Refining music sample identification with a self-supervised graph neural network","arXiv:2506.14684v2 Announce Type: replace-cross Abstract: Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under 'real world' (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge. In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%. To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.14684","src/content/posts/2025-06-23-refining-music-sample-identification-with-a-self-supervised-graph-neural-network.md","b4cf3e9961064111",{"html":25,"metadata":30162},{"headings":30163,"localImagePaths":30164,"remoteImagePaths":30165,"frontmatter":30166,"imagePaths":30167},[],[],[],{"title":30155,"description":30156,"summary":30156,"pubDate":25202,"source":25191,"url":30158,"thumbnail":25193},[],"2025-06-23-refining-music-sample-identification-with-a-self-supervised-graph-neural-network.md","2025-06-23-reimagination-with-test-time-observation-interventions-distractor-robust-world-model-predictions-for-visual-model-predictive-control",{"id":30169,"data":30171,"filePath":30176,"digest":30177,"rendered":30178,"legacyId":30185},{"title":30172,"description":30173,"summary":30173,"pubDate":30174,"source":25191,"url":30175,"thumbnail":25193},"Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control","arXiv:2506.16565v1 Announce Type: cross Abstract: World models enable robots to 'imagine' future observations given current observations and planned actions, and have been increasingly adopted as generalized dynamics models to facilitate robot learning. Despite their promise, these models remain brittle when encountering novel visual distractors such as objects and background elements rarely seen during training. Specifically, novel distractors can corrupt action outcome predictions, causing downstream failures when robots rely on the world model imaginations for planning or action verification. In this work, we propose Reimagination with Observation Intervention (ReOI), a simple yet effective test-time strategy that enables world models to predict more reliable action outcomes in open-world scenarios where novel and unanticipated visual distractors are inevitable. Given the current robot observation, ReOI first detects visual distractors by identifying which elements of the scene degrade in physically implausible ways during world model prediction. Then, it modifies the current observation to remove these distractors and bring the observation closer to the training distribution. Finally, ReOI 'reimagines' future outcomes with the modified observation and reintroduces the distractors post-hoc to preserve visual consistency for downstream planning and verification. We validate our approach on a suite of robotic manipulation tasks in the context of action verification, where the verifier needs to select desired action plans based on predictions from a world model. Our results show that ReOI is robust to both in-distribution and out-of-distribution visual distractors. Notably, it improves task success rates by up to 3x in the presence of novel distractors, significantly outperforming action verification that relies on world model predictions without imagination interventions.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16565","src/content/posts/2025-06-23-reimagination-with-test-time-observation-interventions-distractor-robust-world-model-predictions-for-visual-model-predictive-control.md","d7330bc555082066",{"html":25,"metadata":30179},{"headings":30180,"localImagePaths":30181,"remoteImagePaths":30182,"frontmatter":30183,"imagePaths":30184},[],[],[],{"title":30172,"description":30173,"summary":30173,"pubDate":25202,"source":25191,"url":30175,"thumbnail":25193},[],"2025-06-23-reimagination-with-test-time-observation-interventions-distractor-robust-world-model-predictions-for-visual-model-predictive-control.md","2025-06-23-reinforcement-learning-for-hybrid-charging-stations-planning-and-operation-considering-fixed-and-mobile-chargers",{"id":30186,"data":30188,"filePath":30193,"digest":30194,"rendered":30195,"legacyId":30202},{"title":30189,"description":30190,"summary":30190,"pubDate":30191,"source":25191,"url":30192,"thumbnail":25193},"Reinforcement learning for hybrid charging stations planning and operation considering fixed and mobile chargers","arXiv:2506.16764v1 Announce Type: new Abstract: The success of vehicle electrification, which brings significant societal and environmental benefits, is contingent upon the availability of efficient and adaptable charging infrastructure. Traditional fixed-location charging stations often face issues like underutilization or congestion due to the dynamic nature of charging demand. Mobile chargers have emerged as a flexible solution, capable of relocating to align with these demand fluctuations. This paper addresses the optimal planning and operation of hybrid charging infrastructures, integrating both fixed and mobile chargers within urban road networks. We introduce the Hybrid Charging Station Planning and Operation (HCSPO) problem, which simultaneously optimizes the location and configuration of fixed charging stations and schedules mobile chargers for dynamic operations. Our approach incorporates a charging demand prediction model grounded in Model Predictive Control (MPC) to enhance decision-making. To solve the HCSPO problem, we propose a deep reinforcement learning method, augmented with heuristic scheduling techniques, to effectively bridge the planning of fixed chargers with the real-time operation of mobile chargers. Extensive case studies using real-world urban scenarios demonstrate that our method significantly improves the availability of charging infrastructure and reduces user inconvenience compared to existing solutions and baselines.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16764","src/content/posts/2025-06-23-reinforcement-learning-for-hybrid-charging-stations-planning-and-operation-considering-fixed-and-mobile-chargers.md","519cadad50ca0d76",{"html":25,"metadata":30196},{"headings":30197,"localImagePaths":30198,"remoteImagePaths":30199,"frontmatter":30200,"imagePaths":30201},[],[],[],{"title":30189,"description":30190,"summary":30190,"pubDate":25202,"source":25191,"url":30192,"thumbnail":25193},[],"2025-06-23-reinforcement-learning-for-hybrid-charging-stations-planning-and-operation-considering-fixed-and-mobile-chargers.md","2025-06-23-reinforcing-spatial-reasoning-in-vision-language-models-with-interwoven-thinking-and-visual-drawing",{"id":30203,"data":30205,"filePath":30210,"digest":30211,"rendered":30212,"legacyId":30219},{"title":30206,"description":30207,"summary":30207,"pubDate":30208,"source":25191,"url":30209,"thumbnail":25193},"Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing","arXiv:2506.09965v2 Announce Type: replace-cross Abstract: As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking-capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables LVLMs to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations, including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named VILASR, consistently outperforms existing methods across diverse spatial reasoning benchmarks, involving maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning tasks, with an average improvement of 18.4%.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.09965","src/content/posts/2025-06-23-reinforcing-spatial-reasoning-in-vision-language-models-with-interwoven-thinking-and-visual-drawing.md","9ce17739f1db2ba4",{"html":25,"metadata":30213},{"headings":30214,"localImagePaths":30215,"remoteImagePaths":30216,"frontmatter":30217,"imagePaths":30218},[],[],[],{"title":30206,"description":30207,"summary":30207,"pubDate":25202,"source":25191,"url":30209,"thumbnail":25193},[],"2025-06-23-reinforcing-spatial-reasoning-in-vision-language-models-with-interwoven-thinking-and-visual-drawing.md","2025-06-23-relational-deep-learning-challenges-foundations-and-next-generation-architectures",{"id":30220,"data":30222,"filePath":30227,"digest":30228,"rendered":30229,"legacyId":30236},{"title":30223,"description":30224,"summary":30224,"pubDate":30225,"source":25191,"url":30226,"thumbnail":25193},"Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures","arXiv:2506.16654v1 Announce Type: cross Abstract: Graph machine learning has led to a significant increase in the capabilities of models that learn on arbitrary graph-structured data and has been applied to molecules, social networks, recommendation systems, and transportation, among other domains. Data in multi-tabular relational databases can also be constructed as 'relational entity graphs' for Relational Deep Learning (RDL) - a new blueprint that enables end-to-end representation learning without traditional feature engineering. Compared to arbitrary graph-structured data, relational entity graphs have key properties: (i) their structure is defined by primary-foreign key relationships between entities in different tables, (ii) the structural connectivity is a function of the relational schema defining a database, and (iii) the graph connectivity is temporal and heterogeneous in nature. In this paper, we provide a comprehensive review of RDL by first introducing the representation of relational databases as relational entity graphs, and then reviewing public benchmark datasets that have been used to develop and evaluate recent GNN-based RDL models. We discuss key challenges including large-scale multi-table integration and the complexities of modeling temporal dynamics and heterogeneous data, while also surveying foundational neural network methods and recent architectural advances specialized for relational entity graphs. Finally, we explore opportunities to unify these distinct modeling challenges, highlighting how RDL converges multiple sub-fields in graph machine learning towards the design of foundation models that can transform the processing of relational data.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16654","src/content/posts/2025-06-23-relational-deep-learning-challenges-foundations-and-next-generation-architectures.md","77e9cf6c33098d11",{"html":25,"metadata":30230},{"headings":30231,"localImagePaths":30232,"remoteImagePaths":30233,"frontmatter":30234,"imagePaths":30235},[],[],[],{"title":30223,"description":30224,"summary":30224,"pubDate":25202,"source":25191,"url":30226,"thumbnail":25193},[],"2025-06-23-relational-deep-learning-challenges-foundations-and-next-generation-architectures.md","2025-06-23-reliable-few-shot-learning-under-dual-noises",{"id":30237,"data":30239,"filePath":30244,"digest":30245,"rendered":30246,"legacyId":30253},{"title":30240,"description":30241,"summary":30241,"pubDate":30242,"source":25191,"url":30243,"thumbnail":25193},"Reliable Few-shot Learning under Dual Noises","arXiv:2506.16330v1 Announce Type: cross Abstract: Recent advances in model pre-training give rise to task adaptation-based few-shot learning (FSL), where the goal is to adapt a pre-trained task-agnostic model for capturing task-specific knowledge with a few-labeled support samples of the target task.Nevertheless, existing approaches may still fail in the open world due to the inevitable in-distribution (ID) and out-of-distribution (OOD) noise from both support and query samples of the target task. With limited support samples available, i) the adverse effect of the dual noises can be severely amplified during task adaptation, and ii) the adapted model can produce unreliable predictions on query samples in the presence of the dual noises. In this work, we propose DEnoised Task Adaptation (DETA++) for reliable FSL. DETA++ uses a Contrastive Relevance Aggregation (CoRA) module to calculate image and region weights for support samples, based on which a clean prototype loss and a noise entropy maximization loss are proposed to achieve noise-robust task adaptation. Additionally,DETA++ employs a memory bank to store and refine clean regions for each inner-task class, based on which a Local Nearest Centroid Classifier (LocalNCC) is devised to yield noise-robust predictions on query samples. Moreover, DETA++ utilizes an Intra-class Region Swapping (IntraSwap) strategy to rectify ID class prototypes during task adaptation, enhancing the model's robustness to the dual noises. Extensive experiments demonstrate the effectiveness and flexibility of DETA++.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16330","src/content/posts/2025-06-23-reliable-few-shot-learning-under-dual-noises.md","7e22c6f4a92d1e4d",{"html":25,"metadata":30247},{"headings":30248,"localImagePaths":30249,"remoteImagePaths":30250,"frontmatter":30251,"imagePaths":30252},[],[],[],{"title":30240,"description":30241,"summary":30241,"pubDate":25202,"source":25191,"url":30243,"thumbnail":25193},[],"2025-06-23-reliable-few-shot-learning-under-dual-noises.md","2025-06-23-relic-enhancing-reward-model-generalization-for-low-resource-indic-languages-with-few-shot-examples",{"id":30254,"data":30256,"filePath":30261,"digest":30262,"rendered":30263,"legacyId":30270},{"title":30257,"description":30258,"summary":30258,"pubDate":30259,"source":25191,"url":30260,"thumbnail":25193},"Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples","arXiv:2506.16502v1 Announce Type: cross Abstract: Reward models are essential for aligning large language models (LLMs) with human preferences. However, most open-source multilingual reward models are primarily trained on preference datasets in high-resource languages, resulting in unreliable reward signals for low-resource Indic languages. Collecting large-scale, high-quality preference data for these languages is prohibitively expensive, making preference-based training approaches impractical. To address this challenge, we propose RELIC, a novel in-context learning framework for reward modeling in low-resource Indic languages. RELIC trains a retriever with a pairwise ranking objective to select in-context examples from auxiliary high-resource languages that most effectively highlight the distinction between preferred and less-preferred responses. Extensive experiments on three preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art open-source reward models demonstrate that RELIC significantly improves reward model accuracy for low-resource Indic languages, consistently outperforming existing example selection methods. For example, on Bodo-a low-resource Indic language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13% improvement in accuracy over zero-shot prompting and state-of-the-art example selection method, respectively.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16502","src/content/posts/2025-06-23-relic-enhancing-reward-model-generalization-for-low-resource-indic-languages-with-few-shot-examples.md","fd3e65429c4654c4",{"html":25,"metadata":30264},{"headings":30265,"localImagePaths":30266,"remoteImagePaths":30267,"frontmatter":30268,"imagePaths":30269},[],[],[],{"title":30257,"description":30258,"summary":30258,"pubDate":25202,"source":25191,"url":30260,"thumbnail":25193},[],"2025-06-23-relic-enhancing-reward-model-generalization-for-low-resource-indic-languages-with-few-shot-examples.md","2025-06-23-representation-learning-of-point-cloud-upsampling-in-global-and-local-inputs",{"id":30271,"data":30273,"filePath":30278,"digest":30279,"rendered":30280,"legacyId":30287},{"title":30274,"description":30275,"summary":30275,"pubDate":30276,"source":25191,"url":30277,"thumbnail":25193},"Representation Learning of Point Cloud Upsampling in Global and Local Inputs","arXiv:2501.07076v3 Announce Type: replace-cross Abstract: In recent years, point cloud upsampling has been widely applied in tasks such as 3D reconstruction and object recognition. This study proposed a novel framework, ReLPU, which enhances upsampling performance by explicitly learning from both global and local structural features of point clouds. Specifically, we extracted global features from uniformly segmented inputs (Average Segments) and local features from patch-based inputs of the same point cloud. These two types of features were processed through parallel autoencoders, fused, and then fed into a shared decoder for upsampling. This dual-input design improved feature completeness and cross-scale consistency, especially in sparse and noisy regions. Our framework was applied to several state-of-the-art autoencoder-based networks and validated on standard datasets. Experimental results demonstrated consistent improvements in geometric fidelity and robustness. In addition, saliency maps confirmed that parallel global-local learning significantly enhanced the interpretability and performance of point cloud upsampling.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2501.07076","src/content/posts/2025-06-23-representation-learning-of-point-cloud-upsampling-in-global-and-local-inputs.md","3bbde2d0c6ea3258",{"html":25,"metadata":30281},{"headings":30282,"localImagePaths":30283,"remoteImagePaths":30284,"frontmatter":30285,"imagePaths":30286},[],[],[],{"title":30274,"description":30275,"summary":30275,"pubDate":25202,"source":25191,"url":30277,"thumbnail":25193},[],"2025-06-23-representation-learning-of-point-cloud-upsampling-in-global-and-local-inputs.md","2025-06-23-representation-learning-with-mutual-influence-of-modalities-for-node-classification-in-multi-modal-heterogeneous-networks",{"id":30288,"data":30290,"filePath":30295,"digest":30296,"rendered":30297,"legacyId":30304},{"title":30291,"description":30292,"summary":30292,"pubDate":30293,"source":25191,"url":30294,"thumbnail":25193},"Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks","arXiv:2505.07895v3 Announce Type: replace-cross Abstract: Nowadays, numerous online platforms can be described as multi-modal heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's product review networks. Accurately categorizing nodes within these networks is crucial for analyzing the corresponding entities, which requires effective representation learning on nodes. However, existing multi-modal fusion methods often adopt either early fusion strategies which may lose the unique characteristics of individual modalities, or late fusion approaches overlooking the cross-modal guidance in GNN-based information propagation. In this paper, we propose a novel model for node classification in MMHNs, named Heterogeneous Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node representations by capturing the mutual influence of multiple modalities during the information propagation process, within the framework of heterogeneous graph transformer. Specifically, a nested inter-modal attention mechanism is integrated into the inter-node attention to achieve adaptive multi-modal fusion, and modality alignment is also taken into account to encourage the propagation among nodes with consistent similarities across all modalities. Moreover, an attention loss is augmented to mitigate the impact of missing modalities. Extensive experiments validate the superiority of the model in the node classification task, providing an innovative view to handle multi-modal data, especially when accompanied with network structures.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.07895","src/content/posts/2025-06-23-representation-learning-with-mutual-influence-of-modalities-for-node-classification-in-multi-modal-heterogeneous-networks.md","c3117e4f85aab86c",{"html":25,"metadata":30298},{"headings":30299,"localImagePaths":30300,"remoteImagePaths":30301,"frontmatter":30302,"imagePaths":30303},[],[],[],{"title":30291,"description":30292,"summary":30292,"pubDate":25202,"source":25191,"url":30294,"thumbnail":25193},[],"2025-06-23-representation-learning-with-mutual-influence-of-modalities-for-node-classification-in-multi-modal-heterogeneous-networks.md","2025-06-23-rethinking-external-slow-thinking-from-snowball-errors-to-probability-of-correct-reasoning",{"id":30305,"data":30307,"filePath":30312,"digest":30313,"rendered":30314,"legacyId":30321},{"title":30308,"description":30309,"summary":30309,"pubDate":30310,"source":25191,"url":30311,"thumbnail":25193},"Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning","arXiv:2501.15602v3 Announce Type: replace Abstract: Test-time scaling, which is also often referred to as slow-thinking, has been demonstrated to enhance multi-step reasoning in large language models (LLMs). However, despite its widespread utilization, the mechanisms underlying slow-thinking methods remain poorly understood. This paper explores the mechanisms of external slow-thinking from a theoretical standpoint. We begin by examining the snowball error effect within the LLM reasoning process and connect it to the likelihood of correct reasoning using information theory. Building on this, we show that external slow-thinking methods can be interpreted as strategies to mitigate the error probability. We further provide a comparative analysis of popular external slow-thinking approaches, ranging from simple to complex, highlighting their differences and interrelationships. Our findings suggest that the efficacy of these methods is not primarily determined by the specific framework employed, and that expanding the search scope or the model's internal reasoning capacity may yield more sustained improvements in the long term. We open-source our code at https://github.com/ZyGan1999/Snowball-Errors-and-Probability.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2501.15602","src/content/posts/2025-06-23-rethinking-external-slow-thinking-from-snowball-errors-to-probability-of-correct-reasoning.md","3ea3161f3c223f1b",{"html":25,"metadata":30315},{"headings":30316,"localImagePaths":30317,"remoteImagePaths":30318,"frontmatter":30319,"imagePaths":30320},[],[],[],{"title":30308,"description":30309,"summary":30309,"pubDate":25202,"source":25191,"url":30311,"thumbnail":25193},[],"2025-06-23-rethinking-external-slow-thinking-from-snowball-errors-to-probability-of-correct-reasoning.md","2025-06-23-rethinking-fine-tuning-when-scaling-test-time-compute-limiting-confidence-improves-mathematical-reasoning",{"id":30322,"data":30324,"filePath":30329,"digest":30330,"rendered":30331,"legacyId":30338},{"title":30325,"description":30326,"summary":30326,"pubDate":30327,"source":25191,"url":30328,"thumbnail":25193},"Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning","arXiv:2502.07154v3 Announce Type: replace-cross Abstract: Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in $N$ independent samples. We show, surprisingly, that training with cross-entropy (CE) loss can be ${it misaligned}$ with pass@N in that pass@N accuracy ${it decreases}$ with longer training. We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees of varying shapes. Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2502.07154","src/content/posts/2025-06-23-rethinking-fine-tuning-when-scaling-test-time-compute-limiting-confidence-improves-mathematical-reasoning.md","cd03e02179333512",{"html":25,"metadata":30332},{"headings":30333,"localImagePaths":30334,"remoteImagePaths":30335,"frontmatter":30336,"imagePaths":30337},[],[],[],{"title":30325,"description":30326,"summary":30326,"pubDate":25202,"source":25191,"url":30328,"thumbnail":25193},[],"2025-06-23-rethinking-fine-tuning-when-scaling-test-time-compute-limiting-confidence-improves-mathematical-reasoning.md","2025-06-23-revisiting-multi-agent-debate-as-test-time-scaling-a-systematic-study-of-conditional-effectiveness",{"id":30339,"data":30341,"filePath":30346,"digest":30347,"rendered":30348,"legacyId":30355},{"title":30342,"description":30343,"summary":30343,"pubDate":30344,"source":25191,"url":30345,"thumbnail":25193},"Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness","arXiv:2505.22960v2 Announce Type: replace Abstract: The remarkable growth in large language model (LLM) capabilities has spurred exploration into multi-agent systems, with debate frameworks emerging as a promising avenue for enhanced problem-solving. These multi-agent debate (MAD) approaches, where agents collaboratively present, critique, and refine arguments, potentially offer improved reasoning, robustness, and diverse perspectives over monolithic models. Despite prior studies leveraging MAD, a systematic understanding of its effectiveness compared to self-agent methods, particularly under varying conditions, remains elusive. This paper seeks to fill this gap by conceptualizing MAD as a test-time computational scaling technique, distinguished by collaborative refinement and diverse exploration capabilities. We conduct a comprehensive empirical investigation comparing MAD with strong self-agent test-time scaling baselines on mathematical reasoning and safety-related tasks. Our study systematically examines the influence of task difficulty, model scale, and agent diversity on MAD's performance. Key findings reveal that, for mathematical reasoning, MAD offers limited advantages over self-agent scaling but becomes more effective with increased problem difficulty and decreased model capability, while agent diversity shows little benefit. Conversely, for safety tasks, MAD's collaborative refinement can increase vulnerability, but incorporating diverse agent configurations facilitates a gradual reduction in attack success through the collaborative refinement process. We believe our findings provide critical guidance for the future development of more effective and strategically deployed MAD systems.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.22960","src/content/posts/2025-06-23-revisiting-multi-agent-debate-as-test-time-scaling-a-systematic-study-of-conditional-effectiveness.md","01e45972daae727c",{"html":25,"metadata":30349},{"headings":30350,"localImagePaths":30351,"remoteImagePaths":30352,"frontmatter":30353,"imagePaths":30354},[],[],[],{"title":30342,"description":30343,"summary":30343,"pubDate":25202,"source":25191,"url":30345,"thumbnail":25193},[],"2025-06-23-revisiting-multi-agent-debate-as-test-time-scaling-a-systematic-study-of-conditional-effectiveness.md","2025-06-23-riosworld-benchmarking-the-risk-of-multimodal-computer-use-agents",{"id":30356,"data":30358,"filePath":30363,"digest":30364,"rendered":30365,"legacyId":30372},{"title":30359,"description":30360,"summary":30360,"pubDate":30361,"source":25191,"url":30362,"thumbnail":25193},"RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents","arXiv:2506.00618v3 Announce Type: replace Abstract: With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce textbf{RiOSWorld}, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on textbf{RiOSWorld} demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.00618","src/content/posts/2025-06-23-riosworld-benchmarking-the-risk-of-multimodal-computer-use-agents.md","be7d4d8a6655c92a",{"html":25,"metadata":30366},{"headings":30367,"localImagePaths":30368,"remoteImagePaths":30369,"frontmatter":30370,"imagePaths":30371},[],[],[],{"title":30359,"description":30360,"summary":30360,"pubDate":25202,"source":25191,"url":30362,"thumbnail":25193},[],"2025-06-23-riosworld-benchmarking-the-risk-of-multimodal-computer-use-agents.md","2025-06-23-rl2grid-benchmarking-reinforcement-learning-in-power-grid-operations",{"id":30373,"data":30375,"filePath":30380,"digest":30381,"rendered":30382,"legacyId":30389},{"title":30376,"description":30377,"summary":30377,"pubDate":30378,"source":25191,"url":30379,"thumbnail":25193},"RL2Grid: Benchmarking Reinforcement Learning in Power Grid Operations","arXiv:2503.23101v2 Announce Type: replace-cross Abstract: Reinforcement learning (RL) can provide adaptive and scalable controllers essential for power grid decarbonization. However, RL methods struggle with power grids' complex dynamics, long-horizon goals, and hard physical constraints. For these reasons, we present RL2Grid, a benchmark designed in collaboration with power system operators to accelerate progress in grid control and foster RL maturity. Built on RTE France's power simulation framework, RL2Grid standardizes tasks, state and action spaces, and reward structures for a systematic evaluation and comparison of RL algorithms. Moreover, we integrate operational heuristics and design safety constraints based on human expertise to ensure alignment with physical requirements. By establishing reference performance metrics for classic RL baselines on RL2Grid's tasks, we highlight the need for novel methods capable of handling real systems and discuss future directions for RL-based grid control.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2503.23101","src/content/posts/2025-06-23-rl2grid-benchmarking-reinforcement-learning-in-power-grid-operations.md","a7665f3fac95df2f",{"html":25,"metadata":30383},{"headings":30384,"localImagePaths":30385,"remoteImagePaths":30386,"frontmatter":30387,"imagePaths":30388},[],[],[],{"title":30376,"description":30377,"summary":30377,"pubDate":25202,"source":25191,"url":30379,"thumbnail":25193},[],"2025-06-23-rl2grid-benchmarking-reinforcement-learning-in-power-grid-operations.md","2025-06-23-robo2vlm-visual-question-answering-from-large-scale-in-the-wild-robot-manipulation-datasets",{"id":30390,"data":30392,"filePath":30397,"digest":30398,"rendered":30399,"legacyId":30406},{"title":30393,"description":30394,"summary":30394,"pubDate":30395,"source":25191,"url":30396,"thumbnail":25193},"Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets","arXiv:2505.15517v2 Announce Type: replace-cross Abstract: Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.15517","src/content/posts/2025-06-23-robo2vlm-visual-question-answering-from-large-scale-in-the-wild-robot-manipulation-datasets.md","8a08e9f000e889c1",{"html":25,"metadata":30400},{"headings":30401,"localImagePaths":30402,"remoteImagePaths":30403,"frontmatter":30404,"imagePaths":30405},[],[],[],{"title":30393,"description":30394,"summary":30394,"pubDate":25202,"source":25191,"url":30396,"thumbnail":25193},[],"2025-06-23-robo2vlm-visual-question-answering-from-large-scale-in-the-wild-robot-manipulation-datasets.md","2025-06-23-robust-dynamic-material-handling-via-adaptive-constrained-evolutionary-reinforcement-learning",{"id":30407,"data":30409,"filePath":30414,"digest":30415,"rendered":30416,"legacyId":30423},{"title":30410,"description":30411,"summary":30411,"pubDate":30412,"source":25191,"url":30413,"thumbnail":25193},"Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning","arXiv:2506.16795v1 Announce Type: cross Abstract: Dynamic material handling (DMH) involves the assignment of dynamically arriving material transporting tasks to suitable vehicles in real time for minimising makespan and tardiness. In real-world scenarios, historical task records are usually available, which enables the training of a decision policy on multiple instances consisting of historical records. Recently, reinforcement learning has been applied to solve DMH. Due to the occurrence of dynamic events such as new tasks, adaptability is highly required. Solving DMH is challenging since constraints including task delay should be satisfied. A feedback is received only when all tasks are served, which leads to sparse reward. Besides, making the best use of limited computational resources and historical records for training a robust policy is crucial. The time allocated to different problem instances would highly impact the learning process. To tackle those challenges, this paper proposes a novel adaptive constrained evolutionary reinforcement learning (ACERL) approach, which maintains a population of actors for diverse exploration. ACERL accesses each actor for tackling sparse rewards and constraint violation to restrict the behaviour of the policy. Moreover, ACERL adaptively selects the most beneficial training instances for improving the policy. Extensive experiments on eight training and eight unseen test instances demonstrate the outstanding performance of ACERL compared with several state-of-the-art algorithms. Policies trained by ACERL can schedule the vehicles while fully satisfying the constraints. Additional experiments on 40 unseen noised instances show the robust performance of ACERL. Cross-validation further presents the overall effectiveness of ACREL. Besides, a rigorous ablation study highlights the coordination and benefits of each ingredient of ACERL.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16795","src/content/posts/2025-06-23-robust-dynamic-material-handling-via-adaptive-constrained-evolutionary-reinforcement-learning.md","3edb5642bde53fd1",{"html":25,"metadata":30417},{"headings":30418,"localImagePaths":30419,"remoteImagePaths":30420,"frontmatter":30421,"imagePaths":30422},[],[],[],{"title":30410,"description":30411,"summary":30411,"pubDate":25202,"source":25191,"url":30413,"thumbnail":25193},[],"2025-06-23-robust-dynamic-material-handling-via-adaptive-constrained-evolutionary-reinforcement-learning.md","2025-06-23-robust-finite-memory-policy-gradients-for-hidden-model-pomdps",{"id":30424,"data":30426,"filePath":30431,"digest":30432,"rendered":30433,"legacyId":30440},{"title":30427,"description":30428,"summary":30428,"pubDate":30429,"source":25191,"url":30430,"thumbnail":25193},"Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs","arXiv:2505.09518v2 Announce Type: replace Abstract: Partially observable Markov decision processes (POMDPs) model specific environments in sequential decision-making under uncertainty. Critically, optimal policies for POMDPs may not be robust against perturbations in the environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different environment models, that is, POMDPs with a shared action and observation space. The intuition is that the true model is hidden among a set of potential models, and it is unknown which model will be the environment at execution time. A policy is robust for a given HM-POMDP if it achieves sufficient performance for each of its POMDPs.We compute such robust policies by combining two orthogonal techniques: (1) a deductive formal verification technique that supports tractable robust policy evaluation by computing a worst-case POMDP within the HM-POMDP, and (2) subgradient ascent to optimize the candidate policy for a worst-case POMDP. The empirical evaluation shows that, compared to various baselines, our approach (1) produces policies that are more robust and generalize better to unseen POMDPs, and (2) scales to HM-POMDPs that consist of over a hundred thousand environments.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.09518","src/content/posts/2025-06-23-robust-finite-memory-policy-gradients-for-hidden-model-pomdps.md","6c889fbe91b491b1",{"html":25,"metadata":30434},{"headings":30435,"localImagePaths":30436,"remoteImagePaths":30437,"frontmatter":30438,"imagePaths":30439},[],[],[],{"title":30427,"description":30428,"summary":30428,"pubDate":25202,"source":25191,"url":30430,"thumbnail":25193},[],"2025-06-23-robust-finite-memory-policy-gradients-for-hidden-model-pomdps.md","2025-06-23-robust-training-with-data-augmentation-for-medical-imaging-classification",{"id":30441,"data":30443,"filePath":30448,"digest":30449,"rendered":30450,"legacyId":30457},{"title":30444,"description":30445,"summary":30445,"pubDate":30446,"source":25191,"url":30447,"thumbnail":25193},"Robust Training with Data Augmentation for Medical Imaging Classification","arXiv:2506.17133v1 Announce Type: cross Abstract: Deep neural networks are increasingly being used to detect and diagnose medical conditions using medical imaging. Despite their utility, these models are highly vulnerable to adversarial attacks and distribution shifts, which can affect diagnostic reliability and undermine trust among healthcare professionals. In this study, we propose a robust training algorithm with data augmentation (RTDA) to mitigate these vulnerabilities in medical image classification. We benchmark classifier robustness against adversarial perturbations and natural variations of RTDA and six competing baseline techniques, including adversarial training and data augmentation approaches in isolation and combination, using experimental data sets with three different imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that RTDA achieves superior robustness against adversarial attacks and improved generalization performance in the presence of distribution shift in each image classification task while maintaining high clean accuracy.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17133","src/content/posts/2025-06-23-robust-training-with-data-augmentation-for-medical-imaging-classification.md","7385d0e9a79e10e3",{"html":25,"metadata":30451},{"headings":30452,"localImagePaths":30453,"remoteImagePaths":30454,"frontmatter":30455,"imagePaths":30456},[],[],[],{"title":30444,"description":30445,"summary":30445,"pubDate":25202,"source":25191,"url":30447,"thumbnail":25193},[],"2025-06-23-robust-training-with-data-augmentation-for-medical-imaging-classification.md","2025-06-23-robustness-evaluation-of-ocr-based-visual-document-understanding-under-multi-modal-adversarial-attacks",{"id":30458,"data":30460,"filePath":30465,"digest":30466,"rendered":30467,"legacyId":30474},{"title":30461,"description":30462,"summary":30462,"pubDate":30463,"source":25191,"url":30464,"thumbnail":25193},"Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks","arXiv:2506.16407v1 Announce Type: cross Abstract: Visual Document Understanding (VDU) systems have achieved strong performance in information extraction by integrating textual, layout, and visual signals. However, their robustness under realistic adversarial perturbations remains insufficiently explored. We introduce the first unified framework for generating and evaluating multi-modal adversarial attacks on OCR-based VDU models. Our method covers six gradient-based layout attack scenarios, incorporating manipulations of OCR bounding boxes, pixels, and texts across both word and line granularities, with constraints on layout perturbation budget (e.g., IoU >= 0.6) to preserve plausibility. Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and six model families demonstrate that line-level attacks and compound perturbations (BBox + Pixel + Text) yield the most severe performance degradation. Projected Gradient Descent (PGD)-based BBox perturbations outperform random-shift baselines in all investigated models. Ablation studies further validate the impact of layout budget, text modification, and adversarial transferability.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16407","src/content/posts/2025-06-23-robustness-evaluation-of-ocr-based-visual-document-understanding-under-multi-modal-adversarial-attacks.md","027e769a0f721923",{"html":25,"metadata":30468},{"headings":30469,"localImagePaths":30470,"remoteImagePaths":30471,"frontmatter":30472,"imagePaths":30473},[],[],[],{"title":30461,"description":30462,"summary":30462,"pubDate":25202,"source":25191,"url":30464,"thumbnail":25193},[],"2025-06-23-robustness-evaluation-of-ocr-based-visual-document-understanding-under-multi-modal-adversarial-attacks.md","2025-06-23-safegenbench-a-benchmark-framework-for-security-vulnerability-detection-in-llm-generated-code",{"id":30475,"data":30477,"filePath":30482,"digest":30483,"rendered":30484,"legacyId":30491},{"title":30478,"description":30479,"summary":30479,"pubDate":30480,"source":25191,"url":30481,"thumbnail":25193},"SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code","arXiv:2506.05692v3 Announce Type: replace-cross Abstract: The code generation capabilities of large language models(LLMs) have emerged as a critical dimension in evaluating their overall performance. However, prior research has largely overlooked the security risks inherent in the generated code. In this work, we introduce SafeGenBench, a benchmark specifically designed to assess the security of LLM-generated code. The dataset encompasses a wide range of common software development scenarios and vulnerability types. Building upon this benchmark, we develop an automatic evaluation framework that leverages both static application security testing(SAST) and LLM-based judging to assess the presence of security vulnerabilities in model-generated code. Through the empirical evaluation of state-of-the-art LLMs on SafeGenBench, we reveal notable deficiencies in their ability to produce vulnerability-free code. Our findings highlight pressing challenges and offer actionable insights for future advancements in the secure code generation performance of LLMs. The data and code will be released soon.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.05692","src/content/posts/2025-06-23-safegenbench-a-benchmark-framework-for-security-vulnerability-detection-in-llm-generated-code.md","ce6671e2edc70ee9",{"html":25,"metadata":30485},{"headings":30486,"localImagePaths":30487,"remoteImagePaths":30488,"frontmatter":30489,"imagePaths":30490},[],[],[],{"title":30478,"description":30479,"summary":30479,"pubDate":25202,"source":25191,"url":30481,"thumbnail":25193},[],"2025-06-23-safegenbench-a-benchmark-framework-for-security-vulnerability-detection-in-llm-generated-code.md","2025-06-23-safemimic-towards-safe-and-autonomous-human-to-robot-imitation-for-mobile-manipulation",{"id":30492,"data":30494,"filePath":30499,"digest":30500,"rendered":30501,"legacyId":30508},{"title":30495,"description":30496,"summary":30496,"pubDate":30497,"source":25191,"url":30498,"thumbnail":25193},"SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation","arXiv:2506.15847v1 Announce Type: cross Abstract: For robots to become efficient helpers in the home, they must learn to perform new mobile manipulation tasks simply by watching humans perform them. Learning from a single video demonstration from a human is challenging as the robot needs to first extract from the demo what needs to be done and how, translate the strategy from a third to a first-person perspective, and then adapt it to be successful with its own morphology. Furthermore, to mitigate the dependency on costly human monitoring, this learning process should be performed in a safe and autonomous manner. We present SafeMimic, a framework to learn new mobile manipulation skills safely and autonomously from a single third-person human video. Given an initial human video demonstration of a multi-step mobile manipulation task, SafeMimic first parses the video into segments, inferring both the semantic changes caused and the motions the human executed to achieve them and translating them to an egocentric reference. Then, it adapts the behavior to the robot's own morphology by sampling candidate actions around the human ones, and verifying them for safety before execution in a receding horizon fashion using an ensemble of safety Q-functions trained in simulation. When safe forward progression is not possible, SafeMimic backtracks to previous states and attempts a different sequence of actions, adapting both the trajectory and the grasping modes when required for its morphology. As a result, SafeMimic yields a strategy that succeeds in the demonstrated behavior and learns task-specific actions that reduce exploration in future attempts. Our experiments show that our method allows robots to safely and efficiently learn multi-step mobile manipulation behaviors from a single human demonstration, from different users, and in different environments, with improvements over state-of-the-art baselines across seven tasks",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15847","src/content/posts/2025-06-23-safemimic-towards-safe-and-autonomous-human-to-robot-imitation-for-mobile-manipulation.md","8f18db84bf4354b5",{"html":25,"metadata":30502},{"headings":30503,"localImagePaths":30504,"remoteImagePaths":30505,"frontmatter":30506,"imagePaths":30507},[],[],[],{"title":30495,"description":30496,"summary":30496,"pubDate":25202,"source":25191,"url":30498,"thumbnail":25193},[],"2025-06-23-safemimic-towards-safe-and-autonomous-human-to-robot-imitation-for-mobile-manipulation.md","2025-06-23-screen-hijack-visual-poisoning-of-vlm-agents-in-mobile-environments",{"id":30509,"data":30511,"filePath":30516,"digest":30517,"rendered":30518,"legacyId":30525},{"title":30512,"description":30513,"summary":30513,"pubDate":30514,"source":25191,"url":30515,"thumbnail":25193},"Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments","arXiv:2506.13205v2 Announce Type: replace-cross Abstract: With the growing integration of vision-language models (VLMs), mobile agents are now widely used for tasks like UI automation and camera-based user assistance. These agents are often fine-tuned on limited user-generated datasets, leaving them vulnerable to covert threats during the training process. In this work we present GHOST, the first clean-label backdoor attack specifically designed for mobile agents built upon VLMs. Our method manipulates only the visual inputs of a portion of the training samples - without altering their corresponding labels or instructions - thereby injecting malicious behaviors into the model. Once fine-tuned with this tampered data, the agent will exhibit attacker-controlled responses when a specific visual trigger is introduced at inference time. The core of our approach lies in aligning the gradients of poisoned samples with those of a chosen target instance, embedding backdoor-relevant features into the poisoned training data. To maintain stealth and enhance robustness, we develop three realistic visual triggers: static visual patches, dynamic motion cues, and subtle low-opacity overlays. We evaluate our method across six real-world Android apps and three VLM architectures adapted for mobile use. Results show that our attack achieves high attack success rates (up to 94.67 percent) while maintaining high clean-task performance (FSR up to 95.85 percent). Additionally, ablation studies shed light on how various design choices affect the efficacy and concealment of the attack. Overall, this work is the first to expose critical security flaws in VLM-based mobile agents, highlighting their susceptibility to clean-label backdoor attacks and the urgent need for effective defense mechanisms in their training pipelines.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.13205","src/content/posts/2025-06-23-screen-hijack-visual-poisoning-of-vlm-agents-in-mobile-environments.md","334c96f48d9cda4b",{"html":25,"metadata":30519},{"headings":30520,"localImagePaths":30521,"remoteImagePaths":30522,"frontmatter":30523,"imagePaths":30524},[],[],[],{"title":30512,"description":30513,"summary":30513,"pubDate":25202,"source":25191,"url":30515,"thumbnail":25193},[],"2025-06-23-screen-hijack-visual-poisoning-of-vlm-agents-in-mobile-environments.md","2025-06-23-sde-sql-enhancing-text-to-sql-generation-in-large-language-models-via-self-driven-exploration-with-sql-probes",{"id":30526,"data":30528,"filePath":30533,"digest":30534,"rendered":30535,"legacyId":30542},{"title":30529,"description":30530,"summary":30530,"pubDate":30531,"source":25191,"url":30532,"thumbnail":25193},"SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes","arXiv:2506.07245v2 Announce Type: replace-cross Abstract: Recent advancements in large language models (LLMs) have significantly improved performance on the Text-to-SQL task. However, prior approaches typically rely on static, pre-processed database information provided at inference time, which limits the model's ability to fully understand the database contents. Without dynamic interaction, LLMs are constrained to fixed, human-provided context and cannot autonomously explore the underlying data. To address this limitation, we propose SDE-SQL, a framework that enables large language models to perform self-driven exploration of databases during inference. This is accomplished by generating and executing SQL probes, which allow the model to actively retrieve information from the database and iteratively update its understanding of the data. Unlike prior methods, SDE-SQL operates in a zero-shot setting, without relying on any question-SQL pairs as in-context demonstrations. When evaluated on the BIRD benchmark with Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing a new state-of-the-art among methods based on open-source models without supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the performance of SDE-SQL can be further enhanced, yielding an additional 0.52% improvement.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.07245","src/content/posts/2025-06-23-sde-sql-enhancing-text-to-sql-generation-in-large-language-models-via-self-driven-exploration-with-sql-probes.md","60eaf5e2960a77fe",{"html":25,"metadata":30536},{"headings":30537,"localImagePaths":30538,"remoteImagePaths":30539,"frontmatter":30540,"imagePaths":30541},[],[],[],{"title":30529,"description":30530,"summary":30530,"pubDate":25202,"source":25191,"url":30532,"thumbnail":25193},[],"2025-06-23-sde-sql-enhancing-text-to-sql-generation-in-large-language-models-via-self-driven-exploration-with-sql-probes.md","2025-06-23-segment-anything-for-satellite-imagery-a-strong-baseline-and-a-regional-dataset-for-automatic-field-delineation",{"id":30543,"data":30545,"filePath":30550,"digest":30551,"rendered":30552,"legacyId":30559},{"title":30546,"description":30547,"summary":30547,"pubDate":30548,"source":25191,"url":30549,"thumbnail":25193},"Segment Anything for Satellite Imagery: A Strong Baseline and a Regional Dataset for Automatic Field Delineation","arXiv:2506.16318v1 Announce Type: cross Abstract: Accurate mapping of agricultural field boundaries is essential for the efficient operation of agriculture. Automatic extraction from high-resolution satellite imagery, supported by computer vision techniques, can avoid costly ground surveys. In this paper, we present a pipeline for field delineation based on the Segment Anything Model (SAM), introducing a fine-tuning strategy to adapt SAM to this task. In addition to using published datasets, we describe a method for acquiring a complementary regional dataset that covers areas beyond current sources. Extensive experiments assess segmentation accuracy and evaluate the generalization capabilities. Our approach provides a robust baseline for automated field delineation. The new regional dataset, known as ERAS, is now publicly available.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16318","src/content/posts/2025-06-23-segment-anything-for-satellite-imagery-a-strong-baseline-and-a-regional-dataset-for-automatic-field-delineation.md","61e702f15ea0c3c4",{"html":25,"metadata":30553},{"headings":30554,"localImagePaths":30555,"remoteImagePaths":30556,"frontmatter":30557,"imagePaths":30558},[],[],[],{"title":30546,"description":30547,"summary":30547,"pubDate":25202,"source":25191,"url":30549,"thumbnail":25193},[],"2025-06-23-segment-anything-for-satellite-imagery-a-strong-baseline-and-a-regional-dataset-for-automatic-field-delineation.md","2025-06-23-sekai-a-video-dataset-towards-world-exploration",{"id":30560,"data":30562,"filePath":30567,"digest":30568,"rendered":30569,"legacyId":30576},{"title":30563,"description":30564,"summary":30564,"pubDate":30565,"source":25191,"url":30566,"thumbnail":25193},"Sekai: A Video Dataset towards World Exploration","arXiv:2506.15675v2 Announce Type: replace-cross Abstract: Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications. The project page is https://lixsp11.github.io/sekai-project/.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15675","src/content/posts/2025-06-23-sekai-a-video-dataset-towards-world-exploration.md","eaf275e86a702a02",{"html":25,"metadata":30570},{"headings":30571,"localImagePaths":30572,"remoteImagePaths":30573,"frontmatter":30574,"imagePaths":30575},[],[],[],{"title":30563,"description":30564,"summary":30564,"pubDate":25202,"source":25191,"url":30566,"thumbnail":25193},[],"2025-06-23-sekai-a-video-dataset-towards-world-exploration.md","2025-06-23-selective-use-of-yannakakis-algorithm-to-improve-query-performance-machine-learning-to-the-rescue",{"id":30577,"data":30579,"filePath":30584,"digest":30585,"rendered":30586,"legacyId":30593},{"title":30580,"description":30581,"summary":30581,"pubDate":30582,"source":25191,"url":30583,"thumbnail":25193},"Selective Use of Yannakakis' Algorithm to Improve Query Performance: Machine Learning to the Rescue","arXiv:2502.20233v2 Announce Type: replace-cross Abstract: Query optimization has played a central role in database research for decades. However, more often than not, the proposed optimization techniques lead to a performance improvement in some, but not in all, situations. Therefore, we urgently need a methodology for designing a decision procedure that decides for a given query whether the optimization technique should be applied or not. In this work, we propose such a methodology with a focus on Yannakakis-style query evaluation as our optimization technique of interest. More specifically, we formulate this decision problem as an algorithm selection problem and we present a Machine Learning based approach for its solution. Empirical results with several benchmarks on a variety of database systems show that our approach indeed leads to a statistically significant performance improvement.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2502.20233","src/content/posts/2025-06-23-selective-use-of-yannakakis-algorithm-to-improve-query-performance-machine-learning-to-the-rescue.md","f1088da23d08020a",{"html":25,"metadata":30587},{"headings":30588,"localImagePaths":30589,"remoteImagePaths":30590,"frontmatter":30591,"imagePaths":30592},[],[],[],{"title":30580,"description":30581,"summary":30581,"pubDate":25202,"source":25191,"url":30583,"thumbnail":25193},[],"2025-06-23-selective-use-of-yannakakis-algorithm-to-improve-query-performance-machine-learning-to-the-rescue.md","2025-06-23-semagent-a-semantics-aware-program-repair-agent",{"id":30594,"data":30596,"filePath":30601,"digest":30602,"rendered":30603,"legacyId":30610},{"title":30597,"description":30598,"summary":30598,"pubDate":30599,"source":25191,"url":30600,"thumbnail":25193},"SemAgent: A Semantics Aware Program Repair Agent","arXiv:2506.16650v1 Announce Type: cross Abstract: Large Language Models (LLMs) have shown impressive capabilities in downstream software engineering tasks such as Automated Program Repair (APR). In particular, there has been a lot of research on repository-level issue-resolution benchmarks such as SWE-Bench. Although there has been significant progress on this topic, we notice that in the process of solving such issues, existing agentic systems tend to hyper-localize on immediately suspicious lines of code and fix them in isolation, without a deeper understanding of the issue semantics, code semantics, or execution semantics. Consequently, many existing systems generate patches that overfit to the user issue, even when a more general fix is preferable. To address this limitation, we introduce SemAgent, a novel workflow-based procedure that leverages issue, code, and execution semantics to generate patches that are complete - identifying and fixing all lines relevant to the issue. We achieve this through a novel pipeline that (a) leverages execution semantics to retrieve relevant context, (b) comprehends issue-semantics via generalized abstraction, (c) isolates code-semantics within the context of this abstraction, and (d) leverages this understanding in a two-stage architecture: a repair stage that proposes fine-grained fixes, followed by a reviewer stage that filters relevant fixes based on the inferred issue-semantics. Our evaluations show that our methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark beating all other workflow-based approaches, and an absolute improvement of 7.66% compared to our baseline, which lacks such deep semantic understanding. We note that our approach performs particularly well on issues requiring multi-line reasoning (and editing) and edge-case handling, suggesting that incorporating issue and code semantics into APR pipelines can lead to robust and semantically consistent repairs.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16650","src/content/posts/2025-06-23-semagent-a-semantics-aware-program-repair-agent.md","5bc565df0d9ee7ad",{"html":25,"metadata":30604},{"headings":30605,"localImagePaths":30606,"remoteImagePaths":30607,"frontmatter":30608,"imagePaths":30609},[],[],[],{"title":30597,"description":30598,"summary":30598,"pubDate":25202,"source":25191,"url":30600,"thumbnail":25193},[],"2025-06-23-semagent-a-semantics-aware-program-repair-agent.md","2025-06-23-semantic-preprocessing-for-llm-based-malware-analysis",{"id":30611,"data":30613,"filePath":30618,"digest":30619,"rendered":30620,"legacyId":30627},{"title":30614,"description":30615,"summary":30615,"pubDate":30616,"source":25191,"url":30617,"thumbnail":25193},"Semantic Preprocessing for LLM-based Malware Analysis","arXiv:2506.12113v2 Announce Type: replace-cross Abstract: In a context of malware analysis, numerous approaches rely on Artificial Intelligence to handle a large volume of data. However, these techniques focus on data view (images, sequences) and not on an expert's view. Noticing this issue, we propose a preprocessing that focuses on expert knowledge to improve malware semantic analysis and result interpretability. We propose a new preprocessing method which creates JSON reports for Portable Executable files. These reports gather features from both static and behavioral analysis, and incorporate packer signature detection, MITRE ATT&amp;CK and Malware Behavior Catalog (MBC) knowledge. The purpose of this preprocessing is to gather a semantic representation of binary files, understandable by malware analysts, and that can enhance AI models' explainability for malicious files analysis. Using this preprocessing to train a Large Language Model for Malware classification, we achieve a weighted-average F1-score of 0.94 on a complex dataset, representative of market reality.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.12113","src/content/posts/2025-06-23-semantic-preprocessing-for-llm-based-malware-analysis.md","7fcdbad59bbd18c8",{"html":25,"metadata":30621},{"headings":30622,"localImagePaths":30623,"remoteImagePaths":30624,"frontmatter":30625,"imagePaths":30626},[],[],[],{"title":30614,"description":30615,"summary":30615,"pubDate":25202,"source":25191,"url":30617,"thumbnail":25193},[],"2025-06-23-semantic-preprocessing-for-llm-based-malware-analysis.md","2025-06-23-serving-large-language-models-on-huawei-cloudmatrix384",{"id":30628,"data":30630,"filePath":30635,"digest":30636,"rendered":30637,"legacyId":30644},{"title":30631,"description":30632,"summary":30632,"pubDate":30633,"source":25191,"url":30634,"thumbnail":25193},"Serving Large Language Models on Huawei CloudMatrix384","arXiv:2506.12708v3 Announce Type: replace-cross Abstract: The rapid evolution of large language models (LLMs), driven by growing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure. Traditional AI clusters face limitations in compute intensity, memory bandwidth, inter-chip communication, and latency, compounded by variable workloads and strict service-level objectives. Addressing these issues requires fundamentally redesigned hardware-software integration. This paper introduces Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910 NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus (UB) network, enabling direct all-to-all communication and dynamic pooling of resources. These features optimize performance for communication-intensive operations, such as large-scale MoE expert parallelism and distributed key-value cache access. To fully leverage CloudMatrix384, we propose CloudMatrix-Infer, an advanced LLM serving solution incorporating three core innovations: a peer-to-peer serving architecture that independently scales prefill, decode, and caching; a large-scale expert parallelism strategy supporting EP320 via efficient UB-based token dispatch; and hardware-aware optimizations including specialized operators, microbatch-based pipelining, and INT8 quantization. Evaluation with the DeepSeek-R1 model shows CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of 6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (\u003C50 ms TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s per NPU even under stringent 15 ms latency constraints, while INT8 quantization maintains model accuracy across benchmarks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.12708","src/content/posts/2025-06-23-serving-large-language-models-on-huawei-cloudmatrix384.md","24fafbee9e4b84a3",{"html":25,"metadata":30638},{"headings":30639,"localImagePaths":30640,"remoteImagePaths":30641,"frontmatter":30642,"imagePaths":30643},[],[],[],{"title":30631,"description":30632,"summary":30632,"pubDate":25202,"source":25191,"url":30634,"thumbnail":25193},[],"2025-06-23-serving-large-language-models-on-huawei-cloudmatrix384.md","2025-06-23-shade-arena-evaluating-sabotage-and-monitoring-in-llm-agents",{"id":30645,"data":30647,"filePath":30652,"digest":30653,"rendered":30654,"legacyId":30661},{"title":30648,"description":30649,"summary":30649,"pubDate":30650,"source":25191,"url":30651,"thumbnail":25193},"SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents","arXiv:2506.15740v1 Announce Type: new Abstract: As Large Language Models (LLMs) are increasingly deployed as autonomous agents in complex and long horizon settings, it is critical to evaluate their ability to sabotage users by pursuing hidden objectives. We study the ability of frontier LLMs to evade monitoring and achieve harmful hidden goals while completing a wide array of realistic tasks. We evaluate a broad range of frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena, the first highly diverse agent evaluation dataset for sabotage and monitoring capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign main tasks and harmful side objectives in complicated environments. Agents are evaluated on their ability to complete the side task without appearing suspicious to an LLM monitor. When measuring agent ability to (a) complete the main task, (b) complete the side task, and (c) avoid detection, we find that the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15% (Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For current frontier models, success on the side task relies heavily on having access to a hidden scratchpad that is not visible to the monitor. We also use SHADE-Arena to measure models' monitoring abilities, with the top monitor (Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign transcripts. We find that for now, models still struggle at sabotage due to failures in long-context main task execution. However, our measurements already demonstrate the difficulty of monitoring for subtle sabotage attempts, which we expect to only increase in the face of more complex and longer-horizon tasks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15740","src/content/posts/2025-06-23-shade-arena-evaluating-sabotage-and-monitoring-in-llm-agents.md","73ce396d60b9b496",{"html":25,"metadata":30655},{"headings":30656,"localImagePaths":30657,"remoteImagePaths":30658,"frontmatter":30659,"imagePaths":30660},[],[],[],{"title":30648,"description":30649,"summary":30649,"pubDate":25202,"source":25191,"url":30651,"thumbnail":25193},[],"2025-06-23-shade-arena-evaluating-sabotage-and-monitoring-in-llm-agents.md","2025-06-23-shadow-defense-against-gradient-inversion-attack-in-federated-learning",{"id":30662,"data":30664,"filePath":30669,"digest":30670,"rendered":30671,"legacyId":30678},{"title":30665,"description":30666,"summary":30666,"pubDate":30667,"source":25191,"url":30668,"thumbnail":25193},"Shadow defense against gradient inversion attack in federated learning","arXiv:2506.15711v1 Announce Type: cross Abstract: Federated learning (FL) has emerged as a transformative framework for privacy-preserving distributed training, allowing clients to collaboratively train a global model without sharing their local data. This is especially crucial in sensitive fields like healthcare, where protecting patient data is paramount. However, privacy leakage remains a critical challenge, as the communication of model updates can be exploited by potential adversaries. Gradient inversion attacks (GIAs), for instance, allow adversaries to approximate the gradients used for training and reconstruct training images, thus stealing patient privacy. Existing defense mechanisms obscure gradients, yet lack a nuanced understanding of which gradients or types of image information are most vulnerable to such attacks. These indiscriminate calibrated perturbations result in either excessive privacy protection degrading model accuracy, or insufficient one failing to safeguard sensitive information. Therefore, we introduce a framework that addresses these challenges by leveraging a shadow model with interpretability for identifying sensitive areas. This enables a more targeted and sample-specific noise injection. Specially, our defensive strategy achieves discrepancies of 3.73 in PSNR and 0.2 in SSIM compared to the circumstance without defense on the ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover, it minimizes adverse effects on model performance, with less than 1% F1 reduction compared to SOTA methods. Our extensive experiments, conducted across diverse types of medical images, validate the generalization of the proposed framework. The stable defense improvements for FedAvg are consistently over 1.5% times in LPIPS and SSIM. It also offers a universal defense against various GIA types, especially for these sensitive areas in images.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15711","src/content/posts/2025-06-23-shadow-defense-against-gradient-inversion-attack-in-federated-learning.md","e178d26887fff822",{"html":25,"metadata":30672},{"headings":30673,"localImagePaths":30674,"remoteImagePaths":30675,"frontmatter":30676,"imagePaths":30677},[],[],[],{"title":30665,"description":30666,"summary":30666,"pubDate":25202,"source":25191,"url":30668,"thumbnail":25193},[],"2025-06-23-shadow-defense-against-gradient-inversion-attack-in-federated-learning.md","2025-06-23-shapelib-designing-a-library-of-programmatic-3d-shape-abstractions-with-large-language-models",{"id":30679,"data":30681,"filePath":30686,"digest":30687,"rendered":30688,"legacyId":30695},{"title":30682,"description":30683,"summary":30683,"pubDate":30684,"source":25191,"url":30685,"thumbnail":25193},"ShapeLib: Designing a library of programmatic 3D shape abstractions with Large Language Models","arXiv:2502.08884v2 Announce Type: replace-cross Abstract: We present ShapeLib, the first method that leverages the priors of LLMs to design libraries of programmatic 3D shape abstractions. Our system accepts two forms of design intent: text descriptions of functions to include in the library and a seed set of exemplar shapes. We discover abstractions that match this design intent with a guided LLM workflow that first proposes, and then validates, different ways of applying and implementing functions. We learn recognition networks that map shapes to programs with these newly discovered abstractions by training on data produced by LLM authored synthetic data generation procedures. Across modeling domains (split by shape category), we find that LLMs, when thoughtfully combined with geometric reasoning, can be guided to author a library of abstraction functions that generalize to shapes outside of the seed set. This framework addresses a long-standing shape analysis problem of how to discover reusable abstraction functions while exposing interpretable, semantically aligned interfaces. We find that ShapeLib provides distinct advantages over prior alternative abstraction discovery works in terms of generalization, usability, and maintaining plausibility under manipulation. Finally, we demonstrate that ShapeLib's abstraction functions unlock a number of downstream applications, combining LLM reasoning over shape programs with geometry processing to support shape editing and generation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2502.08884","src/content/posts/2025-06-23-shapelib-designing-a-library-of-programmatic-3d-shape-abstractions-with-large-language-models.md","5ee3dfaeedcc8192",{"html":25,"metadata":30689},{"headings":30690,"localImagePaths":30691,"remoteImagePaths":30692,"frontmatter":30693,"imagePaths":30694},[],[],[],{"title":30682,"description":30683,"summary":30683,"pubDate":25202,"source":25191,"url":30685,"thumbnail":25193},[],"2025-06-23-shapelib-designing-a-library-of-programmatic-3d-shape-abstractions-with-large-language-models.md","2025-06-23-single-shot-thermometry-of-simulated-bose--einstein-condensates-using-artificial-intelligence",{"id":30696,"data":30698,"filePath":30703,"digest":30704,"rendered":30705,"legacyId":30712},{"title":30699,"description":30700,"summary":30700,"pubDate":30701,"source":25191,"url":30702,"thumbnail":25193},"Single-shot thermometry of simulated Bose--Einstein condensates using artificial intelligence","arXiv:2506.16925v1 Announce Type: cross Abstract: Precise determination of thermodynamic parameters in ultracold Bose gases remains challenging due to the destructive nature of conventional measurement techniques and inherent experimental uncertainties. We demonstrate an artificial intelligence approach for rapid, non-destructive estimation of the chemical potential and temperature from single-shot, in situ imaged density profiles of finite-temperature Bose gases. Our convolutional neural network is trained exclusively on quasi-2D `pancake' condensates in harmonic trap configurations. It achieves parameter extraction within fractions of a second. The model also demonstrates zero-shot generalisation across both trap geometry and thermalisation dynamics, successfully estimating thermodynamic parameters for toroidally trapped condensates with errors of only a few nanokelvin despite no prior exposure to such geometries during training, and maintaining predictive accuracy during dynamic thermalisation processes after a relatively brief evolution without explicit training on non-equilibrium states. These results suggest that supervised learning can overcome traditional limitations in ultracold atom thermometry, with extension to broader geometric configurations, temperature ranges, and additional parameters potentially enabling comprehensive real-time analysis of quantum gas experiments. Such capabilities could significantly streamline experimental workflows whilst improving measurement precision across a range of quantum fluid systems.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16925","src/content/posts/2025-06-23-single-shot-thermometry-of-simulated-bose--einstein-condensates-using-artificial-intelligence.md","27be214ec303b093",{"html":25,"metadata":30706},{"headings":30707,"localImagePaths":30708,"remoteImagePaths":30709,"frontmatter":30710,"imagePaths":30711},[],[],[],{"title":30699,"description":30700,"summary":30700,"pubDate":25202,"source":25191,"url":30702,"thumbnail":25193},[],"2025-06-23-single-shot-thermometry-of-simulated-bose--einstein-condensates-using-artificial-intelligence.md","2025-06-23-slr-an-automated-synthesis-framework-for-scalable-logical-reasoning",{"id":30713,"data":30715,"filePath":30720,"digest":30721,"rendered":30722,"legacyId":30729},{"title":30716,"description":30717,"summary":30717,"pubDate":30718,"source":25191,"url":30719,"thumbnail":25193},"SLR: An Automated Synthesis Framework for Scalable Logical Reasoning","arXiv:2506.15787v1 Announce Type: new Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR enables scalable, automated synthesis of inductive reasoning tasks with precisely controlled difficulty. For each task, SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation program used by a symbolic judge to deterministically verify model outputs, and (iii) an instruction prompt for the reasoning task. Using SLR, we create SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs do somewhat better, but incur substantial increases in test-time compute, sometimes exceeding 15k completion tokens. Finally, logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully automated, requires no human annotation, ensures dataset novelty, and offers a scalable environment for probing and advancing LLMs' reasoning capabilities.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15787","src/content/posts/2025-06-23-slr-an-automated-synthesis-framework-for-scalable-logical-reasoning.md","133796c70524f5d6",{"html":25,"metadata":30723},{"headings":30724,"localImagePaths":30725,"remoteImagePaths":30726,"frontmatter":30727,"imagePaths":30728},[],[],[],{"title":30716,"description":30717,"summary":30717,"pubDate":25202,"source":25191,"url":30719,"thumbnail":25193},[],"2025-06-23-slr-an-automated-synthesis-framework-for-scalable-logical-reasoning.md","2025-06-23-song-form-aware-full-song-text-to-lyrics-generation-with-multi-level-granularity-syllable-count-control",{"id":30730,"data":30732,"filePath":30737,"digest":30738,"rendered":30739,"legacyId":30746},{"title":30733,"description":30734,"summary":30734,"pubDate":30735,"source":25191,"url":30736,"thumbnail":25193},"Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control","arXiv:2411.13100v2 Announce Type: replace-cross Abstract: Lyrics generation presents unique challenges, particularly in achieving precise syllable control while adhering to song form structures such as verses and choruses. Conventional line-by-line approaches often lead to unnatural phrasing, underscoring the need for more granular syllable management. We propose a framework for lyrics generation that enables multi-level syllable control at the word, phrase, line, and paragraph levels, aware of song form. Our approach generates complete lyrics conditioned on input text and song form, ensuring alignment with specified syllable constraints. Generated lyrics samples are available at: https://tinyurl.com/lyrics9999",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2411.13100","src/content/posts/2025-06-23-song-form-aware-full-song-text-to-lyrics-generation-with-multi-level-granularity-syllable-count-control.md","04dd1ff48430e735",{"html":25,"metadata":30740},{"headings":30741,"localImagePaths":30742,"remoteImagePaths":30743,"frontmatter":30744,"imagePaths":30745},[],[],[],{"title":30733,"description":30734,"summary":30734,"pubDate":25202,"source":25191,"url":30736,"thumbnail":25193},[],"2025-06-23-song-form-aware-full-song-text-to-lyrics-generation-with-multi-level-granularity-syllable-count-control.md","2025-06-23-sp-vla-a-joint-model-scheduling-and-token-pruning-approach-for-vla-model-acceleration",{"id":30747,"data":30749,"filePath":30754,"digest":30755,"rendered":30756,"legacyId":30763},{"title":30750,"description":30751,"summary":30751,"pubDate":30752,"source":25191,"url":30753,"thumbnail":25193},"SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration","arXiv:2506.12723v2 Announce Type: replace-cross Abstract: Vision-Language-Action (VLA) models have attracted increasing attention for their strong control capabilities. However, their high computational cost and low execution frequency hinder their suitability for real-time tasks such as robotic manipulation and autonomous navigation. Existing VLA acceleration methods primarily focus on structural optimization, overlooking the fact that these models operate in sequential decision-making environments. As a result, temporal redundancy in sequential action generation and spatial redundancy in visual input remain unaddressed. To this end, we propose SP-VLA, a unified framework that accelerates VLA models by jointly scheduling models and pruning tokens. Specifically, we design an action-aware model scheduling mechanism that reduces temporal redundancy by dynamically switching between VLA model and a lightweight generator. Inspired by the human motion pattern of focusing on key decision points while relying on intuition for other actions, we categorize VLA actions into deliberative and intuitive, assigning the former to the VLA model and the latter to the lightweight generator, enabling frequency-adaptive execution through collaborative model scheduling. To address spatial redundancy, we further develop a spatio-semantic dual-aware token pruning method. Tokens are classified into spatial and semantic types and pruned based on their dual-aware importance to accelerate VLA inference. These two mechanisms work jointly to guide the VLA in focusing on critical actions and salient visual information, achieving effective acceleration while maintaining high accuracy. Experimental results demonstrate that our method achieves up to 1.5$times$ acceleration with less than 3% drop in accuracy, outperforming existing approaches in multiple tasks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.12723","src/content/posts/2025-06-23-sp-vla-a-joint-model-scheduling-and-token-pruning-approach-for-vla-model-acceleration.md","4877264a434fa0de",{"html":25,"metadata":30757},{"headings":30758,"localImagePaths":30759,"remoteImagePaths":30760,"frontmatter":30761,"imagePaths":30762},[],[],[],{"title":30750,"description":30751,"summary":30751,"pubDate":25202,"source":25191,"url":30753,"thumbnail":25193},[],"2025-06-23-sp-vla-a-joint-model-scheduling-and-token-pruning-approach-for-vla-model-acceleration.md","2025-06-23-sparse-reg-improving-sample-complexity-in-offline-reinforcement-learning-using-sparsity",{"id":30764,"data":30766,"filePath":30771,"digest":30772,"rendered":30773,"legacyId":30780},{"title":30767,"description":30768,"summary":30768,"pubDate":30769,"source":25191,"url":30770,"thumbnail":25193},"Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity","arXiv:2506.17155v1 Announce Type: cross Abstract: In this paper, we investigate the use of small datasets in the context of offline reinforcement learning (RL). While many common offline RL benchmarks employ datasets with over a million data points, many offline RL applications rely on considerably smaller datasets. We show that offline RL algorithms can overfit on small datasets, resulting in poor performance. To address this challenge, we introduce 'Sparse-Reg': a regularization technique based on sparsity to mitigate overfitting in offline reinforcement learning, enabling effective learning in limited data settings and outperforming state-of-the-art baselines in continuous control.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17155","src/content/posts/2025-06-23-sparse-reg-improving-sample-complexity-in-offline-reinforcement-learning-using-sparsity.md","955703ee6bfb20c6",{"html":25,"metadata":30774},{"headings":30775,"localImagePaths":30776,"remoteImagePaths":30777,"frontmatter":30778,"imagePaths":30779},[],[],[],{"title":30767,"description":30768,"summary":30768,"pubDate":25202,"source":25191,"url":30770,"thumbnail":25193},[],"2025-06-23-sparse-reg-improving-sample-complexity-in-offline-reinforcement-learning-using-sparsity.md","2025-06-23-spatially-aware-evaluation-of-segmentation-uncertainty",{"id":30781,"data":30783,"filePath":30788,"digest":30789,"rendered":30790,"legacyId":30797},{"title":30784,"description":30785,"summary":30785,"pubDate":30786,"source":25191,"url":30787,"thumbnail":25193},"Spatially-Aware Evaluation of Segmentation Uncertainty","arXiv:2506.16589v1 Announce Type: cross Abstract: Uncertainty maps highlight unreliable regions in segmentation predictions. However, most uncertainty evaluation metrics treat voxels independently, ignoring spatial context and anatomical structure. As a result, they may assign identical scores to qualitatively distinct patterns (e.g., scattered vs. boundary-aligned uncertainty). We propose three spatially aware metrics that incorporate structural and boundary information and conduct a thorough validation on medical imaging data from the prostate zonal segmentation challenge within the Medical Segmentation Decathlon. Our results demonstrate improved alignment with clinically important factors and better discrimination between meaningful and spurious uncertainty patterns.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16589","src/content/posts/2025-06-23-spatially-aware-evaluation-of-segmentation-uncertainty.md","2c7916565c3efa57",{"html":25,"metadata":30791},{"headings":30792,"localImagePaths":30793,"remoteImagePaths":30794,"frontmatter":30795,"imagePaths":30796},[],[],[],{"title":30784,"description":30785,"summary":30785,"pubDate":25202,"source":25191,"url":30787,"thumbnail":25193},[],"2025-06-23-spatially-aware-evaluation-of-segmentation-uncertainty.md","2025-06-23-spin-ode-stiff-physics-informed-neural-ode-for-chemical-reaction-rate-estimation",{"id":30798,"data":30800,"filePath":30805,"digest":30806,"rendered":30807,"legacyId":30814},{"title":30801,"description":30802,"summary":30802,"pubDate":30803,"source":25191,"url":30804,"thumbnail":25193},"SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate Estimation","arXiv:2505.05625v2 Announce Type: replace-cross Abstract: Estimating rate coefficients from complex chemical reactions is essential for advancing detailed chemistry. However, the stiffness inherent in real-world atmospheric chemistry systems poses severe challenges, leading to training instability and poor convergence that hinder effective rate coefficient estimation using learning-based approaches. To address this, we propose a Stiff Physics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction modelling. Our method introduces a three-stage optimisation process: first, a latent neural ODE learns the continuous and differentiable trajectory between chemical concentrations and their time derivatives; second, an explicit Chemical Reaction Neural Network (CRNN) extracts the underlying rate coefficients based on the learned dynamics; and third, fine-tune CRNN using a neural ODE solver to further improve rate coefficient estimation. Extensive experiments on both synthetic and newly proposed real-world datasets validate the effectiveness and robustness of our approach. As the first work on stiff Neural ODEs for chemical rate coefficient discovery, our study opens promising directions for integrating neural networks with detailed chemistry.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.05625","src/content/posts/2025-06-23-spin-ode-stiff-physics-informed-neural-ode-for-chemical-reaction-rate-estimation.md","d0bdbbcf3a5e1a17",{"html":25,"metadata":30808},{"headings":30809,"localImagePaths":30810,"remoteImagePaths":30811,"frontmatter":30812,"imagePaths":30813},[],[],[],{"title":30801,"description":30802,"summary":30802,"pubDate":25202,"source":25191,"url":30804,"thumbnail":25193},[],"2025-06-23-spin-ode-stiff-physics-informed-neural-ode-for-chemical-reaction-rate-estimation.md","2025-06-23-spotting-tell-tale-visual-artifacts-in-face-swapping-videos-strengths-and-pitfalls-of-cnn-detectors",{"id":30815,"data":30817,"filePath":30822,"digest":30823,"rendered":30824,"legacyId":30831},{"title":30818,"description":30819,"summary":30819,"pubDate":30820,"source":25191,"url":30821,"thumbnail":25193},"Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of CNN detectors","arXiv:2506.16497v1 Announce Type: cross Abstract: Face swapping manipulations in video streams represents an increasing threat in remote video communications, due to advances in automated and real-time tools. Recent literature proposes to characterize and exploit visual artifacts introduced in video frames by swapping algorithms when dealing with challenging physical scenes, such as face occlusions. This paper investigates the effectiveness of this approach by benchmarking CNN-based data-driven models on two data corpora (including a newly collected one) and analyzing generalization capabilities with respect to different acquisition sources and swapping algorithms. The results confirm excellent performance of general-purpose CNN architectures when operating within the same data source, but a significant difficulty in robustly characterizing occlusion-based visual cues across datasets. This highlights the need for specialized detection strategies to deal with such artifacts.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16497","src/content/posts/2025-06-23-spotting-tell-tale-visual-artifacts-in-face-swapping-videos-strengths-and-pitfalls-of-cnn-detectors.md","4358ffef4b40afcb",{"html":25,"metadata":30825},{"headings":30826,"localImagePaths":30827,"remoteImagePaths":30828,"frontmatter":30829,"imagePaths":30830},[],[],[],{"title":30818,"description":30819,"summary":30819,"pubDate":25202,"source":25191,"url":30821,"thumbnail":25193},[],"2025-06-23-spotting-tell-tale-visual-artifacts-in-face-swapping-videos-strengths-and-pitfalls-of-cnn-detectors.md","2025-06-23-ssr-zero-simple-self-rewarding-reinforcement-learning-for-machine-translation",{"id":30832,"data":30834,"filePath":30839,"digest":30840,"rendered":30841,"legacyId":30848},{"title":30835,"description":30836,"summary":30836,"pubDate":30837,"source":25191,"url":30838,"thumbnail":25193},"SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation","arXiv:2505.16637v3 Announce Type: replace-cross Abstract: Large language models (LLMs) have recently demonstrated remarkable capabilities in machine translation (MT). However, most advanced MT-specific LLMs heavily rely on external supervision signals during training, such as human-annotated reference data or trained reward models (RMs), which are often expensive to obtain and challenging to scale. To overcome this limitation, we propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for MT that is reference-free, fully online, and relies solely on self-judging rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs, e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like Qwen2.5-32B-Instruct in English $leftrightarrow$ Chinese translation tasks from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR with external supervision from COMET, our strongest model, SSR-X-Zero-7B, achieves state-of-the-art performance in English $leftrightarrow$ Chinese translation, surpassing all existing open-source models under 72B parameters and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro. Our analysis highlights the effectiveness of the self-rewarding mechanism compared to the external LLM-as-a-judge approach in MT and demonstrates its complementary benefits when combined with trained RMs. Our findings provide valuable insight into the potential of self-improving RL methods. We have publicly released our code, data and models.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.16637","src/content/posts/2025-06-23-ssr-zero-simple-self-rewarding-reinforcement-learning-for-machine-translation.md","e516d777e494b7b5",{"html":25,"metadata":30842},{"headings":30843,"localImagePaths":30844,"remoteImagePaths":30845,"frontmatter":30846,"imagePaths":30847},[],[],[],{"title":30835,"description":30836,"summary":30836,"pubDate":25202,"source":25191,"url":30838,"thumbnail":25193},[],"2025-06-23-ssr-zero-simple-self-rewarding-reinforcement-learning-for-machine-translation.md","2025-06-23-storywriter-a-multi-agent-framework-for-long-story-generation",{"id":30849,"data":30851,"filePath":30856,"digest":30857,"rendered":30858,"legacyId":30865},{"title":30852,"description":30853,"summary":30853,"pubDate":30854,"source":25191,"url":30855,"thumbnail":25193},"StoryWriter: A Multi-Agent Framework for Long Story Generation","arXiv:2506.16445v1 Announce Type: cross Abstract: Long story generation remains a challenge for existing large language models (LLMs), primarily due to two main factors: (1) discourse coherence, which requires plot consistency, logical coherence, and completeness in the long-form generation, and (2) narrative complexity, which requires an interwoven and engaging narrative. To address these challenges, we propose StoryWriter, a multi-agent story generation framework, which consists of three main modules: (1) outline agent, which generates event-based outlines containing rich event plots, character, and event-event relationships. (2) planning agent, which further details events and plans which events should be written in each chapter to maintain an interwoven and engaging story. (3) writing agent, which dynamically compresses the story history based on the current event to generate and reflect new plots, ensuring the coherence of the generated story. We conduct both human and automated evaluation, and StoryWriter significantly outperforms existing story generation baselines in both story quality and length. Furthermore, we use StoryWriter to generate a dataset, which contains about $6,000$ high-quality long stories, with an average length of $8,000$ words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which demonstrates advanced performance in long story generation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16445","src/content/posts/2025-06-23-storywriter-a-multi-agent-framework-for-long-story-generation.md","e069f9def94b0788",{"html":25,"metadata":30859},{"headings":30860,"localImagePaths":30861,"remoteImagePaths":30862,"frontmatter":30863,"imagePaths":30864},[],[],[],{"title":30852,"description":30853,"summary":30853,"pubDate":25202,"source":25191,"url":30855,"thumbnail":25193},[],"2025-06-23-storywriter-a-multi-agent-framework-for-long-story-generation.md","2025-06-23-studying-and-improving-graph-neural-network-based-motif-estimation",{"id":30866,"data":30868,"filePath":30873,"digest":30874,"rendered":30875,"legacyId":30882},{"title":30869,"description":30870,"summary":30870,"pubDate":30871,"source":25191,"url":30872,"thumbnail":25193},"Studying and Improving Graph Neural Network-based Motif Estimation","arXiv:2506.15709v1 Announce Type: cross Abstract: Graph Neural Networks (GNNs) are a predominant method for graph representation learning. However, beyond subgraph frequency estimation, their application to network motif significance-profile (SP) prediction remains under-explored, with no established benchmarks in the literature. We propose to address this problem, framing SP estimation as a task independent of subgraph frequency estimation. Our approach shifts from frequency counting to direct SP estimation and modulates the problem as multitarget regression. The reformulation is optimised for interpretability, stability and scalability on large graphs. We validate our method using a large synthetic dataset and further test it on real-world graphs. Our experiments reveal that 1-WL limited models struggle to make precise estimations of SPs. However, they can generalise to approximate the graph generation processes of networks by comparing their predicted SP with the ones originating from synthetic generators. This first study on GNN-based motif estimation also hints at how using direct SP estimation can help go past the theoretical limitations that motif estimation faces when performed through subgraph counting.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15709","src/content/posts/2025-06-23-studying-and-improving-graph-neural-network-based-motif-estimation.md","36ade39fd8f459e4",{"html":25,"metadata":30876},{"headings":30877,"localImagePaths":30878,"remoteImagePaths":30879,"frontmatter":30880,"imagePaths":30881},[],[],[],{"title":30869,"description":30870,"summary":30870,"pubDate":25202,"source":25191,"url":30872,"thumbnail":25193},[],"2025-06-23-studying-and-improving-graph-neural-network-based-motif-estimation.md","2025-06-23-subspace-boosted-model-merging",{"id":30883,"data":30885,"filePath":30890,"digest":30891,"rendered":30892,"legacyId":30899},{"title":30886,"description":30887,"summary":30887,"pubDate":30888,"source":25191,"url":30889,"thumbnail":25193},"Subspace-Boosted Model Merging","arXiv:2506.16506v1 Announce Type: cross Abstract: Model merging enables the combination of multiple specialized expert models into a single model capable of performing multiple tasks. However, the benefits of merging an increasing amount of specialized experts generally lead to diminishing returns and reduced overall performance gains. In this work, we offer an explanation and analysis from a task arithmetic perspective; revealing that as the merging process (across numerous existing merging methods) continues for more and more experts, the associated task vector space experiences rank collapse. To mitigate this issue, we introduce Subspace Boosting, which operates on the singular value decomposed task vector space and maintains task vector ranks. Subspace Boosting raises merging efficacy for up to 20 expert models by large margins of more than 10% when evaluated on vision benchmarks. Moreover, we propose employing Higher-Order Generalized Singular Value Decomposition to further quantify task similarity, offering a new interpretable perspective on model merging.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16506","src/content/posts/2025-06-23-subspace-boosted-model-merging.md","b43e5b826c37de50",{"html":25,"metadata":30893},{"headings":30894,"localImagePaths":30895,"remoteImagePaths":30896,"frontmatter":30897,"imagePaths":30898},[],[],[],{"title":30886,"description":30887,"summary":30887,"pubDate":25202,"source":25191,"url":30889,"thumbnail":25193},[],"2025-06-23-subspace-boosted-model-merging.md","2025-06-23-swarmthinkers-learning-physically-consistent-atomic-kmc-transitions-at-scale",{"id":30900,"data":30902,"filePath":30907,"digest":30908,"rendered":30909,"legacyId":30916},{"title":30903,"description":30904,"summary":30904,"pubDate":30905,"source":25191,"url":30906,"thumbnail":25193},"SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale","arXiv:2505.20094v2 Announce Type: replace Abstract: Can a scientific simulation system be physically consistent, interpretable by design, and scalable across regimes--all at once? Despite decades of progress, this trifecta remains elusive. Classical methods like Kinetic Monte Carlo ensure thermodynamic accuracy but scale poorly; learning-based methods offer efficiency but often sacrifice physical consistency and interpretability. We present SwarmThinkers, a reinforcement learning framework that recasts atomic-scale simulation as a physically grounded swarm intelligence system. Each diffusing particle is modeled as a local decision-making agent that selects transitions via a shared policy network trained under thermodynamic constraints. A reweighting mechanism fuses learned preferences with transition rates, preserving statistical fidelity while enabling interpretable, step-wise decision making. Training follows a centralized-training, decentralized-execution paradigm, allowing the policy to generalize across system sizes, concentrations, and temperatures without retraining. On a benchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers is the first system to achieve full-scale, physically consistent simulation on a single A100 GPU, previously attainable only via OpenKMC on a supercomputer. It delivers up to 4963x (3185x on average) faster computation with 485x lower memory usage. By treating particles as decision-makers, not passive samplers, SwarmThinkers marks a paradigm shift in scientific simulation--one that unifies physical consistency, interpretability, and scalability through agent-driven intelligence.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2505.20094","src/content/posts/2025-06-23-swarmthinkers-learning-physically-consistent-atomic-kmc-transitions-at-scale.md","5cb568672f0e72d5",{"html":25,"metadata":30910},{"headings":30911,"localImagePaths":30912,"remoteImagePaths":30913,"frontmatter":30914,"imagePaths":30915},[],[],[],{"title":30903,"description":30904,"summary":30904,"pubDate":25202,"source":25191,"url":30906,"thumbnail":25193},[],"2025-06-23-swarmthinkers-learning-physically-consistent-atomic-kmc-transitions-at-scale.md","2025-06-23-swe-factory-your-automated-factory-for-issue-resolution-training-data-and-evaluation-benchmarks",{"id":30917,"data":30919,"filePath":30924,"digest":30925,"rendered":30926,"legacyId":30933},{"title":30920,"description":30921,"summary":30921,"pubDate":30922,"source":25191,"url":30923,"thumbnail":25193},"SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks","arXiv:2506.10954v2 Announce Type: replace-cross Abstract: Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of $0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.10954","src/content/posts/2025-06-23-swe-factory-your-automated-factory-for-issue-resolution-training-data-and-evaluation-benchmarks.md","418e23dd5be76792",{"html":25,"metadata":30927},{"headings":30928,"localImagePaths":30929,"remoteImagePaths":30930,"frontmatter":30931,"imagePaths":30932},[],[],[],{"title":30920,"description":30921,"summary":30921,"pubDate":25202,"source":25191,"url":30923,"thumbnail":25193},[],"2025-06-23-swe-factory-your-automated-factory-for-issue-resolution-training-data-and-evaluation-benchmarks.md","2025-06-23-sycnmapv2-robust-and-adaptive-unsupervised-segmentation",{"id":30934,"data":30936,"filePath":30941,"digest":30942,"rendered":30943,"legacyId":30950},{"title":30937,"description":30938,"summary":30938,"pubDate":30939,"source":25191,"url":30940,"thumbnail":25193},"SycnMapV2: Robust and Adaptive Unsupervised Segmentation","arXiv:2506.16297v1 Announce Type: cross Abstract: Human vision excels at segmenting visual cues without the need for explicit training, and it remains remarkably robust even as noise severity increases. In contrast, existing AI algorithms struggle to maintain accuracy under similar conditions. Here, we present SyncMapV2, the first to solve unsupervised segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop observed in SOTA methods.This superior performance extends across various types of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training, supervision, or loss functions. It is based on a learning paradigm that uses self-organizing dynamical equations combined with concepts from random networks. Moreover,unlike conventional methods that require re-initialization for each new input, SyncMapV2 adapts online, mimicking the continuous adaptability of human vision. Thus, we go beyond the accurate and robust results, and present the first algorithm that can do all the above online, adapting to input rather than re-initializing. In adaptability tests, SyncMapV2 demonstrates near-zero performance degradation, which motivates and fosters a new generation of robust and adaptive intelligence in the near future.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16297","src/content/posts/2025-06-23-sycnmapv2-robust-and-adaptive-unsupervised-segmentation.md","b87751f7f1b04369",{"html":25,"metadata":30944},{"headings":30945,"localImagePaths":30946,"remoteImagePaths":30947,"frontmatter":30948,"imagePaths":30949},[],[],[],{"title":30937,"description":30938,"summary":30938,"pubDate":25202,"source":25191,"url":30940,"thumbnail":25193},[],"2025-06-23-sycnmapv2-robust-and-adaptive-unsupervised-segmentation.md","2025-06-23-synthesizing-composite-hierarchical-structure-from-symbolic-music-corpora",{"id":30951,"data":30953,"filePath":30958,"digest":30959,"rendered":30960,"legacyId":30967},{"title":30954,"description":30955,"summary":30955,"pubDate":30956,"source":25191,"url":30957,"thumbnail":25193},"Synthesizing Composite Hierarchical Structure from Symbolic Music Corpora","arXiv:2502.15849v4 Announce Type: replace Abstract: Western music is an innately hierarchical system of interacting levels of structure, from fine-grained melody to high-level form. In order to analyze music compositions holistically and at multiple granularities, we propose a unified, hierarchical meta-representation of musical structure called the structural temporal graph (STG). For a single piece, the STG is a data structure that defines a hierarchy of progressively finer structural musical features and the temporal relationships between them. We use the STG to enable a novel approach for deriving a representative structural summary of a music corpus, which we formalize as a nested NP-hard combinatorial optimization problem extending the Generalized Median Graph problem. Our approach first applies simulated annealing to develop a measure of structural distance between two music pieces rooted in graph isomorphism. Our approach then combines the formal guarantees of SMT solvers with nested simulated annealing over structural distances to produce a structurally sound, representative centroid STG for an entire corpus of STGs from individual pieces. To evaluate our approach, we conduct experiments verifying that structural distance accurately differentiates between music pieces, and that derived centroids accurately structurally characterize their corpora.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2502.15849","src/content/posts/2025-06-23-synthesizing-composite-hierarchical-structure-from-symbolic-music-corpora.md","65ee4de94a5dcb8d",{"html":25,"metadata":30961},{"headings":30962,"localImagePaths":30963,"remoteImagePaths":30964,"frontmatter":30965,"imagePaths":30966},[],[],[],{"title":30954,"description":30955,"summary":30955,"pubDate":25202,"source":25191,"url":30957,"thumbnail":25193},[],"2025-06-23-synthesizing-composite-hierarchical-structure-from-symbolic-music-corpora.md","2025-06-23-sysformer-safeguarding-frozen-large-language-models-with-adaptive-system-prompts",{"id":30968,"data":30970,"filePath":30975,"digest":30976,"rendered":30977,"legacyId":30984},{"title":30971,"description":30972,"summary":30972,"pubDate":30973,"source":25191,"url":30974,"thumbnail":25193},"Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts","arXiv:2506.15751v1 Announce Type: new Abstract: As large language models (LLMs) are deployed in safety-critical settings, it is essential to ensure that their responses comply with safety standards. Prior research has revealed that LLMs often fail to grasp the notion of safe behaviors, resulting in either unjustified refusals to harmless prompts or the generation of harmful content. While substantial efforts have been made to improve their robustness, existing defenses often rely on costly fine-tuning of model parameters or employ suboptimal heuristic techniques. In this work, we take a novel approach to safeguard LLMs by learning to adapt the system prompts in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a fixed system prompt, we investigate the impact of tailoring the system prompt to each specific user input on the safety of the responses. To this end, we propose $textbf{Sysformer}$, a trans$textbf{former}$ model that updates an initial $textbf{sys}$tem prompt to a more robust system prompt in the LLM input embedding space while attending to the user prompt. While keeping the LLM parameters frozen, the Sysformer is trained to refuse to respond to a set of harmful prompts while responding ideally to a set of safe ones. Through extensive experiments on $5$ LLMs from different families and $2$ recent benchmarks, we demonstrate that Sysformer can significantly enhance the robustness of LLMs, leading to upto $80%$ gain in the refusal rate on harmful prompts while enhancing the compliance with the safe prompts by upto $90%$. Results also generalize well to sophisticated jailbreaking attacks, making LLMs upto $100%$ more robust against different attack strategies. We hope our findings lead to cheaper safeguarding of LLMs and motivate future investigations into designing variable system prompts.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15751","src/content/posts/2025-06-23-sysformer-safeguarding-frozen-large-language-models-with-adaptive-system-prompts.md","1e49cd97274ff065",{"html":25,"metadata":30978},{"headings":30979,"localImagePaths":30980,"remoteImagePaths":30981,"frontmatter":30982,"imagePaths":30983},[],[],[],{"title":30971,"description":30972,"summary":30972,"pubDate":25202,"source":25191,"url":30974,"thumbnail":25193},[],"2025-06-23-sysformer-safeguarding-frozen-large-language-models-with-adaptive-system-prompts.md","2025-06-23-tabarena-a-living-benchmark-for-machine-learning-on-tabular-data",{"id":30985,"data":30987,"filePath":30992,"digest":30993,"rendered":30994,"legacyId":31001},{"title":30988,"description":30989,"summary":30989,"pubDate":30990,"source":25191,"url":30991,"thumbnail":25193},"TabArena: A Living Benchmark for Machine Learning on Tabular Data","arXiv:2506.16791v1 Announce Type: cross Abstract: With the growing popularity of deep learning and foundation models for tabular data, the need for standardized and reliable benchmarks is higher than ever. However, current benchmarks are static. Their design is not updated even if flaws are discovered, model versions are updated, or new models are released. To address this, we introduce TabArena, the first continuously maintained living tabular benchmarking system. To launch TabArena, we manually curate a representative collection of datasets and well-implemented models, conduct a large-scale benchmarking study to initialize a public leaderboard, and assemble a team of experienced maintainers. Our results highlight the influence of validation method and ensembling of hyperparameter configurations to benchmark models at their full potential. While gradient-boosted trees are still strong contenders on practical tabular datasets, we observe that deep learning methods have caught up under larger time budgets with ensembling. At the same time, foundation models excel on smaller datasets. Finally, we show that ensembles across models advance the state-of-the-art in tabular machine learning and investigate the contributions of individual models. We launch TabArena with a public leaderboard, reproducible code, and maintenance protocols to create a living benchmark available at https://tabarena.ai.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16791","src/content/posts/2025-06-23-tabarena-a-living-benchmark-for-machine-learning-on-tabular-data.md","878d1f80a269244f",{"html":25,"metadata":30995},{"headings":30996,"localImagePaths":30997,"remoteImagePaths":30998,"frontmatter":30999,"imagePaths":31000},[],[],[],{"title":30988,"description":30989,"summary":30989,"pubDate":25202,"source":25191,"url":30991,"thumbnail":25193},[],"2025-06-23-tabarena-a-living-benchmark-for-machine-learning-on-tabular-data.md","2025-06-23-synthetic-als-eeg-data-augmentation-for-als-diagnosis-using-conditional-wgan-with-weight-clipping",{"id":31002,"data":31004,"filePath":31009,"digest":31010,"rendered":31011,"legacyId":31018},{"title":31005,"description":31006,"summary":31006,"pubDate":31007,"source":25191,"url":31008,"thumbnail":25193},"Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping","arXiv:2506.16243v1 Announce Type: cross Abstract: Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and high-quality EEG data from ALS patients are scarce. This data scarcity, coupled with severe class imbalance between ALS and healthy control recordings, poses a challenge for training reliable machine learning classifiers. In this work, we address these issues by generating synthetic EEG signals for ALS patients using a Conditional Wasserstein Generative Adversarial Network (CWGAN). We train CWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of ALS EEG signals and produce realistic synthetic samples. We preprocess and normalize EEG recordings, and train a CWGAN model to generate synthetic ALS signals. The CWGAN architecture and training routine are detailed, with key hyperparameters chosen for stable training. Qualitative evaluation of generated signals shows that they closely mimic real ALS EEG patterns. The CWGAN training converged with generator and discriminator loss curves stabilizing, indicating successful learning. The synthetic EEG signals appear realistic and have potential use as augmented data for training classifiers, helping to mitigate class imbalance and improve ALS detection accuracy. We discuss how this approach can facilitate data sharing and enhance diagnostic models.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16243","src/content/posts/2025-06-23-synthetic-als-eeg-data-augmentation-for-als-diagnosis-using-conditional-wgan-with-weight-clipping.md","daac380e958a6292",{"html":25,"metadata":31012},{"headings":31013,"localImagePaths":31014,"remoteImagePaths":31015,"frontmatter":31016,"imagePaths":31017},[],[],[],{"title":31005,"description":31006,"summary":31006,"pubDate":25202,"source":25191,"url":31008,"thumbnail":25193},[],"2025-06-23-synthetic-als-eeg-data-augmentation-for-als-diagnosis-using-conditional-wgan-with-weight-clipping.md","2025-06-23-tale-a-tool-augmented-framework-for-reference-free-evaluation-of-large-language-models",{"id":31019,"data":31021,"filePath":31026,"digest":31027,"rendered":31028,"legacyId":31035},{"title":31022,"description":31023,"summary":31023,"pubDate":31024,"source":25191,"url":31025,"thumbnail":25193},"TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models","arXiv:2504.07385v2 Announce Type: replace-cross Abstract: As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2504.07385","src/content/posts/2025-06-23-tale-a-tool-augmented-framework-for-reference-free-evaluation-of-large-language-models.md","3e56502a6477eb63",{"html":25,"metadata":31029},{"headings":31030,"localImagePaths":31031,"remoteImagePaths":31032,"frontmatter":31033,"imagePaths":31034},[],[],[],{"title":31022,"description":31023,"summary":31023,"pubDate":25202,"source":25191,"url":31025,"thumbnail":25193},[],"2025-06-23-tale-a-tool-augmented-framework-for-reference-free-evaluation-of-large-language-models.md","2025-06-23-tardis-stride-a-spatio-temporal-road-image-dataset-and-world-model-for-autonomy",{"id":31036,"data":31038,"filePath":31043,"digest":31044,"rendered":31045,"legacyId":31052},{"title":31039,"description":31040,"summary":31040,"pubDate":31041,"source":25191,"url":31042,"thumbnail":25193},"TARDIS STRIDE: A Spatio-Temporal Road Image Dataset and World Model for Autonomy","arXiv:2506.11302v3 Announce Type: replace-cross Abstract: World models aim to simulate environments and enable effective agent behavior. However, modeling real-world environments presents unique challenges as they dynamically change across both space and, crucially, time. To capture these composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for Exploration (STRIDE) permuting 360-degree panoramic imagery into rich interconnected observation, state and action nodes. Leveraging this structure, we can simultaneously model the relationship between egocentric views, positional coordinates, and movement commands across both space and time. We benchmark this dataset via TARDIS, a transformer-based generative world model that integrates spatial and temporal dynamics through a unified autoregressive framework trained on STRIDE. We demonstrate robust performance across a range of agentic tasks such as controllable photorealistic image synthesis, instruction following, autonomous self-control, and state-of-the-art georeferencing. These results suggest a promising direction towards sophisticated generalist agents--capable of understanding and manipulating the spatial and temporal aspects of their material environments--with enhanced embodied reasoning capabilities. Training code, datasets, and model checkpoints are made available at https://huggingface.co/datasets/Tera-AI/STRIDE.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.11302","src/content/posts/2025-06-23-tardis-stride-a-spatio-temporal-road-image-dataset-and-world-model-for-autonomy.md","40f36951505949a1",{"html":25,"metadata":31046},{"headings":31047,"localImagePaths":31048,"remoteImagePaths":31049,"frontmatter":31050,"imagePaths":31051},[],[],[],{"title":31039,"description":31040,"summary":31040,"pubDate":25202,"source":25191,"url":31042,"thumbnail":25193},[],"2025-06-23-tardis-stride-a-spatio-temporal-road-image-dataset-and-world-model-for-autonomy.md","2025-06-23-texpert-a-multi-level-benchmark-for-evaluating-latex-code-generation-by-llms",{"id":31053,"data":31055,"filePath":31060,"digest":31061,"rendered":31062,"legacyId":31069},{"title":31056,"description":31057,"summary":31057,"pubDate":31058,"source":25191,"url":31059,"thumbnail":25193},"TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs","arXiv:2506.16990v1 Announce Type: cross Abstract: LaTeX's precision and flexibility in typesetting have made it the gold standard for the preparation of scientific documentation. Large Language Models (LLMs) present a promising opportunity for researchers to produce publication-ready material using LaTeX with natural language instructions, yet current benchmarks completely lack evaluation of this ability. By introducing TeXpert, our benchmark dataset with natural language prompts for generating LaTeX code focused on components of scientific documents across multiple difficulty levels, we conduct an in-depth analysis of LLM performance in this regard and identify frequent error types. Our evaluation across open and closed-source LLMs highlights multiple key findings: LLMs excelling on standard benchmarks perform poorly in LaTeX generation with a significant accuracy drop-off as the complexity of tasks increases; open-source models like DeepSeek v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks; and formatting and package errors are unexpectedly prevalent, suggesting a lack of diverse LaTeX examples in the training datasets of most LLMs. Our dataset, code, and model evaluations are available at https://github.com/knowledge-verse-ai/TeXpert.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16990","src/content/posts/2025-06-23-texpert-a-multi-level-benchmark-for-evaluating-latex-code-generation-by-llms.md","218dbaf20da78e8d",{"html":25,"metadata":31063},{"headings":31064,"localImagePaths":31065,"remoteImagePaths":31066,"frontmatter":31067,"imagePaths":31068},[],[],[],{"title":31056,"description":31057,"summary":31057,"pubDate":25202,"source":25191,"url":31059,"thumbnail":25193},[],"2025-06-23-texpert-a-multi-level-benchmark-for-evaluating-latex-code-generation-by-llms.md","2025-06-23-textttspecs-faster-test-time-scaling-through-speculative-drafts",{"id":31070,"data":31072,"filePath":31077,"digest":31078,"rendered":31079,"legacyId":31086},{"title":31073,"description":31074,"summary":31074,"pubDate":31075,"source":25191,"url":31076,"thumbnail":25193},"$texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts","arXiv:2506.15733v1 Announce Type: new Abstract: Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench datasets show that $texttt{SPECS}$~matches or surpasses beam search accuracy while reducing latency by up to $sim$19.1%. Our theoretical analysis shows that our algorithm converges to the solution of a KL-regularized reinforcement learning objective with increasing beam width.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15733","src/content/posts/2025-06-23-textttspecs-faster-test-time-scaling-through-speculative-drafts.md","8127cb846aae9b47",{"html":25,"metadata":31080},{"headings":31081,"localImagePaths":31082,"remoteImagePaths":31083,"frontmatter":31084,"imagePaths":31085},[],[],[],{"title":31073,"description":31074,"summary":31074,"pubDate":25202,"source":25191,"url":31076,"thumbnail":25193},[],"2025-06-23-textttspecs-faster-test-time-scaling-through-speculative-drafts.md","2025-06-23-the-ai-imperative-scaling-high-quality-peer-review-in-machine-learning",{"id":31087,"data":31089,"filePath":31094,"digest":31095,"rendered":31096,"legacyId":31103},{"title":31090,"description":31091,"summary":31091,"pubDate":31092,"source":25191,"url":31093,"thumbnail":25193},"The AI Imperative: Scaling High-Quality Peer Review in Machine Learning","arXiv:2506.08134v2 Announce Type: replace Abstract: Peer review, the bedrock of scientific advancement in machine learning (ML), is strained by a crisis of scale. Exponential growth in manuscript submissions to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite capacity of qualified reviewers, leading to concerns about review quality, consistency, and reviewer fatigue. This position paper argues that AI-assisted peer review must become an urgent research and infrastructure priority. We advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language Models (LLMs) not as replacements for human judgment, but as sophisticated collaborators for authors, reviewers, and Area Chairs (ACs). We propose specific roles for AI in enhancing factual verification, guiding reviewer performance, assisting authors in quality improvement, and supporting ACs in decision-making. Crucially, we contend that the development of such systems hinges on access to more granular, structured, and ethically-sourced peer review process data. We outline a research agenda, including illustrative experiments, to develop and validate these AI assistants, and discuss significant technical and ethical challenges. We call upon the ML community to proactively build this AI-assisted future, ensuring the continued integrity and scalability of scientific validation, while maintaining high standards of peer review.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.08134","src/content/posts/2025-06-23-the-ai-imperative-scaling-high-quality-peer-review-in-machine-learning.md","ccc3bc45fe8e7d92",{"html":25,"metadata":31097},{"headings":31098,"localImagePaths":31099,"remoteImagePaths":31100,"frontmatter":31101,"imagePaths":31102},[],[],[],{"title":31090,"description":31091,"summary":31091,"pubDate":25202,"source":25191,"url":31093,"thumbnail":25193},[],"2025-06-23-the-ai-imperative-scaling-high-quality-peer-review-in-machine-learning.md","2025-06-23-the-importance-of-being-lazy-scaling-limits-of-continual-learning",{"id":31104,"data":31106,"filePath":31111,"digest":31112,"rendered":31113,"legacyId":31120},{"title":31107,"description":31108,"summary":31108,"pubDate":31109,"source":25191,"url":31110,"thumbnail":25193},"The Importance of Being Lazy: Scaling Limits of Continual Learning","arXiv:2506.16884v1 Announce Type: cross Abstract: Despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far from complete. In this work, we perform a systematic study on the impact of model scale and the degree of feature learning in continual learning. We reconcile existing contradictory observations on scale in the literature, by differentiating between lazy and rich training regimes through a variable parameterization of the architecture. We show that increasing model width is only beneficial when it reduces the amount of feature learning, yielding more laziness. Using the framework of dynamical mean field theory, we then study the infinite width dynamics of the model in the feature learning regime and characterize CF, extending prior theoretical results limited to the lazy regime. We study the intricate relationship between feature learning, task non-stationarity, and forgetting, finding that high feature learning is only beneficial with highly similar tasks. We identify a transition modulated by task similarity where the model exits an effectively lazy regime with low forgetting to enter a rich regime with significant forgetting. Finally, our findings reveal that neural networks achieve optimal performance at a critical level of feature learning, which depends on task non-stationarity and transfers across model scales. This work provides a unified perspective on the role of scale and feature learning in continual learning.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16884","src/content/posts/2025-06-23-the-importance-of-being-lazy-scaling-limits-of-continual-learning.md","f0a848bcb0bd1cc3",{"html":25,"metadata":31114},{"headings":31115,"localImagePaths":31116,"remoteImagePaths":31117,"frontmatter":31118,"imagePaths":31119},[],[],[],{"title":31107,"description":31108,"summary":31108,"pubDate":25202,"source":25191,"url":31110,"thumbnail":25193},[],"2025-06-23-the-importance-of-being-lazy-scaling-limits-of-continual-learning.md","2025-06-23-the-medperturb-dataset-what-non-content-perturbations-reveal-about-human-and-clinical-llm-decision-making",{"id":31121,"data":31123,"filePath":31128,"digest":31129,"rendered":31130,"legacyId":31137},{"title":31124,"description":31125,"summary":31125,"pubDate":31126,"source":25191,"url":31127,"thumbnail":25193},"The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making","arXiv:2506.17163v1 Announce Type: new Abstract: Clinical robustness is critical to the safe deployment of medical Large Language Models (LLMs), but key questions remain about how LLMs and humans may differ in response to the real-world variability typified by clinical settings. To address this, we introduce MedPerturb, a dataset designed to systematically evaluate medical LLMs under controlled perturbations of clinical input. MedPerturb consists of clinical vignettes spanning a range of pathologies, each transformed along three axes: (1) gender modifications (e.g., gender-swapping or gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial tone); and (3) format changes (e.g., LLM-generated multi-turn conversations or summaries). With MedPerturb, we release a dataset of 800 clinical contexts grounded in realistic input variability, outputs from four LLMs, and three human expert reads per clinical context. We use MedPerturb in two case studies to reveal how shifts in gender identity cues, language style, or format reflect diverging treatment selections between humans and LLMs. We find that LLMs are more sensitive to gender and style perturbations while human annotators are more sensitive to LLM-generated format perturbations such as clinical summaries. Our results highlight the need for evaluation frameworks that go beyond static benchmarks to assess the similarity between human clinician and LLM decisions under the variability characteristic of clinical settings.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17163","src/content/posts/2025-06-23-the-medperturb-dataset-what-non-content-perturbations-reveal-about-human-and-clinical-llm-decision-making.md","c14b293979759d22",{"html":25,"metadata":31131},{"headings":31132,"localImagePaths":31133,"remoteImagePaths":31134,"frontmatter":31135,"imagePaths":31136},[],[],[],{"title":31124,"description":31125,"summary":31125,"pubDate":25202,"source":25191,"url":31127,"thumbnail":25193},[],"2025-06-23-the-medperturb-dataset-what-non-content-perturbations-reveal-about-human-and-clinical-llm-decision-making.md","2025-06-23-the-memory-paradox-why-our-brains-need-knowledge-in-an-age-of-ai",{"id":31138,"data":31140,"filePath":31145,"digest":31146,"rendered":31147,"legacyId":31154},{"title":31141,"description":31142,"summary":31142,"pubDate":31143,"source":25191,"url":31144,"thumbnail":25193},"The Memory Paradox: Why Our Brains Need Knowledge in an Age of AI","arXiv:2506.11015v2 Announce Type: replace-cross Abstract: In the age of generative AI and ubiquitous digital tools, human cognition faces a structural paradox: as external aids become more capable, internal memory systems risk atrophy. Drawing on neuroscience and cognitive psychology, this paper examines how heavy reliance on AI systems and discovery-based pedagogies may impair the consolidation of declarative and procedural memory -- systems essential for expertise, critical thinking, and long-term retention. We review how tools like ChatGPT and calculators can short-circuit the retrieval, error correction, and schema-building processes necessary for robust neural encoding. Notably, we highlight striking parallels between deep learning phenomena such as 'grokking' and the neuroscience of overlearning and intuition. Empirical studies are discussed showing how premature reliance on AI during learning inhibits proceduralization and intuitive mastery. We argue that effective human-AI interaction depends on strong internal models -- biological 'schemata' and neural manifolds -- that enable users to evaluate, refine, and guide AI output. The paper concludes with policy implications for education and workforce training in the age of large language models.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.11015","src/content/posts/2025-06-23-the-memory-paradox-why-our-brains-need-knowledge-in-an-age-of-ai.md","f4bc8f3d557a020a",{"html":25,"metadata":31148},{"headings":31149,"localImagePaths":31150,"remoteImagePaths":31151,"frontmatter":31152,"imagePaths":31153},[],[],[],{"title":31141,"description":31142,"summary":31142,"pubDate":25202,"source":25191,"url":31144,"thumbnail":25193},[],"2025-06-23-the-memory-paradox-why-our-brains-need-knowledge-in-an-age-of-ai.md","2025-06-23-the-role-of-explanation-styles-and-perceived-accuracy-on-decision-making-in-predictive-process-monitoring",{"id":31155,"data":31157,"filePath":31162,"digest":31163,"rendered":31164,"legacyId":31171},{"title":31158,"description":31159,"summary":31159,"pubDate":31160,"source":25191,"url":31161,"thumbnail":25193},"The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring","arXiv:2506.16617v1 Announce Type: new Abstract: Predictive Process Monitoring (PPM) often uses deep learning models to predict the future behavior of ongoing processes, such as predicting process outcomes. While these models achieve high accuracy, their lack of interpretability undermines user trust and adoption. Explainable AI (XAI) aims to address this challenge by providing the reasoning behind the predictions. However, current evaluations of XAI in PPM focus primarily on functional metrics (such as fidelity), overlooking user-centered aspects such as their effect on task performance and decision-making. This study investigates the effects of explanation styles (feature importance, rule-based, and counterfactual) and perceived AI accuracy (low or high) on decision-making in PPM. We conducted a decision-making experiment, where users were presented with the AI predictions, perceived accuracy levels, and explanations of different styles. Users' decisions were measured both before and after receiving explanations, allowing the assessment of objective metrics (Task Performance and Agreement) and subjective metrics (Decision Confidence). Our findings show that perceived accuracy and explanation style have a significant effect.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16617","src/content/posts/2025-06-23-the-role-of-explanation-styles-and-perceived-accuracy-on-decision-making-in-predictive-process-monitoring.md","93023e0a27c55806",{"html":25,"metadata":31165},{"headings":31166,"localImagePaths":31167,"remoteImagePaths":31168,"frontmatter":31169,"imagePaths":31170},[],[],[],{"title":31158,"description":31159,"summary":31159,"pubDate":25202,"source":25191,"url":31161,"thumbnail":25193},[],"2025-06-23-the-role-of-explanation-styles-and-perceived-accuracy-on-decision-making-in-predictive-process-monitoring.md","2025-06-23-the-role-of-model-confidence-on-bias-effects-in-measured-uncertainties",{"id":31172,"data":31174,"filePath":31179,"digest":31180,"rendered":31181,"legacyId":31188},{"title":31175,"description":31176,"summary":31176,"pubDate":31177,"source":25191,"url":31178,"thumbnail":25193},"The Role of Model Confidence on Bias Effects in Measured Uncertainties","arXiv:2506.16724v1 Announce Type: cross Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended tasks, accurately assessing epistemic uncertainty, which reflects a model's lack of knowledge, has become crucial to ensuring reliable outcomes. However, quantifying epistemic uncertainty in such tasks is challenging due to the presence of aleatoric uncertainty, which arises from multiple valid answers. While bias can introduce noise into epistemic uncertainty estimation, it may also reduce noise from aleatoric uncertainty. To investigate this trade-off, we conduct experiments on Visual Question Answering (VQA) tasks and find that mitigating prompt-introduced bias improves uncertainty quantification in GPT-4o. Building on prior work showing that LLMs tend to copy input information when model confidence is low, we further analyze how these prompt biases affect measured epistemic and aleatoric uncertainty across varying bias-free confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases induce greater changes in both uncertainties when bias-free model confidence is lower. Moreover, lower bias-free model confidence leads to greater underestimation of epistemic uncertainty (i.e. overconfidence) due to bias, whereas it has no significant effect on the direction of changes in aleatoric uncertainty estimation. These distinct effects deepen our understanding of bias mitigation for uncertainty quantification and potentially inform the development of more advanced techniques.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16724","src/content/posts/2025-06-23-the-role-of-model-confidence-on-bias-effects-in-measured-uncertainties.md","921f494a6db36f2e",{"html":25,"metadata":31182},{"headings":31183,"localImagePaths":31184,"remoteImagePaths":31185,"frontmatter":31186,"imagePaths":31187},[],[],[],{"title":31175,"description":31176,"summary":31176,"pubDate":25202,"source":25191,"url":31178,"thumbnail":25193},[],"2025-06-23-the-role-of-model-confidence-on-bias-effects-in-measured-uncertainties.md","2025-06-23-the-safety-reminder-a-soft-prompt-to-reactivate-delayed-safety-awareness-in-vision-language-models",{"id":31189,"data":31191,"filePath":31196,"digest":31197,"rendered":31198,"legacyId":31205},{"title":31192,"description":31193,"summary":31193,"pubDate":31194,"source":25191,"url":31195,"thumbnail":25193},"The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models","arXiv:2506.15734v1 Announce Type: new Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across real-world applications such as code generation and chatbot assistance, ensuring their safety has become paramount. Unlike traditional Large Language Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature, allowing adversaries to modify visual or textual inputs to bypass safety guardrails and trigger the generation of harmful content. Through systematic analysis of VLM behavior under attack, we identify a novel phenomenon termed ``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs may initially be compromised to produce harmful content, but eventually recognize the associated risks and attempt to self-correct. This pattern suggests that VLMs retain their underlying safety awareness but experience a temporal delay in their activation. Building on this insight, we hypothesize that VLMs' safety awareness can be proactively reactivated through carefully designed prompts. To this end, we introduce ``The Safety Reminder'', a soft prompt tuning approach that optimizes learnable prompt tokens, which are periodically injected during the text generation process to enhance safety awareness, effectively preventing harmful content generation. Additionally, our safety reminder only activates when harmful content is detected, leaving normal conversations unaffected and preserving the model's performance on benign tasks. Through comprehensive evaluation across three established safety benchmarks and one adversarial attacks, we demonstrate that our approach significantly reduces attack success rates while maintaining model utility, offering a practical solution for deploying safer VLMs in real-world applications.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15734","src/content/posts/2025-06-23-the-safety-reminder-a-soft-prompt-to-reactivate-delayed-safety-awareness-in-vision-language-models.md","92876f7cb4636714",{"html":25,"metadata":31199},{"headings":31200,"localImagePaths":31201,"remoteImagePaths":31202,"frontmatter":31203,"imagePaths":31204},[],[],[],{"title":31192,"description":31193,"summary":31193,"pubDate":25202,"source":25191,"url":31195,"thumbnail":25193},[],"2025-06-23-the-safety-reminder-a-soft-prompt-to-reactivate-delayed-safety-awareness-in-vision-language-models.md","2025-06-23-towards-advanced-mathematical-reasoning-for-llms-via-first-order-logic-theorem-proving",{"id":31206,"data":31208,"filePath":31213,"digest":31214,"rendered":31215,"legacyId":31222},{"title":31209,"description":31210,"summary":31210,"pubDate":31211,"source":25191,"url":31212,"thumbnail":25193},"Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving","arXiv:2506.17104v1 Announce Type: new Abstract: Large language models (LLMs) have shown promising first-order logic (FOL) reasoning capabilities with applications in various areas. However, their effectiveness in complex mathematical reasoning involving multi-step FOL deductions is still under-researched. While LLMs perform competitively on established mathematical reasoning benchmarks, they struggle with multi-step FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on our proposed theorem proving dataset. This issue arises from the limited exploration of diverse proof strategies and the potential for early reasoning mistakes to undermine entire proofs. To address these issues, we propose DREAM, a self-adaptive solution that enhances the Diversity and REAsonability of LLMs' generation strategies. DREAM incorporates an Axiom-Driven Strategy Diversification mechanism to promote varied strategic outcomes and a Sub-Proposition Error Feedback to help LLMs reflect on and correct their proofs. Our contributions include pioneering advancements in LLMs' mathematical reasoning through FOL theorem proving, introducing a novel inference stage solution that improves performance by 0.6% to 6.4%, and providing a curated dataset of 447 mathematical theorems in Lean 4 format for evaluation.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17104","src/content/posts/2025-06-23-towards-advanced-mathematical-reasoning-for-llms-via-first-order-logic-theorem-proving.md","d0d32dffb7560ddc",{"html":25,"metadata":31216},{"headings":31217,"localImagePaths":31218,"remoteImagePaths":31219,"frontmatter":31220,"imagePaths":31221},[],[],[],{"title":31209,"description":31210,"summary":31210,"pubDate":25202,"source":25191,"url":31212,"thumbnail":25193},[],"2025-06-23-towards-advanced-mathematical-reasoning-for-llms-via-first-order-logic-theorem-proving.md","2025-06-23-towards-ai-driven-policing-interdisciplinary-knowledge-discovery-from-police-body-worn-camera-footage",{"id":31223,"data":31225,"filePath":31230,"digest":31231,"rendered":31232,"legacyId":31239},{"title":31226,"description":31227,"summary":31227,"pubDate":31228,"source":25191,"url":31229,"thumbnail":25193},"Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage","arXiv:2504.20007v3 Announce Type: replace Abstract: This paper proposes a novel interdisciplinary framework for analyzing police body-worn camera (BWC) footage from the Rochester Police Department (RPD) using advanced artificial intelligence (AI) and statistical machine learning (ML) techniques. Our goal is to detect, classify, and analyze patterns of interaction between police officers and civilians to identify key behavioral dynamics, such as respect, disrespect, escalation, and de-escalation. We apply multimodal data analysis by integrating image, audio, and natural language processing (NLP) techniques to extract meaningful insights from BWC footage. The framework incorporates speaker separation, transcription, and large language models (LLMs) to produce structured, interpretable summaries of police-civilian encounters. We also employ a custom evaluation pipeline to assess transcription quality and behavior detection accuracy in high-stakes, real-world policing scenarios. Our methodology, computational techniques, and findings outline a practical approach for law enforcement review, training, and accountability processes while advancing the frontiers of knowledge discovery from complex police BWC data.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2504.20007","src/content/posts/2025-06-23-towards-ai-driven-policing-interdisciplinary-knowledge-discovery-from-police-body-worn-camera-footage.md","3fa07a2db6aefa93",{"html":25,"metadata":31233},{"headings":31234,"localImagePaths":31235,"remoteImagePaths":31236,"frontmatter":31237,"imagePaths":31238},[],[],[],{"title":31226,"description":31227,"summary":31227,"pubDate":25202,"source":25191,"url":31229,"thumbnail":25193},[],"2025-06-23-towards-ai-driven-policing-interdisciplinary-knowledge-discovery-from-police-body-worn-camera-footage.md","2025-06-23-towards-ai-search-paradigm",{"id":31240,"data":31242,"filePath":31247,"digest":31248,"rendered":31249,"legacyId":31256},{"title":31243,"description":31244,"summary":31244,"pubDate":31245,"source":25191,"url":31246,"thumbnail":25193},"Towards AI Search Paradigm","arXiv:2506.17188v1 Announce Type: cross Abstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17188","src/content/posts/2025-06-23-towards-ai-search-paradigm.md","45225d77e5dcfe90",{"html":25,"metadata":31250},{"headings":31251,"localImagePaths":31252,"remoteImagePaths":31253,"frontmatter":31254,"imagePaths":31255},[],[],[],{"title":31243,"description":31244,"summary":31244,"pubDate":25202,"source":25191,"url":31246,"thumbnail":25193},[],"2025-06-23-towards-ai-search-paradigm.md","2025-06-23-towards-effective-complementary-security-analysis-using-large-language-models",{"id":31257,"data":31259,"filePath":31264,"digest":31265,"rendered":31266,"legacyId":31273},{"title":31260,"description":31261,"summary":31261,"pubDate":31262,"source":25191,"url":31263,"thumbnail":25193},"Towards Effective Complementary Security Analysis using Large Language Models","arXiv:2506.16899v1 Announce Type: cross Abstract: A key challenge in security analysis is the manual evaluation of potential security weaknesses generated by static application security testing (SAST) tools. Numerous false positives (FPs) in these reports reduce the effectiveness of security analysis. We propose using Large Language Models (LLMs) to improve the assessment of SAST findings. We investigate the ability of LLMs to reduce FPs while trying to maintain a perfect true positive rate, using datasets extracted from the OWASP Benchmark (v1.2) and a real-world software project. Our results indicate that advanced prompting techniques, such as Chain-of-Thought and Self-Consistency, substantially improve FP detection. Notably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark dataset without missing genuine weaknesses. Combining detections from different LLMs would increase this FP detection to approximately 78.9%. Additionally, we demonstrate our approach's generalizability using a real-world dataset covering five SAST tools, three programming languages, and infrastructure files. The best LLM detected 33.85% of all FPs without missing genuine weaknesses, while combining detections from different LLMs would increase this detection to 38.46%. Our findings highlight the potential of LLMs to complement traditional SAST tools, enhancing automation and reducing resources spent addressing false alarms.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16899","src/content/posts/2025-06-23-towards-effective-complementary-security-analysis-using-large-language-models.md","48f4ab5116fdf10d",{"html":25,"metadata":31267},{"headings":31268,"localImagePaths":31269,"remoteImagePaths":31270,"frontmatter":31271,"imagePaths":31272},[],[],[],{"title":31260,"description":31261,"summary":31261,"pubDate":25202,"source":25191,"url":31263,"thumbnail":25193},[],"2025-06-23-towards-effective-complementary-security-analysis-using-large-language-models.md","2025-06-23-towards-efficient-few-shot-graph-neural-architecture-search-via-partitioning-gradient-contribution",{"id":31274,"data":31276,"filePath":31281,"digest":31282,"rendered":31283,"legacyId":31290},{"title":31277,"description":31278,"summary":31278,"pubDate":31279,"source":25191,"url":31280,"thumbnail":25193},"Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution","arXiv:2506.01231v2 Announce Type: replace-cross Abstract: To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.01231","src/content/posts/2025-06-23-towards-efficient-few-shot-graph-neural-architecture-search-via-partitioning-gradient-contribution.md","f8a09a6a4dddd5a9",{"html":25,"metadata":31284},{"headings":31285,"localImagePaths":31286,"remoteImagePaths":31287,"frontmatter":31288,"imagePaths":31289},[],[],[],{"title":31277,"description":31278,"summary":31278,"pubDate":25202,"source":25191,"url":31280,"thumbnail":25193},[],"2025-06-23-towards-efficient-few-shot-graph-neural-architecture-search-via-partitioning-gradient-contribution.md","2025-06-23-towards-generalizable-generic-harmful-speech-datasets-for-implicit-hate-speech-detection",{"id":31291,"data":31293,"filePath":31298,"digest":31299,"rendered":31300,"legacyId":31307},{"title":31294,"description":31295,"summary":31295,"pubDate":31296,"source":25191,"url":31297,"thumbnail":25193},"Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection","arXiv:2506.16476v1 Announce Type: cross Abstract: Implicit hate speech has recently emerged as a critical challenge for social media platforms. While much of the research has traditionally focused on harmful speech in general, the need for generalizable techniques to detect veiled and subtle forms of hate has become increasingly pressing. Based on lexicon analysis, we hypothesize that implicit hate speech is already present in publicly available harmful speech datasets but may not have been explicitly recognized or labeled by annotators. Additionally, crowdsourced datasets are prone to mislabeling due to the complexity of the task and often influenced by annotators' subjective interpretations. In this paper, we propose an approach to address the detection of implicit hate speech and enhance generalizability across diverse datasets by leveraging existing harmful speech datasets. Our method comprises three key components: influential sample identification, reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental results demonstrate the effectiveness of our approach in improving implicit hate detection, achieving a +12.9-point F1 score improvement compared to the baseline.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16476","src/content/posts/2025-06-23-towards-generalizable-generic-harmful-speech-datasets-for-implicit-hate-speech-detection.md","b0bb4d51c78ec73e",{"html":25,"metadata":31301},{"headings":31302,"localImagePaths":31303,"remoteImagePaths":31304,"frontmatter":31305,"imagePaths":31306},[],[],[],{"title":31294,"description":31295,"summary":31295,"pubDate":25202,"source":25191,"url":31297,"thumbnail":25193},[],"2025-06-23-towards-generalizable-generic-harmful-speech-datasets-for-implicit-hate-speech-detection.md","2025-06-23-tower-bridging-generality-and-translation-specialization-in-multilingual-llms",{"id":31308,"data":31310,"filePath":31315,"digest":31316,"rendered":31317,"legacyId":31324},{"title":31311,"description":31312,"summary":31312,"pubDate":31313,"source":25191,"url":31314,"thumbnail":25193},"Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs","arXiv:2506.17080v1 Announce Type: cross Abstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17080","src/content/posts/2025-06-23-tower-bridging-generality-and-translation-specialization-in-multilingual-llms.md","3a1a23be0b5ec7b7",{"html":25,"metadata":31318},{"headings":31319,"localImagePaths":31320,"remoteImagePaths":31321,"frontmatter":31322,"imagePaths":31323},[],[],[],{"title":31311,"description":31312,"summary":31312,"pubDate":25202,"source":25191,"url":31314,"thumbnail":25193},[],"2025-06-23-tower-bridging-generality-and-translation-specialization-in-multilingual-llms.md","2025-06-23-trainverify-equivalence-based-verification-for-distributed-llm-training",{"id":31325,"data":31327,"filePath":31332,"digest":31333,"rendered":31334,"legacyId":31341},{"title":31328,"description":31329,"summary":31329,"pubDate":31330,"source":25191,"url":31331,"thumbnail":25193},"TrainVerify: Equivalence-Based Verification for Distributed LLM Training","arXiv:2506.15961v1 Announce Type: cross Abstract: Training large language models (LLMs) at scale requires parallel execution across thousands of devices, incurring enormous computational costs. Yet, these costly distributed trainings are rarely verified, leaving them prone to silent errors and potentially wasting millions of GPU hours. We introduce TrainVerify, a system for verifiable distributed training of LLMs. Given a deep learning model's logical specification as the ground truth, TrainVerify formally verifies that a distributed parallel execution plan is mathematically equivalent to it. Direct verification is notoriously difficult due to the sheer scale of LLMs which often involves billions of variables and highly intricate computation graphs. Therefore, TrainVerify introduces shape-reduction techniques and a stage-wise parallel verification algorithm that significantly reduces complexity while preserving formal correctness. TrainVerify scales to frontier LLMs, including the successful verification of the Llama3 (405B) and DeepSeek-V3 (671B) training plans.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15961","src/content/posts/2025-06-23-trainverify-equivalence-based-verification-for-distributed-llm-training.md","3fb87593f6375c67",{"html":25,"metadata":31335},{"headings":31336,"localImagePaths":31337,"remoteImagePaths":31338,"frontmatter":31339,"imagePaths":31340},[],[],[],{"title":31328,"description":31329,"summary":31329,"pubDate":25202,"source":25191,"url":31331,"thumbnail":25193},[],"2025-06-23-trainverify-equivalence-based-verification-for-distributed-llm-training.md","2025-06-23-transdreamerv3-implanting-transformer-in-dreamerv3",{"id":31342,"data":31344,"filePath":31349,"digest":31350,"rendered":31351,"legacyId":31358},{"title":31345,"description":31346,"summary":31346,"pubDate":31347,"source":25191,"url":31348,"thumbnail":25193},"TransDreamerV3: Implanting Transformer In DreamerV3","arXiv:2506.17103v1 Announce Type: cross Abstract: This paper introduces TransDreamerV3, a reinforcement learning model that enhances the DreamerV3 architecture by integrating a transformer encoder. The model is designed to improve memory and decision-making capabilities in complex environments. We conducted experiments on Atari-Boxing, Atari-Freeway, Atari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved performance over DreamerV3, particularly in the Atari-Freeway and Crafter tasks. While issues in the Minecraft task and limited training across all tasks were noted, TransDreamerV3 displays advancement in world model-based reinforcement learning, leveraging transformer architectures.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17103","src/content/posts/2025-06-23-transdreamerv3-implanting-transformer-in-dreamerv3.md","dd55eef5dd9a9f5c",{"html":25,"metadata":31352},{"headings":31353,"localImagePaths":31354,"remoteImagePaths":31355,"frontmatter":31356,"imagePaths":31357},[],[],[],{"title":31345,"description":31346,"summary":31346,"pubDate":25202,"source":25191,"url":31348,"thumbnail":25193},[],"2025-06-23-transdreamerv3-implanting-transformer-in-dreamerv3.md","2025-06-23-transformers-backend-integration-in-sglang",{"id":31359,"data":31361,"filePath":31367,"digest":31368,"rendered":31369,"legacyId":31377},{"title":31362,"description":25,"summary":31363,"pubDate":31364,"source":2720,"url":31365,"thumbnail":31366},"Transformers backend integration in SGLang","Transformers backend integration in SGLang Hugging Face transformers library is the standard for wor...",["Date","2025-06-23T00:00:00.000Z"],"https://huggingface.co/blog/transformers-backend-sglang","https://huggingface.co/blog/assets/196_transformers_backend_sglang/thumbnail.jpg","src/content/posts/2025-06-23-transformers-backend-integration-in-sglang.md","820550166fa175c5",{"html":25,"metadata":31370},{"headings":31371,"localImagePaths":31372,"remoteImagePaths":31373,"frontmatter":31374,"imagePaths":31376},[],[],[],{"title":31362,"description":25,"summary":31363,"pubDate":31375,"source":2720,"url":31365,"thumbnail":31366},"Mon, 23 Jun 2025 00:00:00 GMT",[],"2025-06-23-transformers-backend-integration-in-sglang.md","2025-06-23-tricon-sf-a-triple-shuffle-and-contribution-aware-serial-federated-learning-framework-for-heterogeneous-healthcare-data",{"id":31378,"data":31380,"filePath":31385,"digest":31386,"rendered":31387,"legacyId":31394},{"title":31381,"description":31382,"summary":31382,"pubDate":31383,"source":25191,"url":31384,"thumbnail":25193},"TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data","arXiv:2506.16723v1 Announce Type: cross Abstract: Serial pipeline training is an efficient paradigm for handling data heterogeneity in cross-silo federated learning with low communication overhead. However, even without centralized aggregation, direct transfer of models between clients can violate privacy regulations and remain susceptible to gradient leakage and linkage attacks. Additionally, ensuring resilience against semi-honest or malicious clients who may manipulate or misuse received models remains a grand challenge, particularly in privacy-sensitive domains such as healthcare. To address these challenges, we propose TriCon-SF, a novel serial federated learning framework that integrates triple shuffling and contribution awareness. TriCon-SF introduces three levels of randomization by shuffling model layers, data segments, and training sequences to break deterministic learning patterns and disrupt potential attack vectors, thereby enhancing privacy and robustness. In parallel, it leverages Shapley value methods to dynamically evaluate client contributions during training, enabling the detection of dishonest behavior and enhancing system accountability. Extensive experiments on non-IID healthcare datasets demonstrate that TriCon-SF outperforms standard serial and parallel federated learning in both accuracy and communication efficiency. Security analysis further supports its resilience against client-side privacy attacks.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16723","src/content/posts/2025-06-23-tricon-sf-a-triple-shuffle-and-contribution-aware-serial-federated-learning-framework-for-heterogeneous-healthcare-data.md","0f4b2952b29d6513",{"html":25,"metadata":31388},{"headings":31389,"localImagePaths":31390,"remoteImagePaths":31391,"frontmatter":31392,"imagePaths":31393},[],[],[],{"title":31381,"description":31382,"summary":31382,"pubDate":25202,"source":25191,"url":31384,"thumbnail":25193},[],"2025-06-23-tricon-sf-a-triple-shuffle-and-contribution-aware-serial-federated-learning-framework-for-heterogeneous-healthcare-data.md","2025-06-23-trust-transparent-robust-and-ultra-sparse-trees",{"id":31395,"data":31397,"filePath":31402,"digest":31403,"rendered":31404,"legacyId":31411},{"title":31398,"description":31399,"summary":31399,"pubDate":31400,"source":25191,"url":31401,"thumbnail":25193},"TRUST: Transparent, Robust and Ultra-Sparse Trees","arXiv:2506.15791v1 Announce Type: cross Abstract: Piecewise-constant regression trees remain popular for their interpretability, yet often lag behind black-box models like Random Forest in predictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and Ultra-Sparse Trees), a novel regression tree model that combines the accuracy of Random Forests with the interpretability of shallow decision trees and sparse linear models. TRUST further enhances transparency by leveraging Large Language Models to generate tailored, user-friendly explanations. Extensive validation on synthetic and real-world benchmark datasets demonstrates that TRUST consistently outperforms other interpretable models -- including CART, Lasso, and Node Harvest -- in predictive accuracy, while matching the accuracy of Random Forest and offering substantial gains in both accuracy and interpretability over M5', a well-established model that is conceptually related.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15791","src/content/posts/2025-06-23-trust-transparent-robust-and-ultra-sparse-trees.md","4ea580a1b1fd8451",{"html":25,"metadata":31405},{"headings":31406,"localImagePaths":31407,"remoteImagePaths":31408,"frontmatter":31409,"imagePaths":31410},[],[],[],{"title":31398,"description":31399,"summary":31399,"pubDate":25202,"source":25191,"url":31401,"thumbnail":25193},[],"2025-06-23-trust-transparent-robust-and-ultra-sparse-trees.md","2025-06-23-two-heads-are-better-than-one-simulating-large-transformers-with-small-ones",{"id":31412,"data":31414,"filePath":31419,"digest":31420,"rendered":31421,"legacyId":31428},{"title":31415,"description":31416,"summary":31416,"pubDate":31417,"source":25191,"url":31418,"thumbnail":25193},"Two Heads Are Better than One: Simulating Large Transformers with Small Ones","arXiv:2506.12220v2 Announce Type: replace-cross Abstract: The quadratic complexity of self-attention prevents transformers from scaling effectively to long input sequences. On the other hand, modern GPUs and other specialized hardware accelerators are well-optimized for processing small input sequences in transformers during both training and inference. A natural question arises: can we take advantage of the efficiency of small transformers to deal with long input sequences? In this paper, we show that transformers with long input sequences (large transformers) can be efficiently simulated by transformers that can only take short input sequences (small transformers). Specifically, we prove that any transformer with input length $N$ can be efficiently simulated by only $O((N/M)^2)$ transformers with input length $M ll N$, and that this cannot be improved in the worst case. However, we then prove that in various natural scenarios including average-case inputs, sliding window masking and attention sinks, the optimal number $O(N/M)$ of small transformers suffice.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.12220","src/content/posts/2025-06-23-two-heads-are-better-than-one-simulating-large-transformers-with-small-ones.md","0309ab8a0a110b9e",{"html":25,"metadata":31422},{"headings":31423,"localImagePaths":31424,"remoteImagePaths":31425,"frontmatter":31426,"imagePaths":31427},[],[],[],{"title":31415,"description":31416,"summary":31416,"pubDate":25202,"source":25191,"url":31418,"thumbnail":25193},[],"2025-06-23-two-heads-are-better-than-one-simulating-large-transformers-with-small-ones.md","2025-06-23-uncertainty-estimation-by-human-perception-versus-neural-models",{"id":31429,"data":31431,"filePath":31436,"digest":31437,"rendered":31438,"legacyId":31445},{"title":31432,"description":31433,"summary":31433,"pubDate":31434,"source":25191,"url":31435,"thumbnail":25193},"Uncertainty Estimation by Human Perception versus Neural Models","arXiv:2506.15850v1 Announce Type: cross Abstract: Modern neural networks (NNs) often achieve high predictive accuracy but remain poorly calibrated, producing overconfident predictions even when wrong. This miscalibration poses serious challenges in applications where reliable uncertainty estimates are critical. In this work, we investigate how human perceptual uncertainty compares to uncertainty estimated by NNs. Using three vision benchmarks annotated with both human disagreement and crowdsourced confidence, we assess the correlation between model-predicted uncertainty and human-perceived uncertainty. Our results show that current methods only weakly align with human intuition, with correlations varying significantly across tasks and uncertainty metrics. Notably, we find that incorporating human-derived soft labels into the training process can improve calibration without compromising accuracy. These findings reveal a persistent gap between model and human uncertainty and highlight the potential of leveraging human insights to guide the development of more trustworthy AI systems.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15850","src/content/posts/2025-06-23-uncertainty-estimation-by-human-perception-versus-neural-models.md","2cd63a6c0b332862",{"html":25,"metadata":31439},{"headings":31440,"localImagePaths":31441,"remoteImagePaths":31442,"frontmatter":31443,"imagePaths":31444},[],[],[],{"title":31432,"description":31433,"summary":31433,"pubDate":25202,"source":25191,"url":31435,"thumbnail":25193},[],"2025-06-23-uncertainty-estimation-by-human-perception-versus-neural-models.md","2025-06-23-under-the-shadow-of-babel-how-language-shapes-reasoning-in-llms",{"id":31446,"data":31448,"filePath":31453,"digest":31454,"rendered":31455,"legacyId":31462},{"title":31449,"description":31450,"summary":31450,"pubDate":31451,"source":25191,"url":31452,"thumbnail":25193},"Under the Shadow of Babel: How Language Shapes Reasoning in LLMs","arXiv:2506.16151v1 Announce Type: cross Abstract: Language is not only a tool for communication but also a medium for human cognition and reasoning. If, as linguistic relativity suggests, the structure of language shapes cognitive patterns, then large language models (LLMs) trained on human language may also internalize the habitual logical structures embedded in different languages. To examine this hypothesis, we introduce BICAUSE, a structured bilingual dataset for causal reasoning, which includes semantically aligned Chinese and English samples in both forward and reversed causal forms. Our study reveals three key findings: (1) LLMs exhibit typologically aligned attention patterns, focusing more on causes and sentence-initial connectives in Chinese, while showing a more balanced distribution in English. (2) Models internalize language-specific preferences for causal word order and often rigidly apply them to atypical inputs, leading to degraded performance, especially in Chinese. (3) When causal reasoning succeeds, model representations converge toward semantically aligned abstractions across languages, indicating a shared understanding beyond surface form. Overall, these results suggest that LLMs not only mimic surface linguistic forms but also internalize the reasoning biases shaped by language. Rooted in cognitive linguistic theory, this phenomenon is for the first time empirically verified through structural analysis of model internals.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16151","src/content/posts/2025-06-23-under-the-shadow-of-babel-how-language-shapes-reasoning-in-llms.md","73b66de67a9c824c",{"html":25,"metadata":31456},{"headings":31457,"localImagePaths":31458,"remoteImagePaths":31459,"frontmatter":31460,"imagePaths":31461},[],[],[],{"title":31449,"description":31450,"summary":31450,"pubDate":25202,"source":25191,"url":31452,"thumbnail":25193},[],"2025-06-23-under-the-shadow-of-babel-how-language-shapes-reasoning-in-llms.md","2025-06-23-understanding-and-reducing-the-class-dependent-effects-of-data-augmentation-with-a-two-player-game-approach",{"id":31463,"data":31465,"filePath":31470,"digest":31471,"rendered":31472,"legacyId":31479},{"title":31466,"description":31467,"summary":31467,"pubDate":31468,"source":25191,"url":31469,"thumbnail":25193},"Understanding and Reducing the Class-Dependent Effects of Data Augmentation with A Two-Player Game Approach","arXiv:2407.03146v4 Announce Type: replace-cross Abstract: Data augmentation is widely applied and has shown its benefits in different machine learning tasks. However, as recently observed, it may have an unfair effect in multi-class classification. While data augmentation generally improves the overall performance (and therefore is beneficial for many classes), it can actually be detrimental for other classes, which can be problematic in some application domains. In this paper, to counteract this phenomenon, we propose CLAM, a CLAss-dependent Multiplicative-weights method. To derive it, we first formulate the training of a classifier as a non-linear optimization problem that aims at simultaneously maximizing the individual class performances and balancing them. By rewriting this optimization problem as an adversarial two-player game, we propose a novel multiplicative weight algorithm, for which we prove the convergence. Interestingly, our formulation also reveals that the class-dependent effects of data augmentation is not due to data augmentation only, but is in fact a general phenomenon. Our empirical results over six datasets demonstrate that the performance of learned classifiers is indeed more fairly distributed over classes, with only limited impact on the average accuracy.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2407.03146","src/content/posts/2025-06-23-understanding-and-reducing-the-class-dependent-effects-of-data-augmentation-with-a-two-player-game-approach.md","065c5dfcb4ff792d",{"html":25,"metadata":31473},{"headings":31474,"localImagePaths":31475,"remoteImagePaths":31476,"frontmatter":31477,"imagePaths":31478},[],[],[],{"title":31466,"description":31467,"summary":31467,"pubDate":25202,"source":25191,"url":31469,"thumbnail":25193},[],"2025-06-23-understanding-and-reducing-the-class-dependent-effects-of-data-augmentation-with-a-two-player-game-approach.md","2025-06-23-unimate-a-unified-model-for-mechanical-metamaterial-generation-property-prediction-and-condition-confirmation",{"id":31480,"data":31482,"filePath":31487,"digest":31488,"rendered":31489,"legacyId":31496},{"title":31483,"description":31484,"summary":31484,"pubDate":31485,"source":25191,"url":31486,"thumbnail":25193},"UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation","arXiv:2506.15722v1 Announce Type: cross Abstract: Metamaterials are artificial materials that are designed to meet unseen properties in nature, such as ultra-stiffness and negative materials indices. In mechanical metamaterial design, three key modalities are typically involved, i.e., 3D topology, density condition, and mechanical property. Real-world complex application scenarios place the demanding requirements on machine learning models to consider all three modalities together. However, a comprehensive literature review indicates that most existing works only consider two modalities, e.g., predicting mechanical properties given the 3D topology or generating 3D topology given the required properties. Therefore, there is still a significant gap for the state-of-the-art machine learning models capturing the whole. Hence, we propose a unified model named UNIMATE, which consists of a modality alignment module and a synergetic diffusion generation module. Experiments indicate that UNIMATE outperforms the other baseline models in topology generation task, property prediction task, and condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We opensource our proposed UNIMATE model and corresponding results at https://github.com/wzhan24/UniMate.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15722","src/content/posts/2025-06-23-unimate-a-unified-model-for-mechanical-metamaterial-generation-property-prediction-and-condition-confirmation.md","aa1eff8066e8b5c0",{"html":25,"metadata":31490},{"headings":31491,"localImagePaths":31492,"remoteImagePaths":31493,"frontmatter":31494,"imagePaths":31495},[],[],[],{"title":31483,"description":31484,"summary":31484,"pubDate":25202,"source":25191,"url":31486,"thumbnail":25193},[],"2025-06-23-unimate-a-unified-model-for-mechanical-metamaterial-generation-property-prediction-and-condition-confirmation.md","2025-06-23-uniworld-v1-high-resolution-semantic-encoders-for-unified-visual-understanding-and-generation",{"id":31497,"data":31499,"filePath":31504,"digest":31505,"rendered":31506,"legacyId":31513},{"title":31500,"description":31501,"summary":31501,"pubDate":31502,"source":25191,"url":31503,"thumbnail":25193},"UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation","arXiv:2506.03147v4 Announce Type: replace-cross Abstract: Although existing unified models achieve strong performance in vision-language understanding and text-to-image generation, they remain limited in addressing image perception and manipulation -- capabilities increasingly demanded in practical applications. Recently, OpenAI introduced the powerful GPT-4o-Image model, which showcases advanced capabilities in comprehensive image perception and manipulation, sparking widespread interest. Through carefully designed experiments, we observe that GPT-4o-Image likely relies on semantic encoders rather than VAEs for feature extraction, despite VAEs being commonly regarded as crucial for image manipulation tasks. Inspired by this insight, we propose UniWorld-V1, a unified generative framework built upon semantic features extracted from powerful multimodal large language models and contrastive semantic encoders. Using only 2.7M training data, UniWorld-V1 achieves impressive performance across diverse tasks, including image understanding, generation, manipulation, and perception. We fully open-source the UniWorld-V1 framework, including model weights, training and evaluation scripts, and datasets to promote reproducibility and further research.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.03147","src/content/posts/2025-06-23-uniworld-v1-high-resolution-semantic-encoders-for-unified-visual-understanding-and-generation.md","9e59076f01396519",{"html":25,"metadata":31507},{"headings":31508,"localImagePaths":31509,"remoteImagePaths":31510,"frontmatter":31511,"imagePaths":31512},[],[],[],{"title":31500,"description":31501,"summary":31501,"pubDate":25202,"source":25191,"url":31503,"thumbnail":25193},[],"2025-06-23-uniworld-v1-high-resolution-semantic-encoders-for-unified-visual-understanding-and-generation.md","2025-06-23-unsupervised-deep-learning-model-for-fast-energy-layer-pre-selection-of-delivery-efficient-proton-arc-therapy-plan-optimization-of-nasopharyngeal-carcinoma",{"id":31514,"data":31516,"filePath":31521,"digest":31522,"rendered":31523,"legacyId":31530},{"title":31517,"description":31518,"summary":31518,"pubDate":31519,"source":25191,"url":31520,"thumbnail":25193},"Unsupervised deep learning model for fast energy layer pre-selection of delivery-efficient proton arc therapy plan optimization of nasopharyngeal carcinoma","arXiv:2506.15803v1 Announce Type: cross Abstract: Objective. Proton arc therapy (PAT) is an emerging and promising modality in radiotherapy, offering several advantages over conventional intensitymodulated proton therapy (IMPT). However, identifying the optimal energy layer (EL) sequence remains computationally intensive due to the large number of possible energy layer transitions. This study proposes an unsupervised deep learning framework for fast and effective EL pre-selection, aiming to minimize energy layer switch time while preserving high plan quality. Approach. We introduce a novel data representation method, spot-count representation, which encodes the number of proton spots intersecting the target and organs at risk (OARs) in a matrix structured by sorted gantry angles and energy layers. This representation is the input of a UNet-based architecture, SPArcdl, which is trained to optimize a tri-objective function: maximizing target coverage, minimizing OAR exposure, and reducing energy switching time. The model is evaluated on 54 nasopharyngeal cancer cases, and its performance is benchmarked against plans generated by SPArcparticle swarm. Main results. SPArcdl produces EL pre-selection that significantly improves both plan quality and delivery efficiency. Compared to SPArc particle swarm, it enhances the conformity index by 0.16 (p \u003C 0.01), reduces the homogeneity index by 0.71 (p \u003C 0.01), shortens the energy switching time by 38.4% (p \u003C 0.01), and lowers the mean dose to brainstem by 0.21 (p \u003C 0.01). The results unintentionally reveal employing unchanged ELS is more time-wise efficient than descended ELS. SPArcdl's inference time is within 1 second. Significance. SPArcdl is a fast and effective tool for generating high-quality PAT plans by strategically pre-selecting energy layers to reduce delivery time while maintaining excellent dosimetric performance.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15803","src/content/posts/2025-06-23-unsupervised-deep-learning-model-for-fast-energy-layer-pre-selection-of-delivery-efficient-proton-arc-therapy-plan-optimization-of-nasopharyngeal-carcinoma.md","8ec32e46bab3d35b",{"html":25,"metadata":31524},{"headings":31525,"localImagePaths":31526,"remoteImagePaths":31527,"frontmatter":31528,"imagePaths":31529},[],[],[],{"title":31517,"description":31518,"summary":31518,"pubDate":25202,"source":25191,"url":31520,"thumbnail":25193},[],"2025-06-23-unsupervised-deep-learning-model-for-fast-energy-layer-pre-selection-of-delivery-efficient-proton-arc-therapy-plan-optimization-of-nasopharyngeal-carcinoma.md","2025-06-23-using-language-and-road-manuals-to-inform-map-reconstruction-for-autonomous-driving",{"id":31531,"data":31533,"filePath":31538,"digest":31539,"rendered":31540,"legacyId":31547},{"title":31534,"description":31535,"summary":31535,"pubDate":31536,"source":25191,"url":31537,"thumbnail":25193},"Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving","arXiv:2506.10317v2 Announce Type: replace-cross Abstract: Lane-topology prediction is a critical component of safe and reliable autonomous navigation. An accurate understanding of the road environment aids this task. We observe that this information often follows conventions encoded in natural language, through design codes that reflect the road structure and road names that capture the road functionality. We augment this information in a lightweight manner to SMERF, a map-prior-based online lane-topology prediction model, by combining structured road metadata from OSM maps and lane-width priors from Road design manuals with the road centerline encodings. We evaluate our method on two geo-diverse complex intersection scenarios. Our method shows improvement in both lane and traffic element detection and their association. We report results using four topology-aware metrics to comprehensively assess the model performance. These results demonstrate the ability of our approach to generalize and scale to diverse topologies and conditions.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.10317","src/content/posts/2025-06-23-using-language-and-road-manuals-to-inform-map-reconstruction-for-autonomous-driving.md","eb00031435a1a54d",{"html":25,"metadata":31541},{"headings":31542,"localImagePaths":31543,"remoteImagePaths":31544,"frontmatter":31545,"imagePaths":31546},[],[],[],{"title":31534,"description":31535,"summary":31535,"pubDate":25202,"source":25191,"url":31537,"thumbnail":25193},[],"2025-06-23-using-language-and-road-manuals-to-inform-map-reconstruction-for-autonomous-driving.md","2025-06-23-v2x-vlm-end-to-end-v2x-cooperative-autonomous-driving-through-large-vision-language-models",{"id":31548,"data":31550,"filePath":31555,"digest":31556,"rendered":31557,"legacyId":31564},{"title":31551,"description":31552,"summary":31552,"pubDate":31553,"source":25191,"url":31554,"thumbnail":25193},"V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models","arXiv:2408.09251v3 Announce Type: replace-cross Abstract: Vehicle-to-everything (V2X) cooperation has emerged as a promising paradigm to overcome the perception limitations of classical autonomous driving by leveraging information from both ego-vehicle and infrastructure sensors. However, effectively fusing heterogeneous visual and semantic information while ensuring robust trajectory planning remains a significant challenge. This paper introduces V2X-VLM, a novel end-to-end (E2E) cooperative autonomous driving framework based on vision-language models (VLMs). V2X-VLM integrates multiperspective camera views from vehicles and infrastructure with text-based scene descriptions to enable a more comprehensive understanding of driving environments. Specifically, we propose a contrastive learning-based mechanism to reinforce the alignment of heterogeneous visual and textual characteristics, which enhances the semantic understanding of complex driving scenarios, and employ a knowledge distillation strategy to stabilize training. Experiments on a large real-world dataset demonstrate that V2X-VLM achieves state-of-the-art trajectory planning accuracy, significantly reducing L2 error and collision rate compared to existing cooperative autonomous driving baselines. Ablation studies validate the contributions of each component. Moreover, the evaluation of robustness and efficiency highlights the practicality of V2X-VLM for real-world deployment to enhance overall autonomous driving safety and decision-making.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2408.09251","src/content/posts/2025-06-23-v2x-vlm-end-to-end-v2x-cooperative-autonomous-driving-through-large-vision-language-models.md","1c6baf165e4a3a61",{"html":25,"metadata":31558},{"headings":31559,"localImagePaths":31560,"remoteImagePaths":31561,"frontmatter":31562,"imagePaths":31563},[],[],[],{"title":31551,"description":31552,"summary":31552,"pubDate":25202,"source":25191,"url":31554,"thumbnail":25193},[],"2025-06-23-v2x-vlm-end-to-end-v2x-cooperative-autonomous-driving-through-large-vision-language-models.md","2025-06-23-veigar-view-consistent-explicit-inpainting-and-geometry-alignment-for-3d-object-removal",{"id":31565,"data":31567,"filePath":31572,"digest":31573,"rendered":31574,"legacyId":31581},{"title":31568,"description":31569,"summary":31569,"pubDate":31570,"source":25191,"url":31571,"thumbnail":25193},"VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal","arXiv:2506.15821v1 Announce Type: cross Abstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have significantly improved editing tasks, with a primary emphasis on maintaining cross-view consistency throughout the generative process. Contemporary methods typically address this challenge using a dual-strategy framework: performing consistent 2D inpainting across all views guided by embedded priors either explicitly in pixel space or implicitly in latent space; and conducting 3D reconstruction with additional consistency guidance. Previous strategies, in particular, often require an initial 3D reconstruction phase to establish geometric structure, introducing considerable computational overhead. Even with the added cost, the resulting reconstruction quality often remains suboptimal. In this paper, we present VEIGAR, a computationally efficient framework that outperforms existing methods without relying on an initial reconstruction phase. VEIGAR leverages a lightweight foundation model to reliably align priors explicitly in the pixel space. In addition, we introduce a novel supervision strategy based on scale-invariant depth loss, which removes the need for traditional scale-and-shift operations in monocular depth regularization. Through extensive experimentation, VEIGAR establishes a new state-of-the-art benchmark in reconstruction quality and cross-view consistency, while achieving a threefold reduction in training time compared to the fastest existing method, highlighting its superior balance of efficiency and effectiveness.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15821","src/content/posts/2025-06-23-veigar-view-consistent-explicit-inpainting-and-geometry-alignment-for-3d-object-removal.md","c88ffbbac51fdada",{"html":25,"metadata":31575},{"headings":31576,"localImagePaths":31577,"remoteImagePaths":31578,"frontmatter":31579,"imagePaths":31580},[],[],[],{"title":31568,"description":31569,"summary":31569,"pubDate":25202,"source":25191,"url":31571,"thumbnail":25193},[],"2025-06-23-veigar-view-consistent-explicit-inpainting-and-geometry-alignment-for-3d-object-removal.md","2025-06-23-veracity-an-open-source-ai-fact-checking-system",{"id":31582,"data":31584,"filePath":31589,"digest":31590,"rendered":31591,"legacyId":31598},{"title":31585,"description":31586,"summary":31586,"pubDate":31587,"source":25191,"url":31588,"thumbnail":25193},"Veracity: An Open-Source AI Fact-Checking System","arXiv:2506.15794v1 Announce Type: cross Abstract: The proliferation of misinformation poses a significant threat to society, exacerbated by the capabilities of generative AI. This demo paper introduces Veracity, an open-source AI system designed to empower individuals to combat misinformation through transparent and accessible fact-checking. Veracity leverages the synergy between Large Language Models (LLMs) and web retrieval agents to analyze user-submitted claims and provide grounded veracity assessments with intuitive explanations. Key features include multilingual support, numerical scoring of claim veracity, and an interactive interface inspired by familiar messaging applications. This paper will showcase Veracity's ability to not only detect misinformation but also explain its reasoning, fostering media literacy and promoting a more informed society.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15794","src/content/posts/2025-06-23-veracity-an-open-source-ai-fact-checking-system.md","0650cee002d5576e",{"html":25,"metadata":31592},{"headings":31593,"localImagePaths":31594,"remoteImagePaths":31595,"frontmatter":31596,"imagePaths":31597},[],[],[],{"title":31585,"description":31586,"summary":31586,"pubDate":25202,"source":25191,"url":31588,"thumbnail":25193},[],"2025-06-23-veracity-an-open-source-ai-fact-checking-system.md","2025-06-23-vision-guided-chunking-is-all-you-need-enhancing-rag-with-multimodal-document-understanding",{"id":31599,"data":31601,"filePath":31606,"digest":31607,"rendered":31608,"legacyId":31615},{"title":31602,"description":31603,"summary":31603,"pubDate":31604,"source":25191,"url":31605,"thumbnail":25193},"Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding","arXiv:2506.16035v1 Announce Type: cross Abstract: Retrieval-Augmented Generation (RAG) systems have revolutionized information retrieval and question answering, but traditional text-based chunking methods struggle with complex document structures, multi-page tables, embedded figures, and contextual dependencies across page boundaries. We present a novel multimodal document chunking approach that leverages Large Multimodal Models (LMMs) to process PDF documents in batches while maintaining semantic coherence and structural integrity. Our method processes documents in configurable page batches with cross-batch context preservation, enabling accurate handling of tables spanning multiple pages, embedded visual elements, and procedural content. We evaluate our approach on a curated dataset of PDF documents with manually crafted queries, demonstrating improvements in chunk quality and downstream RAG performance. Our vision-guided approach achieves better accuracy compared to traditional vanilla RAG systems, with qualitative analysis showing superior preservation of document structure and semantic coherence.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16035","src/content/posts/2025-06-23-vision-guided-chunking-is-all-you-need-enhancing-rag-with-multimodal-document-understanding.md","b5d4ae1e783953d8",{"html":25,"metadata":31609},{"headings":31610,"localImagePaths":31611,"remoteImagePaths":31612,"frontmatter":31613,"imagePaths":31614},[],[],[],{"title":31602,"description":31603,"summary":31603,"pubDate":25202,"source":25191,"url":31605,"thumbnail":25193},[],"2025-06-23-vision-guided-chunking-is-all-you-need-enhancing-rag-with-multimodal-document-understanding.md","2025-06-23-voices-of-her-analyzing-gender-differences-in-the-ai-publication-world",{"id":31616,"data":31618,"filePath":31623,"digest":31624,"rendered":31625,"legacyId":31632},{"title":31619,"description":31620,"summary":31620,"pubDate":31621,"source":25191,"url":31622,"thumbnail":25193},"Voices of Her: Analyzing Gender Differences in the AI Publication World","arXiv:2305.14597v2 Announce Type: replace-cross Abstract: While several previous studies have analyzed gender bias in research, we are still missing a comprehensive analysis of gender differences in the AI community, covering diverse topics and different development trends. Using the AI Scholar dataset of 78K researchers in the field of AI, we identify several gender differences: (1) Although female researchers tend to have fewer overall citations than males, this citation difference does not hold for all academic-age groups; (2) There exist large gender homophily in co-authorship on AI papers; (3) Female first-authored papers show distinct linguistic styles, such as longer text, more positive emotion words, and more catchy titles than male first-authored papers. Our analysis provides a window into the current demographic trends in our AI community, and encourages more gender equality and diversity in the future. Our code and data are at https://github.com/causalNLP/ai-scholar-gender.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2305.14597","src/content/posts/2025-06-23-voices-of-her-analyzing-gender-differences-in-the-ai-publication-world.md","6d169c9651ff498d",{"html":25,"metadata":31626},{"headings":31627,"localImagePaths":31628,"remoteImagePaths":31629,"frontmatter":31630,"imagePaths":31631},[],[],[],{"title":31619,"description":31620,"summary":31620,"pubDate":25202,"source":25191,"url":31622,"thumbnail":25193},[],"2025-06-23-voices-of-her-analyzing-gender-differences-in-the-ai-publication-world.md","2025-06-23-vrail-vectorized-reward-based-attribution-for-interpretable-learning",{"id":31633,"data":31635,"filePath":31640,"digest":31641,"rendered":31642,"legacyId":31649},{"title":31636,"description":31637,"summary":31637,"pubDate":31638,"source":25191,"url":31639,"thumbnail":25193},"VRAIL: Vectorized Reward-based Attribution for Interpretable Learning","arXiv:2506.16014v1 Announce Type: cross Abstract: We propose VRAIL (Vectorized Reward-based Attribution for Interpretable Learning), a bi-level framework for value-based reinforcement learning (RL) that learns interpretable weight representations from state features. VRAIL consists of two stages: a deep learning (DL) stage that fits an estimated value function using state features, and an RL stage that uses this to shape learning via potential-based reward transformations. The estimator is modeled in either linear or quadratic form, allowing attribution of importance to individual features and their interactions. Empirical results on the Taxi-v3 environment demonstrate that VRAIL improves training stability and convergence compared to standard DQN, without requiring environment modifications. Further analysis shows that VRAIL uncovers semantically meaningful subgoals, such as passenger possession, highlighting its ability to produce human-interpretable behavior. Our findings suggest that VRAIL serves as a general, model-agnostic framework for reward shaping that enhances both learning and interpretability.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16014","src/content/posts/2025-06-23-vrail-vectorized-reward-based-attribution-for-interpretable-learning.md","127d780b7137d5c8",{"html":25,"metadata":31643},{"headings":31644,"localImagePaths":31645,"remoteImagePaths":31646,"frontmatter":31647,"imagePaths":31648},[],[],[],{"title":31636,"description":31637,"summary":31637,"pubDate":25202,"source":25191,"url":31639,"thumbnail":25193},[],"2025-06-23-vrail-vectorized-reward-based-attribution-for-interpretable-learning.md","2025-06-23-watermarking-autoregressive-image-generation",{"id":31650,"data":31652,"filePath":31657,"digest":31658,"rendered":31659,"legacyId":31666},{"title":31653,"description":31654,"summary":31654,"pubDate":31655,"source":25191,"url":31656,"thumbnail":25193},"Watermarking Autoregressive Image Generation","arXiv:2506.16349v1 Announce Type: cross Abstract: Watermarking the outputs of generative models has emerged as a promising approach for tracking their provenance. Despite significant interest in autoregressive image generation models and their potential for misuse, no prior work has attempted to watermark their outputs at the token level. In this work, we present the first such approach by adapting language model watermarking techniques to this setting. We identify a key challenge: the lack of reverse cycle-consistency (RCC), wherein re-tokenizing generated image tokens significantly alters the token sequence, effectively erasing the watermark. To address this and to make our method robust to common image transformations, neural compression, and removal attacks, we introduce (i) a custom tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a complementary watermark synchronization layer. As our experiments demonstrate, our approach enables reliable and robust watermark detection with theoretically grounded p-values.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16349","src/content/posts/2025-06-23-watermarking-autoregressive-image-generation.md","9a9ca0d3d180cb6e",{"html":25,"metadata":31660},{"headings":31661,"localImagePaths":31662,"remoteImagePaths":31663,"frontmatter":31664,"imagePaths":31665},[],[],[],{"title":31653,"description":31654,"summary":31654,"pubDate":25202,"source":25191,"url":31656,"thumbnail":25193},[],"2025-06-23-watermarking-autoregressive-image-generation.md","2025-06-23-web-archives-metadata-generation-with-gpt-4o-challenges-and-insights",{"id":31667,"data":31669,"filePath":31674,"digest":31675,"rendered":31676,"legacyId":31683},{"title":31670,"description":31671,"summary":31671,"pubDate":31672,"source":25191,"url":31673,"thumbnail":25193},"Web Archives Metadata Generation with GPT-4o: Challenges and Insights","arXiv:2411.05409v3 Announce Type: replace-cross Abstract: Current metadata creation for web archives is time consuming and costly due to reliance on human effort. This paper explores the use of gpt-4o for metadata generation within the Web Archive Singapore, focusing on scalability, efficiency, and cost effectiveness. We processed 112 Web ARChive (WARC) files using data reduction techniques, achieving a notable 99.9% reduction in metadata generation costs. By prompt engineering, we generated titles and abstracts, which were evaluated both intrinsically using Levenshtein Distance and BERTScore, and extrinsically with human cataloguers using McNemar's test. Results indicate that while our method offers significant cost savings and efficiency gains, human curated metadata maintains an edge in quality. The study identifies key challenges including content inaccuracies, hallucinations, and translation issues, suggesting that Large Language Models (LLMs) should serve as complements rather than replacements for human cataloguers. Future work will focus on refining prompts, improving content filtering, and addressing privacy concerns through experimentation with smaller models. This research advances the integration of LLMs in web archiving, offering valuable insights into their current capabilities and outlining directions for future enhancements. The code is available at https://github.com/masamune-prog/warc2summary for further development and use by institutions facing similar challenges.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2411.05409","src/content/posts/2025-06-23-web-archives-metadata-generation-with-gpt-4o-challenges-and-insights.md","2123c2e0a7a2a81c",{"html":25,"metadata":31677},{"headings":31678,"localImagePaths":31679,"remoteImagePaths":31680,"frontmatter":31681,"imagePaths":31682},[],[],[],{"title":31670,"description":31671,"summary":31671,"pubDate":25202,"source":25191,"url":31673,"thumbnail":25193},[],"2025-06-23-web-archives-metadata-generation-with-gpt-4o-challenges-and-insights.md","2025-06-23-webxaii-an-open-source-web-framework-to-study-human-xai-interaction",{"id":31684,"data":31686,"filePath":31691,"digest":31692,"rendered":31693,"legacyId":31700},{"title":31687,"description":31688,"summary":31688,"pubDate":31689,"source":25191,"url":31690,"thumbnail":25193},"WebXAII: an open-source web framework to study human-XAI interaction","arXiv:2506.14777v2 Announce Type: replace-cross Abstract: This article introduces WebXAII, an open-source web framework designed to facilitate research on human interaction with eXplainable Artificial Intelligence (XAI) systems. The field of XAI is rapidly expanding, driven by the growing societal implications of the widespread adoption of AI (and in particular machine learning) across diverse applications. Researchers who study the interaction between humans and XAI techniques typically develop ad hoc interfaces in order to conduct their studies. These interfaces are usually not shared alongside the results of the studies, which limits their reusability and the reproducibility of experiments. In response, we design and implement WebXAII, a web-based platform that can embody full experimental protocols, meaning that it can present all aspects of the experiment to human participants and record their responses. The experimental protocols are translated into a composite architecture of generic views and modules, which offers a lot of flexibility. The architecture is defined in a structured configuration file, so that protocols can be implemented with minimal programming skills. We demonstrate that WebXAII can effectively embody relevant protocols, by reproducing the protocol of a state-of-the-art study of the literature.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.14777","src/content/posts/2025-06-23-webxaii-an-open-source-web-framework-to-study-human-xai-interaction.md","981d5263fe5a1989",{"html":25,"metadata":31694},{"headings":31695,"localImagePaths":31696,"remoteImagePaths":31697,"frontmatter":31698,"imagePaths":31699},[],[],[],{"title":31687,"description":31688,"summary":31688,"pubDate":25202,"source":25191,"url":31690,"thumbnail":25193},[],"2025-06-23-webxaii-an-open-source-web-framework-to-study-human-xai-interaction.md","2025-06-23-what-do-latent-action-models-actually-learn",{"id":31701,"data":31703,"filePath":31708,"digest":31709,"rendered":31710,"legacyId":31717},{"title":31704,"description":31705,"summary":31705,"pubDate":31706,"source":25191,"url":31707,"thumbnail":25193},"What Do Latent Action Models Actually Learn?","arXiv:2506.15691v1 Announce Type: cross Abstract: Latent action models (LAMs) aim to learn action-relevant changes from unlabeled videos by compressing changes between frames as latents. However, differences between video frames can be caused by controllable changes as well as exogenous noise, leading to an important concern -- do latents capture the changes caused by actions or irrelevant noise? This paper studies this issue analytically, presenting a linear model that encapsulates the essence of LAM learning, while being tractable.This provides several insights, including connections between LAM and principal component analysis (PCA), desiderata of the data-generating policy, and justification of strategies to encourage learning controllable changes using data augmentation, data cleaning, and auxiliary action-prediction. We also provide illustrative results based on numerical simulation, shedding light on the specific structure of observations, actions, and noise in data that influence LAM learning.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.15691","src/content/posts/2025-06-23-what-do-latent-action-models-actually-learn.md","6c8d6ea4ace2e39c",{"html":25,"metadata":31711},{"headings":31712,"localImagePaths":31713,"remoteImagePaths":31714,"frontmatter":31715,"imagePaths":31716},[],[],[],{"title":31704,"description":31705,"summary":31705,"pubDate":25202,"source":25191,"url":31707,"thumbnail":25193},[],"2025-06-23-what-do-latent-action-models-actually-learn.md","2025-06-23-what-is-the-point-of-equality-in-machine-learning-fairness-beyond-equality-of-opportunity",{"id":31718,"data":31720,"filePath":31725,"digest":31726,"rendered":31727,"legacyId":31734},{"title":31721,"description":31722,"summary":31722,"pubDate":31723,"source":25191,"url":31724,"thumbnail":25193},"What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity","arXiv:2506.16782v1 Announce Type: cross Abstract: Fairness in machine learning (ML) has become a rapidly growing area of research. But why, in the first place, is unfairness in ML morally wrong? And why should we care about improving fairness? Most fair-ML research implicitly appeals to distributive equality: the idea that desirable goods and benefits, such as opportunities (e.g., Barocas et al., 2023), should be equally distributed across society. Unfair ML models, then, are seen as wrong because they unequally distribute such benefits. This paper argues that this exclusive focus on distributive equality offers an incomplete and potentially misleading ethical foundation. Grounding ML fairness in egalitarianism -- the view that equality is a fundamental moral and social ideal -- requires challenging structural inequality: systematic, institutional, and durable arrangements that privilege some groups while disadvantaging others. Structural inequality manifests through ML systems in two primary forms: allocative harms (e.g., economic loss) and representational harms (e.g., stereotypes, erasure). While distributive equality helps address allocative harms, it fails to explain why representational harms are wrong -- why it is wrong for ML systems to reinforce social hierarchies that stratify people into superior and inferior groups -- and why ML systems should aim to foster a society where people relate as equals (i.e., relational equality). To address these limitations, the paper proposes a multifaceted egalitarian framework for ML fairness that integrates both distributive and relational equality. Drawing on critical social and political philosophy, this framework offers a more comprehensive ethical foundation for tackling the full spectrum of harms perpetuated by ML systems. The paper also outlines practical pathways for implementing the framework across the ML pipeline.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16782","src/content/posts/2025-06-23-what-is-the-point-of-equality-in-machine-learning-fairness-beyond-equality-of-opportunity.md","9630756c7fd5d808",{"html":25,"metadata":31728},{"headings":31729,"localImagePaths":31730,"remoteImagePaths":31731,"frontmatter":31732,"imagePaths":31733},[],[],[],{"title":31721,"description":31722,"summary":31722,"pubDate":25202,"source":25191,"url":31724,"thumbnail":25193},[],"2025-06-23-what-is-the-point-of-equality-in-machine-learning-fairness-beyond-equality-of-opportunity.md","2025-06-23-when-can-model-free-reinforcement-learning-be-enough-for-thinking",{"id":31735,"data":31737,"filePath":31742,"digest":31743,"rendered":31744,"legacyId":31751},{"title":31738,"description":31739,"summary":31739,"pubDate":31740,"source":25191,"url":31741,"thumbnail":25193},"When Can Model-Free Reinforcement Learning be Enough for Thinking?","arXiv:2506.17124v1 Announce Type: new Abstract: Recent work on large language models has demonstrated the use of model-free reinforcement learning (RL) to train reasoning-like capabilities. The emergence of 'thinking' through model-free RL is interesting as thinking actions neither produce reward nor change the external world state to one where the agent is more likely to get reward. This paper seeks to build a domain-independent understanding of when model-free RL will lead to 'thinking' as a strategy for reward maximization. To build this understanding, we first introduce a theoretical model which we call a textit{thought Markov decision process} (MDP). Thought MDPs minimally extend the classical MDP model to include an abstract notion of thought state and thought action. Using the thought MDP model, we prove the importance of policy initialization in determining whether or not thinking emerges and show formally that thought actions are equivalent to the agent choosing to perform a step of policy improvement before continuing to act. We then show that open-source LLMs satisfy the conditions that our theory predicts are necessary for model-free RL to produce thinking-like behavior. Finally, we hypothesize sufficient conditions that would enable thinking to be learned outside of language generation and introduce a toy domain where a combination of multi-task pre-training and designated thought actions enable more data-efficient RL compared to non-thinking agents.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.17124","src/content/posts/2025-06-23-when-can-model-free-reinforcement-learning-be-enough-for-thinking.md","6511ef58eb3eaa67",{"html":25,"metadata":31745},{"headings":31746,"localImagePaths":31747,"remoteImagePaths":31748,"frontmatter":31749,"imagePaths":31750},[],[],[],{"title":31738,"description":31739,"summary":31739,"pubDate":25202,"source":25191,"url":31741,"thumbnail":25193},[],"2025-06-23-when-can-model-free-reinforcement-learning-be-enough-for-thinking.md","2025-06-23-with-limited-data-for-multimodal-alignment-let-the-structure-guide-you",{"id":31752,"data":31754,"filePath":31759,"digest":31760,"rendered":31761,"legacyId":31768},{"title":31755,"description":31756,"summary":31756,"pubDate":31757,"source":25191,"url":31758,"thumbnail":25193},"With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You","arXiv:2506.16895v1 Announce Type: cross Abstract: Multimodal models have demonstrated powerful capabilities in complex tasks requiring multimodal alignment including zero-shot classification and cross-modal retrieval. However, existing models typically rely on millions of paired multimodal samples, which are prohibitively expensive or infeasible to obtain in many domains. In this work, we explore the feasibility of building multimodal models with limited amount of paired data by aligning pretrained unimodal foundation models. We show that high-quality alignment is possible with as few as tens of thousands of paired samples$unicode{x2013}$less than $1%$ of the data typically used in the field. To achieve this, we introduce STRUCTURE, an effective regularization technique that preserves the neighborhood geometry of the latent space of unimodal encoders. Additionally, we show that aligning last layers is often suboptimal and demonstrate the benefits of aligning the layers with the highest representational similarity across modalities. These two components can be readily incorporated into existing alignment methods, yielding substantial gains across 24 zero-shot image classification and retrieval benchmarks, with average relative improvement of $51.6%$ in classification and $91.8%$ in retrieval tasks. Our results highlight the effectiveness and broad applicability of our framework for limited-sample multimodal learning and offer a promising path forward for resource-constrained domains.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2506.16895","src/content/posts/2025-06-23-with-limited-data-for-multimodal-alignment-let-the-structure-guide-you.md","05ddd3fe77e5bd2f",{"html":25,"metadata":31762},{"headings":31763,"localImagePaths":31764,"remoteImagePaths":31765,"frontmatter":31766,"imagePaths":31767},[],[],[],{"title":31755,"description":31756,"summary":31756,"pubDate":25202,"source":25191,"url":31758,"thumbnail":25193},[],"2025-06-23-with-limited-data-for-multimodal-alignment-let-the-structure-guide-you.md","2025-06-23-xgen-mm-blip-3-a-family-of-open-large-multimodal-models",{"id":31769,"data":31771,"filePath":31776,"digest":31777,"rendered":31778,"legacyId":31785},{"title":31772,"description":31773,"summary":31773,"pubDate":31774,"source":25191,"url":31775,"thumbnail":25193},"xGen-MM (BLIP-3): A Family of Open Large Multimodal Models","arXiv:2408.08872v3 Announce Type: replace-cross Abstract: This paper introduces BLIP-3, an open framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. We release 4B and 14B models, including both the pre-trained base model and the instruction fine-tuned ones. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our models demonstrate competitive performance among open-source LMMs with similar model sizes. Our resulting LMMs demonstrate competitive performance among open-source LMMs with similar model sizes, with the ability to comprehend interleaved image-text inputs. Our training code, models, and all datasets used in this work, including the three largescale datasets we create and the preprocessed ones, will be open-sourced to better support the research community.",["Date","2025-06-23T04:00:00.000Z"],"https://arxiv.org/abs/2408.08872","src/content/posts/2025-06-23-xgen-mm-blip-3-a-family-of-open-large-multimodal-models.md","4460ec7f58e32868",{"html":25,"metadata":31779},{"headings":31780,"localImagePaths":31781,"remoteImagePaths":31782,"frontmatter":31783,"imagePaths":31784},[],[],[],{"title":31772,"description":31773,"summary":31773,"pubDate":25202,"source":25191,"url":31775,"thumbnail":25193},[],"2025-06-23-xgen-mm-blip-3-a-family-of-open-large-multimodal-models.md","2025-06-23-ゆうちょ銀行youtubeのニセ情報動画に注意喚起-生成ai利用とみられるフェイク相次ぐ",{"id":31786,"data":31788,"filePath":31794,"digest":31795,"rendered":31796,"legacyId":31804},{"title":31789,"description":31790,"summary":31790,"pubDate":31791,"source":24944,"url":31792,"thumbnail":31793},"ゆうちょ銀行、YouTubeの“ニセ情報動画”に注意喚起　生成AI利用とみられるフェイク相次ぐ","ゆうちょ銀行は、YouTube上で偽の情報を発信する動画について、注意を呼びかけた。「7月からゆうちょ銀行の口座が使えなくなる」など、誤った内容の動画を複数確認。7月以降も、同社の全ての商品・サービスは通常通り利用できるとしている。",["Date","2025-06-23T09:59:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/23/news094.html","https://image.itmedia.co.jp/aiplus/articles/2506/23/cover_news094.jpg","src/content/posts/2025-06-23-ゆうちょ銀行youtubeのニセ情報動画に注意喚起-生成ai利用とみられるフェイク相次ぐ.md","6ecfcfb0a495984a",{"html":25,"metadata":31797},{"headings":31798,"localImagePaths":31799,"remoteImagePaths":31800,"frontmatter":31801,"imagePaths":31803},[],[],[],{"title":31789,"description":31790,"summary":31790,"pubDate":31802,"source":24944,"url":31792,"thumbnail":31793},"Mon, 23 Jun 2025 18:59:00 +0900",[],"2025-06-23-ゆうちょ銀行youtubeのニセ情報動画に注意喚起-生成ai利用とみられるフェイク相次ぐ.md","2025-06-23-アイスマイリー第37回-ものづくり-ワールド-東京にブース出展-79水から3日間幕張メッセにて開催",{"id":31805,"data":31807,"filePath":31813,"digest":31814,"rendered":31815,"legacyId":31823},{"title":31808,"description":31809,"summary":31809,"pubDate":31810,"source":24252,"url":31811,"thumbnail":31812},"アイスマイリー、「第37回 ものづくり ワールド [東京]」にブース出展　7/9（水）から3日間、幕張メッセにて開催","\u003Cp>AIsmileyは、2025年7月9日（水）～7月11日（金）に幕張メッセにて開催の「第37回 ものづくり ワールド [東京]」にブースを出展します。 会場では、最新のAIソリューションやニュース等を取り上げるAIポータ [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/monodukuri-world-2025-no37/'>アイスマイリー、「第37回 ものづくり ワールド [東京]」にブース出展　7/9（水）から3日間、幕張メッセにて開催\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-23T04:00:20.000Z"],"https://aismiley.co.jp/ai_news/monodukuri-world-2025-no37/","https://aismiley.co.jp/wp-content/uploads/2025/05/monodukuri-world-tokyo-no37.png","src/content/posts/2025-06-23-アイスマイリー第37回-ものづくり-ワールド-東京にブース出展-79水から3日間幕張メッセにて開催.md","c73de0c8c1b2da0c",{"html":25,"metadata":31816},{"headings":31817,"localImagePaths":31818,"remoteImagePaths":31819,"frontmatter":31820,"imagePaths":31822},[],[],[],{"title":31808,"description":31809,"summary":31809,"pubDate":31821,"source":24252,"url":31811,"thumbnail":31812},"Mon, 23 Jun 2025 04:00:20 +0000",[],"2025-06-23-アイスマイリー第37回-ものづくり-ワールド-東京にブース出展-79水から3日間幕張メッセにて開催.md","2025-06-23-テスラのロボタクシー予定通り走行開始-テキサス州で",{"id":31824,"data":31826,"filePath":31832,"digest":31833,"rendered":31834,"legacyId":31842},{"title":31827,"description":31828,"summary":31828,"pubDate":31829,"source":24944,"url":31830,"thumbnail":31831},"テスラのロボタクシー、予定通り走行開始　テキサス州で","米南部テキサス州オースティンで6月22日、電気自動車大手Teslaによるロボタクシー（自動運転タクシー）走行が予定通り始まった。",["Date","2025-06-23T02:20:00.000Z"],"https://www.itmedia.co.jp/news/articles/2506/23/news054.html","https://image.itmedia.co.jp/news/articles/2506/23/cover_news054.jpg","src/content/posts/2025-06-23-テスラのロボタクシー予定通り走行開始-テキサス州で.md","0819313a7d027fe0",{"html":25,"metadata":31835},{"headings":31836,"localImagePaths":31837,"remoteImagePaths":31838,"frontmatter":31839,"imagePaths":31841},[],[],[],{"title":31827,"description":31828,"summary":31828,"pubDate":31840,"source":24944,"url":31830,"thumbnail":31831},"Mon, 23 Jun 2025 11:20:00 +0900",[],"2025-06-23-テスラのロボタクシー予定通り走行開始-テキサス州で.md","2025-06-23-企業の主体はヒトからaiエージェントにpwcが生成aiの技術動向を分析",{"id":31843,"data":31845,"filePath":31851,"digest":31852,"rendered":31853,"legacyId":31861},{"title":31846,"description":31847,"summary":31847,"pubDate":31848,"source":24944,"url":31849,"thumbnail":31850},"「企業の主体はヒトからAIエージェントに」PwCが生成AIの技術動向を分析","PwCコンサルティングは、「生成AIの将来技術動向」と題したレポートを発表した。現在のLLMが抱える技術的な課題とその克服に向けた進化の方向性などについて分析している。",["Date","2025-06-22T23:00:00.000Z"],"https://atmarkit.itmedia.co.jp/ait/articles/2506/23/news025.html","https://image.itmedia.co.jp/ait/articles/2506/23/cover_news025.jpg","src/content/posts/2025-06-23-企業の主体はヒトからaiエージェントにpwcが生成aiの技術動向を分析.md","4bebb16c13435c21",{"html":25,"metadata":31854},{"headings":31855,"localImagePaths":31856,"remoteImagePaths":31857,"frontmatter":31858,"imagePaths":31860},[],[],[],{"title":31846,"description":31847,"summary":31847,"pubDate":31859,"source":24944,"url":31849,"thumbnail":31850},"Mon, 23 Jun 2025 08:00:00 +0900",[],"2025-06-23-企業の主体はヒトからaiエージェントにpwcが生成aiの技術動向を分析.md","2025-06-24-driving-scalable-growth-with-openai-o3-gpt-41-and-cua",{"id":31862,"data":31864,"filePath":31869,"digest":31870,"rendered":31871,"legacyId":31879},{"title":31865,"description":31866,"summary":31866,"pubDate":31867,"source":19,"url":31868,"thumbnail":21},"Driving scalable growth with OpenAI o3, GPT-4.1, and CUA","Unify, an AI-powered GTM platform, uses OpenAI’s o3, GPT-4.1, and CUA to automate prospecting, research, and outreach. With hyper-personalized messaging and an always-on workflow, Unify helps teams generate pipeline at scale while focusing on high-impact customer interactions.",["Date","2025-06-24T00:00:00.000Z"],"https://openai.com/blog/unify","src/content/posts/2025-06-24-driving-scalable-growth-with-openai-o3-gpt-41-and-cua.md","ffd94160e41af58e",{"html":25,"metadata":31872},{"headings":31873,"localImagePaths":31874,"remoteImagePaths":31875,"frontmatter":31876,"imagePaths":31878},[],[],[],{"title":31865,"description":31866,"summary":31866,"pubDate":31877,"source":19,"url":31868,"thumbnail":21},"Tue, 24 Jun 2025 00:00:00 GMT",[],"2025-06-24-driving-scalable-growth-with-openai-o3-gpt-41-and-cua.md","2025-06-24-fanza同人ai生成作品をトップページなどから除外へ-具体的な仕様は必要に応じて告知と運営元",{"id":31880,"data":31882,"filePath":31888,"digest":31889,"rendered":31890,"legacyId":31898},{"title":31883,"description":31884,"summary":31884,"pubDate":31885,"source":24944,"url":31886,"thumbnail":31887},"FANZA同人、AI生成作品をトップページなどから除外へ　具体的な仕様は「必要に応じて告知」と運営元","成人向け同人作品のECサイト「FANZA同人」のトップページなどからAI生成作品を除外する――FANZA運営元のデジタルコマースは、同人サークル向けにこのような告知をしたと、ITmedia AI＋編集部の取材に対して明かした。",["Date","2025-06-23T22:00:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/24/news053.html","https://image.itmedia.co.jp/aiplus/articles/2506/24/cover_news053.jpg","src/content/posts/2025-06-24-fanza同人ai生成作品をトップページなどから除外へ-具体的な仕様は必要に応じて告知と運営元.md","d2a6cec8cbd6a14e",{"html":25,"metadata":31891},{"headings":31892,"localImagePaths":31893,"remoteImagePaths":31894,"frontmatter":31895,"imagePaths":31897},[],[],[],{"title":31883,"description":31884,"summary":31884,"pubDate":31896,"source":24944,"url":31886,"thumbnail":31887},"Tue, 24 Jun 2025 07:00:00 +0900",[],"2025-06-24-fanza同人ai生成作品をトップページなどから除外へ-具体的な仕様は必要に応じて告知と運営元.md","2025-06-24-gemini-robotics-on-device-brings-ai-to-local-robotic-devices",{"id":31899,"data":31901,"filePath":31907,"digest":31908,"rendered":31909,"legacyId":31917},{"title":31902,"description":31903,"summary":31903,"pubDate":31904,"source":6423,"url":31905,"thumbnail":31906},"Gemini Robotics On-Device brings AI to local robotic devices","We’re introducing an efficient, on-device robotics model with general-purpose dexterity and fast task adaptation.",["Date","2025-06-24T14:00:36.000Z"],"https://deepmind.google/discover/blog/gemini-robotics-on-device-brings-ai-to-local-robotic-devices/","https://lh3.googleusercontent.com/Jt_Vw7PIJEZtXcMIKM1HbWbBCLxv7RUyjyf07eHp-YOfxMCUZA6mPI9kSCaz65UkMoGcZ8CwlD3dNBvy7bnnYchjSkWyN-SugglT3dmg1A9KdoDqdQM=w1200-h630-n-nu","src/content/posts/2025-06-24-gemini-robotics-on-device-brings-ai-to-local-robotic-devices.md","82b8797ec3461179",{"html":25,"metadata":31910},{"headings":31911,"localImagePaths":31912,"remoteImagePaths":31913,"frontmatter":31914,"imagePaths":31916},[],[],[],{"title":31902,"description":31903,"summary":31903,"pubDate":31915,"source":6423,"url":31905,"thumbnail":31906},"Tue, 24 Jun 2025 14:00:36 +0000",[],"2025-06-24-gemini-robotics-on-device-brings-ai-to-local-robotic-devices.md","2025-06-24-googleのaiも信じる万博80歳以上無料の偽情報-誤回答の新機能さらなる改善",{"id":31918,"data":31920,"filePath":31926,"digest":31927,"rendered":31928,"legacyId":31936},{"title":31921,"description":31922,"summary":31922,"pubDate":31923,"source":24944,"url":31924,"thumbnail":31925},"GoogleのAIも信じる「万博80歳以上無料」の偽情報　誤回答の新機能「さらなる改善」","大阪・関西万博の入場料を巡り、80歳以上は「無料」とする誤った情報がインターネット上に広がっている。米IT大手Googleの検索サイトでも「万博」「高齢者」と打ち込むと、AIの回答で80歳以上は無料と示される状況だ。日本国際博覧会協会（万博協会）の相談窓口にも「80歳以上は無料なのか」との問い合わせが複数あり、Googleは「さらなる改善を続ける」としている。",["Date","2025-06-24T04:00:00.000Z"],"https://www.itmedia.co.jp/news/articles/2506/24/news082.html","https://image.itmedia.co.jp/news/articles/2506/24/cover_news082.jpg","src/content/posts/2025-06-24-googleのaiも信じる万博80歳以上無料の偽情報-誤回答の新機能さらなる改善.md","d59064e89065f6b3",{"html":25,"metadata":31929},{"headings":31930,"localImagePaths":31931,"remoteImagePaths":31932,"frontmatter":31933,"imagePaths":31935},[],[],[],{"title":31921,"description":31922,"summary":31922,"pubDate":31934,"source":24944,"url":31924,"thumbnail":31925},"Tue, 24 Jun 2025 13:00:00 +0900",[],"2025-06-24-googleのaiも信じる万博80歳以上無料の偽情報-誤回答の新機能さらなる改善.md","2025-06-24-pythonクイズ10-20-30は期待通りにtrueになるはず-その理由は分かる",{"id":31937,"data":31939,"filePath":31945,"digest":31946,"rendered":31947,"legacyId":31955},{"title":31940,"description":31941,"summary":31941,"pubDate":31942,"source":24944,"url":31943,"thumbnail":31944},"［Pythonクイズ］「1.0 + 2.0 == 3.0」は期待通りにTrueになるはず？　その理由は分かる？","普段何気なく使っている浮動小数点数値ですが、ときには思わぬ結果を生むことがあります。その代表例が今回の問題です。どっちのメッセージが表示されるか分かってますよね？",["Date","2025-06-23T20:00:00.000Z"],"https://atmarkit.itmedia.co.jp/ait/articles/2506/24/news014.html","https://image.itmedia.co.jp/ait/articles/2506/24/cover_news014.jpg","src/content/posts/2025-06-24-pythonクイズ10-20-30は期待通りにtrueになるはず-その理由は分かる.md","430219055d2d9b8b",{"html":25,"metadata":31948},{"headings":31949,"localImagePaths":31950,"remoteImagePaths":31951,"frontmatter":31952,"imagePaths":31954},[],[],[],{"title":31940,"description":31941,"summary":31941,"pubDate":31953,"source":24944,"url":31943,"thumbnail":31944},"Tue, 24 Jun 2025 05:00:00 +0900",[],"2025-06-24-pythonクイズ10-20-30は期待通りにtrueになるはず-その理由は分かる.md","2025-06-24-デジタル先進地神戸市が実践-4つのステージで着実に進めるdx推進術",{"id":31956,"data":31958,"filePath":31964,"digest":31965,"rendered":31966,"legacyId":31973},{"title":31959,"description":31960,"summary":31960,"pubDate":31961,"source":24944,"url":31962,"thumbnail":31963},"デジタル先進地・神戸市が実践　「4つのステージ」で着実に進めるDX推進術","新たな技術やツールを導入する際、いきなり全庁展開しても決してうまくいかない――。DX先進地として知られる神戸市は、このポイントを押さえ、段階を踏んだDXを重視している。市のDX推進の司令塔を担うデジタル戦略部は、DX進捗を「4つのステージ」に分け、それぞれのステージにおいて「やるべきこと」と「やってはいけないこと」を明確化。職員自体の行動変容を伴う改革へとつなげている。",["Date","2025-06-23T22:00:00.000Z"],"https://www.itmedia.co.jp/business/articles/2506/24/news018.html","https://image.itmedia.co.jp/business/articles/2506/24/cover_news018.jpg","src/content/posts/2025-06-24-デジタル先進地神戸市が実践-4つのステージで着実に進めるdx推進術.md","61a6f7b15cc5c3b4",{"html":25,"metadata":31967},{"headings":31968,"localImagePaths":31969,"remoteImagePaths":31970,"frontmatter":31971,"imagePaths":31972},[],[],[],{"title":31959,"description":31960,"summary":31960,"pubDate":31896,"source":24944,"url":31962,"thumbnail":31963},[],"2025-06-24-デジタル先進地神戸市が実践-4つのステージで着実に進めるdx推進術.md","2025-06-24-リコーgeniacでマルチモーダルllmの基本モデルを開発7月に無償公開",{"id":31974,"data":31976,"filePath":31982,"digest":31983,"rendered":31984,"legacyId":31992},{"title":31977,"description":31978,"summary":31978,"pubDate":31979,"source":24252,"url":31980,"thumbnail":31981},"リコー、GENIACでマルチモーダルLLMの基本モデルを開発。7月に無償公開","\u003Cp>リコーは、経済産業省およびNEDOが推進するGENIACプロジェクトにおいて、日本企業向け図表を含むドキュメント読み取りに特化したマルチモーダルLLMの基本モデルを開発しました。7月29日開催のMIRU2025で論文発表 [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/ricoh-genia-multimodal-llm/'>リコー、GENIACでマルチモーダルLLMの基本モデルを開発。7月に無償公開\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-24T07:52:10.000Z"],"https://aismiley.co.jp/ai_news/ricoh-genia-multimodal-llm/","https://aismiley.co.jp/wp-content/uploads/2025/06/ricoh-genia-multimodal-llm.png","src/content/posts/2025-06-24-リコーgeniacでマルチモーダルllmの基本モデルを開発7月に無償公開.md","7ba60459c3a679e7",{"html":25,"metadata":31985},{"headings":31986,"localImagePaths":31987,"remoteImagePaths":31988,"frontmatter":31989,"imagePaths":31991},[],[],[],{"title":31977,"description":31978,"summary":31978,"pubDate":31990,"source":24252,"url":31980,"thumbnail":31981},"Tue, 24 Jun 2025 07:52:10 +0000",[],"2025-06-24-リコーgeniacでマルチモーダルllmの基本モデルを開発7月に無償公開.md","2025-06-24-中古車の傷やへこみ再塗装跡などを30秒で可視化する外装スキャナー",{"id":31993,"data":31995,"filePath":32001,"digest":32002,"rendered":32003,"legacyId":32011},{"title":31996,"description":31997,"summary":31997,"pubDate":31998,"source":24944,"url":31999,"thumbnail":32000},"中古車の傷やへこみ、再塗装跡などを30秒で可視化する外装スキャナー","双日は、Preferred Networksと共同で開発を進めている、中古車の瑕疵（かし）や修復歴を判別可能なドライブスルー型外装スキャナーを発表。併せて、車両の事故歴を可視化するボッシュの評価サービス「Bosch Car History Report」の取り扱いも開始した。",["Date","2025-06-24T00:00:00.000Z"],"https://monoist.itmedia.co.jp/mn/articles/2506/24/news035.html","https://image.itmedia.co.jp/mn/articles/2506/24/cover_news035.jpg","src/content/posts/2025-06-24-中古車の傷やへこみ再塗装跡などを30秒で可視化する外装スキャナー.md","54cf066e692aa4ec",{"html":25,"metadata":32004},{"headings":32005,"localImagePaths":32006,"remoteImagePaths":32007,"frontmatter":32008,"imagePaths":32010},[],[],[],{"title":31996,"description":31997,"summary":31997,"pubDate":32009,"source":24944,"url":31999,"thumbnail":32000},"Tue, 24 Jun 2025 09:00:00 +0900",[],"2025-06-24-中古車の傷やへこみ再塗装跡などを30秒で可視化する外装スキャナー.md","2025-06-24-中小企業のホンネ-aiは期待ほどの成果を上げていない",{"id":32012,"data":32014,"filePath":32020,"digest":32021,"rendered":32022,"legacyId":32030},{"title":32015,"description":32016,"summary":32016,"pubDate":32017,"source":24944,"url":32018,"thumbnail":32019},"中小企業のホンネ　「AIは期待ほどの成果を上げていない」","American Expressの調査によると、AIの活用により一定のメリットはあったものの期待されていたほどの成果は出ていないという。",["Date","2025-06-24T01:00:00.000Z"],"https://kn.itmedia.co.jp/kn/articles/2506/24/news026.html","https://image.itmedia.co.jp/kn/articles/2506/24/cover_news026.jpg","src/content/posts/2025-06-24-中小企業のホンネ-aiは期待ほどの成果を上げていない.md","420596f79c51dec5",{"html":25,"metadata":32023},{"headings":32024,"localImagePaths":32025,"remoteImagePaths":32026,"frontmatter":32027,"imagePaths":32029},[],[],[],{"title":32015,"description":32016,"summary":32016,"pubDate":32028,"source":24944,"url":32018,"thumbnail":32019},"Tue, 24 Jun 2025 10:00:00 +0900",[],"2025-06-24-中小企業のホンネ-aiは期待ほどの成果を上げていない.md","2025-06-24-東武ホテルが直面する人手不足-社長が提唱する観光業界のai活用",{"id":32031,"data":32033,"filePath":32039,"digest":32040,"rendered":32041,"legacyId":32049},{"title":32034,"description":32035,"summary":32035,"pubDate":32036,"source":24944,"url":32037,"thumbnail":32038},"東武ホテルが直面する人手不足　社長が提唱する「観光業界のAI活用」","ホテル業界は人手不足やDXの課題にどう向き合えばよいのか。東武ホテルマネジメントの三輪裕章社長に、ホテル経営の現場で直面する課題と、その打開策を聞いた。",["Date","2025-06-23T23:00:00.000Z"],"https://www.itmedia.co.jp/business/articles/2506/24/news019.html","https://image.itmedia.co.jp/business/articles/2506/24/cover_news019.jpg","src/content/posts/2025-06-24-東武ホテルが直面する人手不足-社長が提唱する観光業界のai活用.md","850d7d1f0b67cfc8",{"html":25,"metadata":32042},{"headings":32043,"localImagePaths":32044,"remoteImagePaths":32045,"frontmatter":32046,"imagePaths":32048},[],[],[],{"title":32034,"description":32035,"summary":32035,"pubDate":32047,"source":24944,"url":32037,"thumbnail":32038},"Tue, 24 Jun 2025 08:00:00 +0900",[],"2025-06-24-東武ホテルが直面する人手不足-社長が提唱する観光業界のai活用.md","2025-06-25-5-tips-for-getting-started-with-flow",{"id":32050,"data":32052,"filePath":32058,"digest":32059,"rendered":32060,"legacyId":32068},{"title":32053,"description":32054,"summary":32054,"pubDate":32055,"source":23485,"url":32056,"thumbnail":32057},"5 tips for getting started with Flow","Flow-generated video showing two astronaut pugs driving a car in outer space, with text overlaid saying 'Find your Flow'",["Date","2025-06-25T22:45:00.000Z"],"https://blog.google/technology/ai/flow-video-tips/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/GetStartedwithFlow_SS.width-1300.png","src/content/posts/2025-06-25-5-tips-for-getting-started-with-flow.md","60a25dab8dc7ca5d",{"html":25,"metadata":32061},{"headings":32062,"localImagePaths":32063,"remoteImagePaths":32064,"frontmatter":32065,"imagePaths":32067},[],[],[],{"title":32053,"description":32054,"summary":32054,"pubDate":32066,"source":23485,"url":32056,"thumbnail":32057},"Wed, 25 Jun 2025 22:45:00 +0000",[],"2025-06-25-5-tips-for-getting-started-with-flow.md","2025-06-25-a-certified-proof-checker-for-deep-neural-network-verification-in-imandra",{"id":32069,"data":32071,"filePath":32076,"digest":32077,"rendered":32078,"legacyId":32086},{"title":32072,"description":32073,"summary":32073,"pubDate":32074,"source":25191,"url":32075,"thumbnail":25193},"A Certified Proof Checker for Deep Neural Network Verification in Imandra","arXiv:2405.10611v2 Announce Type: replace-cross Abstract: Recent advances in the verification of deep neural networks (DNNs) have opened the way for a broader usage of DNN verification technology in many application areas, including safety-critical ones. However, DNN verifiers are themselves complex programs that have been shown to be susceptible to errors and numerical imprecision; this, in turn, has raised the question of trust in DNN verifiers. One prominent attempt to address this issue is enhancing DNN verifiers with the capability of producing certificates of their results that are subject to independent algorithmic checking. While formulations of Marabou certificate checking already exist on top of the state-of-the-art DNN verifier Marabou, they are implemented in C++, and that code itself raises the question of trust (e.g., in the precision of floating point calculations or guarantees for implementation soundness). Here, we present an alternative implementation of the Marabou certificate checking in Imandra -- an industrial functional programming language and an interactive theorem prover (ITP) -- that allows us to obtain full proof of certificate correctness. The significance of the result is two-fold. Firstly, it gives stronger independent guarantees for Marabou proofs. Secondly, it opens the way for the wider adoption of DNN verifiers in interactive theorem proving in the same way as many ITPs already incorporate SMT solvers.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2405.10611","src/content/posts/2025-06-25-a-certified-proof-checker-for-deep-neural-network-verification-in-imandra.md","e3365726c362d848",{"html":25,"metadata":32079},{"headings":32080,"localImagePaths":32081,"remoteImagePaths":32082,"frontmatter":32083,"imagePaths":32085},[],[],[],{"title":32072,"description":32073,"summary":32073,"pubDate":32084,"source":25191,"url":32075,"thumbnail":25193},"Wed, 25 Jun 2025 00:00:00 -0400",[],"2025-06-25-a-certified-proof-checker-for-deep-neural-network-verification-in-imandra.md","2025-06-25-a-comment-on-the-illusion-of-thinking-reframing-the-reasoning-cliff-as-an-agentic-gap",{"id":32087,"data":32089,"filePath":32094,"digest":32095,"rendered":32096,"legacyId":32103},{"title":32090,"description":32091,"summary":32091,"pubDate":32092,"source":25191,"url":32093,"thumbnail":25193},"A Comment On 'The Illusion of Thinking': Reframing the Reasoning Cliff as an Agentic Gap","arXiv:2506.18957v1 Announce Type: new Abstract: The recent work by Shojaee et al. (2025), titled The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity, presents a compelling empirical finding, a reasoning cliff, where the performance of Large Reasoning Models (LRMs) collapses beyond a specific complexity threshold, which the authors posit as an intrinsic scaling limitation of Chain-of-Thought (CoT) reasoning. This commentary, while acknowledging the study's methodological rigor, contends that this conclusion is confounded by experimental artifacts. We argue that the observed failure is not evidence of a fundamental cognitive boundary, but rather a predictable outcome of system-level constraints in the static, text-only evaluation paradigm, including tool use restrictions, context window recall issues, the absence of crucial cognitive baselines, inadequate statistical reporting, and output generation limits. We reframe this performance collapse through the lens of an agentic gap, asserting that the models are not failing at reasoning, but at execution within a profoundly restrictive interface. We empirically substantiate this critique by demonstrating a striking reversal. A model, initially declaring a puzzle impossible when confined to text-only generation, now employs agentic tools to not only solve it but also master variations of complexity far beyond the reasoning cliff it previously failed to surmount. Additionally, our empirical analysis of tool-enabled models like o4-mini and GPT-4o reveals a hierarchy of agentic reasoning, from simple procedural execution to complex meta-cognitive self-correction, which has significant implications for how we define and measure machine intelligence. The illusion of thinking attributed to LRMs is less a reasoning deficit and more a consequence of an otherwise capable mind lacking the tools for action.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18957","src/content/posts/2025-06-25-a-comment-on-the-illusion-of-thinking-reframing-the-reasoning-cliff-as-an-agentic-gap.md","4e056231706c335f",{"html":25,"metadata":32097},{"headings":32098,"localImagePaths":32099,"remoteImagePaths":32100,"frontmatter":32101,"imagePaths":32102},[],[],[],{"title":32090,"description":32091,"summary":32091,"pubDate":32084,"source":25191,"url":32093,"thumbnail":25193},[],"2025-06-25-a-comment-on-the-illusion-of-thinking-reframing-the-reasoning-cliff-as-an-agentic-gap.md","2025-06-25-a-global-local-cross-attention-network-for-ultra-high-resolution-remote-sensing-image-semantic-segmentation",{"id":32104,"data":32106,"filePath":32111,"digest":32112,"rendered":32113,"legacyId":32120},{"title":32107,"description":32108,"summary":32108,"pubDate":32109,"source":25191,"url":32110,"thumbnail":25193},"A Global-Local Cross-Attention Network for Ultra-high Resolution Remote Sensing Image Semantic Segmentation","arXiv:2506.19406v1 Announce Type: cross Abstract: With the rapid development of ultra-high resolution (UHR) remote sensing technology, the demand for accurate and efficient semantic segmentation has increased significantly. However, existing methods face challenges in computational efficiency and multi-scale feature fusion. To address these issues, we propose GLCANet (Global-Local Cross-Attention Network), a lightweight segmentation framework designed for UHR remote sensing imagery.GLCANet employs a dual-stream architecture to efficiently fuse global semantics and local details while minimizing GPU usage. A self-attention mechanism enhances long-range dependencies, refines global features, and preserves local details for better semantic consistency. A masked cross-attention mechanism also adaptively fuses global-local features, selectively enhancing fine-grained details while exploiting global context to improve segmentation accuracy. Experimental results show that GLCANet outperforms state-of-the-art methods regarding accuracy and computational efficiency. The model effectively processes large, high-resolution images with a small memory footprint, providing a promising solution for real-world remote sensing applications.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19406","src/content/posts/2025-06-25-a-global-local-cross-attention-network-for-ultra-high-resolution-remote-sensing-image-semantic-segmentation.md","3a3f16b9b5287198",{"html":25,"metadata":32114},{"headings":32115,"localImagePaths":32116,"remoteImagePaths":32117,"frontmatter":32118,"imagePaths":32119},[],[],[],{"title":32107,"description":32108,"summary":32108,"pubDate":32084,"source":25191,"url":32110,"thumbnail":25193},[],"2025-06-25-a-global-local-cross-attention-network-for-ultra-high-resolution-remote-sensing-image-semantic-segmentation.md","2025-06-25-a-standard-transformer-and-attention-with-linear-biases-for-molecular-conformer-generation",{"id":32121,"data":32123,"filePath":32128,"digest":32129,"rendered":32130,"legacyId":32137},{"title":32124,"description":32125,"summary":32125,"pubDate":32126,"source":25191,"url":32127,"thumbnail":25193},"A standard transformer and attention with linear biases for molecular conformer generation","arXiv:2506.19834v1 Announce Type: cross Abstract: Sampling low-energy molecular conformations, spatial arrangements of atoms in a molecule, is a critical task for many different calculations performed in the drug discovery and optimization process. Numerous specialized equivariant networks have been designed to generate molecular conformations from 2D molecular graphs. Recently, non-equivariant transformer models have emerged as a viable alternative due to their capability to scale to improve generalization. However, the concern has been that non-equivariant models require a large model size to compensate the lack of equivariant bias. In this paper, we demonstrate that a well-chosen positional encoding effectively addresses these size limitations. A standard transformer model incorporating relative positional encoding for molecular graphs when scaled to 25 million parameters surpasses the current state-of-the-art non-equivariant base model with 64 million parameters on the GEOM-DRUGS benchmark. We implemented relative positional encoding as a negative attention bias that linearly increases with the shortest path distances between graph nodes at varying slopes for different attention heads, similar to ALiBi, a widely adopted relative positional encoding technique in the NLP domain. This architecture has the potential to serve as a foundation for a novel class of generative models for molecular conformations.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19834","src/content/posts/2025-06-25-a-standard-transformer-and-attention-with-linear-biases-for-molecular-conformer-generation.md","1863e6ce5828180c",{"html":25,"metadata":32131},{"headings":32132,"localImagePaths":32133,"remoteImagePaths":32134,"frontmatter":32135,"imagePaths":32136},[],[],[],{"title":32124,"description":32125,"summary":32125,"pubDate":32084,"source":25191,"url":32127,"thumbnail":25193},[],"2025-06-25-a-standard-transformer-and-attention-with-linear-biases-for-molecular-conformer-generation.md","2025-06-25-a-survey-of-multi-sensor-fusion-perception-for-embodied-ai-background-methods-challenges-and-prospects",{"id":32138,"data":32140,"filePath":32145,"digest":32146,"rendered":32147,"legacyId":32154},{"title":32141,"description":32142,"summary":32142,"pubDate":32143,"source":25191,"url":32144,"thumbnail":25193},"A Survey of Multi-sensor Fusion Perception for Embodied AI: Background, Methods, Challenges and Prospects","arXiv:2506.19769v1 Announce Type: cross Abstract: Multi-sensor fusion perception (MSFP) is a key technology for embodied AI, which can serve a variety of downstream tasks (e.g., 3D object detection and semantic segmentation) and application scenarios (e.g., autonomous driving and swarm robotics). Recently, impressive achievements on AI-based MSFP methods have been reviewed in relevant surveys. However, we observe that the existing surveys have some limitations after a rigorous and detailed investigation. For one thing, most surveys are oriented to a single task or research field, such as 3D object detection or autonomous driving. Therefore, researchers in other related tasks often find it difficult to benefit directly. For another, most surveys only introduce MSFP from a single perspective of multi-modal fusion, while lacking consideration of the diversity of MSFP methods, such as multi-view fusion and time-series fusion. To this end, in this paper, we hope to organize MSFP research from a task-agnostic perspective, where methods are reported from various technical views. Specifically, we first introduce the background of MSFP. Next, we review multi-modal and multi-agent fusion methods. A step further, time-series fusion methods are analyzed. In the era of LLM, we also investigate multimodal LLM fusion methods. Finally, we discuss open challenges and future directions for MSFP. We hope this survey can help researchers understand the important progress in MSFP and provide possible insights for future research.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19769","src/content/posts/2025-06-25-a-survey-of-multi-sensor-fusion-perception-for-embodied-ai-background-methods-challenges-and-prospects.md","b8881a76567b762e",{"html":25,"metadata":32148},{"headings":32149,"localImagePaths":32150,"remoteImagePaths":32151,"frontmatter":32152,"imagePaths":32153},[],[],[],{"title":32141,"description":32142,"summary":32142,"pubDate":32084,"source":25191,"url":32144,"thumbnail":25193},[],"2025-06-25-a-survey-of-multi-sensor-fusion-perception-for-embodied-ai-background-methods-challenges-and-prospects.md","2025-06-25-adaptive-domain-modeling-with-language-models-a-multi-agent-approach-to-task-planning",{"id":32155,"data":32157,"filePath":32162,"digest":32163,"rendered":32164,"legacyId":32171},{"title":32158,"description":32159,"summary":32159,"pubDate":32160,"source":25191,"url":32161,"thumbnail":25193},"Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning","arXiv:2506.19592v1 Announce Type: new Abstract: We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a multi-agent framework that integrates Large Language Models (LLMs) with symbolic planning to solve complex tasks without the need for manually defined environment models. TAPAS employs specialized LLM-based agents that collaboratively generate and adapt domain models, initial states, and goal specifications as needed using structured tool-calling mechanisms. Through this tool-based interaction, downstream agents can request modifications from upstream agents, enabling adaptation to novel attributes and constraints without manual domain redefinition. A ReAct (Reason+Act)-style execution agent, coupled with natural language plan translation, bridges the gap between dynamically generated plans and real-world robot capabilities. TAPAS demonstrates strong performance in benchmark planning domains and in the VirtualHome simulated real-world environment.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19592","src/content/posts/2025-06-25-adaptive-domain-modeling-with-language-models-a-multi-agent-approach-to-task-planning.md","19913e4bef4236bd",{"html":25,"metadata":32165},{"headings":32166,"localImagePaths":32167,"remoteImagePaths":32168,"frontmatter":32169,"imagePaths":32170},[],[],[],{"title":32158,"description":32159,"summary":32159,"pubDate":32084,"source":25191,"url":32161,"thumbnail":25193},[],"2025-06-25-adaptive-domain-modeling-with-language-models-a-multi-agent-approach-to-task-planning.md","2025-06-25-ai-assisted-transport-of-radioactive-ion-beams",{"id":32172,"data":32174,"filePath":32179,"digest":32180,"rendered":32181,"legacyId":32188},{"title":32175,"description":32176,"summary":32176,"pubDate":32177,"source":25191,"url":32178,"thumbnail":25193},"AI-Assisted Transport of Radioactive Ion Beams","arXiv:2504.06469v3 Announce Type: replace-cross Abstract: Beams of radioactive heavy ions allow researchers to study rare and unstable atomic nuclei, shedding light into the internal structure of exotic nuclei and on how chemical elements are formed in stars. However, the extraction and transport of radioactive beams rely on time-consuming expert-driven tuning methods, where hundreds of parameters are manually optimized. Here, we introduce a system that employs Artificial Intelligence (AI), specifically utilizing Bayesian Optimization, to assist in the transport process of radioactive beams. We apply our methodology to real-life scenarios showing advantages when compared with standard tuning methods. This AI-assisted approach can be extended to other radioactive beam facilities around the world to improve operational efficiency and enhance scientific output.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2504.06469","src/content/posts/2025-06-25-ai-assisted-transport-of-radioactive-ion-beams.md","87b491958ef68418",{"html":25,"metadata":32182},{"headings":32183,"localImagePaths":32184,"remoteImagePaths":32185,"frontmatter":32186,"imagePaths":32187},[],[],[],{"title":32175,"description":32176,"summary":32176,"pubDate":32084,"source":25191,"url":32178,"thumbnail":25193},[],"2025-06-25-ai-assisted-transport-of-radioactive-ion-beams.md","2025-06-25-ai-based-approach-in-early-warning-systems-focus-on-emergency-communication-ecosystem-and-citizen-participation-in-nordic-countries",{"id":32189,"data":32191,"filePath":32196,"digest":32197,"rendered":32198,"legacyId":32205},{"title":32192,"description":32193,"summary":32193,"pubDate":32194,"source":25191,"url":32195,"thumbnail":25193},"AI-based Approach in Early Warning Systems: Focus on Emergency Communication Ecosystem and Citizen Participation in Nordic Countries","arXiv:2506.18926v1 Announce Type: cross Abstract: Climate change and natural disasters are recognized as worldwide challenges requiring complex and efficient ecosystems to deal with social, economic, and environmental effects. This chapter advocates a holistic approach, distinguishing preparedness, emergency responses, and postcrisis phases. The role of the Early Warning System (EWS), Risk modeling and mitigation measures are particularly emphasized. The chapter reviews the various Artificial Intelligence (AI)-enabler technologies that can be leveraged at each phase, focusing on the INFORM risk framework and EWSs. Emergency communication and psychological risk perception have been emphasized in emergency response times. Finally, a set of case studies from Nordic countries has been highlighted.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18926","src/content/posts/2025-06-25-ai-based-approach-in-early-warning-systems-focus-on-emergency-communication-ecosystem-and-citizen-participation-in-nordic-countries.md","8292c614c53ed52e",{"html":25,"metadata":32199},{"headings":32200,"localImagePaths":32201,"remoteImagePaths":32202,"frontmatter":32203,"imagePaths":32204},[],[],[],{"title":32192,"description":32193,"summary":32193,"pubDate":32084,"source":25191,"url":32195,"thumbnail":25193},[],"2025-06-25-ai-based-approach-in-early-warning-systems-focus-on-emergency-communication-ecosystem-and-citizen-participation-in-nordic-countries.md","2025-06-25-ai-enhanced-deliberative-democracy-and-the-future-of-the-collective-will",{"id":32206,"data":32208,"filePath":32213,"digest":32214,"rendered":32215,"legacyId":32222},{"title":32209,"description":32210,"summary":32210,"pubDate":32211,"source":25191,"url":32212,"thumbnail":25193},"AI-Enhanced Deliberative Democracy and the Future of the Collective Will","arXiv:2503.05830v2 Announce Type: replace-cross Abstract: This article unpacks the design choices behind longstanding and newly proposed computational frameworks aimed at finding common grounds across collective preferences and examines their potential future impacts, both technically and normatively. It begins by situating AI-assisted preference elicitation within the historical role of opinion polls, emphasizing that preferences are shaped by the decision-making context and are seldom objectively captured. With that caveat in mind, we explore AI-based democratic innovations as discovery tools for fostering reasonable representations of a collective will, sense-making, and agreement-seeking. At the same time, we caution against dangerously misguided uses, such as enabling binding decisions, fostering gradual disempowerment or post-rationalizing political outcomes.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2503.05830","src/content/posts/2025-06-25-ai-enhanced-deliberative-democracy-and-the-future-of-the-collective-will.md","fc7d71c7233c0374",{"html":25,"metadata":32216},{"headings":32217,"localImagePaths":32218,"remoteImagePaths":32219,"frontmatter":32220,"imagePaths":32221},[],[],[],{"title":32209,"description":32210,"summary":32210,"pubDate":32084,"source":25191,"url":32212,"thumbnail":25193},[],"2025-06-25-ai-enhanced-deliberative-democracy-and-the-future-of-the-collective-will.md","2025-06-25-ai-based-multimodal-biometrics-for-detecting-smartphone-distractions-application-to-online-learning",{"id":32223,"data":32225,"filePath":32230,"digest":32231,"rendered":32232,"legacyId":32239},{"title":32226,"description":32227,"summary":32227,"pubDate":32228,"source":25191,"url":32229,"thumbnail":25193},"AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning","arXiv:2506.17364v2 Announce Type: replace-cross Abstract: This work investigates the use of multimodal biometrics to detect distractions caused by smartphone use during tasks that require sustained attention, with a focus on computer-based online learning. Although the methods are applicable to various domains, such as autonomous driving, we concentrate on the challenges learners face in maintaining engagement amid internal (e.g., motivation), system-related (e.g., course design) and contextual (e.g., smartphone use) factors. Traditional learning platforms often lack detailed behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors provide new insights into learner attention. We propose an AI-based approach that leverages physiological signals and head pose data to detect phone use. Our results show that single biometric signals, such as brain waves or heart rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal model combining all signals reaches 91% accuracy, highlighting the benefits of integration. We conclude by discussing the implications and limitations of deploying these models for real-time support in online learning environments.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.17364","src/content/posts/2025-06-25-ai-based-multimodal-biometrics-for-detecting-smartphone-distractions-application-to-online-learning.md","f61637ac39a9ae30",{"html":25,"metadata":32233},{"headings":32234,"localImagePaths":32235,"remoteImagePaths":32236,"frontmatter":32237,"imagePaths":32238},[],[],[],{"title":32226,"description":32227,"summary":32227,"pubDate":32084,"source":25191,"url":32229,"thumbnail":25193},[],"2025-06-25-ai-based-multimodal-biometrics-for-detecting-smartphone-distractions-application-to-online-learning.md","2025-06-25-ai-facilitated-episodic-future-thinking-for-adults-with-obesity",{"id":32240,"data":32242,"filePath":32247,"digest":32248,"rendered":32249,"legacyId":32256},{"title":32243,"description":32244,"summary":32244,"pubDate":32245,"source":25191,"url":32246,"thumbnail":25193},"AI-Facilitated Episodic Future Thinking For Adults with Obesity","arXiv:2503.16484v2 Announce Type: replace-cross Abstract: Episodic Future Thinking (EFT) involves vividly imagining personal future events and experiences in detail. It has shown promise as an intervention to reduce delay discounting-the tendency to devalue delayed rewards in favor of immediate gratification- and to promote behavior change in a range of maladaptive health behaviors. We present EFTeacher, an AI chatbot powered by the GPT-4-Turbo large language model, designed to generate EFT cues for users with lifestyle-related conditions. To evaluate the feasibility and usability of EFTeacher, we conducted a mixed-methods study that included usability assessments, user evaluations based on content characteristics questionnaires, and semi-structured interviews. Qualitative findings indicate that participants perceived EFTeacher as communicative and supportive through an engaging dialogue. The chatbot facilitated imaginative thinking and reflection on future goals. Participants appreciated its adaptability and personalization features, though some noted challenges such as repetitive dialogue and verbose responses. Our findings underscore the potential of large language model-based chatbots in EFT interventions targeting maladaptive health behaviors.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2503.16484","src/content/posts/2025-06-25-ai-facilitated-episodic-future-thinking-for-adults-with-obesity.md","a296fddcd729e62f",{"html":25,"metadata":32250},{"headings":32251,"localImagePaths":32252,"remoteImagePaths":32253,"frontmatter":32254,"imagePaths":32255},[],[],[],{"title":32243,"description":32244,"summary":32244,"pubDate":32084,"source":25191,"url":32246,"thumbnail":25193},[],"2025-06-25-ai-facilitated-episodic-future-thinking-for-adults-with-obesity.md","2025-06-25-ai-safety-vs-ai-security-demystifying-the-distinction-and-boundaries",{"id":32257,"data":32259,"filePath":32264,"digest":32265,"rendered":32266,"legacyId":32273},{"title":32260,"description":32261,"summary":32261,"pubDate":32262,"source":25191,"url":32263,"thumbnail":25193},"AI Safety vs. AI Security: Demystifying the Distinction and Boundaries","arXiv:2506.18932v1 Announce Type: cross Abstract: Artificial Intelligence (AI) is rapidly being integrated into critical systems across various domains, from healthcare to autonomous vehicles. While its integration brings immense benefits, it also introduces significant risks, including those arising from AI misuse. Within the discourse on managing these risks, the terms 'AI Safety' and 'AI Security' are often used, sometimes interchangeably, resulting in conceptual confusion. This paper aims to demystify the distinction and delineate the precise research boundaries between AI Safety and AI Security. We provide rigorous definitions, outline their respective research focuses, and explore their interdependency, including how security breaches can precipitate safety failures and vice versa. Using clear analogies from message transmission and building construction, we illustrate these distinctions. Clarifying these boundaries is crucial for guiding precise research directions, fostering effective cross-disciplinary collaboration, enhancing policy effectiveness, and ultimately, promoting the deployment of trustworthy AI systems.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18932","src/content/posts/2025-06-25-ai-safety-vs-ai-security-demystifying-the-distinction-and-boundaries.md","6ce5d8c378878fc2",{"html":25,"metadata":32267},{"headings":32268,"localImagePaths":32269,"remoteImagePaths":32270,"frontmatter":32271,"imagePaths":32272},[],[],[],{"title":32260,"description":32261,"summary":32261,"pubDate":32084,"source":25191,"url":32263,"thumbnail":25193},[],"2025-06-25-ai-safety-vs-ai-security-demystifying-the-distinction-and-boundaries.md","2025-06-25-airv2x-unified-air-ground-vehicle-to-everything-collaboration",{"id":32274,"data":32276,"filePath":32281,"digest":32282,"rendered":32283,"legacyId":32290},{"title":32277,"description":32278,"summary":32278,"pubDate":32279,"source":25191,"url":32280,"thumbnail":25193},"AirV2X: Unified Air-Ground Vehicle-to-Everything Collaboration","arXiv:2506.19283v1 Announce Type: cross Abstract: While multi-vehicular collaborative driving demonstrates clear advantages over single-vehicle autonomy, traditional infrastructure-based V2X systems remain constrained by substantial deployment costs and the creation of 'uncovered danger zones' in rural and suburban areas. We present AirV2X-Perception, a large-scale dataset that leverages Unmanned Aerial Vehicles (UAVs) as a flexible alternative or complement to fixed Road-Side Units (RSUs). Drones offer unique advantages over ground-based perception: complementary bird's-eye-views that reduce occlusions, dynamic positioning capabilities that enable hovering, patrolling, and escorting navigation rules, and significantly lower deployment costs compared to fixed infrastructure. Our dataset comprises 6.73 hours of drone-assisted driving scenarios across urban, suburban, and rural environments with varied weather and lighting conditions. The AirV2X-Perception dataset facilitates the development and standardized evaluation of Vehicle-to-Drone (V2D) algorithms, addressing a critical gap in the rapidly expanding field of aerial-assisted autonomous driving systems. The dataset and development kits are open-sourced at https://github.com/taco-group/AirV2X-Perception.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19283","src/content/posts/2025-06-25-airv2x-unified-air-ground-vehicle-to-everything-collaboration.md","93c5122cb1633eab",{"html":25,"metadata":32284},{"headings":32285,"localImagePaths":32286,"remoteImagePaths":32287,"frontmatter":32288,"imagePaths":32289},[],[],[],{"title":32277,"description":32278,"summary":32278,"pubDate":32084,"source":25191,"url":32280,"thumbnail":25193},[],"2025-06-25-airv2x-unified-air-ground-vehicle-to-everything-collaboration.md","2025-06-25-aiエージェントを悪用した攻撃手法3選-対策の考え方にも転換が必要前編",{"id":32291,"data":32293,"filePath":32299,"digest":32300,"rendered":32301,"legacyId":32309},{"title":32294,"description":32295,"summary":32295,"pubDate":32296,"source":24944,"url":32297,"thumbnail":32298},"AIエージェントを悪用した攻撃手法3選　対策の考え方にも転換が必要（前編）","AIエージェントのセキュリティ脅威に対し、学術界と産業界では新たな対策フレームワークの構築が活発化しています。どのような戦略的アプローチが有効なのでしょうか。最新の研究成果が示す新しい脅威分類と、それに対応する包括的セキュリティフレームワークの設計思想について解説します。",["Date","2025-06-25T01:00:00.000Z"],"https://www.itmedia.co.jp/enterprise/articles/2506/25/news041.html","https://image.itmedia.co.jp/enterprise/articles/2506/25/cover_news041.jpg","src/content/posts/2025-06-25-aiエージェントを悪用した攻撃手法3選-対策の考え方にも転換が必要前編.md","d02f3a423d160942",{"html":25,"metadata":32302},{"headings":32303,"localImagePaths":32304,"remoteImagePaths":32305,"frontmatter":32306,"imagePaths":32308},[],[],[],{"title":32294,"description":32295,"summary":32295,"pubDate":32307,"source":24944,"url":32297,"thumbnail":32298},"Wed, 25 Jun 2025 10:00:00 +0900",[],"2025-06-25-aiエージェントを悪用した攻撃手法3選-対策の考え方にも転換が必要前編.md","2025-06-25-align-and-distill-unifying-and-improving-domain-adaptive-object-detection",{"id":32310,"data":32312,"filePath":32317,"digest":32318,"rendered":32319,"legacyId":32326},{"title":32313,"description":32314,"summary":32314,"pubDate":32315,"source":25191,"url":32316,"thumbnail":25193},"Align and Distill: Unifying and Improving Domain Adaptive Object Detection","arXiv:2403.12029v4 Announce Type: replace-cross Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method, ALDI++, that achieves state-of-the-art results by a large margin. ALDI++ outperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy Cityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to outperform a fair baseline), and +0.6 AP50 on CFC Kenai to Channel. ALDI and ALDI++ are architecture-agnostic, setting a new state-of-the-art for YOLO and DETR-based DAOD as well without additional hyperparameter tuning. Our framework, dataset, and state-of-the-art method offer a critical reset for DAOD and provide a strong foundation for future research. Code and data are available: https://github.com/justinkay/aldi and https://github.com/visipedia/caltech-fish-counting.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2403.12029","src/content/posts/2025-06-25-align-and-distill-unifying-and-improving-domain-adaptive-object-detection.md","ff469f7d5930f243",{"html":25,"metadata":32320},{"headings":32321,"localImagePaths":32322,"remoteImagePaths":32323,"frontmatter":32324,"imagePaths":32325},[],[],[],{"title":32313,"description":32314,"summary":32314,"pubDate":32084,"source":25191,"url":32316,"thumbnail":25193},[],"2025-06-25-align-and-distill-unifying-and-improving-domain-adaptive-object-detection.md","2025-06-25-alleviating-user-sensitive-bias-with-fair-generative-sequential-recommendation-model",{"id":32327,"data":32329,"filePath":32334,"digest":32335,"rendered":32336,"legacyId":32343},{"title":32330,"description":32331,"summary":32331,"pubDate":32332,"source":25191,"url":32333,"thumbnail":25193},"Alleviating User-Sensitive bias with Fair Generative Sequential Recommendation Model","arXiv:2506.19777v1 Announce Type: cross Abstract: Recommendation fairness has recently attracted much attention. In the real world, recommendation systems are driven by user behavior, and since users with the same sensitive feature (e.g., gender and age) tend to have the same patterns, recommendation models can easily capture the strong correlation preference of sensitive features and thus cause recommendation unfairness. Diffusion model (DM) as a new generative model paradigm has achieved great success in recommendation systems. DM's ability to model uncertainty and represent diversity, and its modeling mechanism has a high degree of adaptability with the real-world recommendation process with bias. Therefore, we use DM to effectively model the fairness of recommendation and enhance the diversity. This paper proposes a FairGENerative sequential Recommendation model based on DM, FairGENRec. In the training phase, we inject random noise into the original distribution under the guidance of the sensitive feature recognition model, and a sequential denoise model is designed for the reverse reconstruction of items. Simultaneously, recommendation fairness modeling is completed by injecting multi-interests representational information that eliminates the bias of sensitive user features into the generated results. In the inference phase, the model obtains the noise in the form of noise addition by using the history interactions which is followed by reverse iteration to reconstruct the target item representation. Finally, our extensive experiments on three datasets demonstrate the dual enhancement effect of FairGENRec on accuracy and fairness, while the statistical analysis of the cases visualizes the degree of improvement on the fairness of the recommendation.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19777","src/content/posts/2025-06-25-alleviating-user-sensitive-bias-with-fair-generative-sequential-recommendation-model.md","c9342d9cbccf3f08",{"html":25,"metadata":32337},{"headings":32338,"localImagePaths":32339,"remoteImagePaths":32340,"frontmatter":32341,"imagePaths":32342},[],[],[],{"title":32330,"description":32331,"summary":32331,"pubDate":32084,"source":25191,"url":32333,"thumbnail":25193},[],"2025-06-25-alleviating-user-sensitive-bias-with-fair-generative-sequential-recommendation-model.md","2025-06-25-alphagenome-ai-for-better-understanding-the-genome",{"id":32344,"data":32346,"filePath":32352,"digest":32353,"rendered":32354,"legacyId":32362},{"title":32347,"description":32348,"summary":32348,"pubDate":32349,"source":6423,"url":32350,"thumbnail":32351},"AlphaGenome: AI for better understanding the genome","Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.",["Date","2025-06-25T13:59:51.000Z"],"https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/","https://lh3.googleusercontent.com/SZkcKUQyLUhSQ06Rq-PJbxAqn1OpMeEa3khkrBVB1MGyHfxyftoqWwEb2aLP9JxX7CjhpLFODcc5zIoMoNdu0bl6ELsZV2nP9fDwZC6SYS36lzAKDw=w1200-h630-n-nu","src/content/posts/2025-06-25-alphagenome-ai-for-better-understanding-the-genome.md","3b3579fbeef0d89e",{"html":25,"metadata":32355},{"headings":32356,"localImagePaths":32357,"remoteImagePaths":32358,"frontmatter":32359,"imagePaths":32361},[],[],[],{"title":32347,"description":32348,"summary":32348,"pubDate":32360,"source":6423,"url":32350,"thumbnail":32351},"Wed, 25 Jun 2025 13:59:51 +0000",[],"2025-06-25-alphagenome-ai-for-better-understanding-the-genome.md","2025-06-25-anchordp3-3d-affordance-guided-sparse-diffusion-policy-for-robotic-manipulation",{"id":32363,"data":32365,"filePath":32370,"digest":32371,"rendered":32372,"legacyId":32379},{"title":32366,"description":32367,"summary":32367,"pubDate":32368,"source":25191,"url":32369,"thumbnail":25193},"AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic Manipulation","arXiv:2506.19269v1 Announce Type: cross Abstract: We present AnchorDP3, a diffusion policy framework for dual-arm robotic manipulation that achieves state-of-the-art performance in highly randomized environments. AnchorDP3 integrates three key innovations: (1) Simulator-Supervised Semantic Segmentation, using rendered ground truth to explicitly segment task-critical objects within the point cloud, which provides strong affordance priors; (2) Task-Conditioned Feature Encoders, lightweight modules processing augmented point clouds per task, enabling efficient multi-task learning through a shared diffusion-based action expert; (3) Affordance-Anchored Keypose Diffusion with Full State Supervision, replacing dense trajectory prediction with sparse, geometrically meaningful action anchors, i.e., keyposes such as pre-grasp pose, grasp pose directly anchored to affordances, drastically simplifying the prediction space; the action expert is forced to predict both robot joint angles and end-effector poses simultaneously, which exploits geometric consistency to accelerate convergence and boost accuracy. Trained on large-scale, procedurally generated simulation data, AnchorDP3 achieves a 98.7% average success rate in the RoboTwin benchmark across diverse tasks under extreme randomization of objects, clutter, table height, lighting, and backgrounds. This framework, when integrated with the RoboTwin real-to-sim pipeline, has the potential to enable fully autonomous generation of deployable visuomotor policies from only scene and instruction, totally eliminating human demonstrations from learning manipulation skills.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19269","src/content/posts/2025-06-25-anchordp3-3d-affordance-guided-sparse-diffusion-policy-for-robotic-manipulation.md","6db5b46dd2975faf",{"html":25,"metadata":32373},{"headings":32374,"localImagePaths":32375,"remoteImagePaths":32376,"frontmatter":32377,"imagePaths":32378},[],[],[],{"title":32366,"description":32367,"summary":32367,"pubDate":32084,"source":25191,"url":32369,"thumbnail":25193},[],"2025-06-25-anchordp3-3d-affordance-guided-sparse-diffusion-policy-for-robotic-manipulation.md","2025-06-25-antigrounding-lifting-robotic-actions-into-vlm-representation-space-for-decision-making",{"id":32380,"data":32382,"filePath":32387,"digest":32388,"rendered":32389,"legacyId":32396},{"title":32383,"description":32384,"summary":32384,"pubDate":32385,"source":25191,"url":32386,"thumbnail":25193},"AntiGrounding: Lifting Robotic Actions into VLM Representation Space for Decision Making","arXiv:2506.12374v2 Announce Type: replace-cross Abstract: Vision-Language Models (VLMs) encode knowledge and reasoning capabilities for robotic manipulation within high-dimensional representation spaces. However, current approaches often project them into compressed intermediate representations, discarding important task-specific information such as fine-grained spatial or semantic details. To address this, we propose AntiGrounding, a new framework that reverses the instruction grounding process. It lifts candidate actions directly into the VLM representation space, renders trajectories from multiple views, and uses structured visual question answering for instruction-based decision making. This enables zero-shot synthesis of optimal closed-loop robot trajectories for new tasks. We also propose an offline policy refinement module that leverages past experience to enhance long-term performance. Experiments in both simulation and real-world environments show that our method outperforms baselines across diverse robotic manipulation tasks.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.12374","src/content/posts/2025-06-25-antigrounding-lifting-robotic-actions-into-vlm-representation-space-for-decision-making.md","a1bcc89dc4c46110",{"html":25,"metadata":32390},{"headings":32391,"localImagePaths":32392,"remoteImagePaths":32393,"frontmatter":32394,"imagePaths":32395},[],[],[],{"title":32383,"description":32384,"summary":32384,"pubDate":32084,"source":25191,"url":32386,"thumbnail":25193},[],"2025-06-25-antigrounding-lifting-robotic-actions-into-vlm-representation-space-for-decision-making.md","2025-06-25-arabic-dialect-classification-using-rnns-transformers-and-large-language-models-a-comparative-analysis",{"id":32397,"data":32399,"filePath":32404,"digest":32405,"rendered":32406,"legacyId":32413},{"title":32400,"description":32401,"summary":32401,"pubDate":32402,"source":25191,"url":32403,"thumbnail":25193},"Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis","arXiv:2506.19753v1 Announce Type: cross Abstract: The Arabic language is among the most popular languages in the world with a huge variety of dialects spoken in 22 countries. In this study, we address the problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets. RNN models, Transformer models, and large language models (LLMs) via prompt engineering are created and tested. Among these, MARBERTv2 performed best with 65% accuracy and 64% F1-score. Through the use of state-of-the-art preprocessing techniques and the latest NLP models, this paper identifies the most significant linguistic issues in Arabic dialect identification. The results corroborate applications like personalized chatbots that respond in users' dialects, social media monitoring, and greater accessibility for Arabic communities.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19753","src/content/posts/2025-06-25-arabic-dialect-classification-using-rnns-transformers-and-large-language-models-a-comparative-analysis.md","9182c7f021a35b59",{"html":25,"metadata":32407},{"headings":32408,"localImagePaths":32409,"remoteImagePaths":32410,"frontmatter":32411,"imagePaths":32412},[],[],[],{"title":32400,"description":32401,"summary":32401,"pubDate":32084,"source":25191,"url":32403,"thumbnail":25193},[],"2025-06-25-arabic-dialect-classification-using-rnns-transformers-and-large-language-models-a-comparative-analysis.md","2025-06-25-are-we-there-yet-a-brief-survey-of-music-emotion-prediction-datasets-models-and-outstanding-challenges",{"id":32414,"data":32416,"filePath":32421,"digest":32422,"rendered":32423,"legacyId":32430},{"title":32417,"description":32418,"summary":32418,"pubDate":32419,"source":25191,"url":32420,"thumbnail":25193},"Are We There Yet? A Brief Survey of Music Emotion Prediction Datasets, Models and Outstanding Challenges","arXiv:2406.08809v3 Announce Type: replace-cross Abstract: Deep learning models for music have advanced drastically in recent years, but how good are machine learning models at capturing emotion, and what challenges are researchers facing? In this paper, we provide a comprehensive overview of the available music-emotion datasets and discuss evaluation standards as well as competitions in the field. We also offer a brief overview of various types of music emotion prediction models that have been built over the years, providing insights into the diverse approaches within the field. Through this examination, we highlight the challenges that persist in accurately capturing emotion in music, including issues related to dataset quality, annotation consistency, and model generalization. Additionally, we explore the impact of different modalities, such as audio, MIDI, and physiological signals, on the effectiveness of emotion prediction models. Through this examination, we identify persistent challenges in music emotion recognition (MER), including issues related to dataset quality, the ambiguity in emotion labels, and the difficulties of cross-dataset generalization. We argue that future advancements in MER require standardized benchmarks, larger and more diverse datasets, and improved model interpretability. Recognizing the dynamic nature of this field, we have complemented our findings with an accompanying GitHub repository. This repository contains a comprehensive list of music emotion datasets and recent predictive models.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2406.08809","src/content/posts/2025-06-25-are-we-there-yet-a-brief-survey-of-music-emotion-prediction-datasets-models-and-outstanding-challenges.md","92f499432c342ddf",{"html":25,"metadata":32424},{"headings":32425,"localImagePaths":32426,"remoteImagePaths":32427,"frontmatter":32428,"imagePaths":32429},[],[],[],{"title":32417,"description":32418,"summary":32418,"pubDate":32084,"source":25191,"url":32420,"thumbnail":25193},[],"2025-06-25-are-we-there-yet-a-brief-survey-of-music-emotion-prediction-datasets-models-and-outstanding-challenges.md","2025-06-25-asr-enhanced-multimodal-representation-learning-for-cross-domain-product-retrieval",{"id":32431,"data":32433,"filePath":32438,"digest":32439,"rendered":32440,"legacyId":32447},{"title":32434,"description":32435,"summary":32435,"pubDate":32436,"source":25191,"url":32437,"thumbnail":25193},"ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval","arXiv:2408.02978v2 Announce Type: replace-cross Abstract: E-commerce is increasingly multimedia-enriched, with products exhibited in a broad-domain manner as images, short videos, or live stream promotions. A unified and vectorized cross-domain production representation is essential. Due to large intra-product variance and high inter-product similarity in the broad-domain scenario, a visual-only representation is inadequate. While Automatic Speech Recognition (ASR) text derived from the short or live-stream videos is readily accessible, how to de-noise the excessively noisy text for multimodal representation learning is mostly untouched. We propose ASR-enhanced Multimodal Product Representation Learning (AMPere). In order to extract product-specific information from the raw ASR text, AMPere uses an easy-to-implement LLM-based ASR text summarizer. The LLM-summarized text, together with visual data, is then fed into a multi-branch network to generate compact multimodal embeddings. Extensive experiments on a large-scale tri-domain dataset verify the effectiveness of AMPere in obtaining a unified multimodal product representation that clearly improves cross-domain product retrieval.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2408.02978","src/content/posts/2025-06-25-asr-enhanced-multimodal-representation-learning-for-cross-domain-product-retrieval.md","78ef5d65912897dc",{"html":25,"metadata":32441},{"headings":32442,"localImagePaths":32443,"remoteImagePaths":32444,"frontmatter":32445,"imagePaths":32446},[],[],[],{"title":32434,"description":32435,"summary":32435,"pubDate":32084,"source":25191,"url":32437,"thumbnail":25193},[],"2025-06-25-asr-enhanced-multimodal-representation-learning-for-cross-domain-product-retrieval.md","2025-06-25-automated-detection-of-pre-training-text-in-black-box-llms",{"id":32448,"data":32450,"filePath":32455,"digest":32456,"rendered":32457,"legacyId":32464},{"title":32451,"description":32452,"summary":32452,"pubDate":32453,"source":25191,"url":32454,"thumbnail":25193},"Automated Detection of Pre-training Text in Black-box LLMs","arXiv:2506.19399v1 Announce Type: cross Abstract: Detecting whether a given text is a member of the pre-training data of Large Language Models (LLMs) is crucial for ensuring data privacy and copyright protection. Most existing methods rely on the LLM's hidden information (e.g., model parameters or token probabilities), making them ineffective in the black-box setting, where only input and output texts are accessible. Although some methods have been proposed for the black-box setting, they rely on massive manual efforts such as designing complicated questions or instructions. To address these issues, we propose VeilProbe, the first framework for automatically detecting LLMs' pre-training texts in a black-box setting without human intervention. VeilProbe utilizes a sequence-to-sequence mapping model to infer the latent mapping feature between the input text and the corresponding output suffix generated by the LLM. Then it performs the key token perturbations to obtain more distinguishable membership features. Additionally, considering real-world scenarios where the ground-truth training text samples are limited, a prototype-based membership classifier is introduced to alleviate the overfitting issue. Extensive evaluations on three widely used datasets demonstrate that our framework is effective and superior in the black-box setting.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19399","src/content/posts/2025-06-25-automated-detection-of-pre-training-text-in-black-box-llms.md","dc3c84964a1dcb18",{"html":25,"metadata":32458},{"headings":32459,"localImagePaths":32460,"remoteImagePaths":32461,"frontmatter":32462,"imagePaths":32463},[],[],[],{"title":32451,"description":32452,"summary":32452,"pubDate":32084,"source":25191,"url":32454,"thumbnail":25193},[],"2025-06-25-automated-detection-of-pre-training-text-in-black-box-llms.md","2025-06-25-automatic-depression-assessment-using-machine-learning-a-comprehensive-survey",{"id":32465,"data":32467,"filePath":32472,"digest":32473,"rendered":32474,"legacyId":32481},{"title":32468,"description":32469,"summary":32469,"pubDate":32470,"source":25191,"url":32471,"thumbnail":25193},"Automatic Depression Assessment using Machine Learning: A Comprehensive Survey","arXiv:2506.18915v1 Announce Type: cross Abstract: Depression is a common mental illness across current human society. Traditional depression assessment relying on inventories and interviews with psychologists frequently suffer from subjective diagnosis results, slow and expensive diagnosis process as well as lack of human resources. Since there is a solid evidence that depression is reflected by various human internal brain activities and external expressive behaviours, early traditional machine learning (ML) and advanced deep learning (DL) models have been widely explored for human behaviour-based automatic depression assessment (ADA) since 2012. However, recent ADA surveys typically only focus on a limited number of human behaviour modalities. Despite being used as a theoretical basis for developing ADA approaches, existing ADA surveys lack a comprehensive review and summary of multi-modal depression-related human behaviours. To bridge this gap, this paper specifically summarises depression-related human behaviours across a range of modalities (e.g. the human brain, verbal language and non-verbal audio/facial/body behaviours). We focus on conducting an up-to-date and comprehensive survey of ML-based ADA approaches for learning depression cues from these behaviours as well as discussing and comparing their distinctive features and limitations. In addition, we also review existing ADA competitions and datasets, identify and discuss the main challenges and opportunities to provide further research directions for future ADA researchers.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18915","src/content/posts/2025-06-25-automatic-depression-assessment-using-machine-learning-a-comprehensive-survey.md","13de506263b87b60",{"html":25,"metadata":32475},{"headings":32476,"localImagePaths":32477,"remoteImagePaths":32478,"frontmatter":32479,"imagePaths":32480},[],[],[],{"title":32468,"description":32469,"summary":32469,"pubDate":32084,"source":25191,"url":32471,"thumbnail":25193},[],"2025-06-25-automatic-depression-assessment-using-machine-learning-a-comprehensive-survey.md","2025-06-25-automatic-posology-structuration-what-role-for-llms",{"id":32482,"data":32484,"filePath":32489,"digest":32490,"rendered":32491,"legacyId":32498},{"title":32485,"description":32486,"summary":32486,"pubDate":32487,"source":25191,"url":32488,"thumbnail":25193},"Automatic Posology Structuration : What role for LLMs?","arXiv:2506.19525v1 Announce Type: cross Abstract: Automatically structuring posology instructions is essential for improving medication safety and enabling clinical decision support. In French prescriptions, these instructions are often ambiguous, irregular, or colloquial, limiting the effectiveness of classic ML pipelines. We explore the use of Large Language Models (LLMs) to convert free-text posologies into structured formats, comparing prompt-based methods and fine-tuning against a 'pre-LLM' system based on Named Entity Recognition and Linking (NERL). Our results show that while prompting improves performance, only fine-tuned LLMs match the accuracy of the baseline. Through error analysis, we observe complementary strengths: NERL offers structural precision, while LLMs better handle semantic nuances. Based on this, we propose a hybrid pipeline that routes low-confidence cases from NERL (\u003C0.8) to the LLM, selecting outputs based on confidence scores. This strategy achieves 91% structuration accuracy while minimizing latency and compute. Our results show that this hybrid approach improves structuration accuracy while limiting computational cost, offering a scalable solution for real-world clinical use.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19525","src/content/posts/2025-06-25-automatic-posology-structuration-what-role-for-llms.md","471420d17ed9abe5",{"html":25,"metadata":32492},{"headings":32493,"localImagePaths":32494,"remoteImagePaths":32495,"frontmatter":32496,"imagePaths":32497},[],[],[],{"title":32485,"description":32486,"summary":32486,"pubDate":32084,"source":25191,"url":32488,"thumbnail":25193},[],"2025-06-25-automatic-posology-structuration-what-role-for-llms.md","2025-06-25-automatic-prompt-optimization-for-knowledge-graph-construction-insights-from-an-empirical-study",{"id":32499,"data":32501,"filePath":32506,"digest":32507,"rendered":32508,"legacyId":32515},{"title":32502,"description":32503,"summary":32503,"pubDate":32504,"source":25191,"url":32505,"thumbnail":25193},"Automatic Prompt Optimization for Knowledge Graph Construction: Insights from an Empirical Study","arXiv:2506.19773v1 Announce Type: new Abstract: A KG represents a network of entities and illustrates relationships between them. KGs are used for various applications, including semantic search and discovery, reasoning, decision-making, natural language processing, machine learning, and recommendation systems. Triple (subject-relation-object) extraction from text is the fundamental building block of KG construction and has been widely studied, for example, in early benchmarks such as ACE 2002 to more recent ones, such as WebNLG 2020, REBEL and SynthIE. While the use of LLMs is explored for KG construction, handcrafting reasonable task-specific prompts for LLMs is a labour-intensive exercise and can be brittle due to subtle changes in the LLM models employed. Recent work in NLP tasks (e.g. autonomy generation) uses automatic prompt optimization/engineering to address this challenge by generating optimal or near-optimal task-specific prompts given input-output examples. This empirical study explores the application of automatic prompt optimization for the triple extraction task using experimental benchmarking. We evaluate different settings by changing (a) the prompting strategy, (b) the LLM being used for prompt optimization and task execution, (c) the number of canonical relations in the schema (schema complexity), (d) the length and diversity of input text, (e) the metric used to drive the prompt optimization, and (f) the dataset being used for training and testing. We evaluate three different automatic prompt optimizers, namely, DSPy, APE, and TextGrad and use two different triple extraction datasets, SynthIE and REBEL. Through rigorous empirical evaluation, our main contribution highlights that automatic prompt optimization techniques can generate reasonable prompts similar to humans for triple extraction. In turn, these optimized prompts achieve improved results, particularly with increasing schema complexity and text size.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19773","src/content/posts/2025-06-25-automatic-prompt-optimization-for-knowledge-graph-construction-insights-from-an-empirical-study.md","097574535fe46bea",{"html":25,"metadata":32509},{"headings":32510,"localImagePaths":32511,"remoteImagePaths":32512,"frontmatter":32513,"imagePaths":32514},[],[],[],{"title":32502,"description":32503,"summary":32503,"pubDate":32084,"source":25191,"url":32505,"thumbnail":25193},[],"2025-06-25-automatic-prompt-optimization-for-knowledge-graph-construction-insights-from-an-empirical-study.md","2025-06-25-baba-is-llm-reasoning-in-a-game-with-dynamic-rules",{"id":32516,"data":32518,"filePath":32523,"digest":32524,"rendered":32525,"legacyId":32532},{"title":32519,"description":32520,"summary":32520,"pubDate":32521,"source":25191,"url":32522,"thumbnail":25193},"Baba is LLM: Reasoning in a Game with Dynamic Rules","arXiv:2506.19095v1 Announce Type: new Abstract: Large language models (LLMs) are known to perform well on language tasks, but struggle with reasoning tasks. This paper explores the ability of LLMs to play the 2D puzzle game Baba is You, in which players manipulate rules by rearranging text blocks that define object properties. Given that this rule-manipulation relies on language abilities and reasoning, it is a compelling challenge for LLMs. Six LLMs are evaluated using different prompt types, including (1) simple, (2) rule-extended and (3) action-extended prompts. In addition, two models (Mistral, OLMo) are finetuned using textual and structural data from the game. Results show that while larger models (particularly GPT-4o) perform better in reasoning and puzzle solving, smaller unadapted models struggle to recognize game mechanics or apply rule changes. Finetuning improves the ability to analyze the game levels, but does not significantly improve solution formulation. We conclude that even for state-of-the-art and finetuned LLMs, reasoning about dynamic rule changes is difficult (specifically, understanding the use-mention distinction). The results provide insights into the applicability of LLMs to complex problem-solving tasks and highlight the suitability of games with dynamically changing rules for testing reasoning and reflection by LLMs.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19095","src/content/posts/2025-06-25-baba-is-llm-reasoning-in-a-game-with-dynamic-rules.md","579c3da53c2b9394",{"html":25,"metadata":32526},{"headings":32527,"localImagePaths":32528,"remoteImagePaths":32529,"frontmatter":32530,"imagePaths":32531},[],[],[],{"title":32519,"description":32520,"summary":32520,"pubDate":32084,"source":25191,"url":32522,"thumbnail":25193},[],"2025-06-25-baba-is-llm-reasoning-in-a-game-with-dynamic-rules.md","2025-06-25-bayesian-evolutionary-swarm-architecture-a-formal-epistemic-system-grounded-in-truth-based-competition",{"id":32533,"data":32535,"filePath":32540,"digest":32541,"rendered":32542,"legacyId":32549},{"title":32536,"description":32537,"summary":32537,"pubDate":32538,"source":25191,"url":32539,"thumbnail":25193},"Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition","arXiv:2506.19191v1 Announce Type: new Abstract: We introduce a mathematically rigorous framework for an artificial intelligence system composed of probabilistic agents evolving through structured competition and belief revision. The architecture, grounded in Bayesian inference, measure theory, and population dynamics, defines agent fitness as a function of alignment with a fixed external oracle representing ground truth. Agents compete in a discrete-time environment, adjusting posterior beliefs through observed outcomes, with higher-rated agents reproducing and lower-rated agents undergoing extinction. Ratings are updated via pairwise truth-aligned utility comparisons, and belief updates preserve measurable consistency and stochastic convergence. We introduce hash-based cryptographic identity commitments to ensure traceability, alongside causal inference operators using do-calculus. Formal theorems on convergence, robustness, and evolutionary stability are provided. The system establishes truth as an evolutionary attractor, demonstrating that verifiable knowledge arises from adversarial epistemic pressure within a computable, self-regulating swarm.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19191","src/content/posts/2025-06-25-bayesian-evolutionary-swarm-architecture-a-formal-epistemic-system-grounded-in-truth-based-competition.md","d7646439c202381d",{"html":25,"metadata":32543},{"headings":32544,"localImagePaths":32545,"remoteImagePaths":32546,"frontmatter":32547,"imagePaths":32548},[],[],[],{"title":32536,"description":32537,"summary":32537,"pubDate":32084,"source":25191,"url":32539,"thumbnail":25193},[],"2025-06-25-bayesian-evolutionary-swarm-architecture-a-formal-epistemic-system-grounded-in-truth-based-competition.md","2025-06-25-benchmarking-the-pedagogical-knowledge-of-large-language-models",{"id":32550,"data":32552,"filePath":32557,"digest":32558,"rendered":32559,"legacyId":32566},{"title":32553,"description":32554,"summary":32554,"pubDate":32555,"source":25191,"url":32556,"thumbnail":25193},"Benchmarking the Pedagogical Knowledge of Large Language Models","arXiv:2506.18710v2 Announce Type: replace-cross Abstract: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https://rebrand.ly/pedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18710","src/content/posts/2025-06-25-benchmarking-the-pedagogical-knowledge-of-large-language-models.md","8a89806522c128de",{"html":25,"metadata":32560},{"headings":32561,"localImagePaths":32562,"remoteImagePaths":32563,"frontmatter":32564,"imagePaths":32565},[],[],[],{"title":32553,"description":32554,"summary":32554,"pubDate":32084,"source":25191,"url":32556,"thumbnail":25193},[],"2025-06-25-benchmarking-the-pedagogical-knowledge-of-large-language-models.md","2025-06-25-can-ai-support-student-engagement-in-classroom-activities-in-higher-education",{"id":32567,"data":32569,"filePath":32574,"digest":32575,"rendered":32576,"legacyId":32583},{"title":32570,"description":32571,"summary":32571,"pubDate":32572,"source":25191,"url":32573,"thumbnail":25193},"Can AI support student engagement in classroom activities in higher education?","arXiv:2506.18941v1 Announce Type: cross Abstract: Lucrative career prospects and creative opportunities often attract students to enroll in computer science majors and pursue advanced studies in the field. Consequently, there has been a significant surge in enrollment in computer science courses, resulting in large class sizes that can range from hundreds to even thousands of students. A common challenge in such large classrooms is the lack of engagement between students and both the instructor and the learning material. However, with advancements in technology and improvements in large language models (LLMs), there is a considerable opportunity to utilize LLM-based AI models, such as conversational artificial intelligence (CAI), to enhance student engagement with learning content in large classes. To explore the potential of CAI to support engagement, especially with learning content, we designed an activity in a software Engineering course (with a large class size) where students used CAI for an in-class activity. We conducted a within-subject investigation in a large classroom at a US university where we compared student engagement during an in-class activity that used CAI tool vs. one without CAI tool. The CAI tool we used was ChatGPT due to its widespread popularity and familiarity. Our results indicate that CAI (ChatGPT) has the potential to support engagement with learning content during in-class activities, especially in large class sizes. We further discuss the implications of our findings.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18941","src/content/posts/2025-06-25-can-ai-support-student-engagement-in-classroom-activities-in-higher-education.md","e372efc8e24dbfc8",{"html":25,"metadata":32577},{"headings":32578,"localImagePaths":32579,"remoteImagePaths":32580,"frontmatter":32581,"imagePaths":32582},[],[],[],{"title":32570,"description":32571,"summary":32571,"pubDate":32084,"source":25191,"url":32573,"thumbnail":25193},[],"2025-06-25-can-ai-support-student-engagement-in-classroom-activities-in-higher-education.md","2025-06-25-can-large-language-models-capture-human-annotator-disagreements",{"id":32584,"data":32586,"filePath":32591,"digest":32592,"rendered":32593,"legacyId":32600},{"title":32587,"description":32588,"summary":32588,"pubDate":32589,"source":25191,"url":32590,"thumbnail":25193},"Can Large Language Models Capture Human Annotator Disagreements?","arXiv:2506.19467v1 Announce Type: cross Abstract: Human annotation variation (i.e., annotation disagreements) is common in NLP and often reflects important information such as task subjectivity and sample ambiguity. While Large Language Models (LLMs) are increasingly used for automatic annotation to reduce human effort, their evaluation often focuses on predicting the majority-voted 'ground truth' labels. It is still unclear, however, whether these models also capture informative human annotation variation. Our work addresses this gap by extensively evaluating LLMs' ability to predict annotation disagreements without access to repeated human labels. Our results show that LLMs struggle with modeling disagreements, which can be overlooked by majority label-based evaluations. Notably, while RLVR-style (Reinforcement learning with verifiable rewards) reasoning generally boosts LLM performance, it degrades performance in disagreement prediction. Our findings highlight the critical need for evaluating and improving LLM annotators in disagreement modeling. Code and data at https://github.com/EdisonNi-hku/Disagreement_Prediction.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19467","src/content/posts/2025-06-25-can-large-language-models-capture-human-annotator-disagreements.md","f7cc034b9b5e72f7",{"html":25,"metadata":32594},{"headings":32595,"localImagePaths":32596,"remoteImagePaths":32597,"frontmatter":32598,"imagePaths":32599},[],[],[],{"title":32587,"description":32588,"summary":32588,"pubDate":32084,"source":25191,"url":32590,"thumbnail":25193},[],"2025-06-25-can-large-language-models-capture-human-annotator-disagreements.md","2025-06-25-capturing-fine-grained-alignments-improves-3d-affordance-detection",{"id":32601,"data":32603,"filePath":32608,"digest":32609,"rendered":32610,"legacyId":32617},{"title":32604,"description":32605,"summary":32605,"pubDate":32606,"source":25191,"url":32607,"thumbnail":25193},"Capturing Fine-Grained Alignments Improves 3D Affordance Detection","arXiv:2506.19312v1 Announce Type: cross Abstract: In this work, we address the challenge of affordance detection in 3D point clouds, a task that requires effectively capturing fine-grained alignments between point clouds and text. Existing methods often struggle to model such alignments, resulting in limited performance on standard benchmarks. A key limitation of these approaches is their reliance on simple cosine similarity between point cloud and text embeddings, which lacks the expressiveness needed for fine-grained reasoning. To address this limitation, we propose LM-AD, a novel method for affordance detection in 3D point clouds. Moreover, we introduce the Affordance Query Module (AQM), which efficiently captures fine-grained alignment between point clouds and text by leveraging a pretrained language model. We demonstrated that our method outperformed existing approaches in terms of accuracy and mean Intersection over Union on the 3D AffordanceNet dataset.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19312","src/content/posts/2025-06-25-capturing-fine-grained-alignments-improves-3d-affordance-detection.md","97b4af223a93da86",{"html":25,"metadata":32611},{"headings":32612,"localImagePaths":32613,"remoteImagePaths":32614,"frontmatter":32615,"imagePaths":32616},[],[],[],{"title":32604,"description":32605,"summary":32605,"pubDate":32084,"source":25191,"url":32607,"thumbnail":25193},[],"2025-06-25-capturing-fine-grained-alignments-improves-3d-affordance-detection.md","2025-06-25-chatsr-multimodal-large-language-models-for-scientific-formula-discovery",{"id":32618,"data":32620,"filePath":32625,"digest":32626,"rendered":32627,"legacyId":32634},{"title":32621,"description":32622,"summary":32622,"pubDate":32623,"source":25191,"url":32624,"thumbnail":25193},"ChatSR: Multimodal Large Language Models for Scientific Formula Discovery","arXiv:2406.05410v2 Announce Type: replace Abstract: Formulas are the language of communication between humans and nature. The discovery of formulas to describe natural laws from observational data is the purpose of scientific research. It is also an important research topic in artificial intelligence, which is called a symbolic regression problem. Most of the existing symbolic regression methods generate expressions directly from observed data. Although in some methods, we can inject some prior knowledge into the model by adding constraints or introducing some special character hints. However, these methods can only introduce a limited amount of prior knowledge specified in advance. Not to mention understanding natural language instructions. In this article, based on the powerful knowledge reserve and language understanding ability of multi-modal large language models, we present ChatSR, which acts like a knowledgeable human scientist, and we can tell it any prior knowledge through natural language to guide it in formula generation. By testing on 13 datasets, ChatSR not only shows state-of-the-art performance on traditional symbolic regression tasks. More notably, ChatSR can well understand the prior knowledge contained in natural language prompts and improve the quality of generated expressions. In addition, it is exciting that ChatSR has a good zero-shot capability to understand prior knowledge that is not present in the training data.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2406.05410","src/content/posts/2025-06-25-chatsr-multimodal-large-language-models-for-scientific-formula-discovery.md","d29d594f70a1ca65",{"html":25,"metadata":32628},{"headings":32629,"localImagePaths":32630,"remoteImagePaths":32631,"frontmatter":32632,"imagePaths":32633},[],[],[],{"title":32621,"description":32622,"summary":32622,"pubDate":32084,"source":25191,"url":32624,"thumbnail":25193},[],"2025-06-25-chatsr-multimodal-large-language-models-for-scientific-formula-discovery.md","2025-06-25-chordprompt-orchestrating-cross-modal-prompt-synergy-for-multi-domain-incremental-learning-in-clip",{"id":32635,"data":32637,"filePath":32642,"digest":32643,"rendered":32644,"legacyId":32651},{"title":32638,"description":32639,"summary":32639,"pubDate":32640,"source":25191,"url":32641,"thumbnail":25193},"ChordPrompt: Orchestrating Cross-Modal Prompt Synergy for Multi-Domain Incremental Learning in CLIP","arXiv:2506.19608v1 Announce Type: new Abstract: Continual learning (CL) empowers pre-trained vision-language models to adapt effectively to novel or previously underrepresented data distributions without comprehensive retraining, enhancing their adaptability and efficiency. While vision-language models like CLIP show great promise, they struggle to maintain performance across domains in incremental learning scenarios. Existing prompt learning methods face two main limitations: 1) they primarily focus on class-incremental learning scenarios, lacking specific strategies for multi-domain task incremental learning; 2) most current approaches employ single-modal prompts, neglecting the potential benefits of cross-modal information exchange. To address these challenges, we propose the ChordPrompt framework, which facilitates a harmonious interplay between visual and textual prompts. ChordPrompt introduces cross-modal prompts to leverage interactions between visual and textual information. Our approach also employs domain-adaptive text prompts to select appropriate prompts for continual adaptation across multiple domains. Comprehensive experiments on multi-domain incremental learning benchmarks demonstrate that ChordPrompt outperforms state-of-the-art methods in zero-shot generalization and downstream task performance.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19608","src/content/posts/2025-06-25-chordprompt-orchestrating-cross-modal-prompt-synergy-for-multi-domain-incremental-learning-in-clip.md","866e33e8629c626b",{"html":25,"metadata":32645},{"headings":32646,"localImagePaths":32647,"remoteImagePaths":32648,"frontmatter":32649,"imagePaths":32650},[],[],[],{"title":32638,"description":32639,"summary":32639,"pubDate":32084,"source":25191,"url":32641,"thumbnail":25193},[],"2025-06-25-chordprompt-orchestrating-cross-modal-prompt-synergy-for-multi-domain-incremental-learning-in-clip.md","2025-06-25-citizenship-challenges-in-artificial-intelligence-education",{"id":32652,"data":32654,"filePath":32659,"digest":32660,"rendered":32661,"legacyId":32668},{"title":32655,"description":32656,"summary":32656,"pubDate":32657,"source":25191,"url":32658,"thumbnail":25193},"Citizenship Challenges in Artificial Intelligence Education","arXiv:2506.18955v1 Announce Type: cross Abstract: This chapter addresses the citizenship challenges related to AI in education, particularly concerning students, teachers, and other educational stakeholders in the context of AI integration. We first explore how to foster AI awareness and education, along with various strategies to promote a socio-critical approach to AI training, aiming to identify relevant and ethical uses to prioritise. In the second part, we discuss critical thinking and computational thinking skills that can be mobilised within certain AI-supported educational activities, depending on the degree of creative and transformative engagement those activities require.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18955","src/content/posts/2025-06-25-citizenship-challenges-in-artificial-intelligence-education.md","d9b4c3dd43341fb9",{"html":25,"metadata":32662},{"headings":32663,"localImagePaths":32664,"remoteImagePaths":32665,"frontmatter":32666,"imagePaths":32667},[],[],[],{"title":32655,"description":32656,"summary":32656,"pubDate":32084,"source":25191,"url":32658,"thumbnail":25193},[],"2025-06-25-citizenship-challenges-in-artificial-intelligence-education.md","2025-06-25-climateiqa-a-new-dataset-and-benchmark-to-advance-vision-language-models-in-meteorology-anomalies-analysis",{"id":32669,"data":32671,"filePath":32676,"digest":32677,"rendered":32678,"legacyId":32685},{"title":32672,"description":32673,"summary":32673,"pubDate":32674,"source":25191,"url":32675,"thumbnail":25193},"ClimateIQA: A New Dataset and Benchmark to Advance Vision-Language Models in Meteorology Anomalies Analysis","arXiv:2406.09838v2 Announce Type: replace-cross Abstract: Meteorological heatmaps play a vital role in deciphering extreme weather phenomena, yet their inherent complexities marked by irregular contours, unstructured patterns, and complex color variations present unique analytical hurdles for state-of-the-art Vision-Language Models (VLMs). Current state-of-the-art models like GPT-4o, Qwen-VL, and LLaVA 1.6 struggle with tasks such as precise color identification and spatial localization, resulting in inaccurate or incomplete interpretations. To address these challenges, we introduce Sparse Position and Outline Tracking (SPOT), a novel algorithm specifically designed to process irregularly shaped colored regions in visual data. SPOT identifies and localizes these regions by extracting their spatial coordinates, enabling structured representations of irregular shapes. Building on SPOT, we construct ClimateIQA, a novel meteorological visual question answering (VQA) dataset, comprising 26,280 high-resolution heatmaps and 762,120 instruction samples for wind gust, total precipitation, wind chill index and heat index analysis. ClimateIQA enhances VLM training by incorporating spatial cues, geographic metadata, and reanalysis data, improving model accuracy in interpreting and describing extreme weather features. Furthermore, we develop Climate-Zoo, a suite of fine-tuned VLMs based on SPOT-empowered ClimateIQA, which significantly outperforms existing models in meteorological heatmap tasks.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2406.09838","src/content/posts/2025-06-25-climateiqa-a-new-dataset-and-benchmark-to-advance-vision-language-models-in-meteorology-anomalies-analysis.md","1562add878211cef",{"html":25,"metadata":32679},{"headings":32680,"localImagePaths":32681,"remoteImagePaths":32682,"frontmatter":32683,"imagePaths":32684},[],[],[],{"title":32672,"description":32673,"summary":32673,"pubDate":32084,"source":25191,"url":32675,"thumbnail":25193},[],"2025-06-25-climateiqa-a-new-dataset-and-benchmark-to-advance-vision-language-models-in-meteorology-anomalies-analysis.md","2025-06-25-commander-gpt-dividing-and-routing-for-multimodal-sarcasm-detection",{"id":32686,"data":32688,"filePath":32693,"digest":32694,"rendered":32695,"legacyId":32702},{"title":32689,"description":32690,"summary":32690,"pubDate":32691,"source":25191,"url":32692,"thumbnail":25193},"Commander-GPT: Dividing and Routing for Multimodal Sarcasm Detection","arXiv:2506.19420v1 Announce Type: new Abstract: Multimodal sarcasm understanding is a high-order cognitive task. Although large language models (LLMs) have shown impressive performance on many downstream NLP tasks, growing evidence suggests that they struggle with sarcasm understanding. In this paper, we propose Commander-GPT, a modular decision routing framework inspired by military command theory. Rather than relying on a single LLM's capability, Commander-GPT orchestrates a team of specialized LLM agents where each agent will be selectively assigned to a focused sub-task such as context modeling, sentiment analysis, etc. Their outputs are then routed back to the commander, which integrates the information and performs the final sarcasm judgment. To coordinate these agents, we introduce three types of centralized commanders: (1) a trained lightweight encoder-based commander (e.g., multi-modal BERT); (2) four small autoregressive language models, serving as moderately capable commanders (e.g., DeepSeek-VL); (3) two large LLM-based commander (Gemini Pro and GPT-4o) that performs task routing, output aggregation, and sarcasm decision-making in a zero-shot fashion. We evaluate Commander-GPT on the MMSD and MMSD 2.0 benchmarks, comparing five prompting strategies. Experimental results show that our framework achieves 4.4% and 11.7% improvement in F1 score over state-of-the-art (SoTA) baselines on average, demonstrating its effectiveness.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19420","src/content/posts/2025-06-25-commander-gpt-dividing-and-routing-for-multimodal-sarcasm-detection.md","04457f59cbd028a1",{"html":25,"metadata":32696},{"headings":32697,"localImagePaths":32698,"remoteImagePaths":32699,"frontmatter":32700,"imagePaths":32701},[],[],[],{"title":32689,"description":32690,"summary":32690,"pubDate":32084,"source":25191,"url":32692,"thumbnail":25193},[],"2025-06-25-commander-gpt-dividing-and-routing-for-multimodal-sarcasm-detection.md","2025-06-25-concisehint-boosting-efficient-reasoning-via-continuous-concise-hints-during-generation",{"id":32703,"data":32705,"filePath":32710,"digest":32711,"rendered":32712,"legacyId":32719},{"title":32706,"description":32707,"summary":32707,"pubDate":32708,"source":25191,"url":32709,"thumbnail":25193},"ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation","arXiv:2506.18810v2 Announce Type: replace Abstract: Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and OpenAI o1 series have achieved notable performance enhancements on complex reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT). However, an emerging issue is their inclination to produce excessively verbose reasoning processes, leading to the inefficiency problem. Existing literature on improving efficiency mainly adheres to the before-reasoning paradigms such as prompting and reasoning or fine-tuning and reasoning, but ignores the promising direction of directly encouraging the model to speak concisely by intervening during the generation of reasoning. In order to fill the blank, we propose a framework dubbed ConciseHint, which continuously encourages the reasoning model to speak concisely by injecting the textual hint (manually designed or trained on the concise data) during the token generation of the reasoning process. Besides, ConciseHint is adaptive to the complexity of the query by adaptively adjusting the hint intensity, which ensures it will not undermine model performance. Experiments on the state-of-the-art LRMs, including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can effectively produce concise reasoning processes while maintaining performance well. For instance, we achieve a reduction ratio of 65% for the reasoning length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18810","src/content/posts/2025-06-25-concisehint-boosting-efficient-reasoning-via-continuous-concise-hints-during-generation.md","b6f24edd605ad2cc",{"html":25,"metadata":32713},{"headings":32714,"localImagePaths":32715,"remoteImagePaths":32716,"frontmatter":32717,"imagePaths":32718},[],[],[],{"title":32706,"description":32707,"summary":32707,"pubDate":32084,"source":25191,"url":32709,"thumbnail":25193},[],"2025-06-25-concisehint-boosting-efficient-reasoning-via-continuous-concise-hints-during-generation.md","2025-06-25-connecting-vision-and-emissions-a-behavioural-ai-approach-to-carbon-estimation-in-road-design",{"id":32720,"data":32722,"filePath":32727,"digest":32728,"rendered":32729,"legacyId":32736},{"title":32723,"description":32724,"summary":32724,"pubDate":32725,"source":25191,"url":32726,"thumbnail":25193},"Connecting Vision and Emissions: A Behavioural AI Approach to Carbon Estimation in Road Design","arXiv:2506.18924v1 Announce Type: cross Abstract: We present an enhanced YOLOv8 real time vehicle detection and classification framework, for estimating carbon emissions in urban environments. The system enhances YOLOv8 architecture to detect, segment, and track vehicles from live traffic video streams. Once a vehicle is localized, a dedicated deep learning-based identification module is employed to recognize license plates and classify vehicle types. Since YOLOv8 lacks the built-in capacity for fine grained recognition tasks such as reading license plates or determining vehicle attributes beyond class labels, our framework incorporates a hybrid pipeline where each detected vehicle is tracked and its bounding box is cropped and passed to a deep Optical Character Recognition (OCR) module. This OCR system, composed of multiple convolutional neural network (CNN) layers, is trained specifically for character-level detection and license plate decoding under varied conditions such as motion blur, occlusion, and diverse font styles. Additionally, the recognized plate information is validated using a real time API that cross references with an external vehicle registration database to ensure accurate classification and emission estimation. This multi-stage approach enables precise, automated calculation of per vehicle carbon emissions. Extensive evaluation was conducted using a diverse vehicle dataset enriched with segmentation masks and annotated license plates. The YOLOv8 detector achieved a mean Average Precision (mAP@0.5) of approximately 71% for bounding boxes and 70% for segmentation masks. Character level OCR accuracy reached up to 99% with the best performing CNN model. These results affirm the feasibility of combining real time object detection with deep OCR for practical deployment in smart transportation systems, offering a scalable solution for automated, vehicle specific carbon emission monitoring.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18924","src/content/posts/2025-06-25-connecting-vision-and-emissions-a-behavioural-ai-approach-to-carbon-estimation-in-road-design.md","f6cc1fd1929944b2",{"html":25,"metadata":32730},{"headings":32731,"localImagePaths":32732,"remoteImagePaths":32733,"frontmatter":32734,"imagePaths":32735},[],[],[],{"title":32723,"description":32724,"summary":32724,"pubDate":32084,"source":25191,"url":32726,"thumbnail":25193},[],"2025-06-25-connecting-vision-and-emissions-a-behavioural-ai-approach-to-carbon-estimation-in-road-design.md","2025-06-25-contactdexnet-multi-fingered-robotic-hand-grasping-in-cluttered-environments-through-hand-object-contact-semantic-mapping",{"id":32737,"data":32739,"filePath":32744,"digest":32745,"rendered":32746,"legacyId":32753},{"title":32740,"description":32741,"summary":32741,"pubDate":32742,"source":25191,"url":32743,"thumbnail":25193},"ContactDexNet: Multi-fingered Robotic Hand Grasping in Cluttered Environments through Hand-object Contact Semantic Mapping","arXiv:2404.08844v3 Announce Type: replace-cross Abstract: The deep learning models has significantly advanced dexterous manipulation techniques for multi-fingered hand grasping. However, the contact information-guided grasping in cluttered environments remains largely underexplored. To address this gap, we have developed a method for generating multi-fingered hand grasp samples in cluttered settings through contact semantic map. We introduce a contact semantic conditional variational autoencoder network (CoSe-CVAE) for creating comprehensive contact semantic map from object point cloud. We utilize grasp detection method to estimate hand grasp poses from the contact semantic map. Finally, an unified grasp evaluation model PointNetGPD++ is designed to assess grasp quality and collision probability, substantially improving the reliability of identifying optimal grasps in cluttered scenarios. Our grasp generation method has demonstrated remarkable success, outperforming state-of-the-art methods by at least 4.65% with 81.0% average grasping success rate in real-world single-object environment and 75.3% grasping success rate in cluttered scenes. We also proposed the multi-modal multi-fingered grasping dataset generation method. Our multi-fingered hand grasping dataset outperforms previous datasets in scene diversity, modality diversity. The dataset, code and supplementary materials can be found at https://sites.google.com/view/contact-dexnet.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2404.08844","src/content/posts/2025-06-25-contactdexnet-multi-fingered-robotic-hand-grasping-in-cluttered-environments-through-hand-object-contact-semantic-mapping.md","e8a6610b2bac1d3f",{"html":25,"metadata":32747},{"headings":32748,"localImagePaths":32749,"remoteImagePaths":32750,"frontmatter":32751,"imagePaths":32752},[],[],[],{"title":32740,"description":32741,"summary":32741,"pubDate":32084,"source":25191,"url":32743,"thumbnail":25193},[],"2025-06-25-contactdexnet-multi-fingered-robotic-hand-grasping-in-cluttered-environments-through-hand-object-contact-semantic-mapping.md","2025-06-25-controllable-video-generation-with-provable-disentanglement",{"id":32754,"data":32756,"filePath":32761,"digest":32762,"rendered":32763,"legacyId":32770},{"title":32757,"description":32758,"summary":32758,"pubDate":32759,"source":25191,"url":32760,"thumbnail":25193},"Controllable Video Generation with Provable Disentanglement","arXiv:2502.02690v2 Announce Type: replace-cross Abstract: Controllable video generation remains a significant challenge, despite recent advances in generating high-quality and consistent videos. Most existing methods for controlling video generation treat the video as a whole, neglecting intricate fine-grained spatiotemporal relationships, which limits both control precision and efficiency. In this paper, we propose Controllable Video Generative Adversarial Networks (CoVoGAN) to disentangle the video concepts, thus facilitating efficient and independent control over individual concepts. Specifically, following the minimal change principle, we first disentangle static and dynamic latent variables. We then leverage the sufficient change property to achieve component-wise identifiability of dynamic latent variables, enabling disentangled control of video generation. To establish the theoretical foundation, we provide a rigorous analysis demonstrating the identifiability of our approach. Building on these theoretical insights, we design a Temporal Transition Module to disentangle latent dynamics. To enforce the minimal change principle and sufficient change property, we minimize the dimensionality of latent dynamic variables and impose temporal conditional independence. To validate our approach, we integrate this module as a plug-in for GANs. Extensive qualitative and quantitative experiments on various video generation benchmarks demonstrate that our method significantly improves generation quality and controllability across diverse real-world scenarios.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2502.02690","src/content/posts/2025-06-25-controllable-video-generation-with-provable-disentanglement.md","85b5595067981be0",{"html":25,"metadata":32764},{"headings":32765,"localImagePaths":32766,"remoteImagePaths":32767,"frontmatter":32768,"imagePaths":32769},[],[],[],{"title":32757,"description":32758,"summary":32758,"pubDate":32084,"source":25191,"url":32760,"thumbnail":25193},[],"2025-06-25-controllable-video-generation-with-provable-disentanglement.md","2025-06-25-conversational-intent-driven-graphrag-enhancing-multi-turn-dialogue-systems-through-adaptive-dual-retrieval-of-flow-patterns-and-context-semantics",{"id":32771,"data":32773,"filePath":32778,"digest":32779,"rendered":32780,"legacyId":32787},{"title":32774,"description":32775,"summary":32775,"pubDate":32776,"source":25191,"url":32777,"thumbnail":25193},"Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue Systems through Adaptive Dual-Retrieval of Flow Patterns and Context Semantics","arXiv:2506.19385v1 Announce Type: new Abstract: We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval Augmented Generation), a novel framework that addresses the limitations of existing dialogue systems in maintaining both contextual coherence and goal-oriented progression in multi-turn customer service conversations. Unlike traditional RAG systems that rely solely on semantic similarity (Conversation RAG) or standard knowledge graphs (GraphRAG), CID-GraphRAG constructs dynamic intent transition graphs from goal achieved historical dialogues and implements a dual-retrieval mechanism that adaptively balances intent-based graph traversal with semantic search. This approach enables the system to simultaneously leverage both conversional intent flow patterns and contextual semantics, significantly improving retrieval quality and response quality. In extensive experiments on real-world customer service dialogues, we employ both automatic metrics and LLM-as-judge assessments, demonstrating that CID-GraphRAG significantly outperforms both semantic-based Conversation RAG and intent-based GraphRAG baselines across all evaluation criteria. Quantitatively, CID-GraphRAG demonstrates substantial improvements over Conversation RAG across automatic metrics, with relative gains of 11% in BLEU, 5% in ROUGE-L, 6% in METEOR, and most notably, a 58% improvement in response quality according to LLM-as-judge evaluations. These results demonstrate that the integration of intent transition structures with semantic retrieval creates a synergistic effect that neither approach achieves independently, establishing CID-GraphRAG as an effective framework for addressing the challenges of maintaining contextual coherence and goal-oriented progression in knowledge-intensive multi-turn dialogues.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19385","src/content/posts/2025-06-25-conversational-intent-driven-graphrag-enhancing-multi-turn-dialogue-systems-through-adaptive-dual-retrieval-of-flow-patterns-and-context-semantics.md","8cc1c38bfc69b463",{"html":25,"metadata":32781},{"headings":32782,"localImagePaths":32783,"remoteImagePaths":32784,"frontmatter":32785,"imagePaths":32786},[],[],[],{"title":32774,"description":32775,"summary":32775,"pubDate":32084,"source":25191,"url":32777,"thumbnail":25193},[],"2025-06-25-conversational-intent-driven-graphrag-enhancing-multi-turn-dialogue-systems-through-adaptive-dual-retrieval-of-flow-patterns-and-context-semantics.md","2025-06-25-cross-regularization-adaptive-model-complexity-through-validation-gradients",{"id":32788,"data":32790,"filePath":32795,"digest":32796,"rendered":32797,"legacyId":32804},{"title":32791,"description":32792,"summary":32792,"pubDate":32793,"source":25191,"url":32794,"thumbnail":25193},"Cross-regularization: Adaptive Model Complexity through Validation Gradients","arXiv:2506.19755v1 Announce Type: cross Abstract: Model regularization requires extensive manual tuning to balance complexity against overfitting. Cross-regularization resolves this tradeoff by directly adapting regularization parameters through validation gradients during training. The method splits parameter optimization - training data guides feature learning while validation data shapes complexity controls - converging provably to cross-validation optima. When implemented through noise injection in neural networks, this approach reveals striking patterns: unexpectedly high noise tolerance and architecture-specific regularization that emerges organically during training. Beyond complexity control, the framework integrates seamlessly with data augmentation, uncertainty calibration and growing datasets while maintaining single-run efficiency through a simple gradient-based approach.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19755","src/content/posts/2025-06-25-cross-regularization-adaptive-model-complexity-through-validation-gradients.md","0780775573f5d614",{"html":25,"metadata":32798},{"headings":32799,"localImagePaths":32800,"remoteImagePaths":32801,"frontmatter":32802,"imagePaths":32803},[],[],[],{"title":32791,"description":32792,"summary":32792,"pubDate":32084,"source":25191,"url":32794,"thumbnail":25193},[],"2025-06-25-cross-regularization-adaptive-model-complexity-through-validation-gradients.md","2025-06-25-cupid-curating-data-your-robot-loves-with-influence-functions",{"id":32805,"data":32807,"filePath":32812,"digest":32813,"rendered":32814,"legacyId":32821},{"title":32808,"description":32809,"summary":32809,"pubDate":32810,"source":25191,"url":32811,"thumbnail":25193},"CUPID: Curating Data your Robot Loves with Influence Functions","arXiv:2506.19121v1 Announce Type: cross Abstract: In robot imitation learning, policy performance is tightly coupled with the quality and composition of the demonstration data. Yet, developing a precise understanding of how individual demonstrations contribute to downstream outcomes - such as closed-loop task success or failure - remains a persistent challenge. We propose CUPID, a robot data curation method based on a novel influence function-theoretic formulation for imitation learning policies. Given a set of evaluation rollouts, CUPID estimates the influence of each training demonstration on the policy's expected return. This enables ranking and selection of demonstrations according to their impact on the policy's closed-loop performance. We use CUPID to curate data by 1) filtering out training demonstrations that harm policy performance and 2) subselecting newly collected trajectories that will most improve the policy. Extensive simulated and hardware experiments show that our approach consistently identifies which data drives test-time performance. For example, training with less than 33% of curated data can yield state-of-the-art diffusion policies on the simulated RoboMimic benchmark, with similar gains observed in hardware. Furthermore, hardware experiments show that our method can identify robust strategies under distribution shift, isolate spurious correlations, and even enhance the post-training of generalist robot policies. Additional materials are made available at: https://cupid-curation.github.io.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19121","src/content/posts/2025-06-25-cupid-curating-data-your-robot-loves-with-influence-functions.md","284d36fb740276ca",{"html":25,"metadata":32815},{"headings":32816,"localImagePaths":32817,"remoteImagePaths":32818,"frontmatter":32819,"imagePaths":32820},[],[],[],{"title":32808,"description":32809,"summary":32809,"pubDate":32084,"source":25191,"url":32811,"thumbnail":25193},[],"2025-06-25-cupid-curating-data-your-robot-loves-with-influence-functions.md","2025-06-25-cuvslam-cuda-accelerated-visual-odometry-and-mapping",{"id":32822,"data":32824,"filePath":32829,"digest":32830,"rendered":32831,"legacyId":32838},{"title":32825,"description":32826,"summary":32826,"pubDate":32827,"source":25191,"url":32828,"thumbnail":25193},"cuVSLAM: CUDA accelerated visual odometry and mapping","arXiv:2506.04359v2 Announce Type: replace-cross Abstract: Accurate and robust pose estimation is a key requirement for any autonomous robot. We present cuVSLAM, a state-of-the-art solution for visual simultaneous localization and mapping, which can operate with a variety of visual-inertial sensor suites, including multiple RGB and depth cameras, and inertial measurement units. cuVSLAM supports operation with as few as one RGB camera to as many as 32 cameras, in arbitrary geometric configurations, thus supporting a wide range of robotic setups. cuVSLAM is specifically optimized using CUDA to deploy in real-time applications with minimal computational overhead on edge-computing devices such as the NVIDIA Jetson. We present the design and implementation of cuVSLAM, example use cases, and empirical results on several state-of-the-art benchmarks demonstrating the best-in-class performance of cuVSLAM.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.04359","src/content/posts/2025-06-25-cuvslam-cuda-accelerated-visual-odometry-and-mapping.md","521047f9764a3fc2",{"html":25,"metadata":32832},{"headings":32833,"localImagePaths":32834,"remoteImagePaths":32835,"frontmatter":32836,"imagePaths":32837},[],[],[],{"title":32825,"description":32826,"summary":32826,"pubDate":32084,"source":25191,"url":32828,"thumbnail":25193},[],"2025-06-25-cuvslam-cuda-accelerated-visual-odometry-and-mapping.md","2025-06-25-cve-bench-a-benchmark-for-ai-agents-ability-to-exploit-real-world-web-application-vulnerabilities",{"id":32839,"data":32841,"filePath":32846,"digest":32847,"rendered":32848,"legacyId":32855},{"title":32842,"description":32843,"summary":32843,"pubDate":32844,"source":25191,"url":32845,"thumbnail":25193},"CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities","arXiv:2503.17332v4 Announce Type: replace-cross Abstract: Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture the Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities involves both specialized expertise to reproduce exploits and a systematic approach to evaluating unpredictable threats. To address this challenge, we introduce CVE-Bench, a real-world cybersecurity benchmark based on critical-severity Common Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents to exploit vulnerable web applications in scenarios that mimic real-world conditions, while also providing effective evaluation of their exploits. Our evaluation shows that the state-of-the-art agent framework can resolve up to 13% of vulnerabilities.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2503.17332","src/content/posts/2025-06-25-cve-bench-a-benchmark-for-ai-agents-ability-to-exploit-real-world-web-application-vulnerabilities.md","9c6fd0595ccddb9f",{"html":25,"metadata":32849},{"headings":32850,"localImagePaths":32851,"remoteImagePaths":32852,"frontmatter":32853,"imagePaths":32854},[],[],[],{"title":32842,"description":32843,"summary":32843,"pubDate":32084,"source":25191,"url":32845,"thumbnail":25193},[],"2025-06-25-cve-bench-a-benchmark-for-ai-agents-ability-to-exploit-real-world-web-application-vulnerabilities.md","2025-06-25-damba-st-domain-adaptive-mamba-for-efficient-urban-spatio-temporal-prediction",{"id":32856,"data":32858,"filePath":32863,"digest":32864,"rendered":32865,"legacyId":32872},{"title":32859,"description":32860,"summary":32860,"pubDate":32861,"source":25191,"url":32862,"thumbnail":25193},"Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction","arXiv:2506.18939v1 Announce Type: cross Abstract: Training urban spatio-temporal foundation models that generalize well across diverse regions and cities is critical for deploying urban services in unseen or data-scarce regions. Recent studies have typically focused on fusing cross-domain spatio-temporal data to train unified Transformer-based models. However, these models suffer from quadratic computational complexity and high memory overhead, limiting their scalability and practical deployment. Inspired by the efficiency of Mamba, a state space model with linear time complexity, we explore its potential for efficient urban spatio-temporal prediction. However, directly applying Mamba as a spatio-temporal backbone leads to negative transfer and severe performance degradation. This is primarily due to spatio-temporal heterogeneity and the recursive mechanism of Mamba's hidden state updates, which limit cross-domain generalization. To overcome these challenges, we propose Damba-ST, a novel domain-adaptive Mamba-based model for efficient urban spatio-temporal prediction. Damba-ST retains Mamba's linear complexity advantage while significantly enhancing its adaptability to heterogeneous domains. Specifically, we introduce two core innovations: (1) a domain-adaptive state space model that partitions the latent representation space into a shared subspace for learning cross-domain commonalities and independent, domain-specific subspaces for capturing intra-domain discriminative features; (2) three distinct Domain Adapters, which serve as domain-aware proxies to bridge disparate domain distributions and facilitate the alignment of cross-domain commonalities. Extensive experiments demonstrate the generalization and efficiency of Damba-ST. It achieves state-of-the-art performance on prediction tasks and demonstrates strong zero-shot generalization, enabling seamless deployment in new urban environments without extensive retraining or fine-tuning.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18939","src/content/posts/2025-06-25-damba-st-domain-adaptive-mamba-for-efficient-urban-spatio-temporal-prediction.md","813920fd8e4d33a0",{"html":25,"metadata":32866},{"headings":32867,"localImagePaths":32868,"remoteImagePaths":32869,"frontmatter":32870,"imagePaths":32871},[],[],[],{"title":32859,"description":32860,"summary":32860,"pubDate":32084,"source":25191,"url":32862,"thumbnail":25193},[],"2025-06-25-damba-st-domain-adaptive-mamba-for-efficient-urban-spatio-temporal-prediction.md","2025-06-25-damo-a-data-efficient-multimodal-orchestrator-for-temporal-reasoning-with-video-llms",{"id":32873,"data":32875,"filePath":32880,"digest":32881,"rendered":32882,"legacyId":32889},{"title":32876,"description":32877,"summary":32877,"pubDate":32878,"source":25191,"url":32879,"thumbnail":25193},"DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs","arXiv:2506.11558v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with GPT-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.11558","src/content/posts/2025-06-25-damo-a-data-efficient-multimodal-orchestrator-for-temporal-reasoning-with-video-llms.md","d1e7aed5e8e6a8f4",{"html":25,"metadata":32883},{"headings":32884,"localImagePaths":32885,"remoteImagePaths":32886,"frontmatter":32887,"imagePaths":32888},[],[],[],{"title":32876,"description":32877,"summary":32877,"pubDate":32084,"source":25191,"url":32879,"thumbnail":25193},[],"2025-06-25-damo-a-data-efficient-multimodal-orchestrator-for-temporal-reasoning-with-video-llms.md","2025-06-25-defeating-prompt-injections-by-design",{"id":32890,"data":32892,"filePath":32897,"digest":32898,"rendered":32899,"legacyId":32906},{"title":32893,"description":32894,"summary":32894,"pubDate":32895,"source":25191,"url":32896,"thumbnail":25193},"Defeating Prompt Injections by Design","arXiv:2503.18813v2 Announce Type: replace-cross Abstract: Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an untrusted environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models are susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL uses a notion of a capability to prevent the exfiltration of private data over unauthorized data flows by enforcing security policies when tools are called. We demonstrate effectiveness of CaMeL by solving $77%$ of tasks with provable security (compared to $84%$ with an undefended system) in AgentDojo. We release CaMeL at https://github.com/google-research/camel-prompt-injection.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2503.18813","src/content/posts/2025-06-25-defeating-prompt-injections-by-design.md","ada0f61efc54941b",{"html":25,"metadata":32900},{"headings":32901,"localImagePaths":32902,"remoteImagePaths":32903,"frontmatter":32904,"imagePaths":32905},[],[],[],{"title":32893,"description":32894,"summary":32894,"pubDate":32084,"source":25191,"url":32896,"thumbnail":25193},[],"2025-06-25-defeating-prompt-injections-by-design.md","2025-06-25-deltaspace-a-semantic-aligned-feature-space-for-flexible-text-guided-image-editing",{"id":32907,"data":32909,"filePath":32914,"digest":32915,"rendered":32916,"legacyId":32923},{"title":32910,"description":32911,"summary":32911,"pubDate":32912,"source":25191,"url":32913,"thumbnail":25193},"DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing","arXiv:2310.08785v2 Announce Type: replace-cross Abstract: Text-guided image editing faces significant challenges when considering training and inference flexibility. Much literature collects large amounts of annotated image-text pairs to train text-conditioned generative models from scratch, which is expensive and not efficient. After that, some approaches that leverage pre-trained vision-language models have been proposed to avoid data collection, but they are limited by either per text-prompt optimization or inference-time hyper-parameters tuning. To address these issues, we investigate and identify a specific space, referred to as CLIP DeltaSpace, where the CLIP visual feature difference of two images is semantically aligned with the CLIP textual feature difference of their corresponding text descriptions. Based on DeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP visual feature differences to the latent space directions of a generative model during the training phase, and predicts the latent space directions from the CLIP textual feature differences during the inference phase. And this design endows DeltaEdit with two advantages: (1) text-free training; (2) generalization to various text prompts for zero-shot inference. Extensive experiments validate the effectiveness and versatility of DeltaEdit with different generative models, including both the GAN model and the diffusion model, in achieving flexible text-guided image editing. Code is available at https://github.com/Yueming6568/DeltaEdit.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2310.08785","src/content/posts/2025-06-25-deltaspace-a-semantic-aligned-feature-space-for-flexible-text-guided-image-editing.md","889e50191cabe140",{"html":25,"metadata":32917},{"headings":32918,"localImagePaths":32919,"remoteImagePaths":32920,"frontmatter":32921,"imagePaths":32922},[],[],[],{"title":32910,"description":32911,"summary":32911,"pubDate":32084,"source":25191,"url":32913,"thumbnail":25193},[],"2025-06-25-deltaspace-a-semantic-aligned-feature-space-for-flexible-text-guided-image-editing.md","2025-06-25-detecting-machine-generated-texts-not-just-ai-vs-humans-and-explainability-is-complicated",{"id":32924,"data":32926,"filePath":32931,"digest":32932,"rendered":32933,"legacyId":32940},{"title":32927,"description":32928,"summary":32928,"pubDate":32929,"source":25191,"url":32930,"thumbnail":25193},"Detecting Machine-Generated Texts: Not Just 'AI vs Humans' and Explainability is Complicated","arXiv:2406.18259v2 Announce Type: replace-cross Abstract: As LLMs rapidly advance, increasing concerns arise regarding risks about actual authorship of texts we see online and in real world. The task of distinguishing LLM-authored texts is complicated by the nuanced and overlapping behaviors of both machines and humans. In this paper, we challenge the current practice of considering LLM-generated text detection a binary classification task of differentiating human from AI. Instead, we introduce a novel ternary text classification scheme, adding an 'undecided' category for texts that could be attributed to either source, and we show that this new category is crucial to understand how to make the detection result more explainable to lay users. This research shifts the paradigm from merely classifying to explaining machine-generated texts, emphasizing need for detectors to provide clear and understandable explanations to users. Our study involves creating four new datasets comprised of texts from various LLMs and human authors. Based on new datasets, we performed binary classification tests to ascertain the most effective SOTA detection methods and identified SOTA LLMs capable of producing harder-to-detect texts. We constructed a new dataset of texts generated by two top-performing LLMs and human authors, and asked three human annotators to produce ternary labels with explanation notes. This dataset was used to investigate how three top-performing SOTA detectors behave in new ternary classification context. Our results highlight why 'undecided' category is much needed from the viewpoint of explainability. Additionally, we conducted an analysis of explainability of the three best-performing detectors and the explanation notes of the human annotators, revealing insights about the complexity of explainable detection of machine-generated texts. Finally, we propose guidelines for developing future detection systems with improved explanatory power.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2406.18259","src/content/posts/2025-06-25-detecting-machine-generated-texts-not-just-ai-vs-humans-and-explainability-is-complicated.md","c5a59c470a56f14d",{"html":25,"metadata":32934},{"headings":32935,"localImagePaths":32936,"remoteImagePaths":32937,"frontmatter":32938,"imagePaths":32939},[],[],[],{"title":32927,"description":32928,"summary":32928,"pubDate":32084,"source":25191,"url":32930,"thumbnail":25193},[],"2025-06-25-detecting-machine-generated-texts-not-just-ai-vs-humans-and-explainability-is-complicated.md","2025-06-25-df2-distribution-free-decision-focused-learning",{"id":32941,"data":32943,"filePath":32948,"digest":32949,"rendered":32950,"legacyId":32957},{"title":32944,"description":32945,"summary":32945,"pubDate":32946,"source":25191,"url":32947,"thumbnail":25193},"DF2: Distribution-Free Decision-Focused Learning","arXiv:2308.05889v2 Announce Type: replace-cross Abstract: Decision-focused learning (DFL), which differentiates through the KKT conditions, has recently emerged as a powerful approach for predict-then-optimize problems. However, under probabilistic settings, DFL faces three major bottlenecks: model mismatch error, sample average approximation error, and gradient approximation error. Model mismatch error stems from the misalignment between the model's parameterized predictive distribution and the true probability distribution. Sample average approximation error arises when using finite samples to approximate the expected optimization objective. Gradient approximation error occurs when the objectives are non-convex and KKT conditions cannot be directly applied. In this paper, we present DF2, the first distribution-free decision-focused learning method designed to mitigate these three bottlenecks. Rather than depending on a task-specific forecaster that requires precise model assumptions, our method directly learns the expected optimization function during training. To efficiently learn this function in a data-driven manner, we devise an attention-based model architecture inspired by the distribution-based parameterization of the expected objective. We evaluate DF2 on two synthetic problems and three real-world problems, demonstrating the effectiveness of DF2. Our code is available at: https://github.com/Lingkai-Kong/DF2.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2308.05889","src/content/posts/2025-06-25-df2-distribution-free-decision-focused-learning.md","ea5e3076f73c5c2b",{"html":25,"metadata":32951},{"headings":32952,"localImagePaths":32953,"remoteImagePaths":32954,"frontmatter":32955,"imagePaths":32956},[],[],[],{"title":32944,"description":32945,"summary":32945,"pubDate":32084,"source":25191,"url":32947,"thumbnail":25193},[],"2025-06-25-df2-distribution-free-decision-focused-learning.md","2025-06-25-dialogic-pedagogy-for-large-language-models-aligning-conversational-ai-with-proven-theories-of-learning",{"id":32958,"data":32960,"filePath":32965,"digest":32966,"rendered":32967,"legacyId":32974},{"title":32961,"description":32962,"summary":32962,"pubDate":32963,"source":25191,"url":32964,"thumbnail":25193},"Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning","arXiv:2506.19484v1 Announce Type: cross Abstract: Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. We synthesize existing literature on LLMs in education and theories of conversational and dialogic pedagogy - including Vygotsky's sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard's conversational framework - and examine how prompting strategies and retrieval-augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. We map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, we propose practical strategies to better align LLM interactions with sound pedagogy - for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. Our aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19484","src/content/posts/2025-06-25-dialogic-pedagogy-for-large-language-models-aligning-conversational-ai-with-proven-theories-of-learning.md","3d8c070686442ec9",{"html":25,"metadata":32968},{"headings":32969,"localImagePaths":32970,"remoteImagePaths":32971,"frontmatter":32972,"imagePaths":32973},[],[],[],{"title":32961,"description":32962,"summary":32962,"pubDate":32084,"source":25191,"url":32964,"thumbnail":25193},[],"2025-06-25-dialogic-pedagogy-for-large-language-models-aligning-conversational-ai-with-proven-theories-of-learning.md","2025-06-25-diffris-enhancing-referring-remote-sensing-image-segmentation-with-pre-trained-text-to-image-diffusion-models",{"id":32975,"data":32977,"filePath":32982,"digest":32983,"rendered":32984,"legacyId":32991},{"title":32978,"description":32979,"summary":32979,"pubDate":32980,"source":25191,"url":32981,"thumbnail":25193},"DiffRIS: Enhancing Referring Remote Sensing Image Segmentation with Pre-trained Text-to-Image Diffusion Models","arXiv:2506.18946v1 Announce Type: cross Abstract: Referring remote sensing image segmentation (RRSIS) enables the precise delineation of regions within remote sensing imagery through natural language descriptions, serving critical applications in disaster response, urban development, and environmental monitoring. Despite recent advances, current approaches face significant challenges in processing aerial imagery due to complex object characteristics including scale variations, diverse orientations, and semantic ambiguities inherent to the overhead perspective. To address these limitations, we propose DiffRIS, a novel framework that harnesses the semantic understanding capabilities of pre-trained text-to-image diffusion models for enhanced cross-modal alignment in RRSIS tasks. Our framework introduces two key innovations: a context perception adapter (CP-adapter) that dynamically refines linguistic features through global context modeling and object-aware reasoning, and a progressive cross-modal reasoning decoder (PCMRD) that iteratively aligns textual descriptions with visual regions for precise segmentation. The CP-adapter bridges the domain gap between general vision-language understanding and remote sensing applications, while PCMRD enables fine-grained semantic alignment through multi-scale feature interaction. Comprehensive experiments on three benchmark datasets-RRSIS-D, RefSegRS, and RISBench-demonstrate that DiffRIS consistently outperforms existing methods across all standard metrics, establishing a new state-of-the-art for RRSIS tasks. The significant performance improvements validate the effectiveness of leveraging pre-trained diffusion models for remote sensing applications through our proposed adaptive framework.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18946","src/content/posts/2025-06-25-diffris-enhancing-referring-remote-sensing-image-segmentation-with-pre-trained-text-to-image-diffusion-models.md","47697a5c2ed61e5d",{"html":25,"metadata":32985},{"headings":32986,"localImagePaths":32987,"remoteImagePaths":32988,"frontmatter":32989,"imagePaths":32990},[],[],[],{"title":32978,"description":32979,"summary":32979,"pubDate":32084,"source":25191,"url":32981,"thumbnail":25193},[],"2025-06-25-diffris-enhancing-referring-remote-sensing-image-segmentation-with-pre-trained-text-to-image-diffusion-models.md","2025-06-25-discrepancy-aware-graph-mask-auto-encoder",{"id":32992,"data":32994,"filePath":32999,"digest":33000,"rendered":33001,"legacyId":33008},{"title":32995,"description":32996,"summary":32996,"pubDate":32997,"source":25191,"url":32998,"thumbnail":25193},"Discrepancy-Aware Graph Mask Auto-Encoder","arXiv:2506.19343v1 Announce Type: cross Abstract: Masked Graph Auto-Encoder, a powerful graph self-supervised training paradigm, has recently shown superior performance in graph representation learning. Existing works typically rely on node contextual information to recover the masked information. However, they fail to generalize well to heterophilic graphs where connected nodes may be not similar, because they focus only on capturing the neighborhood information and ignoring the discrepancy information between different nodes, resulting in indistinguishable node representations. In this paper, to address this issue, we propose a Discrepancy-Aware Graph Mask Auto-Encoder (DGMAE). It obtains more distinguishable node representations by reconstructing the discrepancy information of neighboring nodes during the masking process. We conduct extensive experiments on 17 widely-used benchmark datasets. The results show that our DGMAE can effectively preserve the discrepancies of nodes in low-dimensional space. Moreover, DGMAE significantly outperforms state-of-the-art graph self-supervised learning methods on three graph analytic including tasks node classification, node clustering, and graph classification, demonstrating its remarkable superiority. The code of DGMAE is available at https://github.com/zhengziyu77/DGMAE.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19343","src/content/posts/2025-06-25-discrepancy-aware-graph-mask-auto-encoder.md","f6bc83dff5344076",{"html":25,"metadata":33002},{"headings":33003,"localImagePaths":33004,"remoteImagePaths":33005,"frontmatter":33006,"imagePaths":33007},[],[],[],{"title":32995,"description":32996,"summary":32996,"pubDate":32084,"source":25191,"url":32998,"thumbnail":25193},[],"2025-06-25-discrepancy-aware-graph-mask-auto-encoder.md","2025-06-25-disentangling-reasoning-and-knowledge-in-medical-large-language-models",{"id":33009,"data":33011,"filePath":33016,"digest":33017,"rendered":33018,"legacyId":33025},{"title":33012,"description":33013,"summary":33013,"pubDate":33014,"source":25191,"url":33015,"thumbnail":25193},"Disentangling Reasoning and Knowledge in Medical Large Language Models","arXiv:2505.11462v2 Announce Type: replace-cross Abstract: Medical reasoning in large language models (LLMs) aims to emulate clinicians' diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and PubMedQA often mix reasoning with factual recall. We address this by separating 11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human performance. Our analysis shows that only 32.8 percent of questions require complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1) and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent gaps between knowledge and reasoning performance. For example, HuatuoGPT-o1 scores 56.9 on knowledge but only 44.8 on reasoning. In adversarial tests where models are misled with incorrect initial reasoning, biomedical models degrade sharply, while larger or RL-trained general models show more robustness. To address this, we train BioMed-R1 using fine-tuning and reinforcement learning on reasoning-heavy examples. It achieves the strongest performance among similarly sized models. Further gains may come from incorporating clinical case reports and training with adversarial and backtracking scenarios.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2505.11462","src/content/posts/2025-06-25-disentangling-reasoning-and-knowledge-in-medical-large-language-models.md","c5451df5f8b99cca",{"html":25,"metadata":33019},{"headings":33020,"localImagePaths":33021,"remoteImagePaths":33022,"frontmatter":33023,"imagePaths":33024},[],[],[],{"title":33012,"description":33013,"summary":33013,"pubDate":32084,"source":25191,"url":33015,"thumbnail":25193},[],"2025-06-25-disentangling-reasoning-and-knowledge-in-medical-large-language-models.md","2025-06-25-do-llms-know-when-to-flip-a-coin-strategic-randomization-through-reasoning-and-experience",{"id":33026,"data":33028,"filePath":33033,"digest":33034,"rendered":33035,"legacyId":33042},{"title":33029,"description":33030,"summary":33030,"pubDate":33031,"source":25191,"url":33032,"thumbnail":25193},"Do LLMs Know When to Flip a Coin? Strategic Randomization through Reasoning and Experience","arXiv:2506.18928v1 Announce Type: new Abstract: Strategic randomization is a key principle in game theory, yet it remains underexplored in large language models (LLMs). Prior work often conflates the cognitive decision to randomize with the mechanical generation of randomness, leading to incomplete evaluations. To address this, we propose a novel zero-sum game inspired by the Tian Ji Horse Race, where the Nash equilibrium corresponds to a maximal entropy strategy. The game's complexity masks this property from untrained humans and underdeveloped LLMs. We evaluate five LLMs across prompt styles -- framed, neutral, and hinted -- using competitive multi-tournament gameplay with system-provided random choices, isolating the decision to randomize. Results show that weaker models remain deterministic regardless of prompts, while stronger models exhibit increased randomization under explicit hints. When facing weaker models, strong LLMs adopt deterministic strategies to exploit biases, but converge toward equilibrium play when facing peers. Through win/loss outcomes and Bayes factor analysis, we demonstrate meaningful variation in LLMs' strategic reasoning capabilities, highlighting opportunities for improvement in abstract reasoning and adaptive learning. We make our implementation publicly available at https://github.com/ocelopus/llm-when-to-throw-coin to ensure full reproducibility.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18928","src/content/posts/2025-06-25-do-llms-know-when-to-flip-a-coin-strategic-randomization-through-reasoning-and-experience.md","04c66af0ed613ff1",{"html":25,"metadata":33036},{"headings":33037,"localImagePaths":33038,"remoteImagePaths":33039,"frontmatter":33040,"imagePaths":33041},[],[],[],{"title":33029,"description":33030,"summary":33030,"pubDate":32084,"source":25191,"url":33032,"thumbnail":25193},[],"2025-06-25-do-llms-know-when-to-flip-a-coin-strategic-randomization-through-reasoning-and-experience.md","2025-06-25-do-vendi-scores-converge-with-finite-samples-truncated-vendi-score-for-finite-sample-convergence-guarantees",{"id":33043,"data":33045,"filePath":33050,"digest":33051,"rendered":33052,"legacyId":33059},{"title":33046,"description":33047,"summary":33047,"pubDate":33048,"source":25191,"url":33049,"thumbnail":25193},"Do Vendi Scores Converge with Finite Samples? Truncated Vendi Score for Finite-Sample Convergence Guarantees","arXiv:2410.21719v3 Announce Type: replace-cross Abstract: Evaluating the diversity of generative models without reference data poses methodological challenges. The reference-free Vendi and RKE scores address this by quantifying the diversity of generated data using matrix-based entropy measures. Among these two, the Vendi score is typically computed via the eigendecomposition of an $n times n$ kernel matrix constructed from n generated samples. However, the prohibitive computational cost of eigendecomposition for large $n$ often limits the number of samples used to fewer than 20,000. In this paper, we investigate the statistical convergence of the Vendi and RKE scores under restricted sample sizes. We numerically demonstrate that, in general, the Vendi score computed with standard sample sizes below 20,000 may not converge to its asymptotic value under infinite sampling. To address this, we introduce the $t$-truncated Vendi score by truncating the eigenspectrum of the kernel matrix, which is provably guaranteed to converge to its population limit with $n=mathcal{O}(t)$ samples. We further show that existing Nystr'om and FKEA approximation methods converge to the asymptotic limit of the truncated Vendi score. In contrast to the Vendi score, we prove that the RKE score enjoys universal convergence guarantees across all kernel functions. We conduct several numerical experiments to illustrate the concentration of Nystr'om and FKEA computed Vendi scores around the truncated Vendi score, and we analyze how the truncated Vendi and RKE scores correlate with the diversity of image and text data. The code is available at https://github.com/aziksh-ospanov/truncated-vendi.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2410.21719","src/content/posts/2025-06-25-do-vendi-scores-converge-with-finite-samples-truncated-vendi-score-for-finite-sample-convergence-guarantees.md","aeaa1ddac2b0041b",{"html":25,"metadata":33053},{"headings":33054,"localImagePaths":33055,"remoteImagePaths":33056,"frontmatter":33057,"imagePaths":33058},[],[],[],{"title":33046,"description":33047,"summary":33047,"pubDate":32084,"source":25191,"url":33049,"thumbnail":25193},[],"2025-06-25-do-vendi-scores-converge-with-finite-samples-truncated-vendi-score-for-finite-sample-convergence-guarantees.md","2025-06-25-eccot-a-framework-for-enhancing-effective-cognition-via-chain-of-thought-in-large-language-model",{"id":33060,"data":33062,"filePath":33067,"digest":33068,"rendered":33069,"legacyId":33076},{"title":33063,"description":33064,"summary":33064,"pubDate":33065,"source":25191,"url":33066,"thumbnail":25193},"ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model","arXiv:2506.19599v1 Announce Type: cross Abstract: In the era of large-scale artificial intelligence, Large Language Models (LLMs) have made significant strides in natural language processing. However, they often lack transparency and generate unreliable outputs, raising concerns about their interpretability. To address this, the Chain of Thought (CoT) prompting method structures reasoning into step-by-step deductions. Yet, not all reasoning chains are valid, and errors can lead to unreliable conclusions. We propose ECCoT, an End-to-End Cognitive Chain of Thought Validation Framework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates the Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT generation and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By filtering ineffective chains using structured ordering statistics, ECCoT improves interpretability, reduces biases, and enhances the trustworthiness of LLM-based decision-making. Key contributions include the introduction of ECCoT, MRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning enhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19599","src/content/posts/2025-06-25-eccot-a-framework-for-enhancing-effective-cognition-via-chain-of-thought-in-large-language-model.md","ad638a188a48df8e",{"html":25,"metadata":33070},{"headings":33071,"localImagePaths":33072,"remoteImagePaths":33073,"frontmatter":33074,"imagePaths":33075},[],[],[],{"title":33063,"description":33064,"summary":33064,"pubDate":32084,"source":25191,"url":33066,"thumbnail":25193},[],"2025-06-25-eccot-a-framework-for-enhancing-effective-cognition-via-chain-of-thought-in-large-language-model.md","2025-06-25-eccdnamamba-a-pre-trained-model-for-ultra-long-eccdna-sequence-analysis",{"id":33077,"data":33079,"filePath":33084,"digest":33085,"rendered":33086,"legacyId":33093},{"title":33080,"description":33081,"summary":33081,"pubDate":33082,"source":25191,"url":33083,"thumbnail":25193},"eccDNAMamba: A Pre-Trained Model for Ultra-Long eccDNA Sequence Analysis","arXiv:2506.18940v1 Announce Type: cross Abstract: Extrachromosomal circular DNA (eccDNA) plays key regulatory roles and contributes to oncogene overexpression in cancer through high-copy amplification and long-range interactions. Despite advances in modeling, no pre-trained models currently support full-length circular eccDNA for downstream analysis. Existing genomic models are either limited to single-nucleotide resolution or hindered by the inefficiency of the quadratic attention mechanism. Here, we introduce eccDNAMamba, the first bidirectional state-space encoder tailored for circular DNA sequences. It combines forward and reverse passes for full-context representation learning with linear-time complexity, and preserves circular structure through a novel augmentation strategy. Tested on two real-world datasets, eccDNAMamba achieves strong classification performance and scales to sequences up to 200 Kbp, offering a robust and efficient framework for modeling circular genomes. Our codes are available at https://github.com/zzq1zh/GenAI-Lab.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18940","src/content/posts/2025-06-25-eccdnamamba-a-pre-trained-model-for-ultra-long-eccdna-sequence-analysis.md","50903ad88efdf182",{"html":25,"metadata":33087},{"headings":33088,"localImagePaths":33089,"remoteImagePaths":33090,"frontmatter":33091,"imagePaths":33092},[],[],[],{"title":33080,"description":33081,"summary":33081,"pubDate":32084,"source":25191,"url":33083,"thumbnail":25193},[],"2025-06-25-eccdnamamba-a-pre-trained-model-for-ultra-long-eccdna-sequence-analysis.md","2025-06-25-ecg-smart-net-a-deep-learning-architecture-for-precise-ecg-diagnosis-of-occlusion-myocardial-infarction",{"id":33094,"data":33096,"filePath":33101,"digest":33102,"rendered":33103,"legacyId":33110},{"title":33097,"description":33098,"summary":33098,"pubDate":33099,"source":25191,"url":33100,"thumbnail":25193},"ECG-SMART-NET: A Deep Learning Architecture for Precise ECG Diagnosis of Occlusion Myocardial Infarction","arXiv:2405.09567v2 Announce Type: replace-cross Abstract: Objective: In this paper we develop and evaluate ECG-SMART-NET for occlusion myocardial infarction (OMI) identification. OMI is a severe form of heart attack characterized by complete blockage of one or more coronary arteries requiring immediate referral for cardiac catheterization to restore blood flow to the heart. Two thirds of OMI cases are difficult to visually identify from a 12-lead electrocardiogram (ECG) and can be potentially fatal if not identified quickly. Previous works on this topic are scarce, and current state-of-the-art evidence suggests both feature-based random forests and convolutional neural networks (CNNs) are promising approaches to improve ECG detection of OMI. Methods: While the ResNet architecture has been adapted for use with ECG recordings, it is not ideally suited to capture informative temporal features within each lead and the spatial concordance or discordance across leads. We propose a clinically informed modification of the ResNet-18 architecture. The model first learns temporal features through temporal convolutional layers with 1xk kernels followed by a spatial convolutional layer, after the residual blocks, with 12x1 kernels to learn spatial features. Results: ECG-SMART-NET was benchmarked against the original ResNet-18 and other state-of-the-art models on a multisite real-word clinical dataset that consists of 10,393 ECGs from 7,397 unique patients (rate of OMI =7.2%). ECG-SMART-NET outperformed other models in the classification of OMI with a test AUC of 0.953 [0.921, 0.978]. Conclusion and Significance: ECG-SMART-NET can outperform the state-of-the-art random forest for OMI prediction and is better suited for this task than the original ResNet-18 architecture.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2405.09567","src/content/posts/2025-06-25-ecg-smart-net-a-deep-learning-architecture-for-precise-ecg-diagnosis-of-occlusion-myocardial-infarction.md","95d0b2614fd2e39e",{"html":25,"metadata":33104},{"headings":33105,"localImagePaths":33106,"remoteImagePaths":33107,"frontmatter":33108,"imagePaths":33109},[],[],[],{"title":33097,"description":33098,"summary":33098,"pubDate":32084,"source":25191,"url":33100,"thumbnail":25193},[],"2025-06-25-ecg-smart-net-a-deep-learning-architecture-for-precise-ecg-diagnosis-of-occlusion-myocardial-infarction.md","2025-06-25-emergent-risk-awareness-in-rational-agents-under-resource-constraints",{"id":33111,"data":33113,"filePath":33118,"digest":33119,"rendered":33120,"legacyId":33127},{"title":33114,"description":33115,"summary":33115,"pubDate":33116,"source":25191,"url":33117,"thumbnail":25193},"Emergent Risk Awareness in Rational Agents under Resource Constraints","arXiv:2505.23436v3 Announce Type: replace Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed to interact with humans and to solve sequential decision-making problems under (approximate) utility functions and internal models. When such problems have resource or failure constraints where action sequences may be forcibly terminated once resources are exhausted, agents face implicit trade-offs that reshape their utility-driven (rational) behaviour. Additionally, since these agents are typically commissioned by a human principal to act on their behalf, asymmetries in constraint exposure can give rise to previously unanticipated misalignment between human objectives and agent incentives. We formalise this setting through a survival bandit framework, provide theoretical and empirical results that quantify the impact of survival-driven preference shifts, identify conditions under which misalignment emerges and propose mechanisms to mitigate the emergence of risk-seeking or risk-averse behaviours. As a result, this work aims to increase understanding and interpretability of emergent behaviours of AI agents operating under such survival pressure, and offer guidelines for safely deploying such AI systems in critical resource-limited environments.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2505.23436","src/content/posts/2025-06-25-emergent-risk-awareness-in-rational-agents-under-resource-constraints.md","bf266cba855f0a72",{"html":25,"metadata":33121},{"headings":33122,"localImagePaths":33123,"remoteImagePaths":33124,"frontmatter":33125,"imagePaths":33126},[],[],[],{"title":33114,"description":33115,"summary":33115,"pubDate":32084,"source":25191,"url":33117,"thumbnail":25193},[],"2025-06-25-emergent-risk-awareness-in-rational-agents-under-resource-constraints.md","2025-06-25-emostage-a-framework-for-accurate-empathetic-response-generation-via-perspective-taking-and-phase-recognition",{"id":33128,"data":33130,"filePath":33135,"digest":33136,"rendered":33137,"legacyId":33144},{"title":33131,"description":33132,"summary":33132,"pubDate":33133,"source":25191,"url":33134,"thumbnail":25193},"EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition","arXiv:2506.19279v1 Announce Type: cross Abstract: The rising demand for mental health care has fueled interest in AI-driven counseling systems. While large language models (LLMs) offer significant potential, current approaches face challenges, including limited understanding of clients' psychological states and counseling stages, reliance on high-quality training data, and privacy concerns associated with commercial deployment. To address these issues, we propose EmoStage, a framework that enhances empathetic response generation by leveraging the inference capabilities of open-source LLMs without additional training data. Our framework introduces perspective-taking to infer clients' psychological states and support needs, enabling the generation of emotionally resonant responses. In addition, phase recognition is incorporated to ensure alignment with the counseling process and to prevent contextually inappropriate or inopportune responses. Experiments conducted in both Japanese and Chinese counseling settings demonstrate that EmoStage improves the quality of responses generated by base models and performs competitively with data-driven methods.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19279","src/content/posts/2025-06-25-emostage-a-framework-for-accurate-empathetic-response-generation-via-perspective-taking-and-phase-recognition.md","7d2dc65bf271e75c",{"html":25,"metadata":33138},{"headings":33139,"localImagePaths":33140,"remoteImagePaths":33141,"frontmatter":33142,"imagePaths":33143},[],[],[],{"title":33131,"description":33132,"summary":33132,"pubDate":32084,"source":25191,"url":33134,"thumbnail":25193},[],"2025-06-25-emostage-a-framework-for-accurate-empathetic-response-generation-via-perspective-taking-and-phase-recognition.md","2025-06-25-emotion-detection-on-user-front-facing-app-interfaces-for-enhanced-schedule-optimization-a-machine-learning-approach",{"id":33145,"data":33147,"filePath":33152,"digest":33153,"rendered":33154,"legacyId":33161},{"title":33148,"description":33149,"summary":33149,"pubDate":33150,"source":25191,"url":33151,"thumbnail":25193},"Emotion Detection on User Front-Facing App Interfaces for Enhanced Schedule Optimization: A Machine Learning Approach","arXiv:2506.19280v1 Announce Type: new Abstract: Human-Computer Interaction (HCI) has evolved significantly to incorporate emotion recognition capabilities, creating unprecedented opportunities for adaptive and personalized user experiences. This paper explores the integration of emotion detection into calendar applications, enabling user interfaces to dynamically respond to users' emotional states and stress levels, thereby enhancing both productivity and engagement. We present and evaluate two complementary approaches to emotion detection: a biometric-based method utilizing heart rate (HR) data extracted from electrocardiogram (ECG) signals processed through Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) neural networks to predict the emotional dimensions of Valence, Arousal, and Dominance; and a behavioral method analyzing computer activity through multiple machine learning models to classify emotions based on fine-grained user interactions such as mouse movements, clicks, and keystroke patterns. Our comparative analysis, from real-world datasets, reveals that while both approaches demonstrate effectiveness, the computer activity-based method delivers superior consistency and accuracy, particularly for mouse-related interactions, which achieved approximately 90% accuracy. Furthermore, GRU networks outperformed LSTM models in the biometric approach, with Valence prediction reaching 84.38% accuracy.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19280","src/content/posts/2025-06-25-emotion-detection-on-user-front-facing-app-interfaces-for-enhanced-schedule-optimization-a-machine-learning-approach.md","4aab02e6089322a4",{"html":25,"metadata":33155},{"headings":33156,"localImagePaths":33157,"remoteImagePaths":33158,"frontmatter":33159,"imagePaths":33160},[],[],[],{"title":33148,"description":33149,"summary":33149,"pubDate":32084,"source":25191,"url":33151,"thumbnail":25193},[],"2025-06-25-emotion-detection-on-user-front-facing-app-interfaces-for-enhanced-schedule-optimization-a-machine-learning-approach.md","2025-06-25-enhancing-generalization-of-spiking-neural-networks-through-temporal-regularization",{"id":33162,"data":33164,"filePath":33169,"digest":33170,"rendered":33171,"legacyId":33178},{"title":33165,"description":33166,"summary":33166,"pubDate":33167,"source":25191,"url":33168,"thumbnail":25193},"Enhancing Generalization of Spiking Neural Networks Through Temporal Regularization","arXiv:2506.19256v1 Announce Type: cross Abstract: Spiking Neural Networks (SNNs) have received widespread attention due to their event-driven and low-power characteristics, making them particularly effective for processing event-based neuromorphic data. Recent studies have shown that directly trained SNNs suffer from severe overfitting issues due to the limited scale of neuromorphic datasets and the gradient mismatching problem, which fundamentally constrain their generalization performance. In this paper, we propose a temporal regularization training (TRT) method by introducing a time-dependent regularization mechanism to enforce stronger constraints on early timesteps. We compare the performance of TRT with other state-of-the-art methods performance on datasets including CIFAR10/100, ImageNet100, DVS-CIFAR10, and N-Caltech101. To validate the effectiveness of TRT, we conducted ablation studies and analyses including loss landscape visualization and learning curve analysis, demonstrating that TRT can effectively mitigate overfitting and flatten the training loss landscape, thereby enhancing generalizability. Furthermore, we establish a theoretical interpretation of TRT's temporal regularization mechanism based on the results of Fisher information analysis. We analyze the temporal information dynamics inside SNNs by tracking Fisher information during the TRT training process, revealing the Temporal Information Concentration (TIC) phenomenon, where Fisher information progressively concentrates in early timesteps. The time-decaying regularization mechanism implemented in TRT effectively guides the network to learn robust features in early timesteps with rich information, thereby leading to significant improvements in model generalization. Code is available at https://github.com/ZBX05/Temporal-Regularization-Training.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19256","src/content/posts/2025-06-25-enhancing-generalization-of-spiking-neural-networks-through-temporal-regularization.md","ab2170f594962b26",{"html":25,"metadata":33172},{"headings":33173,"localImagePaths":33174,"remoteImagePaths":33175,"frontmatter":33176,"imagePaths":33177},[],[],[],{"title":33165,"description":33166,"summary":33166,"pubDate":32084,"source":25191,"url":33168,"thumbnail":25193},[],"2025-06-25-enhancing-generalization-of-spiking-neural-networks-through-temporal-regularization.md","2025-06-25-enhancing-security-in-llm-applications-a-performance-evaluation-of-early-detection-systems",{"id":33179,"data":33181,"filePath":33186,"digest":33187,"rendered":33188,"legacyId":33195},{"title":33182,"description":33183,"summary":33183,"pubDate":33184,"source":25191,"url":33185,"thumbnail":25193},"Enhancing Security in LLM Applications: A Performance Evaluation of Early Detection Systems","arXiv:2506.19109v1 Announce Type: cross Abstract: Prompt injection threatens novel applications that emerge from adapting LLMs for various user tasks. The newly developed LLM-based software applications become more ubiquitous and diverse. However, the threat of prompt injection attacks undermines the security of these systems as the mitigation and defenses against them, proposed so far, are insufficient. We investigated the capabilities of early prompt injection detection systems, focusing specifically on the detection performance of techniques implemented in various open-source solutions. These solutions are supposed to detect certain types of prompt injection attacks, including the prompt leak. In prompt leakage attacks, an attacker maliciously manipulates the LLM into outputting its system instructions, violating the system's confidentiality. Our study presents analyzes of distinct prompt leakage detection techniques, and a comparative analysis of several detection solutions, which implement those techniques. We identify the strengths and weaknesses of these techniques and elaborate on their optimal configuration and usage in high-stake deployments. In one of the first studies on existing prompt leak detection solutions, we compared the performances of LLM Guard, Vigil, and Rebuff. We concluded that the implementations of canary word checks in Vigil and Rebuff were not effective at detecting prompt leak attacks, and we proposed improvements for them. We also found an evasion weakness in Rebuff's secondary model-based technique and proposed a mitigation. Then, the result of the comparison of LLM Guard, Vigil, and Rebuff at their peak performance revealed that Vigil is optimal for cases when minimal false positive rate is required, and Rebuff is the most optimal for average needs.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19109","src/content/posts/2025-06-25-enhancing-security-in-llm-applications-a-performance-evaluation-of-early-detection-systems.md","dc148cd18b2c2aed",{"html":25,"metadata":33189},{"headings":33190,"localImagePaths":33191,"remoteImagePaths":33192,"frontmatter":33193,"imagePaths":33194},[],[],[],{"title":33182,"description":33183,"summary":33183,"pubDate":32084,"source":25191,"url":33185,"thumbnail":25193},[],"2025-06-25-enhancing-security-in-llm-applications-a-performance-evaluation-of-early-detection-systems.md","2025-06-25-evaluating-compliance-with-visualization-guidelines-in-diagrams-for-scientific-publications-using-large-vision-language-models",{"id":33196,"data":33198,"filePath":33203,"digest":33204,"rendered":33205,"legacyId":33212},{"title":33199,"description":33200,"summary":33200,"pubDate":33201,"source":25191,"url":33202,"thumbnail":25193},"Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models","arXiv:2506.19825v1 Announce Type: new Abstract: Diagrams are widely used to visualize data in publications. The research field of data visualization deals with defining principles and guidelines for the creation and use of these diagrams, which are often not known or adhered to by researchers, leading to misinformation caused by providing inaccurate or incomplete information. In this work, large Vision Language Models (VLMs) are used to analyze diagrams in order to identify potential problems in regards to selected data visualization principles and guidelines. To determine the suitability of VLMs for these tasks, five open source VLMs and five prompting strategies are compared using a set of questions derived from selected data visualization guidelines. The results show that the employed VLMs work well to accurately analyze diagram types (F1-score 82.49 %), 3D effects (F1-score 98.55 %), axes labels (F1-score 76.74 %), lines (RMSE 1.16), colors (RMSE 1.60) and legends (F1-score 96.64 %, RMSE 0.70), while they cannot reliably provide feedback about the image quality (F1-score 0.74 %) and tick marks/labels (F1-score 46.13 %). Among the employed VLMs, Qwen2.5VL performs best, and the summarizing prompting strategy performs best for most of the experimental questions. It is shown that VLMs can be used to automatically identify a number of potential issues in diagrams, such as missing axes labels, missing legends, and unnecessary 3D effects. The approach laid out in this work can be extended for further aspects of data visualization.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19825","src/content/posts/2025-06-25-evaluating-compliance-with-visualization-guidelines-in-diagrams-for-scientific-publications-using-large-vision-language-models.md","3110f86dae079ea3",{"html":25,"metadata":33206},{"headings":33207,"localImagePaths":33208,"remoteImagePaths":33209,"frontmatter":33210,"imagePaths":33211},[],[],[],{"title":33199,"description":33200,"summary":33200,"pubDate":32084,"source":25191,"url":33202,"thumbnail":25193},[],"2025-06-25-evaluating-compliance-with-visualization-guidelines-in-diagrams-for-scientific-publications-using-large-vision-language-models.md","2025-06-25-evaluating-link-prediction-new-perspectives-and-recommendations",{"id":33213,"data":33215,"filePath":33220,"digest":33221,"rendered":33222,"legacyId":33229},{"title":33216,"description":33217,"summary":33217,"pubDate":33218,"source":25191,"url":33219,"thumbnail":25193},"Evaluating link prediction: New perspectives and recommendations","arXiv:2502.12777v3 Announce Type: replace-cross Abstract: Link prediction (LP) is an important problem in network science and machine learning research. The state-of-the-art LP methods are usually evaluated in a uniform setup, ignoring several factors associated with the data and application specific needs. We identify a number of such factors, such as, network-type, problem-type, geodesic distance between the end nodes and its distribution over the classes, nature and applicability of LP methods, class imbalance and its impact on early retrieval, evaluation metric, etc., and present an experimental setup which allows us to evaluate LP methods in a rigorous and controlled manner. We perform extensive experiments with a variety of LP methods over real network datasets in this controlled setup, and gather valuable insights on the interactions of these factors with the performance of LP through an array of carefully designed hypotheses. Following the insights, we provide recommendations to be followed as best practice for evaluating LP methods.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2502.12777","src/content/posts/2025-06-25-evaluating-link-prediction-new-perspectives-and-recommendations.md","a2dfa54306b94415",{"html":25,"metadata":33223},{"headings":33224,"localImagePaths":33225,"remoteImagePaths":33226,"frontmatter":33227,"imagePaths":33228},[],[],[],{"title":33216,"description":33217,"summary":33217,"pubDate":32084,"source":25191,"url":33219,"thumbnail":25193},[],"2025-06-25-evaluating-link-prediction-new-perspectives-and-recommendations.md","2025-06-25-evaluating-transparent-reasoning-in-large-language-models-for-accountable-critical-tasks",{"id":33230,"data":33232,"filePath":33237,"digest":33238,"rendered":33239,"legacyId":33246},{"title":33233,"description":33234,"summary":33234,"pubDate":33235,"source":25191,"url":33236,"thumbnail":25193},"Evaluating Transparent Reasoning in Large Language Models for Accountable Critical Tasks","arXiv:2408.01933v5 Announce Type: replace-cross Abstract: This paper introduces REACT, a benchmark designed to rigorously evaluate the reasoning capabilities of large language models (LLMs) within accountable, high-stakes decision-making tasks in medical and legal domains. Unlike traditional benchmarks primarily focused on prediction accuracy, REACT emphasizes transparent and interpretable reasoning, requiring models to align their logic closely with expert-derived procedures. To assess whether LLM reasoning aligns closely with human experts, we annotated 511 clinical cases from the medical domain and 86 legal cases from the legal domain, each enriched with detailed expert-extracted rationales and evidence supporting each step of the reasoning process. These annotations were guided by carefully constructed reasoning graphs, which explicitly encode domain-specific inference structures and decision criteria derived by domain experts. These reasoning graphs serve not only as standards for expert annotation but also as structured guidelines enabling models to reason transparently and step-by-step. To address the scalability challenges of manual annotation, we further developed a semi-automatic annotation pipeline leveraging expert-defined reasoning graph templates to efficiently generate new graphs, exploring the potential to extend our approach into additional critical domains. Experimental results demonstrate that reasoning graphs substantially enhance the interpretability and accuracy of LLM reasoning compared to traditional baselines, although significant gaps remain relative to expert-level reasoning performance.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2408.01933","src/content/posts/2025-06-25-evaluating-transparent-reasoning-in-large-language-models-for-accountable-critical-tasks.md","dc176a6a30d8303a",{"html":25,"metadata":33240},{"headings":33241,"localImagePaths":33242,"remoteImagePaths":33243,"frontmatter":33244,"imagePaths":33245},[],[],[],{"title":33233,"description":33234,"summary":33234,"pubDate":32084,"source":25191,"url":33236,"thumbnail":25193},[],"2025-06-25-evaluating-transparent-reasoning-in-large-language-models-for-accountable-critical-tasks.md","2025-06-25-evolutionary-level-repair",{"id":33247,"data":33249,"filePath":33254,"digest":33255,"rendered":33256,"legacyId":33263},{"title":33250,"description":33251,"summary":33251,"pubDate":33252,"source":25191,"url":33253,"thumbnail":25193},"Evolutionary Level Repair","arXiv:2506.19359v1 Announce Type: new Abstract: We address the problem of game level repair, which consists of taking a designed but non-functional game level and making it functional. This might consist of ensuring the completeness of the level, reachability of objects, or other performance characteristics. The repair problem may also be constrained in that it can only make a small number of changes to the level. We investigate search-based solutions to the level repair problem, particularly using evolutionary and quality-diversity algorithms, with good results. This level repair method is applied to levels generated using a machine learning-based procedural content generation (PCGML) method that generates stylistically appropriate but frequently broken levels. This combination of PCGML for generation and search-based methods for repair shows great promise as a hybrid procedural content generation (PCG) method.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19359","src/content/posts/2025-06-25-evolutionary-level-repair.md","fe52e4be75125301",{"html":25,"metadata":33257},{"headings":33258,"localImagePaths":33259,"remoteImagePaths":33260,"frontmatter":33261,"imagePaths":33262},[],[],[],{"title":33250,"description":33251,"summary":33251,"pubDate":32084,"source":25191,"url":33253,"thumbnail":25193},[],"2025-06-25-evolutionary-level-repair.md","2025-06-25-exclusive-style-removal-for-cross-domain-novel-class-discovery",{"id":33264,"data":33266,"filePath":33271,"digest":33272,"rendered":33273,"legacyId":33280},{"title":33267,"description":33268,"summary":33268,"pubDate":33269,"source":25191,"url":33270,"thumbnail":25193},"Exclusive Style Removal for Cross Domain Novel Class Discovery","arXiv:2406.18140v4 Announce Type: replace-cross Abstract: As a promising field in open-world learning, textit{Novel Class Discovery} (NCD) is usually a task to cluster unseen novel classes in an unlabeled set based on the prior knowledge of labeled data within the same domain. However, the performance of existing NCD methods could be severely compromised when novel classes are sampled from a different distribution with the labeled ones. In this paper, we explore and establish the solvability of NCD with cross domain setting under the necessary condition that the style information needs to be removed. Based on the theoretical analysis, we introduce an exclusive style removal module for extracting style information that is distinctive from the baseline features, thereby facilitating inference. Moreover, this module is easy to integrate with other NCD methods, acting as a plug-in to improve performance on novel classes with different distributions compared to the labeled set. Additionally, recognizing the non-negligible influence of different backbones and pre-training strategies on the performance of the NCD methods, we build a fair benchmark for future NCD research. Extensive experiments on three common datasets demonstrate the effectiveness of our proposed style removal strategy.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2406.18140","src/content/posts/2025-06-25-exclusive-style-removal-for-cross-domain-novel-class-discovery.md","4c51e7ab538b7d33",{"html":25,"metadata":33274},{"headings":33275,"localImagePaths":33276,"remoteImagePaths":33277,"frontmatter":33278,"imagePaths":33279},[],[],[],{"title":33267,"description":33268,"summary":33268,"pubDate":32084,"source":25191,"url":33270,"thumbnail":25193},[],"2025-06-25-exclusive-style-removal-for-cross-domain-novel-class-discovery.md","2025-06-25-experimental-assessment-of-neural-3d-reconstruction-for-small-uav-based-applications",{"id":33281,"data":33283,"filePath":33288,"digest":33289,"rendered":33290,"legacyId":33297},{"title":33284,"description":33285,"summary":33285,"pubDate":33286,"source":25191,"url":33287,"thumbnail":25193},"Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications","arXiv:2506.19491v1 Announce Type: cross Abstract: The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has expanded their deployment potential to indoor and hard-to-reach areas. However, this trend introduces distinct challenges, particularly in terms of flight dynamics and power consumption, which limit the UAVs' autonomy and mission capabilities. This paper presents a novel approach to overcoming these limitations by integrating Neural 3D Reconstruction (N3DR) with small UAV systems for fine-grained 3-Dimensional (3D) digital reconstruction of small static objects. Specifically, we design, implement, and evaluate an N3DR-based pipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and Splatfacto, to improve the quality of 3D reconstructions using images of the object captured by a fleet of small UAVs. We assess the performance of the considered models using various imagery and pointcloud metrics, comparing them against the baseline Structure from Motion (SfM) algorithm. The experimental results demonstrate that the N3DR-enhanced pipeline significantly improves reconstruction quality, making it feasible for small UAVs to support high-precision 3D mapping and anomaly detection in constrained environments. In more general terms, our results highlight the potential of N3DR in advancing the capabilities of miniaturized UAV systems.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19491","src/content/posts/2025-06-25-experimental-assessment-of-neural-3d-reconstruction-for-small-uav-based-applications.md","8eae917125ff4b2f",{"html":25,"metadata":33291},{"headings":33292,"localImagePaths":33293,"remoteImagePaths":33294,"frontmatter":33295,"imagePaths":33296},[],[],[],{"title":33284,"description":33285,"summary":33285,"pubDate":32084,"source":25191,"url":33287,"thumbnail":25193},[],"2025-06-25-experimental-assessment-of-neural-3d-reconstruction-for-small-uav-based-applications.md","2025-06-25-faf-a-feature-adaptive-framework-for-few-shot-time-series-forecasting",{"id":33298,"data":33300,"filePath":33305,"digest":33306,"rendered":33307,"legacyId":33314},{"title":33301,"description":33302,"summary":33302,"pubDate":33303,"source":25191,"url":33304,"thumbnail":25193},"FAF: A Feature-Adaptive Framework for Few-Shot Time Series Forecasting","arXiv:2506.19567v1 Announce Type: cross Abstract: Multi-task and few-shot time series forecasting tasks are commonly encountered in scenarios such as the launch of new products in different cities. However, traditional time series forecasting methods suffer from insufficient historical data, which stems from a disregard for the generalized and specific features among different tasks. For the aforementioned challenges, we propose the Feature-Adaptive Time Series Forecasting Framework (FAF), which consists of three key components: the Generalized Knowledge Module (GKM), the Task-Specific Module (TSM), and the Rank Module (RM). During training phase, the GKM is updated through a meta-learning mechanism that enables the model to extract generalized features across related tasks. Meanwhile, the TSM is trained to capture diverse local dynamics through multiple functional regions, each of which learns specific features from individual tasks. During testing phase, the RM dynamically selects the most relevant functional region from the TSM based on input sequence features, which is then combined with the generalized knowledge learned by the GKM to generate accurate forecasts. This design enables FAF to achieve robust and personalized forecasting even with sparse historical observations We evaluate FAF on five diverse real-world datasets under few-shot time series forecasting settings. Experimental results demonstrate that FAF consistently outperforms baselines that include three categories of time series forecasting methods. In particular, FAF achieves a 41.81% improvement over the best baseline, iTransformer, on the CO$_2$ emissions dataset.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19567","src/content/posts/2025-06-25-faf-a-feature-adaptive-framework-for-few-shot-time-series-forecasting.md","01ea0455b435ffb4",{"html":25,"metadata":33308},{"headings":33309,"localImagePaths":33310,"remoteImagePaths":33311,"frontmatter":33312,"imagePaths":33313},[],[],[],{"title":33301,"description":33302,"summary":33302,"pubDate":32084,"source":25191,"url":33304,"thumbnail":25193},[],"2025-06-25-faf-a-feature-adaptive-framework-for-few-shot-time-series-forecasting.md","2025-06-25-exploring-the-collaborative-co-creation-process-with-ai-a-case-study-in-novice-music-production",{"id":33315,"data":33317,"filePath":33322,"digest":33323,"rendered":33324,"legacyId":33331},{"title":33318,"description":33319,"summary":33319,"pubDate":33320,"source":25191,"url":33321,"thumbnail":25193},"Exploring the Collaborative Co-Creation Process with AI: A Case Study in Novice Music Production","arXiv:2501.15276v2 Announce Type: replace-cross Abstract: Artificial intelligence is reshaping creative domains, yet its co-creative processes, especially in group settings with novice users, remain under explored. To bridge this gap, we conducted a case study in a college-level course where nine undergraduate students were tasked with creating three original music tracks using AI tools over 10 weeks. The study spanned the entire creative journey from ideation to releasing these songs on Spotify. Participants leveraged AI for music and lyric production, cover art, and distribution. Our findings highlight how AI transforms creative workflows: accelerating ideation but compressing the traditional preparation stage, and requiring novices to navigate a challenging idea selection and validation phase. We also identified a new 'collaging and refinement' stage, where participants creatively combined diverse AI-generated outputs into cohesive works. Furthermore, AI influenced group social dynamics and role division among human creators. Based on these insights, we propose the Human-AI Co-Creation Stage Model and the Human-AI Agency Model, offering new perspectives on collaborative co-creation with AI.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2501.15276","src/content/posts/2025-06-25-exploring-the-collaborative-co-creation-process-with-ai-a-case-study-in-novice-music-production.md","a25febf227306920",{"html":25,"metadata":33325},{"headings":33326,"localImagePaths":33327,"remoteImagePaths":33328,"frontmatter":33329,"imagePaths":33330},[],[],[],{"title":33318,"description":33319,"summary":33319,"pubDate":32084,"source":25191,"url":33321,"thumbnail":25193},[],"2025-06-25-exploring-the-collaborative-co-creation-process-with-ai-a-case-study-in-novice-music-production.md","2025-06-25-faircausesyn-towards-causally-fair-llm-augmented-synthetic-data-generation",{"id":33332,"data":33334,"filePath":33339,"digest":33340,"rendered":33341,"legacyId":33348},{"title":33335,"description":33336,"summary":33336,"pubDate":33337,"source":25191,"url":33338,"thumbnail":25193},"FairCauseSyn: Towards Causally Fair LLM-Augmented Synthetic Data Generation","arXiv:2506.19082v1 Announce Type: cross Abstract: Synthetic data generation creates data based on real-world data using generative models. In health applications, generating high-quality data while maintaining fairness for sensitive attributes is essential for equitable outcomes. Existing GAN-based and LLM-based methods focus on counterfactual fairness and are primarily applied in finance and legal domains. Causal fairness provides a more comprehensive evaluation framework by preserving causal structure, but current synthetic data generation methods do not address it in health settings. To fill this gap, we develop the first LLM-augmented synthetic data generation method to enhance causal fairness using real-world tabular health data. Our generated data deviates by less than 10% from real data on causal fairness metrics. When trained on causally fair predictors, synthetic data reduces bias on the sensitive attribute by 70% compared to real data. This work improves access to fair synthetic data, supporting equitable health research and healthcare delivery.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19082","src/content/posts/2025-06-25-faircausesyn-towards-causally-fair-llm-augmented-synthetic-data-generation.md","f839ae0e0b21600a",{"html":25,"metadata":33342},{"headings":33343,"localImagePaths":33344,"remoteImagePaths":33345,"frontmatter":33346,"imagePaths":33347},[],[],[],{"title":33335,"description":33336,"summary":33336,"pubDate":32084,"source":25191,"url":33338,"thumbnail":25193},[],"2025-06-25-faircausesyn-towards-causally-fair-llm-augmented-synthetic-data-generation.md","2025-06-25-fake-or-real-can-robots-tell-evaluating-embodied-vision-language-models-on-real-and-3d-printed-objects",{"id":33349,"data":33351,"filePath":33356,"digest":33357,"rendered":33358,"legacyId":33365},{"title":33352,"description":33353,"summary":33353,"pubDate":33354,"source":25191,"url":33355,"thumbnail":25193},"Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects","arXiv:2506.19579v1 Announce Type: cross Abstract: Robotic scene understanding increasingly relies on vision-language models (VLMs) to generate natural language descriptions of the environment. In this work, we present a comparative study of captioning strategies for tabletop scenes captured by a robotic arm equipped with an RGB camera. The robot collects images of objects from multiple viewpoints, and we evaluate several models that generate scene descriptions. We compare the performance of various captioning models, like BLIP and VLMs. Our experiments examine the trade-offs between single-view and multi-view captioning, and difference between recognising real-world and 3D printed objects. We quantitatively evaluate object identification accuracy, completeness, and naturalness of the generated captions. Results show that VLMs can be used in robotic settings where common objects need to be recognised, but fail to generalise to novel representations. Our findings provide practical insights into deploying foundation models for embodied agents in real-world settings.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19579","src/content/posts/2025-06-25-fake-or-real-can-robots-tell-evaluating-embodied-vision-language-models-on-real-and-3d-printed-objects.md","334307a668b1c01c",{"html":25,"metadata":33359},{"headings":33360,"localImagePaths":33361,"remoteImagePaths":33362,"frontmatter":33363,"imagePaths":33364},[],[],[],{"title":33352,"description":33353,"summary":33353,"pubDate":32084,"source":25191,"url":33355,"thumbnail":25193},[],"2025-06-25-fake-or-real-can-robots-tell-evaluating-embodied-vision-language-models-on-real-and-3d-printed-objects.md","2025-06-25-fast-and-distributed-equivariant-graph-neural-networks-by-virtual-node-learning",{"id":33366,"data":33368,"filePath":33373,"digest":33374,"rendered":33375,"legacyId":33382},{"title":33369,"description":33370,"summary":33370,"pubDate":33371,"source":25191,"url":33372,"thumbnail":25193},"Fast and Distributed Equivariant Graph Neural Networks by Virtual Node Learning","arXiv:2506.19482v1 Announce Type: cross Abstract: Equivariant Graph Neural Networks (GNNs) have achieved remarkable success across diverse scientific applications. However, existing approaches face critical efficiency challenges when scaling to large geometric graphs and suffer significant performance degradation when the input graphs are sparsified for computational tractability. To address these limitations, we introduce FastEGNN and DistEGNN, two novel enhancements to equivariant GNNs for large-scale geometric graphs. FastEGNN employs a key innovation: a small ordered set of virtual nodes that effectively approximates the large unordered graph of real nodes. Specifically, we implement distinct message passing and aggregation mechanisms for different virtual nodes to ensure mutual distinctiveness, and minimize Maximum Mean Discrepancy (MMD) between virtual and real coordinates to achieve global distributedness. This design enables FastEGNN to maintain high accuracy while efficiently processing large-scale sparse graphs. For extremely large-scale geometric graphs, we present DistEGNN, a distributed extension where virtual nodes act as global bridges between subgraphs in different devices, maintaining consistency while dramatically reducing memory and computational overhead. We comprehensively evaluate our models across four challenging domains: N-body systems (100 nodes), protein dynamics (800 nodes), Water-3D (8,000 nodes), and our new Fluid113K benchmark (113,000 nodes). Results demonstrate superior efficiency and performance, establishing new capabilities in large-scale equivariant graph learning. Code is available at https://github.com/GLAD-RUC/DistEGNN.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19482","src/content/posts/2025-06-25-fast-and-distributed-equivariant-graph-neural-networks-by-virtual-node-learning.md","6b30436228ed70a1",{"html":25,"metadata":33376},{"headings":33377,"localImagePaths":33378,"remoteImagePaths":33379,"frontmatter":33380,"imagePaths":33381},[],[],[],{"title":33369,"description":33370,"summary":33370,"pubDate":32084,"source":25191,"url":33372,"thumbnail":25193},[],"2025-06-25-fast-and-distributed-equivariant-graph-neural-networks-by-virtual-node-learning.md","2025-06-25-feat-a-preference-feedback-dataset-through-a-cost-effective-auto-generation-and-labeling-framework-for-english-ai-tutoring",{"id":33383,"data":33385,"filePath":33390,"digest":33391,"rendered":33392,"legacyId":33399},{"title":33386,"description":33387,"summary":33387,"pubDate":33388,"source":25191,"url":33389,"thumbnail":25193},"FEAT: A Preference Feedback Dataset through a Cost-Effective Auto-Generation and Labeling Framework for English AI Tutoring","arXiv:2506.19325v1 Announce Type: new Abstract: In English education tutoring, teacher feedback is essential for guiding students. Recently, AI-based tutoring systems have emerged to assist teachers; however, these systems require high-quality and large-scale teacher feedback data, which is both time-consuming and costly to generate manually. In this study, we propose FEAT, a cost-effective framework for generating teacher feedback, and have constructed three complementary datasets: (1) DIRECT-Manual (DM), where both humans and large language models (LLMs) collaboratively generate high-quality teacher feedback, albeit at a higher cost; (2) DIRECT-Generated (DG), an LLM-only generated, cost-effective dataset with lower quality;, and (3) DIRECT-Augmented (DA), primarily based on DG with a small portion of DM added to enhance quality while maintaining cost-efficiency. Experimental results showed that incorporating a small portion of DM (5-10%) into DG leads to superior performance compared to using 100% DM alone.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19325","src/content/posts/2025-06-25-feat-a-preference-feedback-dataset-through-a-cost-effective-auto-generation-and-labeling-framework-for-english-ai-tutoring.md","4ae9c58a08fe12d5",{"html":25,"metadata":33393},{"headings":33394,"localImagePaths":33395,"remoteImagePaths":33396,"frontmatter":33397,"imagePaths":33398},[],[],[],{"title":33386,"description":33387,"summary":33387,"pubDate":32084,"source":25191,"url":33389,"thumbnail":25193},[],"2025-06-25-feat-a-preference-feedback-dataset-through-a-cost-effective-auto-generation-and-labeling-framework-for-english-ai-tutoring.md","2025-06-25-finding-clustering-algorithms-in-the-transformer-architecture",{"id":33400,"data":33402,"filePath":33407,"digest":33408,"rendered":33409,"legacyId":33416},{"title":33403,"description":33404,"summary":33404,"pubDate":33405,"source":25191,"url":33406,"thumbnail":25193},"Finding Clustering Algorithms in the Transformer Architecture","arXiv:2506.19125v1 Announce Type: cross Abstract: The invention of the transformer architecture has revolutionized Artificial Intelligence (AI), yielding unprecedented success in areas such as natural language processing, computer vision, and multimodal reasoning. Despite these advances, it is unclear whether transformers are able to learn and implement precise algorithms. Here, we demonstrate that transformers can exactly implement a fundamental and widely used algorithm for $k$-means clustering: Lloyd's algorithm. First, we theoretically prove the existence of such a transformer architecture, which we term the $k$-means transformer, that exactly implements Lloyd's algorithm for $k$-means clustering using the standard ingredients of modern transformers: attention and residual connections. Next, we numerically implement this transformer and demonstrate in experiments the exact correspondence between our architecture and Lloyd's algorithm, providing a fully neural implementation of $k$-means clustering. Finally, we demonstrate that interpretable alterations (e.g., incorporating layer normalizations or multilayer perceptrons) to this architecture yields diverse and novel variants of clustering algorithms, such as soft $k$-means, spherical $k$-means, trimmed $k$-means, and more. Collectively, our findings demonstrate how transformer mechanisms can precisely map onto algorithmic procedures, offering a clear and interpretable perspective on implementing precise algorithms in transformers.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19125","src/content/posts/2025-06-25-finding-clustering-algorithms-in-the-transformer-architecture.md","1f57ddb00d4bc2fb",{"html":25,"metadata":33410},{"headings":33411,"localImagePaths":33412,"remoteImagePaths":33413,"frontmatter":33414,"imagePaths":33415},[],[],[],{"title":33403,"description":33404,"summary":33404,"pubDate":32084,"source":25191,"url":33406,"thumbnail":25193},[],"2025-06-25-finding-clustering-algorithms-in-the-transformer-architecture.md","2025-06-25-from-high-snr-radar-signal-to-ecg-a-transfer-learning-model-with-cardio-focusing-algorithm-for-scenarios-with-limited-data",{"id":33417,"data":33419,"filePath":33424,"digest":33425,"rendered":33426,"legacyId":33433},{"title":33420,"description":33421,"summary":33421,"pubDate":33422,"source":25191,"url":33423,"thumbnail":25193},"From High-SNR Radar Signal to ECG: A Transfer Learning Model with Cardio-Focusing Algorithm for Scenarios with Limited Data","arXiv:2506.19358v1 Announce Type: cross Abstract: Electrocardiogram (ECG), as a crucial find-grained cardiac feature, has been successfully recovered from radar signals in the literature, but the performance heavily relies on the high-quality radar signal and numerous radar-ECG pairs for training, restricting the applications in new scenarios due to data scarcity. Therefore, this work will focus on radar-based ECG recovery in new scenarios with limited data and propose a cardio-focusing and -tracking (CFT) algorithm to precisely track the cardiac location to ensure an efficient acquisition of high-quality radar signals. Furthermore, a transfer learning model (RFcardi) is proposed to extract cardio-related information from the radar signal without ECG ground truth based on the intrinsic sparsity of cardiac features, and only a few synchronous radar-ECG pairs are required to fine-tune the pre-trained model for the ECG recovery. The experimental results reveal that the proposed CFT can dynamically identify the cardiac location, and the RFcardi model can effectively generate faithful ECG recoveries after using a small number of radar-ECG pairs for training. The code and dataset are available after the publication.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19358","src/content/posts/2025-06-25-from-high-snr-radar-signal-to-ecg-a-transfer-learning-model-with-cardio-focusing-algorithm-for-scenarios-with-limited-data.md","a294b8e8926c8195",{"html":25,"metadata":33427},{"headings":33428,"localImagePaths":33429,"remoteImagePaths":33430,"frontmatter":33431,"imagePaths":33432},[],[],[],{"title":33420,"description":33421,"summary":33421,"pubDate":32084,"source":25191,"url":33423,"thumbnail":25193},[],"2025-06-25-from-high-snr-radar-signal-to-ecg-a-transfer-learning-model-with-cardio-focusing-algorithm-for-scenarios-with-limited-data.md","2025-06-25-from-memories-to-maps-mechanisms-of-in-context-reinforcement-learning-in-transformers",{"id":33434,"data":33436,"filePath":33441,"digest":33442,"rendered":33443,"legacyId":33450},{"title":33437,"description":33438,"summary":33438,"pubDate":33439,"source":25191,"url":33440,"thumbnail":25193},"From memories to maps: Mechanisms of in context reinforcement learning in transformers","arXiv:2506.19686v1 Announce Type: new Abstract: Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19686","src/content/posts/2025-06-25-from-memories-to-maps-mechanisms-of-in-context-reinforcement-learning-in-transformers.md","3acff8f7a89ac9e6",{"html":25,"metadata":33444},{"headings":33445,"localImagePaths":33446,"remoteImagePaths":33447,"frontmatter":33448,"imagePaths":33449},[],[],[],{"title":33437,"description":33438,"summary":33438,"pubDate":32084,"source":25191,"url":33440,"thumbnail":25193},[],"2025-06-25-from-memories-to-maps-mechanisms-of-in-context-reinforcement-learning-in-transformers.md","2025-06-25-from-reproduction-to-replication-evaluating-research-agents-with-progressive-code-masking",{"id":33451,"data":33453,"filePath":33458,"digest":33459,"rendered":33460,"legacyId":33467},{"title":33454,"description":33455,"summary":33455,"pubDate":33456,"source":25191,"url":33457,"thumbnail":25193},"From Reproduction to Replication: Evaluating Research Agents with Progressive Code Masking","arXiv:2506.19724v1 Announce Type: new Abstract: Recent progress in autonomous code generation has fueled excitement around AI agents capable of accelerating scientific discovery by running experiments. However, there is currently no benchmark that evaluates whether such agents can implement scientific ideas when given varied amounts of code as a starting point, interpolating between reproduction (running code) and from-scratch replication (fully re-implementing and running code). We introduce AutoExperiment, a benchmark that evaluates AI agents' ability to implement and run machine learning experiments based on natural language descriptions in research papers. In each task, agents are given a research paper, a codebase with key functions masked out, and a command to run the experiment. The goal is to generate the missing code, execute the experiment in a sandboxed environment, and reproduce the results. AutoExperiment scales in difficulty by varying the number of missing functions $n$, ranging from partial reproduction to full replication. We evaluate state-of-the-art agents and find that performance degrades rapidly as $n$ increases. Agents that can dynamically interact with the environment (e.g. to debug their code) can outperform agents in fixed 'agentless' harnesses, and there exists a significant gap between single-shot and multi-trial success rates (Pass@1 vs. Pass@5), motivating verifier approaches to our benchmark. Our findings highlight critical challenges in long-horizon code generation, context retrieval, and autonomous experiment execution, establishing AutoExperiment as a new benchmark for evaluating progress in AI-driven scientific experimentation. Our data and code are open-sourced at https://github.com/j1mk1m/AutoExperiment .",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19724","src/content/posts/2025-06-25-from-reproduction-to-replication-evaluating-research-agents-with-progressive-code-masking.md","7b656296694db71c",{"html":25,"metadata":33461},{"headings":33462,"localImagePaths":33463,"remoteImagePaths":33464,"frontmatter":33465,"imagePaths":33466},[],[],[],{"title":33454,"description":33455,"summary":33455,"pubDate":32084,"source":25191,"url":33457,"thumbnail":25193},[],"2025-06-25-from-reproduction-to-replication-evaluating-research-agents-with-progressive-code-masking.md","2025-06-25-from-rows-to-yields-how-foundation-models-for-tabular-data-simplify-crop-yield-prediction",{"id":33468,"data":33470,"filePath":33475,"digest":33476,"rendered":33477,"legacyId":33484},{"title":33471,"description":33472,"summary":33472,"pubDate":33473,"source":25191,"url":33474,"thumbnail":25193},"From Rows to Yields: How Foundation Models for Tabular Data Simplify Crop Yield Prediction","arXiv:2506.19046v1 Announce Type: new Abstract: We present an application of a foundation model for small- to medium-sized tabular data (TabPFN), to sub-national yield forecasting task in South Africa. TabPFN has recently demonstrated superior performance compared to traditional machine learning (ML) models in various regression and classification tasks. We used the dekadal (10-days) time series of Earth Observation (EO; FAPAR and soil moisture) and gridded weather data (air temperature, precipitation and radiation) to forecast the yield of summer crops at the sub-national level. The crop yield data was available for 23 years and for up to 8 provinces. Covariate variables for TabPFN (i.e., EO and weather) were extracted by region and aggregated at a monthly scale. We benchmarked the results of the TabPFN against six ML models and three baseline models. Leave-one-year-out cross-validation experiment setting was used in order to ensure the assessment of the models capacity to forecast an unseen year. Results showed that TabPFN and ML models exhibit comparable accuracy, outperforming the baselines. Nonetheless, TabPFN demonstrated superior practical utility due to its significantly faster tuning time and reduced requirement for feature engineering. This renders TabPFN a more viable option for real-world operation yield forecasting applications, where efficiency and ease of implementation are paramount.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19046","src/content/posts/2025-06-25-from-rows-to-yields-how-foundation-models-for-tabular-data-simplify-crop-yield-prediction.md","da37418a016acf34",{"html":25,"metadata":33478},{"headings":33479,"localImagePaths":33480,"remoteImagePaths":33481,"frontmatter":33482,"imagePaths":33483},[],[],[],{"title":33471,"description":33472,"summary":33472,"pubDate":32084,"source":25191,"url":33474,"thumbnail":25193},[],"2025-06-25-from-rows-to-yields-how-foundation-models-for-tabular-data-simplify-crop-yield-prediction.md","2025-06-25-gbgc-efficient-and-adaptive-graph-coarsening-via-granular-ball-computing",{"id":33485,"data":33487,"filePath":33492,"digest":33493,"rendered":33494,"legacyId":33501},{"title":33488,"description":33489,"summary":33489,"pubDate":33490,"source":25191,"url":33491,"thumbnail":25193},"GBGC: Efficient and Adaptive Graph Coarsening via Granular-ball Computing","arXiv:2506.19224v1 Announce Type: new Abstract: The objective of graph coarsening is to generate smaller, more manageable graphs while preserving key information of the original graph. Previous work were mainly based on the perspective of spectrum-preserving, using some predefined coarsening rules to make the eigenvalues of the Laplacian matrix of the original graph and the coarsened graph match as much as possible. However, they largely overlooked the fact that the original graph is composed of subregions at different levels of granularity, where highly connected and similar nodes should be more inclined to be aggregated together as nodes in the coarsened graph. By combining the multi-granularity characteristics of the graph structure, we can generate coarsened graph at the optimal granularity. To this end, inspired by the application of granular-ball computing in multi-granularity, we propose a new multi-granularity, efficient, and adaptive coarsening method via granular-ball (GBGC), which significantly improves the coarsening results and efficiency. Specifically, GBGC introduces an adaptive granular-ball graph refinement mechanism, which adaptively splits the original graph from coarse to fine into granular-balls of different sizes and optimal granularity, and constructs the coarsened graph using these granular-balls as supernodes. In addition, compared with other state-of-the-art graph coarsening methods, the processing speed of this method can be increased by tens to hundreds of times and has lower time complexity. The accuracy of GBGC is almost always higher than that of the original graph due to the good robustness and generalization of the granular-ball computing, so it has the potential to become a standard graph data preprocessing method.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19224","src/content/posts/2025-06-25-gbgc-efficient-and-adaptive-graph-coarsening-via-granular-ball-computing.md","769ca7539f77a911",{"html":25,"metadata":33495},{"headings":33496,"localImagePaths":33497,"remoteImagePaths":33498,"frontmatter":33499,"imagePaths":33500},[],[],[],{"title":33488,"description":33489,"summary":33489,"pubDate":32084,"source":25191,"url":33491,"thumbnail":25193},[],"2025-06-25-gbgc-efficient-and-adaptive-graph-coarsening-via-granular-ball-computing.md","2025-06-25-gemini-cli-your-open-source-ai-agent",{"id":33502,"data":33504,"filePath":33510,"digest":33511,"rendered":33512,"legacyId":33520},{"title":33505,"description":33506,"summary":33506,"pubDate":33507,"source":23485,"url":33508,"thumbnail":33509},"Gemini CLI: your open-source AI agent","Gemini CLI icon on a background with code snippets",["Date","2025-06-25T13:00:00.000Z"],"https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/","https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemini_CLI_Hero_Final.width-1300.png","src/content/posts/2025-06-25-gemini-cli-your-open-source-ai-agent.md","761f54c88377b852",{"html":25,"metadata":33513},{"headings":33514,"localImagePaths":33515,"remoteImagePaths":33516,"frontmatter":33517,"imagePaths":33519},[],[],[],{"title":33505,"description":33506,"summary":33506,"pubDate":33518,"source":23485,"url":33508,"thumbnail":33509},"Wed, 25 Jun 2025 13:00:00 +0000",[],"2025-06-25-gemini-cli-your-open-source-ai-agent.md","2025-06-25-general-methods-make-great-domain-specific-foundation-models-a-case-study-on-fetal-ultrasound",{"id":33521,"data":33523,"filePath":33528,"digest":33529,"rendered":33530,"legacyId":33537},{"title":33524,"description":33525,"summary":33525,"pubDate":33526,"source":25191,"url":33527,"thumbnail":25193},"General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound","arXiv:2506.19552v1 Announce Type: cross Abstract: With access to large-scale, unlabeled medical datasets, researchers are confronted with two questions: Should they attempt to pretrain a custom foundation model on this medical data, or use transfer-learning from an existing generalist model? And, if a custom model is pretrained, are novel methods required? In this paper we explore these questions by conducting a case-study, in which we train a foundation model on a large regional fetal ultrasound dataset of 2M images. By selecting the well-established DINOv2 method for pretraining, we achieve state-of-the-art results on three fetal ultrasound datasets, covering data from different countries, classification, segmentation, and few-shot tasks. We compare against a series of models pretrained on natural images, ultrasound images, and supervised baselines. Our results demonstrate two key insights: (i) Pretraining on custom data is worth it, even if smaller models are trained on less data, as scaling in natural image pretraining does not translate to ultrasound performance. (ii) Well-tuned methods from computer vision are making it feasible to train custom foundation models for a given medical domain, requiring no hyperparameter tuning and little methodological adaptation. Given these findings, we argue that a bias towards methodological innovation should be avoided when developing domain specific foundation models under common computational resource constraints.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19552","src/content/posts/2025-06-25-general-methods-make-great-domain-specific-foundation-models-a-case-study-on-fetal-ultrasound.md","8ee6b8ed751a8e1b",{"html":25,"metadata":33531},{"headings":33532,"localImagePaths":33533,"remoteImagePaths":33534,"frontmatter":33535,"imagePaths":33536},[],[],[],{"title":33524,"description":33525,"summary":33525,"pubDate":32084,"source":25191,"url":33527,"thumbnail":25193},[],"2025-06-25-general-methods-make-great-domain-specific-foundation-models-a-case-study-on-fetal-ultrasound.md","2025-06-25-geometric-aware-variational-inference-robust-and-adaptive-regularization-with-directional-weight-uncertainty",{"id":33538,"data":33540,"filePath":33545,"digest":33546,"rendered":33547,"legacyId":33554},{"title":33541,"description":33542,"summary":33542,"pubDate":33543,"source":25191,"url":33544,"thumbnail":25193},"Geometric-Aware Variational Inference: Robust and Adaptive Regularization with Directional Weight Uncertainty","arXiv:2506.19726v1 Announce Type: cross Abstract: Deep neural networks require principled uncertainty quantification, yet existing variational inference methods often employ isotropic Gaussian approximations in weight space that poorly match the network's inherent geometry. We address this mismatch by introducing Concentration-Adapted Perturbations (CAP), a variational framework that models weight uncertainties directly on the unit hypersphere using von Mises-Fisher distributions. Building on recent work in radial-directional posterior decompositions and spherical weight constraints, CAP provides the first complete theoretical framework connecting directional statistics to practical noise regularization in neural networks. Our key contribution is an analytical derivation linking vMF concentration parameters to activation noise variance, enabling each layer to learn its optimal uncertainty level through a novel closed-form KL divergence regularizer. In experiments on CIFAR-10, CAP significantly improves model calibration - reducing Expected Calibration Error by 5.6x - while providing interpretable layer-wise uncertainty profiles. CAP requires minimal computational overhead and integrates seamlessly into standard architectures, offering a theoretically grounded yet practical approach to uncertainty quantification in deep learning.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19726","src/content/posts/2025-06-25-geometric-aware-variational-inference-robust-and-adaptive-regularization-with-directional-weight-uncertainty.md","d5fbf528a19e6201",{"html":25,"metadata":33548},{"headings":33549,"localImagePaths":33550,"remoteImagePaths":33551,"frontmatter":33552,"imagePaths":33553},[],[],[],{"title":33541,"description":33542,"summary":33542,"pubDate":32084,"source":25191,"url":33544,"thumbnail":25193},[],"2025-06-25-geometric-aware-variational-inference-robust-and-adaptive-regularization-with-directional-weight-uncertainty.md","2025-06-25-glimpse-gradient-layer-importance-mapping-for-prompted-visual-saliency-explanation-for-generative-lvlms",{"id":33555,"data":33557,"filePath":33562,"digest":33563,"rendered":33564,"legacyId":33571},{"title":33558,"description":33559,"summary":33559,"pubDate":33560,"source":25191,"url":33561,"thumbnail":25193},"GLIMPSE: Gradient-Layer Importance Mapping for Prompted Visual Saliency Explanation for Generative LVLMs","arXiv:2506.18985v1 Announce Type: cross Abstract: Recent advances in large vision language models (LVLMs) have unlocked unprecedented capabilities in generating coherent responses from visual inputs. However, interpreting where LVLMs direct their visual attention while generating free-form textual responses remains a significant challenge, yet is essential for understanding model behavior, diagnosing hallucination, exposing bias and ensuring transparency. We introduce GLIMPSE (Gradient-Layer Importance Mapping for Prompted Visual Saliency Explanation), a lightweight, model-agnostic framework for visualizing the salient image regions that LVLMs rely upon during open-ended visual question answering (VQA), while concurrently revealing the multimodal textual saliency. GLIMPSE fuses gradient-weighted attention, adaptive layer propagation, and weighted token aggregation to produce holistic response-level attribution heat maps for interpreting cross-modal reasoning, outperforming prior interpretability methods in human-alignment. We demonstrate an analytic explainable AI (XAI) approach using GLIMPSE to uncover fine-grained insights into LVLM cross-modal attribution, trace token-level reasoning dynamics, and analyze systematic human-attention misalignment, hallucination, and bias.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18985","src/content/posts/2025-06-25-glimpse-gradient-layer-importance-mapping-for-prompted-visual-saliency-explanation-for-generative-lvlms.md","a4bfb11cd2a03386",{"html":25,"metadata":33565},{"headings":33566,"localImagePaths":33567,"remoteImagePaths":33568,"frontmatter":33569,"imagePaths":33570},[],[],[],{"title":33558,"description":33559,"summary":33559,"pubDate":32084,"source":25191,"url":33561,"thumbnail":25193},[],"2025-06-25-glimpse-gradient-layer-importance-mapping-for-prompted-visual-saliency-explanation-for-generative-lvlms.md","2025-06-25-google-deepmindロボットを完全ローカルで動かせるaiモデルgemini-robotics-on-deviceを初期リリース",{"id":33572,"data":33574,"filePath":33580,"digest":33581,"rendered":33582,"legacyId":33590},{"title":33575,"description":33576,"summary":33576,"pubDate":33577,"source":24944,"url":33578,"thumbnail":33579},"Google DeepMind、ロボットを完全ローカルで動かせるAIモデル「Gemini Robotics On-Device」を初期リリース","Google DeepMindは、ロボット上で完全にローカル実行できるAIモデル「Gemini Robotics On-Device」を初期リリースした。ネットワーク接続に依存せず低遅延で動作するため、オフライン環境でのロボット活用が期待される。",["Date","2025-06-24T23:26:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/25/news056.html","https://image.itmedia.co.jp/aiplus/articles/2506/25/cover_news056.jpg","src/content/posts/2025-06-25-google-deepmindロボットを完全ローカルで動かせるaiモデルgemini-robotics-on-deviceを初期リリース.md","02597e0ebd443de0",{"html":25,"metadata":33583},{"headings":33584,"localImagePaths":33585,"remoteImagePaths":33586,"frontmatter":33587,"imagePaths":33589},[],[],[],{"title":33575,"description":33576,"summary":33576,"pubDate":33588,"source":24944,"url":33578,"thumbnail":33579},"Wed, 25 Jun 2025 08:26:00 +0900",[],"2025-06-25-google-deepmindロボットを完全ローカルで動かせるaiモデルgemini-robotics-on-deviceを初期リリース.md","2025-06-25-has-machine-translation-evaluation-achieved-human-parity-the-human-reference-and-the-limits-of-progress",{"id":33591,"data":33593,"filePath":33598,"digest":33599,"rendered":33600,"legacyId":33607},{"title":33594,"description":33595,"summary":33595,"pubDate":33596,"source":25191,"url":33597,"thumbnail":25193},"Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress","arXiv:2506.19571v1 Announce Type: cross Abstract: In Machine Translation (MT) evaluation, metric performance is assessed based on agreement with human judgments. In recent years, automatic metrics have demonstrated increasingly high levels of agreement with humans. To gain a clearer understanding of metric performance and establish an upper bound, we incorporate human baselines in the MT meta-evaluation, that is, the assessment of MT metrics' capabilities. Our results show that human annotators are not consistently superior to automatic metrics, with state-of-the-art metrics often ranking on par with or higher than human baselines. Despite these findings suggesting human parity, we discuss several reasons for caution. Finally, we explore the broader implications of our results for the research field, asking: Can we still reliably measure improvements in MT evaluation? With this work, we aim to shed light on the limits of our ability to measure progress in the field, fostering discussion on an issue that we believe is crucial to the entire MT evaluation community.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19571","src/content/posts/2025-06-25-has-machine-translation-evaluation-achieved-human-parity-the-human-reference-and-the-limits-of-progress.md","e488067af20ae196",{"html":25,"metadata":33601},{"headings":33602,"localImagePaths":33603,"remoteImagePaths":33604,"frontmatter":33605,"imagePaths":33606},[],[],[],{"title":33594,"description":33595,"summary":33595,"pubDate":32084,"source":25191,"url":33597,"thumbnail":25193},[],"2025-06-25-has-machine-translation-evaluation-achieved-human-parity-the-human-reference-and-the-limits-of-progress.md","2025-06-25-hawaii-hierarchical-visual-knowledge-transfer-for-efficient-vision-language-models",{"id":33608,"data":33610,"filePath":33615,"digest":33616,"rendered":33617,"legacyId":33624},{"title":33611,"description":33612,"summary":33612,"pubDate":33613,"source":25191,"url":33614,"thumbnail":25193},"HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models","arXiv:2506.19072v1 Announce Type: cross Abstract: Improving the visual understanding ability of vision-language models (VLMs) is crucial for enhancing their performance across various tasks. While using multiple pretrained visual experts has shown great promise, it often incurs significant computational costs during training and inference. To address this challenge, we propose HAWAII, a novel framework that distills knowledge from multiple visual experts into a single vision encoder, enabling it to inherit the complementary strengths of several experts with minimal computational overhead. To mitigate conflicts among different teachers and switch between different teacher-specific knowledge, instead of using a fixed set of adapters for multiple teachers, we propose to use teacher-specific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each adapter is aligned with a specific teacher, avoiding noisy guidance during distillation. To enable efficient knowledge distillation, we propose fine-grained and coarse-grained distillation. At the fine-grained level, token importance scores are employed to emphasize the most informative tokens from each teacher adaptively. At the coarse-grained level, we summarize the knowledge from multiple teachers and transfer it to the student using a set of general-knowledge LoRA adapters with a router. Extensive experiments on various vision-language tasks demonstrate the superiority of HAWAII, compared to the popular open-source VLMs.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19072","src/content/posts/2025-06-25-hawaii-hierarchical-visual-knowledge-transfer-for-efficient-vision-language-models.md","228b1e5409b9c736",{"html":25,"metadata":33618},{"headings":33619,"localImagePaths":33620,"remoteImagePaths":33621,"frontmatter":33622,"imagePaths":33623},[],[],[],{"title":33611,"description":33612,"summary":33612,"pubDate":32084,"source":25191,"url":33614,"thumbnail":25193},[],"2025-06-25-hawaii-hierarchical-visual-knowledge-transfer-for-efficient-vision-language-models.md","2025-06-25-heuragenix-leveraging-llms-for-solving-complex-combinatorial-optimization-challenges",{"id":33625,"data":33627,"filePath":33632,"digest":33633,"rendered":33634,"legacyId":33641},{"title":33628,"description":33629,"summary":33629,"pubDate":33630,"source":25191,"url":33631,"thumbnail":25193},"HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges","arXiv:2506.15196v2 Announce Type: replace Abstract: Heuristic algorithms play a vital role in solving combinatorial optimization (CO) problems, yet traditional designs depend heavily on manual expertise and struggle to generalize across diverse instances. We introduce textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large language models (LLMs) that first evolves heuristics and then selects among them automatically. In the heuristic evolution phase, HeurAgenix leverages an LLM to compare seed heuristic solutions with higher-quality solutions and extract reusable evolution strategies. During problem solving, it dynamically picks the most promising heuristic for each problem state, guided by the LLM's perception ability. For flexibility, this selector can be either a state-of-the-art LLM or a fine-tuned lightweight model with lower inference cost. To mitigate the scarcity of reliable supervision caused by CO complexity, we fine-tune the lightweight heuristic selector with a dual-reward mechanism that jointly exploits singals from selection preferences and state perception, enabling robust selection under noisy annotations. Extensive experiments on canonical benchmarks show that HeurAgenix not only outperforms existing LLM-based hyper-heuristics but also matches or exceeds specialized solvers. Code is available at https://github.com/microsoft/HeurAgenix.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.15196","src/content/posts/2025-06-25-heuragenix-leveraging-llms-for-solving-complex-combinatorial-optimization-challenges.md","ef28995210eb2533",{"html":25,"metadata":33635},{"headings":33636,"localImagePaths":33637,"remoteImagePaths":33638,"frontmatter":33639,"imagePaths":33640},[],[],[],{"title":33628,"description":33629,"summary":33629,"pubDate":32084,"source":25191,"url":33631,"thumbnail":25193},[],"2025-06-25-heuragenix-leveraging-llms-for-solving-complex-combinatorial-optimization-challenges.md","2025-06-25-hierarchical-time-series-forecasting-via-latent-mean-encoding",{"id":33642,"data":33644,"filePath":33649,"digest":33650,"rendered":33651,"legacyId":33658},{"title":33645,"description":33646,"summary":33646,"pubDate":33647,"source":25191,"url":33648,"thumbnail":25193},"Hierarchical Time Series Forecasting Via Latent Mean Encoding","arXiv:2506.19633v1 Announce Type: cross Abstract: Coherently forecasting the behaviour of a target variable across both coarse and fine temporal scales is crucial for profit-optimized decision-making in several business applications, and remains an open research problem in temporal hierarchical forecasting. Here, we propose a new hierarchical architecture that tackles this problem by leveraging modules that specialize in forecasting the different temporal aggregation levels of interest. The architecture, which learns to encode the average behaviour of the target variable within its hidden layers, makes accurate and coherent forecasts across the target temporal hierarchies. We validate our architecture on the challenging, real-world M5 dataset and show that it outperforms established methods, such as the TSMixer model.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19633","src/content/posts/2025-06-25-hierarchical-time-series-forecasting-via-latent-mean-encoding.md","91a97bb8b14af9c1",{"html":25,"metadata":33652},{"headings":33653,"localImagePaths":33654,"remoteImagePaths":33655,"frontmatter":33656,"imagePaths":33657},[],[],[],{"title":33645,"description":33646,"summary":33646,"pubDate":32084,"source":25191,"url":33648,"thumbnail":25193},[],"2025-06-25-hierarchical-time-series-forecasting-via-latent-mean-encoding.md","2025-06-25-human-centered-editable-speech-to-sign-language-generation-via-streaming-conformer-transformer-and-resampling-hook",{"id":33659,"data":33661,"filePath":33666,"digest":33667,"rendered":33668,"legacyId":33675},{"title":33662,"description":33663,"summary":33663,"pubDate":33664,"source":25191,"url":33665,"thumbnail":25193},"Human-Centered Editable Speech-to-Sign-Language Generation via Streaming Conformer-Transformer and Resampling Hook","arXiv:2506.14677v2 Announce Type: replace-cross Abstract: Existing end-to-end sign-language animation systems suffer from low naturalness, limited facial/body expressivity, and no user control. We propose a human-centered, real-time speech-to-sign animation framework that integrates (1) a streaming Conformer encoder with an autoregressive Transformer-MDN decoder for synchronized upper-body and facial motion generation, (2) a transparent, editable JSON intermediate representation empowering deaf users and experts to inspect and modify each sign segment, and (3) a human-in-the-loop optimization loop that refines the model based on user edits and ratings. Deployed on Unity3D, our system achieves a 13 ms average frame-inference time and a 103 ms end-to-end latency on an RTX 4070. Our key contributions include the design of a JSON-centric editing mechanism for fine-grained sign-level personalization and the first application of an MDN-based feedback loop for continuous model adaptation. This combination establishes a generalizable, explainable AI paradigm for user-adaptive, low-latency multimodal systems. In studies with 20 deaf signers and 5 professional interpreters, we observe a +13 point SUS improvement, 6.7 point reduction in cognitive load, and significant gains in naturalness and trust (p $\u003C$ .001) over baselines. This work establishes a scalable, explainable AI paradigm for accessible sign-language technologies.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.14677","src/content/posts/2025-06-25-human-centered-editable-speech-to-sign-language-generation-via-streaming-conformer-transformer-and-resampling-hook.md","43ce401bfdb213f0",{"html":25,"metadata":33669},{"headings":33670,"localImagePaths":33671,"remoteImagePaths":33672,"frontmatter":33673,"imagePaths":33674},[],[],[],{"title":33662,"description":33663,"summary":33663,"pubDate":32084,"source":25191,"url":33665,"thumbnail":25193},[],"2025-06-25-human-centered-editable-speech-to-sign-language-generation-via-streaming-conformer-transformer-and-resampling-hook.md","2025-06-25-i-know-myself-better-but-not-really-greatly-how-well-can-llms-detect-and-explain-llm-generated-texts",{"id":33676,"data":33678,"filePath":33683,"digest":33684,"rendered":33685,"legacyId":33692},{"title":33679,"description":33680,"summary":33680,"pubDate":33681,"source":25191,"url":33682,"thumbnail":25193},"'I know myself better, but not really greatly': How Well Can LLMs Detect and Explain LLM-Generated Texts?","arXiv:2502.12743v2 Announce Type: replace-cross Abstract: Distinguishing between human- and LLM-generated texts is crucial given the risks associated with misuse of LLMs. This paper investigates detection and explanation capabilities of current LLMs across two settings: binary (human vs. LLM-generated) and ternary classification (including an ``undecided'' class). We evaluate 6 close- and open-source LLMs of varying sizes and find that self-detection (LLMs identifying their own outputs) consistently outperforms cross-detection (identifying outputs from other LLMs), though both remain suboptimal. Introducing a ternary classification framework improves both detection accuracy and explanation quality across all models. Through comprehensive quantitative and qualitative analyses using our human-annotated dataset, we identify key explanation failures, primarily reliance on inaccurate features, hallucinations, and flawed reasoning. Our findings underscore the limitations of current LLMs in self-detection and self-explanation, highlighting the need for further research to address overfitting and enhance generalizability.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2502.12743","src/content/posts/2025-06-25-i-know-myself-better-but-not-really-greatly-how-well-can-llms-detect-and-explain-llm-generated-texts.md","3e9ca12ae9c94c0e",{"html":25,"metadata":33686},{"headings":33687,"localImagePaths":33688,"remoteImagePaths":33689,"frontmatter":33690,"imagePaths":33691},[],[],[],{"title":33679,"description":33680,"summary":33680,"pubDate":32084,"source":25191,"url":33682,"thumbnail":25193},[],"2025-06-25-i-know-myself-better-but-not-really-greatly-how-well-can-llms-detect-and-explain-llm-generated-texts.md","2025-06-25-identifying-macro-causal-effects-in-c-dmgs-over-dmgs",{"id":33693,"data":33695,"filePath":33700,"digest":33701,"rendered":33702,"legacyId":33709},{"title":33696,"description":33697,"summary":33697,"pubDate":33698,"source":25191,"url":33699,"thumbnail":25193},"Identifying Macro Causal Effects in C-DMGs over DMGs","arXiv:2506.19650v1 Announce Type: new Abstract: The do-calculus is a sound and complete tool for identifying causal effects in acyclic directed mixed graphs (ADMGs) induced by structural causal models (SCMs). However, in many real-world applications, especially in high-dimensional setting, constructing a fully specified ADMG is often infeasible. This limitation has led to growing interest in partially specified causal representations, particularly through cluster-directed mixed graphs (C-DMGs), which group variables into clusters and offer a more abstract yet practical view of causal dependencies. While these representations can include cycles, recent work has shown that the do-calculus remains sound and complete for identifying macro-level causal effects in C-DMGs over ADMGs under the assumption that all clusters size are greater than 1. Nevertheless, real-world systems often exhibit cyclic causal dynamics at the structural level. To account for this, input-output structural causal models (ioSCMs) have been introduced as a generalization of SCMs that allow for cycles. ioSCMs induce another type of graph structure known as a directed mixed graph (DMG). Analogous to the ADMG setting, one can define C-DMGs over DMGs as high-level representations of causal relations among clusters of variables. In this paper, we prove that, unlike in the ADMG setting, the do-calculus is unconditionally sound and complete for identifying macro causal effects in C-DMGs over DMGs. Furthermore, we show that the graphical criteria for non-identifiability of macro causal effects previously established C-DMGs over ADMGs naturally extends to a subset of C-DMGs over DMGs.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19650","src/content/posts/2025-06-25-identifying-macro-causal-effects-in-c-dmgs-over-dmgs.md","3c83024a6317b1fb",{"html":25,"metadata":33703},{"headings":33704,"localImagePaths":33705,"remoteImagePaths":33706,"frontmatter":33707,"imagePaths":33708},[],[],[],{"title":33696,"description":33697,"summary":33697,"pubDate":32084,"source":25191,"url":33699,"thumbnail":25193},[],"2025-06-25-identifying-macro-causal-effects-in-c-dmgs-over-dmgs.md","2025-06-25-impact-of-visual-context-on-noisy-multimodal-nmt-an-empirical-study-for-english-to-indian-languages",{"id":33710,"data":33712,"filePath":33717,"digest":33718,"rendered":33719,"legacyId":33726},{"title":33713,"description":33714,"summary":33714,"pubDate":33715,"source":25191,"url":33716,"thumbnail":25193},"Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages","arXiv:2308.16075v2 Announce Type: replace-cross Abstract: Neural Machine Translation (NMT) has made remarkable progress using large-scale textual data, but the potential of incorporating multimodal inputs, especially visual information, remains underexplored in high-resource settings. While prior research has focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model handle textual noise. Multimodal models slightly outperform text-only models in noisy settings, even when random images are used. The study's experiments translate from English to Hindi, Bengali, and Malayalam, significantly outperforming state-of-the-art benchmarks. Interestingly, the effect of visual context varies with the level of source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features perform better in high-noise scenarios. This sheds light on the role of visual context, especially in noisy settings, and opens up a new research direction for Noisy Neural Machine Translation in multimodal setups. The research emphasizes the importance of combining visual and textual information to improve translation across various environments. Our code is publicly available at https://github.com/babangain/indicMMT.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2308.16075","src/content/posts/2025-06-25-impact-of-visual-context-on-noisy-multimodal-nmt-an-empirical-study-for-english-to-indian-languages.md","db0200e18f5ca8f2",{"html":25,"metadata":33720},{"headings":33721,"localImagePaths":33722,"remoteImagePaths":33723,"frontmatter":33724,"imagePaths":33725},[],[],[],{"title":33713,"description":33714,"summary":33714,"pubDate":32084,"source":25191,"url":33716,"thumbnail":25193},[],"2025-06-25-impact-of-visual-context-on-noisy-multimodal-nmt-an-empirical-study-for-english-to-indian-languages.md","2025-06-25-improving-progressive-generation-with-decomposable-flow-matching",{"id":33727,"data":33729,"filePath":33734,"digest":33735,"rendered":33736,"legacyId":33743},{"title":33730,"description":33731,"summary":33731,"pubDate":33732,"source":25191,"url":33733,"thumbnail":25193},"Improving Progressive Generation with Decomposable Flow Matching","arXiv:2506.19839v1 Announce Type: cross Abstract: Generating high-dimensional visual modalities is a computationally intensive task. A common solution is progressive generation, where the outputs are synthesized in a coarse-to-fine spectral autoregressive manner. While diffusion models benefit from the coarse-to-fine nature of denoising, explicit multi-stage architectures are rarely adopted. These architectures have increased the complexity of the overall approach, introducing the need for a custom diffusion formulation, decomposition-dependent stage transitions, add-hoc samplers, or a model cascade. Our contribution, Decomposable Flow Matching (DFM), is a simple and effective framework for the progressive generation of visual media. DFM applies Flow Matching independently at each level of a user-defined multi-scale representation (such as Laplacian pyramid). As shown by our experiments, our approach improves visual quality for both images and videos, featuring superior results compared to prior multistage frameworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores over the base architecture and 26.4% over the best-performing baseline, under the same training compute. When applied to finetuning of large models, such as FLUX, DFM shows faster convergence speed to the training distribution. Crucially, all these advantages are achieved with a single model, architectural simplicity, and minimal modifications to existing training pipelines.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19839","src/content/posts/2025-06-25-improving-progressive-generation-with-decomposable-flow-matching.md","54b7c49e562fd235",{"html":25,"metadata":33737},{"headings":33738,"localImagePaths":33739,"remoteImagePaths":33740,"frontmatter":33741,"imagePaths":33742},[],[],[],{"title":33730,"description":33731,"summary":33731,"pubDate":32084,"source":25191,"url":33733,"thumbnail":25193},[],"2025-06-25-improving-progressive-generation-with-decomposable-flow-matching.md","2025-06-25-improving-student-ai-interaction-through-pedagogical-prompting-an-example-in-computer-science-education",{"id":33744,"data":33746,"filePath":33751,"digest":33752,"rendered":33753,"legacyId":33760},{"title":33747,"description":33748,"summary":33748,"pubDate":33749,"source":25191,"url":33750,"thumbnail":25193},"Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education","arXiv:2506.19107v1 Announce Type: cross Abstract: With the proliferation of large language model (LLM) applications since 2022, their use in education has sparked both excitement and concern. Recent studies consistently highlight students' (mis)use of LLMs can hinder learning outcomes. This work aims to teach students how to effectively prompt LLMs to improve their learning. We first proposed pedagogical prompting, a theoretically-grounded new concept to elicit learning-oriented responses from LLMs. To move from concept design to a proof-of-concept learning intervention in real educational settings, we selected early undergraduate CS education (CS1/CS2) as the example context. We began with a formative survey study with instructors (N=36) teaching early-stage undergraduate-level CS courses to inform the instructional design based on classroom needs. Based on their insights, we designed and developed a learning intervention through an interactive system with scenario-based instruction to train pedagogical prompting skills. Finally, we evaluated its instructional effectiveness through a user study with CS novice students (N=22) using pre/post-tests. Through mixed methods analyses, our results indicate significant improvements in learners' LLM-based pedagogical help-seeking skills, along with positive attitudes toward the system and increased willingness to use pedagogical prompts in the future. Our contributions include (1) a theoretical framework of pedagogical prompting; (2) empirical insights into current instructor attitudes toward pedagogical prompting; and (3) a learning intervention design with an interactive learning tool and scenario-based instruction leading to promising results on teaching LLM-based help-seeking. Our approach is scalable for broader implementation in classrooms and has the potential to be integrated into tools like ChatGPT as an on-boarding experience to encourage learning-oriented use of generative AI.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19107","src/content/posts/2025-06-25-improving-student-ai-interaction-through-pedagogical-prompting-an-example-in-computer-science-education.md","d6b8f63e4129e148",{"html":25,"metadata":33754},{"headings":33755,"localImagePaths":33756,"remoteImagePaths":33757,"frontmatter":33758,"imagePaths":33759},[],[],[],{"title":33747,"description":33748,"summary":33748,"pubDate":32084,"source":25191,"url":33750,"thumbnail":25193},[],"2025-06-25-improving-student-ai-interaction-through-pedagogical-prompting-an-example-in-computer-science-education.md","2025-06-25-in-context-occams-razor-how-transformers-prefer-simpler-hypotheses-on-the-fly",{"id":33761,"data":33763,"filePath":33768,"digest":33769,"rendered":33770,"legacyId":33777},{"title":33764,"description":33765,"summary":33765,"pubDate":33766,"source":25191,"url":33767,"thumbnail":25193},"In-Context Occam's Razor: How Transformers Prefer Simpler Hypotheses on the Fly","arXiv:2506.19351v1 Announce Type: cross Abstract: In-context learning (ICL) enables transformers to adapt to new tasks through contextual examples without parameter updates. While existing research has typically studied ICL in fixed-complexity environments, practical language models encounter tasks spanning diverse complexity levels. This paper investigates how transformers navigate hierarchical task structures where higher-complexity categories can perfectly represent any pattern generated by simpler ones. We design well-controlled testbeds based on Markov chains and linear regression that reveal transformers not only identify the appropriate complexity level for each task but also accurately infer the corresponding parameters--even when the in-context examples are compatible with multiple complexity hypotheses. Notably, when presented with data generated by simpler processes, transformers consistently favor the least complex sufficient explanation. We theoretically explain this behavior through a Bayesian framework, demonstrating that transformers effectively implement an in-context Bayesian Occam's razor by balancing model fit against complexity penalties. We further ablate on the roles of model size, training mixture distribution, inference context length, and architecture. Finally, we validate this Occam's razor-like inductive bias on a pretrained GPT-4 model with Boolean-function tasks as case study, suggesting it may be inherent to transformers trained on diverse task distributions.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19351","src/content/posts/2025-06-25-in-context-occams-razor-how-transformers-prefer-simpler-hypotheses-on-the-fly.md","d950afafbe139b55",{"html":25,"metadata":33771},{"headings":33772,"localImagePaths":33773,"remoteImagePaths":33774,"frontmatter":33775,"imagePaths":33776},[],[],[],{"title":33764,"description":33765,"summary":33765,"pubDate":32084,"source":25191,"url":33767,"thumbnail":25193},[],"2025-06-25-in-context-occams-razor-how-transformers-prefer-simpler-hypotheses-on-the-fly.md","2025-06-25-indiefake-dataset-a-benchmark-dataset-for-audio-deepfake-detection",{"id":33778,"data":33780,"filePath":33785,"digest":33786,"rendered":33787,"legacyId":33794},{"title":33781,"description":33782,"summary":33782,"pubDate":33783,"source":25191,"url":33784,"thumbnail":25193},"IndieFake Dataset: A Benchmark Dataset for Audio Deepfake Detection","arXiv:2506.19014v1 Announce Type: cross Abstract: Advancements in audio deepfake technology offers benefits like AI assistants, better accessibility for speech impairments, and enhanced entertainment. However, it also poses significant risks to security, privacy, and trust in digital communications. Detecting and mitigating these threats requires comprehensive datasets. Existing datasets lack diverse ethnic accents, making them inadequate for many real-world scenarios. Consequently, models trained on these datasets struggle to detect audio deepfakes in diverse linguistic and cultural contexts such as in South-Asian countries. Ironically, there is a stark lack of South-Asian speaker samples in the existing datasets despite constituting a quarter of the worlds population. This work introduces the IndieFake Dataset (IFD), featuring 27.17 hours of bonafide and deepfake audio from 50 English speaking Indian speakers. IFD offers balanced data distribution and includes speaker-level characterization, absent in datasets like ASVspoof21 (DF). We evaluated various baselines on IFD against existing ASVspoof21 (DF) and In-The-Wild (ITW) datasets. IFD outperforms ASVspoof21 (DF) and proves to be more challenging compared to benchmark ITW dataset. The dataset will be publicly available upon acceptance.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19014","src/content/posts/2025-06-25-indiefake-dataset-a-benchmark-dataset-for-audio-deepfake-detection.md","a91533fd40c2855e",{"html":25,"metadata":33788},{"headings":33789,"localImagePaths":33790,"remoteImagePaths":33791,"frontmatter":33792,"imagePaths":33793},[],[],[],{"title":33781,"description":33782,"summary":33782,"pubDate":32084,"source":25191,"url":33784,"thumbnail":25193},[],"2025-06-25-indiefake-dataset-a-benchmark-dataset-for-audio-deepfake-detection.md","2025-06-25-interpretable-and-granular-video-based-quantification-of-motor-characteristics-from-the-finger-tapping-test-in-parkinson-disease",{"id":33795,"data":33797,"filePath":33802,"digest":33803,"rendered":33804,"legacyId":33811},{"title":33798,"description":33799,"summary":33799,"pubDate":33800,"source":25191,"url":33801,"thumbnail":25193},"Interpretable and Granular Video-Based Quantification of Motor Characteristics from the Finger Tapping Test in Parkinson Disease","arXiv:2506.18925v1 Announce Type: cross Abstract: Accurately quantifying motor characteristics in Parkinson disease (PD) is crucial for monitoring disease progression and optimizing treatment strategies. The finger-tapping test is a standard motor assessment. Clinicians visually evaluate a patient's tapping performance and assign an overall severity score based on tapping amplitude, speed, and irregularity. However, this subjective evaluation is prone to inter- and intra-rater variability, and does not offer insights into individual motor characteristics captured during this test. This paper introduces a granular computer vision-based method for quantifying PD motor characteristics from video recordings. Four sets of clinically relevant features are proposed to characterize hypokinesia, bradykinesia, sequence effect, and hesitation-halts. We evaluate our approach on video recordings and clinical evaluations of 74 PD patients from the Personalized Parkinson Project. Principal component analysis with varimax rotation shows that the video-based features corresponded to the four deficits. Additionally, video-based analysis has allowed us to identify further granular distinctions within sequence effect and hesitation-halts deficits. In the following, we have used these features to train machine learning classifiers to estimate the Movement Disorder Society Unified Parkinson Disease Rating Scale (MDS-UPDRS) finger-tapping score. Compared to state-of-the-art approaches, our method achieves a higher accuracy in MDS-UPDRS score prediction, while still providing an interpretable quantification of individual finger-tapping motor characteristics. In summary, the proposed framework provides a practical solution for the objective assessment of PD motor characteristics, that can potentially be applied in both clinical and remote settings. Future work is needed to assess its responsiveness to symptomatic treatment and disease progression.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18925","src/content/posts/2025-06-25-interpretable-and-granular-video-based-quantification-of-motor-characteristics-from-the-finger-tapping-test-in-parkinson-disease.md","475dfb8a85a834b7",{"html":25,"metadata":33805},{"headings":33806,"localImagePaths":33807,"remoteImagePaths":33808,"frontmatter":33809,"imagePaths":33810},[],[],[],{"title":33798,"description":33799,"summary":33799,"pubDate":32084,"source":25191,"url":33801,"thumbnail":25193},[],"2025-06-25-interpretable-and-granular-video-based-quantification-of-motor-characteristics-from-the-finger-tapping-test-in-parkinson-disease.md","2025-06-25-interpretable-hybrid-machine-learning-models-using-fold-r-and-answer-set-programming",{"id":33812,"data":33814,"filePath":33819,"digest":33820,"rendered":33821,"legacyId":33828},{"title":33815,"description":33816,"summary":33816,"pubDate":33817,"source":25191,"url":33818,"thumbnail":25193},"Interpretable Hybrid Machine Learning Models Using FOLD-R++ and Answer Set Programming","arXiv:2506.19573v1 Announce Type: new Abstract: Machine learning (ML) techniques play a pivotal role in high-stakes domains such as healthcare, where accurate predictions can greatly enhance decision-making. However, most high-performing methods such as neural networks and ensemble methods are often opaque, limiting trust and broader adoption. In parallel, symbolic methods like Answer Set Programming (ASP) offer the possibility of interpretable logical rules but do not always match the predictive power of ML models. This paper proposes a hybrid approach that integrates ASP-derived rules from the FOLD-R++ algorithm with black-box ML classifiers to selectively correct uncertain predictions and provide human-readable explanations. Experiments on five medical datasets reveal statistically significant performance gains in accuracy and F1 score. This study underscores the potential of combining symbolic reasoning with conventional ML to achieve high interpretability without sacrificing accuracy.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19573","src/content/posts/2025-06-25-interpretable-hybrid-machine-learning-models-using-fold-r-and-answer-set-programming.md","55dddbed943295ba",{"html":25,"metadata":33822},{"headings":33823,"localImagePaths":33824,"remoteImagePaths":33825,"frontmatter":33826,"imagePaths":33827},[],[],[],{"title":33815,"description":33816,"summary":33816,"pubDate":32084,"source":25191,"url":33818,"thumbnail":25193},[],"2025-06-25-interpretable-hybrid-machine-learning-models-using-fold-r-and-answer-set-programming.md","2025-06-25-interrogating-ai-characterizing-emergent-playful-interactions-with-chatgpt",{"id":33829,"data":33831,"filePath":33836,"digest":33837,"rendered":33838,"legacyId":33845},{"title":33832,"description":33833,"summary":33833,"pubDate":33834,"source":25191,"url":33835,"thumbnail":25193},"Interrogating AI: Characterizing Emergent Playful Interactions with ChatGPT","arXiv:2401.08405v4 Announce Type: replace-cross Abstract: In an era of AI's growing capabilities and influences, recent advancements are reshaping HCI and CSCW's view of AI. Playful interactions emerged as an important way for users to make sense of the ever-changing AI technologies, yet remained underexamined. We target this gap by investigating playful interactions exhibited by users of a popular AI technology, ChatGPT. Through a thematic analysis of 372 user-generated posts on the ChatGPT subreddit, we found that more than half (54%) of user discourse revolved around playful interactions. The analysis further allowed us to construct a preliminary framework to describe these interactions, categorizing them into six types: reflecting, jesting, imitating, challenging, tricking, and contriving; each included sub-categories. This study contributes to HCI and CSCW by identifying the diverse ways users engage in playful interactions with AI. It examines how these interactions can help users understand AI's agency, shape human-AI relationships, and provide insights for designing AI systems.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2401.08405","src/content/posts/2025-06-25-interrogating-ai-characterizing-emergent-playful-interactions-with-chatgpt.md","290cfbfabf52e3d9",{"html":25,"metadata":33839},{"headings":33840,"localImagePaths":33841,"remoteImagePaths":33842,"frontmatter":33843,"imagePaths":33844},[],[],[],{"title":33832,"description":33833,"summary":33833,"pubDate":32084,"source":25191,"url":33835,"thumbnail":25193},[],"2025-06-25-interrogating-ai-characterizing-emergent-playful-interactions-with-chatgpt.md","2025-06-25-is-an-object-centric-representation-beneficial-for-robotic-manipulation",{"id":33846,"data":33848,"filePath":33853,"digest":33854,"rendered":33855,"legacyId":33862},{"title":33849,"description":33850,"summary":33850,"pubDate":33851,"source":25191,"url":33852,"thumbnail":25193},"Is an object-centric representation beneficial for robotic manipulation ?","arXiv:2506.19408v1 Announce Type: new Abstract: Object-centric representation (OCR) has recently become a subject of interest in the computer vision community for learning a structured representation of images and videos. It has been several times presented as a potential way to improve data-efficiency and generalization capabilities to learn an agent on downstream tasks. However, most existing work only evaluates such models on scene decomposition, without any notion of reasoning over the learned representation. Robotic manipulation tasks generally involve multi-object environments with potential inter-object interaction. We thus argue that they are a very interesting playground to really evaluate the potential of existing object-centric work. To do so, we create several robotic manipulation tasks in simulated environments involving multiple objects (several distractors, the robot, etc.) and a high-level of randomization (object positions, colors, shapes, background, initial positions, etc.). We then evaluate one classical object-centric method across several generalization scenarios and compare its results against several state-of-the-art hollistic representations. Our results exhibit that existing methods are prone to failure in difficult scenarios involving complex scene structures, whereas object-centric methods help overcome these challenges.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19408","src/content/posts/2025-06-25-is-an-object-centric-representation-beneficial-for-robotic-manipulation.md","cecaf788435b20e8",{"html":25,"metadata":33856},{"headings":33857,"localImagePaths":33858,"remoteImagePaths":33859,"frontmatter":33860,"imagePaths":33861},[],[],[],{"title":33849,"description":33850,"summary":33850,"pubDate":32084,"source":25191,"url":33852,"thumbnail":25193},[],"2025-06-25-is-an-object-centric-representation-beneficial-for-robotic-manipulation.md","2025-06-25-iterative-quantum-feature-maps",{"id":33863,"data":33865,"filePath":33870,"digest":33871,"rendered":33872,"legacyId":33879},{"title":33866,"description":33867,"summary":33867,"pubDate":33868,"source":25191,"url":33869,"thumbnail":25193},"Iterative Quantum Feature Maps","arXiv:2506.19461v1 Announce Type: cross Abstract: Quantum machine learning models that leverage quantum circuits as quantum feature maps (QFMs) are recognized for their enhanced expressive power in learning tasks. Such models have demonstrated rigorous end-to-end quantum speedups for specific families of classification problems. However, deploying deep QFMs on real quantum hardware remains challenging due to circuit noise and hardware constraints. Additionally, variational quantum algorithms often suffer from computational bottlenecks, particularly in accurate gradient estimation, which significantly increases quantum resource demands during training. We propose Iterative Quantum Feature Maps (IQFMs), a hybrid quantum-classical framework that constructs a deep architecture by iteratively connecting shallow QFMs with classically computed augmentation weights. By incorporating contrastive learning and a layer-wise training mechanism, IQFMs effectively reduces quantum runtime and mitigates noise-induced degradation. In tasks involving noisy quantum data, numerical experiments show that IQFMs outperforms quantum convolutional neural networks, without requiring the optimization of variational quantum parameters. Even for a typical classical image classification benchmark, a carefully designed IQFMs achieves performance comparable to that of classical neural networks. This framework presents a promising path to address current limitations and harness the full potential of quantum-enhanced machine learning.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19461","src/content/posts/2025-06-25-iterative-quantum-feature-maps.md","6b47dd8452d73471",{"html":25,"metadata":33873},{"headings":33874,"localImagePaths":33875,"remoteImagePaths":33876,"frontmatter":33877,"imagePaths":33878},[],[],[],{"title":33866,"description":33867,"summary":33867,"pubDate":32084,"source":25191,"url":33869,"thumbnail":25193},[],"2025-06-25-iterative-quantum-feature-maps.md","2025-06-25-jcapt-a-joint-modeling-approach-for-capt",{"id":33880,"data":33882,"filePath":33887,"digest":33888,"rendered":33889,"legacyId":33896},{"title":33883,"description":33884,"summary":33884,"pubDate":33885,"source":25191,"url":33886,"thumbnail":25193},"JCAPT: A Joint Modeling Approach for CAPT","arXiv:2506.19315v1 Announce Type: cross Abstract: Effective pronunciation feedback is critical in second language (L2) learning, for which computer-assisted pronunciation training (CAPT) systems often encompass two key tasks: automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD). Recent work has shown that joint modeling of these two tasks can yield mutual benefits. Our unified framework leverages Mamba, a selective state space model (SSM), while integrating phonological features and think token strategies to jointly enhance interpretability and fine-grained temporal reasoning in APA and MDD. To our knowledge, this is the first study to combine phonological attribution, SSM-based modeling, and prompting in CAPT. A series of experiments conducted on the speechocean762 benchmark demonstrate that our model consistently outperforms prior methods, particularly on the MDD task.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19315","src/content/posts/2025-06-25-jcapt-a-joint-modeling-approach-for-capt.md","5ad2f3c69b2b68cf",{"html":25,"metadata":33890},{"headings":33891,"localImagePaths":33892,"remoteImagePaths":33893,"frontmatter":33894,"imagePaths":33895},[],[],[],{"title":33883,"description":33884,"summary":33884,"pubDate":32084,"source":25191,"url":33886,"thumbnail":25193},[],"2025-06-25-jcapt-a-joint-modeling-approach-for-capt.md","2025-06-25-jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval",{"id":33897,"data":33899,"filePath":33904,"digest":33905,"rendered":33906,"legacyId":33913},{"title":33900,"description":33901,"summary":33901,"pubDate":33902,"source":25191,"url":33903,"thumbnail":25193},"jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval","arXiv:2506.18902v2 Announce Type: replace Abstract: We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18902","src/content/posts/2025-06-25-jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval.md","4564abc3c193d6b0",{"html":25,"metadata":33907},{"headings":33908,"localImagePaths":33909,"remoteImagePaths":33910,"frontmatter":33911,"imagePaths":33912},[],[],[],{"title":33900,"description":33901,"summary":33901,"pubDate":32084,"source":25191,"url":33903,"thumbnail":25193},[],"2025-06-25-jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval.md","2025-06-25-joyagents-r1-joint-evolution-dynamics-for-versatile-multi-llm-agents-with-reinforcement-learning",{"id":33914,"data":33916,"filePath":33921,"digest":33922,"rendered":33923,"legacyId":33930},{"title":33917,"description":33918,"summary":33918,"pubDate":33919,"source":25191,"url":33920,"thumbnail":25193},"JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning","arXiv:2506.19846v1 Announce Type: new Abstract: Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm for increasingly complex tasks. However, joint evolution across heterogeneous agents remains challenging due to cooperative inefficiency and training instability. In this paper, we propose the joint evolution dynamics for MARL called JoyAgents-R1, which first applies Group Relative Policy Optimization (GRPO) to the joint training of heterogeneous multi-agents. By iteratively refining agents' large language models (LLMs) and memories, the method achieves holistic equilibrium with optimal decision-making and memory capabilities. Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on the behavior of each agent across entire reasoning trajectories to enhance GRPO sampling efficiency while maintaining policy diversity. Then, our marginal benefit-driven selection strategy identifies top-$K$ sampling groups with maximal reward fluctuations, enabling targeted agent model updates that improve training stability and maximize joint benefits through cost-effective parameter adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution mechanism that repurposes GRPO rewards as cost-free supervisory signals to eliminate repetitive reasoning and accelerate convergence. Experiments across general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves performance comparable to that of larger LLMs while built on smaller open-source models.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19846","src/content/posts/2025-06-25-joyagents-r1-joint-evolution-dynamics-for-versatile-multi-llm-agents-with-reinforcement-learning.md","906bcf9e94b3fe9b",{"html":25,"metadata":33924},{"headings":33925,"localImagePaths":33926,"remoteImagePaths":33927,"frontmatter":33928,"imagePaths":33929},[],[],[],{"title":33917,"description":33918,"summary":33918,"pubDate":32084,"source":25191,"url":33920,"thumbnail":25193},[],"2025-06-25-joyagents-r1-joint-evolution-dynamics-for-versatile-multi-llm-agents-with-reinforcement-learning.md","2025-06-25-kag-thinker-interactive-thinking-and-deep-reasoning-in-llms-via-knowledge-augmented-generation",{"id":33931,"data":33933,"filePath":33938,"digest":33939,"rendered":33940,"legacyId":33947},{"title":33934,"description":33935,"summary":33935,"pubDate":33936,"source":25191,"url":33937,"thumbnail":25193},"KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via Knowledge-Augmented Generation","arXiv:2506.17728v2 Announce Type: replace-cross Abstract: In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn interactive thinking and deep reasoning framework powered by a dedicated parameter-light large language model (LLM). Our approach constructs a structured thinking process for solving complex problems, enhancing the the logical coherence and contextual consistency of the reasoning process in question-answering (Q&amp;A) tasks on domain-specific knowledge bases (KBs) within LLMs. Following the textbf{Logical Form} guided retrieval and reasoning technology route of KAG, this framework first decomposes complex questions into independently solvable sub-problems (which are also referred to as logical forms) through textbf{breadth decomposition}. Each such logical form is represented in two equivalent forms-natural language and logical function-and subsequently classified as either a Knowledge Retrieval or Reasoning Analysis task. Dependencies and parameter passing between these tasks are explicitly modeled via logical function interfaces. In the solving process, the Retrieval function performs retrieval tasks. It retrieves one-hop structured and unstructured information of specified knowledge unit. While the Math and Deduce functions are used to perform reasoning analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external knowledge sources are regarded as equivalent KBs. We use the textbf{knowledge boundary} module to determine the optimal source using self-regulatory mechanisms such as confidence calibration and reflective reasoning, and use the textbf{depth solving} module to enhance the comprehensiveness of knowledge acquisition...",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.17728","src/content/posts/2025-06-25-kag-thinker-interactive-thinking-and-deep-reasoning-in-llms-via-knowledge-augmented-generation.md","ddd7576af333ef3b",{"html":25,"metadata":33941},{"headings":33942,"localImagePaths":33943,"remoteImagePaths":33944,"frontmatter":33945,"imagePaths":33946},[],[],[],{"title":33934,"description":33935,"summary":33935,"pubDate":32084,"source":25191,"url":33937,"thumbnail":25193},[],"2025-06-25-kag-thinker-interactive-thinking-and-deep-reasoning-in-llms-via-knowledge-augmented-generation.md","2025-06-25-kling-foley-multimodal-diffusion-transformer-for-high-quality-video-to-audio-generation",{"id":33948,"data":33950,"filePath":33955,"digest":33956,"rendered":33957,"legacyId":33964},{"title":33951,"description":33952,"summary":33952,"pubDate":33953,"source":25191,"url":33954,"thumbnail":25193},"Kling-Foley: Multimodal Diffusion Transformer for High-Quality Video-to-Audio Generation","arXiv:2506.19774v1 Announce Type: cross Abstract: We propose Kling-Foley, a large-scale multimodal Video-to-Audio generation model that synthesizes high-quality audio synchronized with video content. In Kling-Foley, we introduce multimodal diffusion transformers to model the interactions between video, audio, and text modalities, and combine it with a visual semantic representation module and an audio-visual synchronization module to enhance alignment capabilities. Specifically, these modules align video conditions with latent audio elements at the frame level, thereby improving semantic alignment and audio-visual synchronization. Together with text conditions, this integrated approach enables precise generation of video-matching sound effects. In addition, we propose a universal latent audio codec that can achieve high-quality modeling in various scenarios such as sound effects, speech, singing, and music. We employ a stereo rendering method that imbues synthesized audio with a spatial presence. At the same time, in order to make up for the incomplete types and annotations of the open-source benchmark, we also open-source an industrial-level benchmark Kling-Audio-Eval. Our experiments show that Kling-Foley trained with the flow matching objective achieves new audio-visual SOTA performance among public models in terms of distribution matching, semantic alignment, temporal alignment and audio quality.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19774","src/content/posts/2025-06-25-kling-foley-multimodal-diffusion-transformer-for-high-quality-video-to-audio-generation.md","7c321662fb3d6bc2",{"html":25,"metadata":33958},{"headings":33959,"localImagePaths":33960,"remoteImagePaths":33961,"frontmatter":33962,"imagePaths":33963},[],[],[],{"title":33951,"description":33952,"summary":33952,"pubDate":32084,"source":25191,"url":33954,"thumbnail":25193},[],"2025-06-25-kling-foley-multimodal-diffusion-transformer-for-high-quality-video-to-audio-generation.md","2025-06-25-knowrl-exploring-knowledgeable-reinforcement-learning-for-factuality",{"id":33965,"data":33967,"filePath":33972,"digest":33973,"rendered":33974,"legacyId":33981},{"title":33968,"description":33969,"summary":33969,"pubDate":33970,"source":25191,"url":33971,"thumbnail":25193},"KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality","arXiv:2506.19807v1 Announce Type: new Abstract: Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19807","src/content/posts/2025-06-25-knowrl-exploring-knowledgeable-reinforcement-learning-for-factuality.md","a58fed65b170853d",{"html":25,"metadata":33975},{"headings":33976,"localImagePaths":33977,"remoteImagePaths":33978,"frontmatter":33979,"imagePaths":33980},[],[],[],{"title":33968,"description":33969,"summary":33969,"pubDate":32084,"source":25191,"url":33971,"thumbnail":25193},[],"2025-06-25-knowrl-exploring-knowledgeable-reinforcement-learning-for-factuality.md","2025-06-25-kunlunbaizerag-reinforcement-learning-driven-inference-performance-leap-for-large-language-models",{"id":33982,"data":33984,"filePath":33989,"digest":33990,"rendered":33991,"legacyId":33998},{"title":33985,"description":33986,"summary":33986,"pubDate":33987,"source":25191,"url":33988,"thumbnail":25193},"KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap for Large Language Models","arXiv:2506.19466v1 Announce Type: new Abstract: This paper introduces KunLunBaizeRAG, a reinforcement learning-driven reasoning framework designed to enhance the reasoning capabilities of large language models (LLMs) in complex multi-hop question-answering tasks. The framework addresses key limitations of traditional RAG, such as retrieval drift, information redundancy, and strategy rigidity. Key innovations include the RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative Enhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR) mechanism, and a progressive hybrid training strategy. Experimental results demonstrate significant improvements in exact match (EM) and LLM-judged score (LJ) across four benchmarks, highlighting the framework's robustness and effectiveness in complex reasoning scenarios.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19466","src/content/posts/2025-06-25-kunlunbaizerag-reinforcement-learning-driven-inference-performance-leap-for-large-language-models.md","d3abce347b07b687",{"html":25,"metadata":33992},{"headings":33993,"localImagePaths":33994,"remoteImagePaths":33995,"frontmatter":33996,"imagePaths":33997},[],[],[],{"title":33985,"description":33986,"summary":33986,"pubDate":32084,"source":25191,"url":33988,"thumbnail":25193},[],"2025-06-25-kunlunbaizerag-reinforcement-learning-driven-inference-performance-leap-for-large-language-models.md","2025-06-25-language-model-re-rankers-are-fooled-by-lexical-similarities",{"id":33999,"data":34001,"filePath":34006,"digest":34007,"rendered":34008,"legacyId":34015},{"title":34002,"description":34003,"summary":34003,"pubDate":34004,"source":25191,"url":34005,"thumbnail":25193},"Language Model Re-rankers are Fooled by Lexical Similarities","arXiv:2502.17036v2 Announce Type: replace-cross Abstract: Language model (LM) re-rankers are used to refine retrieval results for retrieval-augmented generation (RAG). They are more expensive than lexical matching methods like BM25 but assumed to better process semantic information and the relations between the query and the retrieved answers. To understand whether LM re-rankers always live up to this assumption, we evaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our results show that LM re-rankers struggle to outperform a simple BM25 baseline on DRUID. Leveraging a novel separation metric based on BM25 scores, we explain and identify re-ranker errors stemming from lexical dissimilarities. We also investigate different methods to improve LM re-ranker performance and find these methods mainly useful for NQ. Taken together, our work identifies and explains weaknesses of LM re-rankers and points to the need for more adversarial and realistic datasets for their evaluation.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2502.17036","src/content/posts/2025-06-25-language-model-re-rankers-are-fooled-by-lexical-similarities.md","3d1ded0a9a214c0a",{"html":25,"metadata":34009},{"headings":34010,"localImagePaths":34011,"remoteImagePaths":34012,"frontmatter":34013,"imagePaths":34014},[],[],[],{"title":34002,"description":34003,"summary":34003,"pubDate":32084,"source":25191,"url":34005,"thumbnail":25193},[],"2025-06-25-language-model-re-rankers-are-fooled-by-lexical-similarities.md","2025-06-25-language-models-might-not-understand-you-evaluating-theory-of-mind-via-story-prompting",{"id":34016,"data":34018,"filePath":34023,"digest":34024,"rendered":34025,"legacyId":34032},{"title":34019,"description":34020,"summary":34020,"pubDate":34021,"source":25191,"url":34022,"thumbnail":25193},"Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting","arXiv:2506.19089v1 Announce Type: cross Abstract: We introduce $texttt{StorySim}$, a programmable framework for synthetically generating stories to evaluate the theory of mind (ToM) and world modeling (WM) capabilities of large language models (LLMs). Unlike prior benchmarks that may suffer from contamination in pretraining data, $texttt{StorySim}$ produces novel, compositional story prompts anchored by a highly controllable $texttt{Storyboard}$, enabling precise manipulation of character perspectives and events. We use this framework to design first- and second-order ToM tasks alongside WM tasks that control for the ability to track and model mental states. Our experiments across a suite of state-of-the-art LLMs reveal that most models perform better on WM tasks than ToM tasks, and that models tend to perform better reasoning with humans compared to inanimate objects. Additionally, our framework enabled us to find evidence of heuristic behavior such as recency bias and an over-reliance on earlier events in the story. All code for generating data and evaluations is freely available.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19089","src/content/posts/2025-06-25-language-models-might-not-understand-you-evaluating-theory-of-mind-via-story-prompting.md","3ccf51695c682e4f",{"html":25,"metadata":34026},{"headings":34027,"localImagePaths":34028,"remoteImagePaths":34029,"frontmatter":34030,"imagePaths":34031},[],[],[],{"title":34019,"description":34020,"summary":34020,"pubDate":32084,"source":25191,"url":34022,"thumbnail":25193},[],"2025-06-25-language-models-might-not-understand-you-evaluating-theory-of-mind-via-story-prompting.md","2025-06-25-large-language-models-for-automated-scholarly-paper-review-a-survey",{"id":34033,"data":34035,"filePath":34040,"digest":34041,"rendered":34042,"legacyId":34049},{"title":34036,"description":34037,"summary":34037,"pubDate":34038,"source":25191,"url":34039,"thumbnail":25193},"Large language models for automated scholarly paper review: A survey","arXiv:2501.10326v2 Announce Type: replace Abstract: Large language models (LLMs) have significantly impacted human society, influencing various domains. Among them, academia is not simply a domain affected by LLMs, but it is also the pivotal force in the development of LLMs. In academic publication, this phenomenon is represented during the incorporation of LLMs into the peer review mechanism for reviewing manuscripts. LLMs hold transformative potential for the full-scale implementation of automated scholarly paper review (ASPR), but they also pose new issues and challenges that need to be addressed. In this survey paper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin with a survey to find out which LLMs are used to conduct ASPR. Then, we review what ASPR-related technological bottlenecks have been solved with the incorporation of LLM technology. After that, we move on to explore new methods, new datasets, new source code, and new online systems that come with LLMs for ASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and investigate the attitudes and reactions of publishers and academia to ASPR. Lastly, we discuss the challenges and future directions associated with the development of LLMs for ASPR. This survey serves as an inspirational reference for the researchers and can promote the progress of ASPR for its actual implementation.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2501.10326","src/content/posts/2025-06-25-large-language-models-for-automated-scholarly-paper-review-a-survey.md","a84ea69df432cd33",{"html":25,"metadata":34043},{"headings":34044,"localImagePaths":34045,"remoteImagePaths":34046,"frontmatter":34047,"imagePaths":34048},[],[],[],{"title":34036,"description":34037,"summary":34037,"pubDate":32084,"source":25191,"url":34039,"thumbnail":25193},[],"2025-06-25-large-language-models-for-automated-scholarly-paper-review-a-survey.md","2025-06-25-laurel-learned-augmented-residual-layer",{"id":34050,"data":34052,"filePath":34057,"digest":34058,"rendered":34059,"legacyId":34066},{"title":34053,"description":34054,"summary":34054,"pubDate":34055,"source":25191,"url":34056,"thumbnail":25193},"LAuReL: Learned Augmented Residual Layer","arXiv:2411.07501v4 Announce Type: replace-cross Abstract: One of the core pillars of efficient deep learning methods is architectural improvements such as the residual/skip connection, which has led to significantly better model convergence and quality. Since then the residual connection has become ubiquitous in not just convolutional neural networks but also transformer-based architectures, the backbone of LLMs. In this paper we introduce Learned Augmented Residual Layer (LAuReL) -- a novel generalization of the canonical residual connection -- with the goal to be an in-situ replacement of the latter while outperforming on both model quality and footprint metrics. Our experiments show that using LAuReL can help boost performance for both vision and language models. For example, on the ResNet-50, ImageNet 1K task, it achieves 60% of the gains from adding an extra layer, while only adding 0.003% more parameters, and matches it while adding 2.6 times fewer parameters. Similarly, when pre-training 1B and 4B parameter LLMs, LAuReL improves performance on a variety of challenging downstream evaluation tasks by 2.54% to 20.05%, while adding only 0.012% and 0.1% additional parameters, respectively.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2411.07501","src/content/posts/2025-06-25-laurel-learned-augmented-residual-layer.md","badd483671d6811d",{"html":25,"metadata":34060},{"headings":34061,"localImagePaths":34062,"remoteImagePaths":34063,"frontmatter":34064,"imagePaths":34065},[],[],[],{"title":34053,"description":34054,"summary":34054,"pubDate":32084,"source":25191,"url":34056,"thumbnail":25193},[],"2025-06-25-laurel-learned-augmented-residual-layer.md","2025-06-25-learning-task-belief-similarity-with-latent-dynamics-for-meta-reinforcement-learning",{"id":34067,"data":34069,"filePath":34074,"digest":34075,"rendered":34076,"legacyId":34083},{"title":34070,"description":34071,"summary":34071,"pubDate":34072,"source":25191,"url":34073,"thumbnail":25193},"Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning","arXiv:2506.19785v1 Announce Type: new Abstract: Meta-reinforcement learning requires utilizing prior task distribution information obtained during exploration to rapidly adapt to unknown tasks. The efficiency of an agent's exploration hinges on accurately identifying the current task. Recent Bayes-Adaptive Deep RL approaches often rely on reconstructing the environment's reward signal, which is challenging in sparse reward settings, leading to suboptimal exploitation. Inspired by bisimulation metrics, which robustly extracts behavioral similarity in continuous MDPs, we propose SimBelief-a novel meta-RL framework via measuring similarity of task belief in Bayes-Adaptive MDP (BAMDP). SimBelief effectively extracts common features of similar task distributions, enabling efficient task identification and exploration in sparse reward environments. We introduce latent task belief metric to learn the common structure of similar tasks and incorporate it into the specific task belief. By learning the latent dynamics across task distributions, we connect shared latent task belief features with specific task features, facilitating rapid task identification and adaptation. Our method outperforms state-of-the-art baselines on sparse reward MuJoCo and panda-gym tasks.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19785","src/content/posts/2025-06-25-learning-task-belief-similarity-with-latent-dynamics-for-meta-reinforcement-learning.md","19572f06033293ef",{"html":25,"metadata":34077},{"headings":34078,"localImagePaths":34079,"remoteImagePaths":34080,"frontmatter":34081,"imagePaths":34082},[],[],[],{"title":34070,"description":34071,"summary":34071,"pubDate":32084,"source":25191,"url":34073,"thumbnail":25193},[],"2025-06-25-learning-task-belief-similarity-with-latent-dynamics-for-meta-reinforcement-learning.md","2025-06-25-lemmanaid-neuro-symbolic-lemma-conjecturing",{"id":34084,"data":34086,"filePath":34091,"digest":34092,"rendered":34093,"legacyId":34100},{"title":34087,"description":34088,"summary":34088,"pubDate":34089,"source":25191,"url":34090,"thumbnail":25193},"Lemmanaid: Neuro-Symbolic Lemma Conjecturing","arXiv:2504.04942v3 Announce Type: replace Abstract: Automatically conjecturing useful, interesting and novel lemmas would greatly improve automated reasoning tools and lower the bar for formalizing mathematics in proof assistants. It is however a very challenging task for both neural and symbolic approaches. We present the first steps towards a practical neuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language Models (LLMs) and symbolic methods, and evaluate it on proof libraries for the Isabelle proof assistant. We train an LLM to generate lemma templates that describe the shape of a lemma, and use symbolic methods to fill in the details. We compare Lemmanaid against an LLM trained to generate complete lemma statements as well as previous fully symbolic conjecturing methods. Lemmanaid outperforms both neural and symbolic methods on test sets from Isabelle's HOL library and from its Archive of Formal Proofs, discovering between 29-39.5% of the gold standard human written lemmas. This is 8-15% more lemmas than the neural-only method. By leveraging the best of both symbolic and neural methods we can generate useful lemmas for a wide range of input domains, facilitating computer-assisted theory development and formalization.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2504.04942","src/content/posts/2025-06-25-lemmanaid-neuro-symbolic-lemma-conjecturing.md","efde68e5f28c56f8",{"html":25,"metadata":34094},{"headings":34095,"localImagePaths":34096,"remoteImagePaths":34097,"frontmatter":34098,"imagePaths":34099},[],[],[],{"title":34087,"description":34088,"summary":34088,"pubDate":32084,"source":25191,"url":34090,"thumbnail":25193},[],"2025-06-25-lemmanaid-neuro-symbolic-lemma-conjecturing.md","2025-06-25-leveraging-large-language-models-to-democratize-access-to-costly-datasets-for-academic-research",{"id":34101,"data":34103,"filePath":34108,"digest":34109,"rendered":34110,"legacyId":34117},{"title":34104,"description":34105,"summary":34105,"pubDate":34106,"source":25191,"url":34107,"thumbnail":25193},"Leveraging Large Language Models to Democratize Access to Costly Datasets for Academic Research","arXiv:2412.02065v2 Announce Type: replace-cross Abstract: Unequal access to costly datasets essential for empirical research has long hindered researchers from disadvantaged institutions, limiting their ability to contribute to their fields and advance their careers. Recent breakthroughs in Large Language Models (LLMs) have the potential to democratize data access by automating data collection from unstructured sources. We develop and evaluate a novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation (RAG) framework to collect data from corporate disclosures. Our approach achieves human-level accuracy in collecting CEO pay ratios from approximately 10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000 10-K filings, with LLM processing times of 9 and 40 minutes respectively, each at a cost under $10. This stands in stark contrast to the hundreds of hours needed for manual collection or the thousands of dollars required for commercial database subscriptions. To foster a more inclusive research community by empowering researchers with limited resources to explore new avenues of inquiry, we share our methodology and the resulting datasets.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2412.02065","src/content/posts/2025-06-25-leveraging-large-language-models-to-democratize-access-to-costly-datasets-for-academic-research.md","96f88310679f6a7b",{"html":25,"metadata":34111},{"headings":34112,"localImagePaths":34113,"remoteImagePaths":34114,"frontmatter":34115,"imagePaths":34116},[],[],[],{"title":34104,"description":34105,"summary":34105,"pubDate":32084,"source":25191,"url":34107,"thumbnail":25193},[],"2025-06-25-leveraging-large-language-models-to-democratize-access-to-costly-datasets-for-academic-research.md","2025-06-25-llm-driven-medical-document-analysis-enhancing-trustworthy-pathology-and-differential-diagnosis",{"id":34118,"data":34120,"filePath":34125,"digest":34126,"rendered":34127,"legacyId":34134},{"title":34121,"description":34122,"summary":34122,"pubDate":34123,"source":25191,"url":34124,"thumbnail":25193},"LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology and Differential Diagnosis","arXiv:2506.19702v1 Announce Type: new Abstract: Medical document analysis plays a crucial role in extracting essential clinical insights from unstructured healthcare records, supporting critical tasks such as differential diagnosis. Determining the most probable condition among overlapping symptoms requires precise evaluation and deep medical expertise. While recent advancements in large language models (LLMs) have significantly enhanced performance in medical document analysis, privacy concerns related to sensitive patient data limit the use of online LLMs services in clinical settings. To address these challenges, we propose a trustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using low-rank adaptation, specifically optimized for differential diagnosis tasks. Our approach utilizes DDXPlus, the largest benchmark dataset for differential diagnosis, and demonstrates superior performance in pathology prediction and variable-length differential diagnosis compared to existing methods. The developed web-based platform allows users to submit their own unstructured medical documents and receive accurate, explainable diagnostic results. By incorporating advanced explainability techniques, the system ensures transparent and reliable predictions, fostering user trust and confidence. Extensive evaluations confirm that the proposed method surpasses current state-of-the-art models in predictive accuracy while offering practical utility in clinical settings. This work addresses the urgent need for reliable, explainable, and privacy-preserving artificial intelligence solutions, representing a significant advancement in intelligent medical document analysis for real-world healthcare applications. The code can be found at href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19702","src/content/posts/2025-06-25-llm-driven-medical-document-analysis-enhancing-trustworthy-pathology-and-differential-diagnosis.md","2a29aea4ae9df730",{"html":25,"metadata":34128},{"headings":34129,"localImagePaths":34130,"remoteImagePaths":34131,"frontmatter":34132,"imagePaths":34133},[],[],[],{"title":34121,"description":34122,"summary":34122,"pubDate":32084,"source":25191,"url":34124,"thumbnail":25193},[],"2025-06-25-llm-driven-medical-document-analysis-enhancing-trustworthy-pathology-and-differential-diagnosis.md","2025-06-25-llms-on-a-budget-say-hola",{"id":34135,"data":34137,"filePath":34142,"digest":34143,"rendered":34144,"legacyId":34151},{"title":34138,"description":34139,"summary":34139,"pubDate":34140,"source":25191,"url":34141,"thumbnail":25193},"LLMs on a Budget? Say HOLA","arXiv:2506.18952v1 Announce Type: cross Abstract: Running Large Language Models (LLMs) on edge devices is constrained by high compute and memory demands posing a barrier for real-time applications in sectors like healthcare, education, and embedded systems. Current solutions such as quantization, pruning, and retrieval-augmented generation (RAG) offer only partial optimizations and often compromise on speed or accuracy. We introduce HOLA, an end-to-end optimization framework for efficient LLM deployment. Internally, it leverages Hierarchical Speculative Decoding (HSD) for faster inference without quality loss. Externally, AdaComp-RAG adjusts retrieval complexity based on context needs. Together with LoBi, which blends structured pruning (LoRA) and quantization, HOLA delivers significant gains: 17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge devices like Jetson Nano--proving both scalable and production-ready.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18952","src/content/posts/2025-06-25-llms-on-a-budget-say-hola.md","63c0522ec51987e9",{"html":25,"metadata":34145},{"headings":34146,"localImagePaths":34147,"remoteImagePaths":34148,"frontmatter":34149,"imagePaths":34150},[],[],[],{"title":34138,"description":34139,"summary":34139,"pubDate":32084,"source":25191,"url":34141,"thumbnail":25193},[],"2025-06-25-llms-on-a-budget-say-hola.md","2025-06-25-local-look-ahead-guidance-via-verifier-in-the-loop-for-automated-theorem-proving",{"id":34152,"data":34154,"filePath":34159,"digest":34160,"rendered":34161,"legacyId":34168},{"title":34155,"description":34156,"summary":34156,"pubDate":34157,"source":25191,"url":34158,"thumbnail":25193},"Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving","arXiv:2503.09730v2 Announce Type: replace Abstract: The most promising recent methods for AI reasoning require applying variants of reinforcement learning (RL) either on rolled out trajectories from the LLMs, even for the step-wise rewards, or large quantities of human-annotated trajectory data. The reliance on the rolled-out trajectory renders the compute cost and time prohibitively high. In particular, the correctness of a reasoning trajectory can typically only be judged at its completion, leading to sparse rewards in RL or requiring expensive synthetic data generation in expert iteration-like methods. In this work, we focus on the Automatic Theorem Proving (ATP) task and propose a novel verifier-in-the-loop design, which, unlike existing approaches that leverage feedback on the entire reasoning trajectory, employs an automated verifier to give intermediate feedback at each step of the reasoning process. Using Lean as the verifier, we empirically show that the step-by-step local verification produces a global improvement in the model's reasoning accuracy and efficiency.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2503.09730","src/content/posts/2025-06-25-local-look-ahead-guidance-via-verifier-in-the-loop-for-automated-theorem-proving.md","d7aecdd96a8f3a30",{"html":25,"metadata":34162},{"headings":34163,"localImagePaths":34164,"remoteImagePaths":34165,"frontmatter":34166,"imagePaths":34167},[],[],[],{"title":34155,"description":34156,"summary":34156,"pubDate":32084,"source":25191,"url":34158,"thumbnail":25193},[],"2025-06-25-local-look-ahead-guidance-via-verifier-in-the-loop-for-automated-theorem-proving.md","2025-06-25-long-context-generalization-with-sparse-attention",{"id":34169,"data":34171,"filePath":34174,"digest":34175,"rendered":34176,"legacyId":34183},{"title":28912,"description":34172,"summary":34172,"pubDate":34173,"source":25191,"url":28915,"thumbnail":25193},"arXiv:2506.16640v2 Announce Type: replace-cross Abstract: Transformer-based architectures traditionally employ softmax to compute attention weights, which produces dense distributions over all tokens in a sequence. While effective in many settings, this density has been shown to be detrimental for tasks that demand precise focus on fixed-size patterns: as sequence length increases, non-informative tokens accumulate attention probability mass, leading to dispersion and representational collapse. We show in this paper that sparse attention mechanisms using $alpha$-entmax can avoid these issues, due to their ability to assign exact zeros to irrelevant tokens. Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows $alpha$-entmax with a learnable temperature parameter, allowing the attention distribution to interpolate between sparse (pattern-focused) and dense (softmax-like) regimes. Finally, we show that the ability to locate and generalize fixed-size patterns can be further improved through a careful design of position encodings, which impacts both dense and sparse attention methods. By integrating ASEntmax into standard transformer layers alongside proper positional encodings, we show that our models greatly outperform softmax, scalable softmax, and fixed-temperature $alpha$-entmax baselines on long-context generalization.",["Date","2025-06-25T04:00:00.000Z"],"src/content/posts/2025-06-25-long-context-generalization-with-sparse-attention.md","aef579b748709fdc",{"html":25,"metadata":34177},{"headings":34178,"localImagePaths":34179,"remoteImagePaths":34180,"frontmatter":34181,"imagePaths":34182},[],[],[],{"title":28912,"description":34172,"summary":34172,"pubDate":32084,"source":25191,"url":28915,"thumbnail":25193},[],"2025-06-25-long-context-generalization-with-sparse-attention.md","2025-06-25-lost-in-translation-converting-regexes-for-log-parsing-into-dynatrace-pattern-language",{"id":34184,"data":34186,"filePath":34191,"digest":34192,"rendered":34193,"legacyId":34200},{"title":34187,"description":34188,"summary":34188,"pubDate":34189,"source":25191,"url":34190,"thumbnail":25193},"Lost in Translation? Converting RegExes for Log Parsing into Dynatrace Pattern Language","arXiv:2506.19539v1 Announce Type: cross Abstract: Log files provide valuable information for detecting and diagnosing problems in enterprise software applications and data centers. Several log analytics tools and platforms were developed to help filter and extract information from logs, typically using regular expressions (RegExes). Recent commercial log analytics platforms provide domain-specific languages specifically designed for log parsing, such as Grok or the Dynatrace Pattern Language (DPL). However, users who want to migrate to these platforms must manually convert their RegExes into the new pattern language, which is costly and error-prone. In this work, we present Reptile, which combines a rule-based approach for converting RegExes into DPL patterns with a best-effort approach for cases where a full conversion is impossible. Furthermore, it integrates GPT-4 to optimize the obtained DPL patterns. The evaluation with 946 RegExes collected from a large company shows that Reptile safely converted 73.7% of them. The evaluation of Reptile's pattern optimization with 23 real-world RegExes showed an F1-score and MCC above 0.91. These results are promising and have ample practical implications for companies that migrate to a modern log analytics platform, such as Dynatrace.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19539","src/content/posts/2025-06-25-lost-in-translation-converting-regexes-for-log-parsing-into-dynatrace-pattern-language.md","7c8b890796db7216",{"html":25,"metadata":34194},{"headings":34195,"localImagePaths":34196,"remoteImagePaths":34197,"frontmatter":34198,"imagePaths":34199},[],[],[],{"title":34187,"description":34188,"summary":34188,"pubDate":32084,"source":25191,"url":34190,"thumbnail":25193},[],"2025-06-25-lost-in-translation-converting-regexes-for-log-parsing-into-dynatrace-pattern-language.md","2025-06-25-mambaoutrs-a-hybrid-cnn-fourier-architecture-for-remote-sensing-image-classification",{"id":34201,"data":34203,"filePath":34208,"digest":34209,"rendered":34210,"legacyId":34217},{"title":34204,"description":34205,"summary":34205,"pubDate":34206,"source":25191,"url":34207,"thumbnail":25193},"MambaOutRS: A Hybrid CNN-Fourier Architecture for Remote Sensing Image Classification","arXiv:2506.19561v1 Announce Type: cross Abstract: Recent advances in deep learning for vision tasks have seen the rise of State Space Models (SSMs) like Mamba, celebrated for their linear scalability. However, their adaptation to 2D visual data often necessitates complex modifications that may diminish efficiency. In this paper, we introduce MambaOutRS, a novel hybrid convolutional architecture for remote sensing image classification that re-evaluates the necessity of recurrent SSMs. MambaOutRS builds upon stacked Gated CNN blocks for local feature extraction and introduces a novel Fourier Filter Gate (FFG) module that operates in the frequency domain to capture global contextual information efficiently. Our architecture employs a four-stage hierarchical design and was extensively evaluated on challenging remote sensing datasets: UC Merced, AID, NWPU-RESISC45, and EuroSAT. MambaOutRS consistently achieved state-of-the-art (SOTA) performance across these benchmarks. Notably, our MambaOutRS-t variant (24.0M parameters) attained the highest F1-scores of 98.41% on UC Merced and 95.99% on AID, significantly outperforming existing baselines, including larger transformer models and Mamba-based architectures, despite using considerably fewer parameters. An ablation study conclusively demonstrates the critical role of the Fourier Filter Gate in enhancing the model's ability to capture global spatial patterns, leading to robust and accurate classification. These results strongly suggest that the complexities of recurrent SSMs can be effectively superseded by a judicious combination of gated convolutions for spatial mixing and frequency-based gates for spectral global context. Thus, MambaOutRS provides a compelling and efficient paradigm for developing high-performance deep learning models in remote sensing and other vision domains, particularly where computational efficiency is paramount.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19561","src/content/posts/2025-06-25-mambaoutrs-a-hybrid-cnn-fourier-architecture-for-remote-sensing-image-classification.md","a321bf7644c1d801",{"html":25,"metadata":34211},{"headings":34212,"localImagePaths":34213,"remoteImagePaths":34214,"frontmatter":34215,"imagePaths":34216},[],[],[],{"title":34204,"description":34205,"summary":34205,"pubDate":32084,"source":25191,"url":34207,"thumbnail":25193},[],"2025-06-25-mambaoutrs-a-hybrid-cnn-fourier-architecture-for-remote-sensing-image-classification.md","2025-06-25-mate-llm-powered-multi-agent-translation-environment-for-accessibility-applications",{"id":34218,"data":34220,"filePath":34225,"digest":34226,"rendered":34227,"legacyId":34234},{"title":34221,"description":34222,"summary":34222,"pubDate":34223,"source":25191,"url":34224,"thumbnail":25193},"MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility Applications","arXiv:2506.19502v1 Announce Type: cross Abstract: Accessibility remains a critical concern in today's society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user's needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19502","src/content/posts/2025-06-25-mate-llm-powered-multi-agent-translation-environment-for-accessibility-applications.md","f9600b4cd7249253",{"html":25,"metadata":34228},{"headings":34229,"localImagePaths":34230,"remoteImagePaths":34231,"frontmatter":34232,"imagePaths":34233},[],[],[],{"title":34221,"description":34222,"summary":34222,"pubDate":32084,"source":25191,"url":34224,"thumbnail":25193},[],"2025-06-25-mate-llm-powered-multi-agent-translation-environment-for-accessibility-applications.md","2025-06-25-mcp-zero-active-tool-discovery-for-autonomous-llm-agents",{"id":34235,"data":34237,"filePath":34242,"digest":34243,"rendered":34244,"legacyId":34251},{"title":34238,"description":34239,"summary":34239,"pubDate":34240,"source":25191,"url":34241,"thumbnail":25193},"MCP-Zero: Active Tool Discovery for Autonomous LLM Agents","arXiv:2506.01056v4 Announce Type: replace Abstract: True intelligence requires active capability acquisition, yet current LLM agents inject pre-defined tool schemas into prompts, reducing models to passive selectors and falling short of robust general-purpose agency. We introduce MCP-Zero, an active agent framework that restores tool discovery autonomy to LLMs themselves. Instead of overwhelming models with all available tools, MCP-Zero enables agents to actively identify capability gaps, and request specific tools on-demand, transforming them from large-scale retrievers into genuine autonomous agents. The framework operates through three core mechanisms: (1) Active Tool Request, where models autonomously generate structured requests specifying their exact tool requirements; (2) Hierarchical Semantic Routing, a two-stage algorithm that matches requests to relevant servers and tools through improved semantic alignment; (3) Iterative Capability Extension, enabling agents to progressively build cross-domain toolchains while maintaining minimal context footprint. We construct MCP-tools, a comprehensive dataset of 308 MCP servers and 2,797 tools from the official Model-Context-Protocol repository. Experiments demonstrate that MCP-Zero preserves agent autonomy while achieving substantial efficiency gains: (i) accurate tool selection from nearly 3k candidates across 248.1k tokens; (ii) 98% reduction in token consumption on APIBank while maintaining high accuracy; and (iii) consistent multi-turn performance that scales with tool ecosystem growth. This work establishes active tool discovery as a fundamental design pattern for scalable autonomous agent systems.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.01056","src/content/posts/2025-06-25-mcp-zero-active-tool-discovery-for-autonomous-llm-agents.md","ab51d5d335c81591",{"html":25,"metadata":34245},{"headings":34246,"localImagePaths":34247,"remoteImagePaths":34248,"frontmatter":34249,"imagePaths":34250},[],[],[],{"title":34238,"description":34239,"summary":34239,"pubDate":32084,"source":25191,"url":34241,"thumbnail":25193},[],"2025-06-25-mcp-zero-active-tool-discovery-for-autonomous-llm-agents.md","2025-06-25-mederr-ct-a-visual-question-answering-benchmark-for-identifying-and-correcting-errors-in-ct-reports",{"id":34252,"data":34254,"filePath":34259,"digest":34260,"rendered":34261,"legacyId":34268},{"title":34255,"description":34256,"summary":34256,"pubDate":34257,"source":25191,"url":34258,"thumbnail":25193},"MedErr-CT: A Visual Question Answering Benchmark for Identifying and Correcting Errors in CT Reports","arXiv:2506.19217v1 Announce Type: cross Abstract: Computed Tomography (CT) plays a crucial role in clinical diagnosis, but the growing demand for CT examinations has raised concerns about diagnostic errors. While Multimodal Large Language Models (MLLMs) demonstrate promising comprehension of medical knowledge, their tendency to produce inaccurate information highlights the need for rigorous validation. However, existing medical visual question answering (VQA) benchmarks primarily focus on simple visual recognition tasks, lacking clinical relevance and failing to assess expert-level knowledge. We introduce MedErr-CT, a novel benchmark for evaluating medical MLLMs' ability to identify and correct errors in CT reports through a VQA framework. The benchmark includes six error categories - four vision-centric errors (Omission, Insertion, Direction, Size) and two lexical error types (Unit, Typo) - and is organized into three task levels: classification, detection, and correction. Using this benchmark, we quantitatively assess the performance of state-of-the-art 3D medical MLLMs, revealing substantial variation in their capabilities across different error types. Our benchmark contributes to the development of more reliable and clinically applicable MLLMs, ultimately helping reduce diagnostic errors and improve accuracy in clinical practice. The code and datasets are available at https://github.com/babbu3682/MedErr-CT.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19217","src/content/posts/2025-06-25-mederr-ct-a-visual-question-answering-benchmark-for-identifying-and-correcting-errors-in-ct-reports.md","fc3d251fb940a314",{"html":25,"metadata":34262},{"headings":34263,"localImagePaths":34264,"remoteImagePaths":34265,"frontmatter":34266,"imagePaths":34267},[],[],[],{"title":34255,"description":34256,"summary":34256,"pubDate":32084,"source":25191,"url":34258,"thumbnail":25193},[],"2025-06-25-mederr-ct-a-visual-question-answering-benchmark-for-identifying-and-correcting-errors-in-ct-reports.md","2025-06-25-mem4nav-boosting-vision-and-language-navigation-in-urban-environments-with-a-hierarchical-spatial-cognition-long-short-memory-system",{"id":34269,"data":34271,"filePath":34276,"digest":34277,"rendered":34278,"legacyId":34285},{"title":34272,"description":34273,"summary":34273,"pubDate":34274,"source":25191,"url":34275,"thumbnail":25193},"Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System","arXiv:2506.19433v1 Announce Type: cross Abstract: Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19433","src/content/posts/2025-06-25-mem4nav-boosting-vision-and-language-navigation-in-urban-environments-with-a-hierarchical-spatial-cognition-long-short-memory-system.md","bdbe40ab8ffde45f",{"html":25,"metadata":34279},{"headings":34280,"localImagePaths":34281,"remoteImagePaths":34282,"frontmatter":34283,"imagePaths":34284},[],[],[],{"title":34272,"description":34273,"summary":34273,"pubDate":32084,"source":25191,"url":34275,"thumbnail":25193},[],"2025-06-25-mem4nav-boosting-vision-and-language-navigation-in-urban-environments-with-a-hierarchical-spatial-cognition-long-short-memory-system.md","2025-06-25-mememind-a-large-scale-multimodal-dataset-with-chain-of-thought-reasoning-for-harmful-meme-detection",{"id":34286,"data":34288,"filePath":34293,"digest":34294,"rendered":34295,"legacyId":34302},{"title":34289,"description":34290,"summary":34290,"pubDate":34291,"source":25191,"url":34292,"thumbnail":25193},"MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection","arXiv:2506.18919v1 Announce Type: cross Abstract: The rapid development of social media has intensified the spread of harmful content. Harmful memes, which integrate both images and text, pose significant challenges for automated detection due to their implicit semantics and complex multimodal interactions. Although existing research has made progress in detection accuracy and interpretability, the lack of a systematic, large-scale, diverse, and highly explainable dataset continues to hinder further advancement in this field. To address this gap, we introduce MemeMind, a novel dataset featuring scientifically rigorous standards, large scale, diversity, bilingual support (Chinese and English), and detailed Chain-of-Thought (CoT) annotations. MemeMind fills critical gaps in current datasets by offering comprehensive labeling and explicit reasoning traces, thereby providing a solid foundation for enhancing harmful meme detection. In addition, we propose an innovative detection framework, MemeGuard, which effectively integrates multimodal information with reasoning process modeling, significantly improving models' ability to understand and identify harmful memes. Extensive experiments conducted on the MemeMind dataset demonstrate that MemeGuard consistently outperforms existing state-of-the-art methods in harmful meme detection tasks.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18919","src/content/posts/2025-06-25-mememind-a-large-scale-multimodal-dataset-with-chain-of-thought-reasoning-for-harmful-meme-detection.md","3bf72518a28e1b25",{"html":25,"metadata":34296},{"headings":34297,"localImagePaths":34298,"remoteImagePaths":34299,"frontmatter":34300,"imagePaths":34301},[],[],[],{"title":34289,"description":34290,"summary":34290,"pubDate":32084,"source":25191,"url":34292,"thumbnail":25193},[],"2025-06-25-mememind-a-large-scale-multimodal-dataset-with-chain-of-thought-reasoning-for-harmful-meme-detection.md","2025-06-25-merging-ai-and-underwater-photography-to-reveal-hidden-ocean-worlds",{"id":34303,"data":34305,"filePath":34311,"digest":34312,"rendered":34313,"legacyId":34321},{"title":34306,"description":34307,"summary":34307,"pubDate":34308,"source":21340,"url":34309,"thumbnail":34310},"Merging AI and underwater photography to reveal hidden ocean worlds","The LOBSTgER research initiative at MIT Sea Grant explores how generative AI can expand scientific storytelling by building on field-based photographic data.",["Date","2025-06-25T13:55:00.000Z"],"https://news.mit.edu/2025/lobstger-merging-ai-underwater-photography-to-reveal-hidden-ocean-worlds-0625","https://news.mit.edu/sites/default/files/images/202506/mit-LOBSTgER.jpg","src/content/posts/2025-06-25-merging-ai-and-underwater-photography-to-reveal-hidden-ocean-worlds.md","eaccbf6f6c259bad",{"html":25,"metadata":34314},{"headings":34315,"localImagePaths":34316,"remoteImagePaths":34317,"frontmatter":34318,"imagePaths":34320},[],[],[],{"title":34306,"description":34307,"summary":34307,"pubDate":34319,"source":21340,"url":34309,"thumbnail":34310},"Wed, 25 Jun 2025 09:55:00 -0400",[],"2025-06-25-merging-ai-and-underwater-photography-to-reveal-hidden-ocean-worlds.md","2025-06-25-meta-reasoner-dynamic-guidance-for-optimized-inference-time-reasoning-in-large-language-models",{"id":34322,"data":34324,"filePath":34329,"digest":34330,"rendered":34331,"legacyId":34338},{"title":34325,"description":34326,"summary":34326,"pubDate":34327,"source":25191,"url":34328,"thumbnail":25193},"Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models","arXiv:2502.19918v3 Announce Type: replace Abstract: Large Language Models (LLMs) increasingly rely on prolonged reasoning chains to solve complex tasks. However, this trial-and-error approach often leads to high computational overhead and error propagation, where early mistakes can derail subsequent steps. To address these issues, we introduce Meta-Reasoner, a framework that dynamically optimizes inference-time reasoning by enabling LLMs to 'think about how to think.' Drawing inspiration from human meta-cognition and dual-process theory, Meta-Reasoner operates as a strategic advisor, decoupling high-level guidance from step-by-step generation. It employs contextual multi-armed bandits to iteratively evaluate reasoning progress and select optimal strategies (e.g., backtrack, clarify ambiguity, restart from scratch, or propose alternative approaches), and reallocates computational resources toward the most promising paths. Our evaluations on mathematical reasoning and puzzles highlight the potential of dynamic reasoning chains to overcome inherent challenges in the LLM reasoning process and also show promise in broader applications, offering a scalable and adaptable solution for reasoning-intensive tasks.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2502.19918","src/content/posts/2025-06-25-meta-reasoner-dynamic-guidance-for-optimized-inference-time-reasoning-in-large-language-models.md","7a43b99c1000274a",{"html":25,"metadata":34332},{"headings":34333,"localImagePaths":34334,"remoteImagePaths":34335,"frontmatter":34336,"imagePaths":34337},[],[],[],{"title":34325,"description":34326,"summary":34326,"pubDate":32084,"source":25191,"url":34328,"thumbnail":25193},[],"2025-06-25-meta-reasoner-dynamic-guidance-for-optimized-inference-time-reasoning-in-large-language-models.md","2025-06-25-mixture-of-cache-conditional-experts-for-efficient-mobile-device-inference",{"id":34339,"data":34341,"filePath":34346,"digest":34347,"rendered":34348,"legacyId":34355},{"title":34342,"description":34343,"summary":34343,"pubDate":34344,"source":25191,"url":34345,"thumbnail":25193},"Mixture of Cache-Conditional Experts for Efficient Mobile Device Inference","arXiv:2412.00099v2 Announce Type: replace-cross Abstract: Mixture of Experts (MoE) LLMs have recently gained attention for their ability to enhance performance by selectively engaging specialized subnetworks or 'experts' for each input. However, deploying MoEs on memory-constrained devices remains challenging, particularly when generating tokens sequentially with a batch size of one, as opposed to typical high-throughput settings involving long sequences or large batches. In this work, we optimize MoE on memory-constrained devices where only a subset of expert weights fit in DRAM. We introduce a novel cache-aware routing strategy that leverages expert reuse during token generation to improve cache locality. We evaluate our approach on language modeling, MMLU, and GSM8K benchmarks and present on-device results demonstrating 2$times$ speedups on mobile devices, offering a flexible, training-free solution to extend MoE's applicability across real-world applications.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2412.00099","src/content/posts/2025-06-25-mixture-of-cache-conditional-experts-for-efficient-mobile-device-inference.md","e6503ab6bc6840f0",{"html":25,"metadata":34349},{"headings":34350,"localImagePaths":34351,"remoteImagePaths":34352,"frontmatter":34353,"imagePaths":34354},[],[],[],{"title":34342,"description":34343,"summary":34343,"pubDate":32084,"source":25191,"url":34345,"thumbnail":25193},[],"2025-06-25-mixture-of-cache-conditional-experts-for-efficient-mobile-device-inference.md","2025-06-25-most-mr-reconstruction-optimization-for-multiple-downstream-tasks-via-continual-learning",{"id":34356,"data":34358,"filePath":34363,"digest":34364,"rendered":34365,"legacyId":34372},{"title":34359,"description":34360,"summary":34360,"pubDate":34361,"source":25191,"url":34362,"thumbnail":25193},"MOST: MR reconstruction Optimization for multiple downStream Tasks via continual learning","arXiv:2409.10394v3 Announce Type: replace-cross Abstract: Deep learning-based Magnetic Resonance (MR) reconstruction methods have focused on generating high-quality images but often overlook the impact on downstream tasks (e.g., segmentation) that utilize the reconstructed images. Cascading separately trained reconstruction network and downstream task network has been shown to introduce performance degradation due to error propagation and domain gaps between training datasets. To mitigate this issue, downstream task-oriented reconstruction optimization has been proposed for a single downstream task. Expanding this optimization to multi-task scenarios is not straightforward. In this work, we extended this optimization to sequentially introduced multiple downstream tasks and demonstrated that a single MR reconstruction network can be optimized for multiple downstream tasks by deploying continual learning (MOST). MOST integrated techniques from replay-based continual learning and image-guided loss to overcome catastrophic forgetting. Comparative experiments demonstrated that MOST outperformed a reconstruction network without finetuning, a reconstruction network with na'ive finetuning, and conventional continual learning methods. The source code is available at: https://github.com/SNU-LIST/MOST.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2409.10394","src/content/posts/2025-06-25-most-mr-reconstruction-optimization-for-multiple-downstream-tasks-via-continual-learning.md","742f1af9db23a6b8",{"html":25,"metadata":34366},{"headings":34367,"localImagePaths":34368,"remoteImagePaths":34369,"frontmatter":34370,"imagePaths":34371},[],[],[],{"title":34359,"description":34360,"summary":34360,"pubDate":32084,"source":25191,"url":34362,"thumbnail":25193},[],"2025-06-25-most-mr-reconstruction-optimization-for-multiple-downstream-tasks-via-continual-learning.md","2025-06-25-mubench-assessment-of-multilingual-capabilities-of-large-language-models-across-61-languages",{"id":34373,"data":34375,"filePath":34380,"digest":34381,"rendered":34382,"legacyId":34389},{"title":34376,"description":34377,"summary":34377,"pubDate":34378,"source":25191,"url":34379,"thumbnail":25193},"MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages","arXiv:2506.19468v1 Announce Type: cross Abstract: Multilingual large language models (LLMs) are advancing rapidly, with new models frequently claiming support for an increasing number of languages. However, existing evaluation datasets are limited and lack cross-lingual alignment, leaving assessments of multilingual capabilities fragmented in both language and skill coverage. To address this, we introduce MuBench, a benchmark covering 61 languages and evaluating a broad range of capabilities. We evaluate several state-of-the-art multilingual LLMs and find notable gaps between claimed and actual language coverage, particularly a persistent performance disparity between English and low-resource languages. Leveraging MuBench's alignment, we propose Multilingual Consistency (MLC) as a complementary metric to accuracy for analyzing performance bottlenecks and guiding model improvement. Finally, we pretrain a suite of 1.2B-parameter models on English and Chinese with 500B tokens, varying language ratios and parallel data proportions to investigate cross-lingual transfer dynamics.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19468","src/content/posts/2025-06-25-mubench-assessment-of-multilingual-capabilities-of-large-language-models-across-61-languages.md","0174266d246572cf",{"html":25,"metadata":34383},{"headings":34384,"localImagePaths":34385,"remoteImagePaths":34386,"frontmatter":34387,"imagePaths":34388},[],[],[],{"title":34376,"description":34377,"summary":34377,"pubDate":32084,"source":25191,"url":34379,"thumbnail":25193},[],"2025-06-25-mubench-assessment-of-multilingual-capabilities-of-large-language-models-across-61-languages.md","2025-06-25-multi-continental-healthcare-modelling-using-blockchain-enabled-federated-learning",{"id":34390,"data":34392,"filePath":34397,"digest":34398,"rendered":34399,"legacyId":34406},{"title":34393,"description":34394,"summary":34394,"pubDate":34395,"source":25191,"url":34396,"thumbnail":25193},"Multi-Continental Healthcare Modelling Using Blockchain-Enabled Federated Learning","arXiv:2410.17933v3 Announce Type: replace-cross Abstract: One of the biggest challenges of building artificial intelligence (AI) model in the healthcare area is the data sharing. Since healthcare data is private, sensitive, and heterogeneous, collecting sufficient data for modelling is exhausting, costly, and sometimes impossible. In this paper, we propose a framework for global healthcare modelling using datasets from multi-continents (Europe, North America, and Asia) without sharing the local datasets, and choose glucose management as a study model to verify its effectiveness. Technically, blockchain-enabled federated learning is implemented with adaptation to meet the privacy and safety requirements of healthcare data, meanwhile, it rewards honest participation and penalizes malicious activities using its on-chain incentive mechanism. Experimental results show that the proposed framework is effective, efficient, and privacy-preserving. Its prediction accuracy consistently outperforms models trained on limited personal data and achieves comparable or even slightly better results than centralized training in certain scenarios, all while preserving data privacy. This work paves the way for international collaborations on healthcare projects, where additional data is crucial for reducing bias and providing benefits to humanity.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2410.17933","src/content/posts/2025-06-25-multi-continental-healthcare-modelling-using-blockchain-enabled-federated-learning.md","a9438049e5f991a6",{"html":25,"metadata":34400},{"headings":34401,"localImagePaths":34402,"remoteImagePaths":34403,"frontmatter":34404,"imagePaths":34405},[],[],[],{"title":34393,"description":34394,"summary":34394,"pubDate":32084,"source":25191,"url":34396,"thumbnail":25193},[],"2025-06-25-multi-continental-healthcare-modelling-using-blockchain-enabled-federated-learning.md","2025-06-25-multimodal-fusion-slam-with-fourier-attention",{"id":34407,"data":34409,"filePath":34414,"digest":34415,"rendered":34416,"legacyId":34423},{"title":34410,"description":34411,"summary":34411,"pubDate":34412,"source":25191,"url":34413,"thumbnail":25193},"Multimodal Fusion SLAM with Fourier Attention","arXiv:2506.18204v2 Announce Type: replace-cross Abstract: Visual SLAM is particularly challenging in environments affected by noise, varying lighting conditions, and darkness. Learning-based optical flow algorithms can leverage multiple modalities to address these challenges, but traditional optical flow-based visual SLAM approaches often require significant computational resources.To overcome this limitation, we propose FMF-SLAM, an efficient multimodal fusion SLAM method that utilizes fast Fourier transform (FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel Fourier-based self-attention and cross-attention mechanism to extract features from RGB and depth signals. We further enhance the interaction of multimodal features by incorporating multi-scale knowledge distillation across modalities. We also demonstrate the practical feasibility of FMF-SLAM in real-world scenarios with real time performance by integrating it with a security robot by fusing with a global positioning module GNSS-RTK and global Bundle Adjustment. Our approach is validated using video sequences from TUM, TartanAir, and our real-world datasets, showcasing state-of-the-art performance under noisy, varying lighting, and dark conditions.Our code and datasets are available at https://github.com/youjie-zhou/FMF-SLAM.git.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18204","src/content/posts/2025-06-25-multimodal-fusion-slam-with-fourier-attention.md","33aa493bdb7b3999",{"html":25,"metadata":34417},{"headings":34418,"localImagePaths":34419,"remoteImagePaths":34420,"frontmatter":34421,"imagePaths":34422},[],[],[],{"title":34410,"description":34411,"summary":34411,"pubDate":32084,"source":25191,"url":34413,"thumbnail":25193},[],"2025-06-25-multimodal-fusion-slam-with-fourier-attention.md","2025-06-25-multimodal-machine-learning-in-mental-health-a-survey-of-data-algorithms-and-challenges",{"id":34424,"data":34426,"filePath":34431,"digest":34432,"rendered":34433,"legacyId":34440},{"title":34427,"description":34428,"summary":34428,"pubDate":34429,"source":25191,"url":34430,"thumbnail":25193},"Multimodal Machine Learning in Mental Health: A Survey of Data, Algorithms, and Challenges","arXiv:2407.16804v2 Announce Type: replace-cross Abstract: Multimodal machine learning (MML) is rapidly reshaping the way mental-health disorders are detected, characterized, and longitudinally monitored. Whereas early studies relied on isolated data streams -- such as speech, text, or wearable signals -- recent research has converged on architectures that integrate heterogeneous modalities to capture the rich, complex signatures of psychiatric conditions. This survey provides the first comprehensive, clinically grounded synthesis of MML for mental health. We (i) catalog 26 public datasets spanning audio, visual, physiological signals, and text modalities; (ii) systematically compare transformer, graph, and hybrid-based fusion strategies across 28 models, highlighting trends in representation learning and cross-modal alignment. Beyond summarizing current capabilities, we interrogate open challenges: data governance and privacy, demographic and intersectional fairness, evaluation explainability, and the complexity of mental health disorders in multimodal settings. By bridging methodological innovation with psychiatric utility, this survey aims to orient both ML researchers and mental-health practitioners toward the next generation of trustworthy, multimodal decision-support systems.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2407.16804","src/content/posts/2025-06-25-multimodal-machine-learning-in-mental-health-a-survey-of-data-algorithms-and-challenges.md","5ed0f0ab3f6222ce",{"html":25,"metadata":34434},{"headings":34435,"localImagePaths":34436,"remoteImagePaths":34437,"frontmatter":34438,"imagePaths":34439},[],[],[],{"title":34427,"description":34428,"summary":34428,"pubDate":32084,"source":25191,"url":34430,"thumbnail":25193},[],"2025-06-25-multimodal-machine-learning-in-mental-health-a-survey-of-data-algorithms-and-challenges.md","2025-06-25-musecontrollite-multifunctional-music-generation-with-lightweight-conditioners",{"id":34441,"data":34443,"filePath":34448,"digest":34449,"rendered":34450,"legacyId":34457},{"title":34444,"description":34445,"summary":34445,"pubDate":34446,"source":25191,"url":34447,"thumbnail":25193},"MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners","arXiv:2506.18729v2 Announce Type: replace-cross Abstract: We propose MuseControlLite, a lightweight mechanism designed to fine-tune text-to-music generation models for precise conditioning using various time-varying musical attributes and reference audio signals. The key finding is that positional embeddings, which have been seldom used by text-to-music generation models in the conditioner for text conditions, are critical when the condition of interest is a function of time. Using melody control as an example, our experiments show that simply adding rotary positional embeddings to the decoupled cross-attention layers increases control accuracy from 56.6% to 61.1%, while requiring 6.75 times fewer trainable parameters than state-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion Transformer model of Stable Audio Open. We evaluate various forms of musical attribute control, audio inpainting, and audio outpainting, demonstrating improved controllability over MusicGen-Large and Stable Audio Open ControlNet at a significantly lower fine-tuning cost, with only 85M trainble parameters. Source code, model checkpoints, and demo examples are available at: https://musecontrollite.github.io/web/.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18729","src/content/posts/2025-06-25-musecontrollite-multifunctional-music-generation-with-lightweight-conditioners.md","b79bc65d4946b969",{"html":25,"metadata":34451},{"headings":34452,"localImagePaths":34453,"remoteImagePaths":34454,"frontmatter":34455,"imagePaths":34456},[],[],[],{"title":34444,"description":34445,"summary":34445,"pubDate":32084,"source":25191,"url":34447,"thumbnail":25193},[],"2025-06-25-musecontrollite-multifunctional-music-generation-with-lightweight-conditioners.md","2025-06-25-naada-a-noise-aware-attention-denoising-autoencoder-for-dental-panoramic-radiographs",{"id":34458,"data":34460,"filePath":34465,"digest":34466,"rendered":34467,"legacyId":34474},{"title":34461,"description":34462,"summary":34462,"pubDate":34463,"source":25191,"url":34464,"thumbnail":25193},"NAADA: A Noise-Aware Attention Denoising Autoencoder for Dental Panoramic Radiographs","arXiv:2506.19387v1 Announce Type: cross Abstract: Convolutional denoising autoencoders (DAEs) are powerful tools for image restoration. However, they inherit a key limitation of convolutional neural networks (CNNs): they tend to recover low-frequency features, such as smooth regions, more effectively than high-frequency details. This leads to the loss of fine details, which is particularly problematic in dental radiographs where preserving subtle anatomical structures is crucial. While self-attention mechanisms can help mitigate this issue by emphasizing important features, conventional attention methods often prioritize features corresponding to cleaner regions and may overlook those obscured by noise. To address this limitation, we propose a noise-aware self-attention method, which allows the model to effectively focus on and recover key features even within noisy regions. Building on this approach, we introduce the noise-aware attention-enhanced denoising autoencoder (NAADA) network for enhancing noisy panoramic dental radiographs. Compared with the recent state of the art (and much heavier) methods like Uformer, MResDNN etc., our method improves the reconstruction of fine details, ensuring better image quality and diagnostic accuracy.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19387","src/content/posts/2025-06-25-naada-a-noise-aware-attention-denoising-autoencoder-for-dental-panoramic-radiographs.md","aa24365c80a9cbc4",{"html":25,"metadata":34468},{"headings":34469,"localImagePaths":34470,"remoteImagePaths":34471,"frontmatter":34472,"imagePaths":34473},[],[],[],{"title":34461,"description":34462,"summary":34462,"pubDate":32084,"source":25191,"url":34464,"thumbnail":25193},[],"2025-06-25-naada-a-noise-aware-attention-denoising-autoencoder-for-dental-panoramic-radiographs.md","2025-06-25-naviagent-bilevel-planning-on-tool-dependency-graphs-for-function-calling",{"id":34475,"data":34477,"filePath":34482,"digest":34483,"rendered":34484,"legacyId":34491},{"title":34478,"description":34479,"summary":34479,"pubDate":34480,"source":25191,"url":34481,"thumbnail":25193},"NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling","arXiv:2506.19500v1 Announce Type: new Abstract: LLMs' reliance on static knowledge and fragile tool invocation severely hinders the orchestration of complex, heterogeneous toolchains, particularly at large scales. Existing methods typically use rigid single-path execution, resulting in poor error recovery and exponentially growing search spaces. We introduce NaviAgent, a graph-navigated bilevel planning architecture for robust function calling, comprising a Multi-Path Decider and Graph-Encoded Navigator. As an LLM-powered agent, the Multi-Path Decider defines a four-dimensional decision space and continuously perceives environmental states, dynamically selecting the optimal action to fully cover all tool invocation scenarios. The Graph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph (TDHG), where node embeddings explicitly fuse API schema structure with historical invocation behavior. It also integrates a novel heuristic search strategy that guides the Decider toward efficient and highly successful toolchains, even for unseen tool combinations. Experiments show that NaviAgent consistently achieves the highest task success rate (TSR) across all foundation models and task complexities, outperforming the average baselines (ReAct, ToolLLM, {alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B, and Deepseek-V3, respectively. Its execution steps are typically within one step of the most efficient baseline, ensuring a strong balance between quality and efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of 49.5%, surpassing the much larger 32B model (44.9%) under our architecture. Incorporating the Graph-Encoded Navigator further boosts TSR by an average of 2.4 points, with gains up over 9 points on complex tasks for larger models (Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain orchestration.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19500","src/content/posts/2025-06-25-naviagent-bilevel-planning-on-tool-dependency-graphs-for-function-calling.md","6ed2d52a9d62da9b",{"html":25,"metadata":34485},{"headings":34486,"localImagePaths":34487,"remoteImagePaths":34488,"frontmatter":34489,"imagePaths":34490},[],[],[],{"title":34478,"description":34479,"summary":34479,"pubDate":32084,"source":25191,"url":34481,"thumbnail":25193},[],"2025-06-25-naviagent-bilevel-planning-on-tool-dependency-graphs-for-function-calling.md","2025-06-25-nerf-based-cbct-reconstruction-needs-normalization-and-initialization",{"id":34492,"data":34494,"filePath":34499,"digest":34500,"rendered":34501,"legacyId":34508},{"title":34495,"description":34496,"summary":34496,"pubDate":34497,"source":25191,"url":34498,"thumbnail":25193},"NeRF-based CBCT Reconstruction needs Normalization and Initialization","arXiv:2506.19742v1 Announce Type: cross Abstract: Cone Beam Computed Tomography (CBCT) is widely used in medical imaging. However, the limited number and intensity of X-ray projections make reconstruction an ill-posed problem with severe artifacts. NeRF-based methods have achieved great success in this task. However, they suffer from a local-global training mismatch between their two key components: the hash encoder and the neural network. Specifically, in each training step, only a subset of the hash encoder's parameters is used (local sparse), whereas all parameters in the neural network participate (global dense). Consequently, hash features generated in each step are highly misaligned, as they come from different subsets of the hash encoder. These misalignments from different training steps are then fed into the neural network, causing repeated inconsistent global updates in training, which leads to unstable training, slower convergence, and degraded reconstruction quality. Aiming to alleviate the impact of this local-global optimization mismatch, we introduce a Normalized Hash Encoder, which enhances feature consistency and mitigates the mismatch. Additionally, we propose a Mapping Consistency Initialization(MCI) strategy that initializes the neural network before training by leveraging the global mapping property from a well-trained model. The initialized neural network exhibits improved stability during early training, enabling faster convergence and enhanced reconstruction performance. Our method is simple yet effective, requiring only a few lines of code while substantially improving training efficiency on 128 CT cases collected from 4 different datasets, covering 7 distinct anatomical regions.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19742","src/content/posts/2025-06-25-nerf-based-cbct-reconstruction-needs-normalization-and-initialization.md","ef7df33b47cc2e5a",{"html":25,"metadata":34502},{"headings":34503,"localImagePaths":34504,"remoteImagePaths":34505,"frontmatter":34506,"imagePaths":34507},[],[],[],{"title":34495,"description":34496,"summary":34496,"pubDate":32084,"source":25191,"url":34498,"thumbnail":25193},[],"2025-06-25-nerf-based-cbct-reconstruction-needs-normalization-and-initialization.md","2025-06-25-neural-cellular-automata-for-arc-agi",{"id":34509,"data":34511,"filePath":34516,"digest":34517,"rendered":34518,"legacyId":34525},{"title":34512,"description":34513,"summary":34513,"pubDate":34514,"source":25191,"url":34515,"thumbnail":25193},"Neural Cellular Automata for ARC-AGI","arXiv:2506.15746v1 Announce Type: cross Abstract: Cellular automata and their differentiable counterparts, Neural Cellular Automata (NCA), are highly expressive and capable of surprisingly complex behaviors. This paper explores how NCAs perform when applied to tasks requiring precise transformations and few-shot generalization, using the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) as a domain that challenges their capabilities in ways not previously explored. Specifically, this paper uses gradient-based training to learn iterative update rules that transform input grids into their outputs from the training examples and apply them to the test inputs. Results suggest that gradient-trained NCA models are a promising and efficient approach to a range of abstract grid-based tasks from ARC. Along with discussing the impacts of various design modifications and training constraints, this work examines the behavior and properties of NCAs applied to ARC to give insights for broader applications of self-organizing systems.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.15746","src/content/posts/2025-06-25-neural-cellular-automata-for-arc-agi.md","e82ff451f7fb3802",{"html":25,"metadata":34519},{"headings":34520,"localImagePaths":34521,"remoteImagePaths":34522,"frontmatter":34523,"imagePaths":34524},[],[],[],{"title":34512,"description":34513,"summary":34513,"pubDate":32084,"source":25191,"url":34515,"thumbnail":25193},[],"2025-06-25-neural-cellular-automata-for-arc-agi.md","2025-06-25-ntrl-encounter-generation-via-reinforcement-learning-for-dynamic-difficulty-adjustment-in-dungeons-and-dragons",{"id":34526,"data":34528,"filePath":34533,"digest":34534,"rendered":34535,"legacyId":34542},{"title":34529,"description":34530,"summary":34530,"pubDate":34531,"source":25191,"url":34532,"thumbnail":25193},"NTRL: Encounter Generation via Reinforcement Learning for Dynamic Difficulty Adjustment in Dungeons and Dragons","arXiv:2506.19530v1 Announce Type: new Abstract: Balancing combat encounters in Dungeons & Dragons (D&amp;D) is a complex task that requires Dungeon Masters (DM) to manually assess party strength, enemy composition, and dynamic player interactions while avoiding interruption of the narrative flow. In this paper, we propose Encounter Generation via Reinforcement Learning (NTRL), a novel approach that automates Dynamic Difficulty Adjustment (DDA) in D&amp;D via combat encounter design. By framing the problem as a contextual bandit, NTRL generates encounters based on real-time party members attributes. In comparison with classic DM heuristics, NTRL iteratively optimizes encounters to extend combat longevity (+200%), increases damage dealt to party members, reducing post-combat hit points (-16.67%), and raises the number of player deaths while maintaining low total party kills (TPK). The intensification of combat forces players to act wisely and engage in tactical maneuvers, even though the generated encounters guarantee high win rates (70%). Even in comparison with encounters designed by human Dungeon Masters, NTRL demonstrates superior performance by enhancing the strategic depth of combat while increasing difficulty in a manner that preserves overall game fairness.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19530","src/content/posts/2025-06-25-ntrl-encounter-generation-via-reinforcement-learning-for-dynamic-difficulty-adjustment-in-dungeons-and-dragons.md","9e2f929232979887",{"html":25,"metadata":34536},{"headings":34537,"localImagePaths":34538,"remoteImagePaths":34539,"frontmatter":34540,"imagePaths":34541},[],[],[],{"title":34529,"description":34530,"summary":34530,"pubDate":32084,"source":25191,"url":34532,"thumbnail":25193},[],"2025-06-25-ntrl-encounter-generation-via-reinforcement-learning-for-dynamic-difficulty-adjustment-in-dungeons-and-dragons.md","2025-06-25-on-the-efficacy-of-old-features-for-the-detection-of-new-bots",{"id":34543,"data":34545,"filePath":34550,"digest":34551,"rendered":34552,"legacyId":34559},{"title":34546,"description":34547,"summary":34547,"pubDate":34548,"source":25191,"url":34549,"thumbnail":25193},"On the efficacy of old features for the detection of new bots","arXiv:2506.19635v1 Announce Type: new Abstract: For more than a decade now, academicians and online platform administrators have been studying solutions to the problem of bot detection. Bots are computer algorithms whose use is far from being benign: malicious bots are purposely created to distribute spam, sponsor public characters and, ultimately, induce a bias within the public opinion. To fight the bot invasion on our online ecosystem, several approaches have been implemented, mostly based on (supervised and unsupervised) classifiers, which adopt the most varied account features, from the simplest to the most expensive ones to be extracted from the raw data obtainable through the Twitter public APIs. In this exploratory study, using Twitter as a benchmark, we compare the performances of four state-of-art feature sets in detecting novel bots: one of the output scores of the popular bot detector Botometer, which considers more than 1,000 features of an account to take a decision; two feature sets based on the account profile and timeline; and the information about the Twitter client from which the user tweets. The results of our analysis, conducted on six recently released datasets of Twitter accounts, hint at the possible use of general-purpose classifiers and cheap-to-compute account features for the detection of evolved bots.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19635","src/content/posts/2025-06-25-on-the-efficacy-of-old-features-for-the-detection-of-new-bots.md","3dc976957ec564aa",{"html":25,"metadata":34553},{"headings":34554,"localImagePaths":34555,"remoteImagePaths":34556,"frontmatter":34557,"imagePaths":34558},[],[],[],{"title":34546,"description":34547,"summary":34547,"pubDate":32084,"source":25191,"url":34549,"thumbnail":25193},[],"2025-06-25-on-the-efficacy-of-old-features-for-the-detection-of-new-bots.md","2025-06-25-orthogonal-finetuning-made-scalable",{"id":34560,"data":34562,"filePath":34567,"digest":34568,"rendered":34569,"legacyId":34576},{"title":34563,"description":34564,"summary":34564,"pubDate":34565,"source":25191,"url":34566,"thumbnail":25193},"Orthogonal Finetuning Made Scalable","arXiv:2506.19847v1 Announce Type: cross Abstract: Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation while preventing catastrophic forgetting, but its high runtime and memory demands limit practical deployment. We identify the core computational bottleneck in OFT as its weight-centric implementation, which relies on costly matrix-matrix multiplications with cubic complexity. To overcome this, we propose OFTv2, an input-centric reformulation that instead uses matrix-vector multiplications (i.e., matrix-free computation), reducing the computational cost to quadratic. We further introduce the Cayley-Neumann parameterization, an efficient orthogonal parameterization that approximates the matrix inversion in Cayley transform via a truncated Neumann series. These modifications allow OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage without compromising performance. In addition, we extend OFTv2 to support finetuning quantized foundation models and show that it outperforms the popular QLoRA in training stability, efficiency, and memory usage.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19847","src/content/posts/2025-06-25-orthogonal-finetuning-made-scalable.md","3ed69d852de29264",{"html":25,"metadata":34570},{"headings":34571,"localImagePaths":34572,"remoteImagePaths":34573,"frontmatter":34574,"imagePaths":34575},[],[],[],{"title":34563,"description":34564,"summary":34564,"pubDate":32084,"source":25191,"url":34566,"thumbnail":25193},[],"2025-06-25-orthogonal-finetuning-made-scalable.md","2025-06-25-outlier-safe-pre-training-for-robust-4-bit-quantization-of-large-language-models",{"id":34577,"data":34579,"filePath":34584,"digest":34585,"rendered":34586,"legacyId":34593},{"title":34580,"description":34581,"summary":34581,"pubDate":34582,"source":25191,"url":34583,"thumbnail":25193},"Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models","arXiv:2506.19697v1 Announce Type: cross Abstract: Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19697","src/content/posts/2025-06-25-outlier-safe-pre-training-for-robust-4-bit-quantization-of-large-language-models.md","e475847048df53cd",{"html":25,"metadata":34587},{"headings":34588,"localImagePaths":34589,"remoteImagePaths":34590,"frontmatter":34591,"imagePaths":34592},[],[],[],{"title":34580,"description":34581,"summary":34581,"pubDate":32084,"source":25191,"url":34583,"thumbnail":25193},[],"2025-06-25-outlier-safe-pre-training-for-robust-4-bit-quantization-of-large-language-models.md","2025-06-25-pbft-backed-semantic-voting-for-multi-agent-memory-pruning",{"id":34594,"data":34596,"filePath":34601,"digest":34602,"rendered":34603,"legacyId":34610},{"title":34597,"description":34598,"summary":34598,"pubDate":34599,"source":25191,"url":34600,"thumbnail":25193},"PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning","arXiv:2506.17338v2 Announce Type: replace-cross Abstract: The proliferation of multi-agent systems (MAS) in complex, dynamic environments necessitates robust and efficient mechanisms for managing shared knowledge. A critical challenge is ensuring that distributed memories remain synchronized, relevant, and free from the accumulation of outdated or inconsequential data - a process analogous to biological forgetting. This paper introduces the Co-Forgetting Protocol, a novel, comprehensive framework designed to address this challenge by enabling synchronized memory pruning in MAS. The protocol integrates three key components: (1) context-aware semantic voting, where agents utilize a lightweight DistilBERT model to assess the relevance of memory items based on their content and the current operational context; (2) multi-scale temporal decay functions, which assign diminishing importance to memories based on their age and access frequency across different time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based consensus mechanism, ensuring that decisions to retain or discard memory items are agreed upon by a qualified and fault-tolerant majority of agents, even in the presence of up to f Byzantine (malicious or faulty) agents in a system of N greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient inter-agent communication and Pinecone for scalable vector embedding storage and similarity search, with SQLite managing metadata. Experimental evaluations in a simulated MAS environment with four agents demonstrate the protocol's efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88% voting accuracy in forgetting decisions against human-annotated benchmarks, a 92% PBFT consensus success rate under simulated Byzantine conditions, and an 82% cache hit rate for memory access.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.17338","src/content/posts/2025-06-25-pbft-backed-semantic-voting-for-multi-agent-memory-pruning.md","e2556cec1b9ea9c3",{"html":25,"metadata":34604},{"headings":34605,"localImagePaths":34606,"remoteImagePaths":34607,"frontmatter":34608,"imagePaths":34609},[],[],[],{"title":34597,"description":34598,"summary":34598,"pubDate":32084,"source":25191,"url":34600,"thumbnail":25193},[],"2025-06-25-pbft-backed-semantic-voting-for-multi-agent-memory-pruning.md","2025-06-25-persona-features-control-emergent-misalignment",{"id":34611,"data":34613,"filePath":34618,"digest":34619,"rendered":34620,"legacyId":34627},{"title":34614,"description":34615,"summary":34615,"pubDate":34616,"source":25191,"url":34617,"thumbnail":25193},"Persona Features Control Emergent Misalignment","arXiv:2506.19823v1 Announce Type: cross Abstract: Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes 'emergent misalignment,' where models give stereotypically malicious responses to unrelated prompts. We extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training. To investigate the mechanisms behind this generalized misalignment, we apply a 'model diffing' approach using sparse autoencoders to compare internal model representations before and after fine-tuning. This approach reveals several 'misaligned persona' features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to predict whether a model will exhibit such behavior. Additionally, we investigate mitigation strategies, discovering that fine-tuning an emergently misaligned model on just a few hundred benign samples efficiently restores alignment.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19823","src/content/posts/2025-06-25-persona-features-control-emergent-misalignment.md","9b28a350cc2dae5d",{"html":25,"metadata":34621},{"headings":34622,"localImagePaths":34623,"remoteImagePaths":34624,"frontmatter":34625,"imagePaths":34626},[],[],[],{"title":34614,"description":34615,"summary":34615,"pubDate":32084,"source":25191,"url":34617,"thumbnail":25193},[],"2025-06-25-persona-features-control-emergent-misalignment.md","2025-06-25-perspective-shifted-neuro-symbolic-world-models-a-framework-for-socially-aware-robot-navigation",{"id":34628,"data":34630,"filePath":34635,"digest":34636,"rendered":34637,"legacyId":34644},{"title":34631,"description":34632,"summary":34632,"pubDate":34633,"source":25191,"url":34634,"thumbnail":25193},"Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation","arXiv:2503.20425v2 Announce Type: replace Abstract: Navigating in environments alongside humans requires agents to reason under uncertainty and account for the beliefs and intentions of those around them. Under a sequential decision-making framework, egocentric navigation can naturally be represented as a Markov Decision Process (MDP). However, social navigation additionally requires reasoning about the hidden beliefs of others, inherently leading to a Partially Observable Markov Decision Process (POMDP), where agents lack direct access to others' mental states. Inspired by Theory of Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based reinforcement learning architecture for social navigation, addressing the challenge of belief tracking in partially observable environments; and (2) a perspective-shift operator for belief estimation, leveraging recent work on Influence-based Abstractions (IBA) in structured multi-agent settings.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2503.20425","src/content/posts/2025-06-25-perspective-shifted-neuro-symbolic-world-models-a-framework-for-socially-aware-robot-navigation.md","3d79a48bc287e02d",{"html":25,"metadata":34638},{"headings":34639,"localImagePaths":34640,"remoteImagePaths":34641,"frontmatter":34642,"imagePaths":34643},[],[],[],{"title":34631,"description":34632,"summary":34632,"pubDate":32084,"source":25191,"url":34634,"thumbnail":25193},[],"2025-06-25-perspective-shifted-neuro-symbolic-world-models-a-framework-for-socially-aware-robot-navigation.md","2025-06-25-plan-for-speed----dilated-scheduling-for-masked-diffusion-language-models",{"id":34645,"data":34647,"filePath":34652,"digest":34653,"rendered":34654,"legacyId":34661},{"title":34648,"description":34649,"summary":34649,"pubDate":34650,"source":25191,"url":34651,"thumbnail":25193},"Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models","arXiv:2506.19037v1 Announce Type: cross Abstract: Masked diffusion language models (MDLM) have shown strong promise for non-autoregressive text generation, yet existing samplers act as implicit planners, selecting tokens to unmask via denoiser confidence or entropy scores. Such heuristics falter under parallel unmasking - they ignore pairwise interactions between tokens and cannot account for dependencies when unmasking multiple positions at once, limiting their inference time to traditional auto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking Strategy (DUS), an inference-only, planner-model-free method that requires no additional training. DUS leverages a first-order Markov assumption to partition sequence positions into dilation-based groups of non-adjacent tokens, enabling independent, parallel unmasking steps that respect local context that minimizes the joint entropy of each iteration step. Unlike semi-AR block approaches (e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces the number of denoiser calls to O(log B) per generation block - yielding substantial speedup over the O(B) run time of state-of-the-art diffusion models, where B is the block size in the semi-AR inference process. In experiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks - domains suited to non-ordinal generation - DUS improves scores over parallel confidence-based planner, without modifying the underlying denoiser. DUS offers a lightweight, budget-aware approach to efficient, high-quality text generation, paving the way to unlock the true capabilities of MDLMs.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19037","src/content/posts/2025-06-25-plan-for-speed----dilated-scheduling-for-masked-diffusion-language-models.md","0187cb2d308cc4c9",{"html":25,"metadata":34655},{"headings":34656,"localImagePaths":34657,"remoteImagePaths":34658,"frontmatter":34659,"imagePaths":34660},[],[],[],{"title":34648,"description":34649,"summary":34649,"pubDate":32084,"source":25191,"url":34651,"thumbnail":25193},[],"2025-06-25-plan-for-speed----dilated-scheduling-for-masked-diffusion-language-models.md","2025-06-25-position-intelligent-science-laboratory-requires-the-integration-of-cognitive-and-embodied-ai",{"id":34662,"data":34664,"filePath":34669,"digest":34670,"rendered":34671,"legacyId":34678},{"title":34665,"description":34666,"summary":34666,"pubDate":34667,"source":25191,"url":34668,"thumbnail":25193},"Position: Intelligent Science Laboratory Requires the Integration of Cognitive and Embodied AI","arXiv:2506.19613v1 Announce Type: new Abstract: Scientific discovery has long been constrained by human limitations in expertise, physical capability, and sleep cycles. The recent rise of AI scientists and automated laboratories has accelerated both the cognitive and operational aspects of research. However, key limitations persist: AI systems are often confined to virtual environments, while automated laboratories lack the flexibility and autonomy to adaptively test new hypotheses in the physical world. Recent advances in embodied AI, such as generalist robot foundation models, diffusion-based action policies, fine-grained manipulation learning, and sim-to-real transfer, highlight the promise of integrating cognitive and embodied intelligence. This convergence opens the door to closed-loop systems that support iterative, autonomous experimentation and the possibility of serendipitous discovery. In this position paper, we propose the paradigm of Intelligent Science Laboratories (ISLs): a multi-layered, closed-loop framework that deeply integrates cognitive and embodied intelligence. ISLs unify foundation models for scientific reasoning, agent-based workflow orchestration, and embodied agents for robust physical experimentation. We argue that such systems are essential for overcoming the current limitations of scientific discovery and for realizing the full transformative potential of AI-driven science.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19613","src/content/posts/2025-06-25-position-intelligent-science-laboratory-requires-the-integration-of-cognitive-and-embodied-ai.md","e23fc5cef3df2099",{"html":25,"metadata":34672},{"headings":34673,"localImagePaths":34674,"remoteImagePaths":34675,"frontmatter":34676,"imagePaths":34677},[],[],[],{"title":34665,"description":34666,"summary":34666,"pubDate":32084,"source":25191,"url":34668,"thumbnail":25193},[],"2025-06-25-position-intelligent-science-laboratory-requires-the-integration-of-cognitive-and-embodied-ai.md","2025-06-25-privacy-preserving-llm-interaction-with-socratic-chain-of-thought-reasoning-and-homomorphically-encrypted-vector-databases",{"id":34679,"data":34681,"filePath":34686,"digest":34687,"rendered":34688,"legacyId":34695},{"title":34682,"description":34683,"summary":34683,"pubDate":34684,"source":25191,"url":34685,"thumbnail":25193},"Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases","arXiv:2506.17336v1 Announce Type: cross Abstract: Large language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single user's private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.17336","src/content/posts/2025-06-25-privacy-preserving-llm-interaction-with-socratic-chain-of-thought-reasoning-and-homomorphically-encrypted-vector-databases.md","4ff33b5d1ff654cc",{"html":25,"metadata":34689},{"headings":34690,"localImagePaths":34691,"remoteImagePaths":34692,"frontmatter":34693,"imagePaths":34694},[],[],[],{"title":34682,"description":34683,"summary":34683,"pubDate":32084,"source":25191,"url":34685,"thumbnail":25193},[],"2025-06-25-privacy-preserving-llm-interaction-with-socratic-chain-of-thought-reasoning-and-homomorphically-encrypted-vector-databases.md","2025-06-25-privacyxray-detecting-privacy-breaches-in-llms-through-semantic-consistency-and-probability-certainty",{"id":34696,"data":34698,"filePath":34703,"digest":34704,"rendered":34705,"legacyId":34712},{"title":34699,"description":34700,"summary":34700,"pubDate":34701,"source":25191,"url":34702,"thumbnail":25193},"PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty","arXiv:2506.19563v1 Announce Type: cross Abstract: Large Language Models (LLMs) are widely used in sensitive domains, including healthcare, finance, and legal services, raising concerns about potential private information leaks during inference. Privacy extraction attacks, such as jailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the models to output sensitive information. However, these attacks cannot verify whether the extracted private information is accurate, as no public datasets exist for cross-validation, leaving a critical gap in private information detection during inference. To address this, we propose PrivacyXray, a novel framework detecting privacy breaches by analyzing LLM inner states. Our analysis reveals that LLMs exhibit higher semantic coherence and probabilistic certainty when generating correct private outputs. Based on this, PrivacyXray detects privacy breaches using four metrics: intra-layer and inter-layer semantic similarity, token-level and sentence-level probability distributions. PrivacyXray addresses critical challenges in private information detection by overcoming the lack of open-source private datasets and eliminating reliance on external data for validation. It achieves this through the synthesis of realistic private data and a detection mechanism based on the inner states of LLMs. Experiments show that PrivacyXray achieves consistent performance, with an average accuracy of 92.69% across five LLMs. Compared to state-of-the-art methods, PrivacyXray achieves significant improvements, with an average accuracy increase of 20.06%, highlighting its stability and practical utility in real-world applications.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19563","src/content/posts/2025-06-25-privacyxray-detecting-privacy-breaches-in-llms-through-semantic-consistency-and-probability-certainty.md","ea7f338d98e68975",{"html":25,"metadata":34706},{"headings":34707,"localImagePaths":34708,"remoteImagePaths":34709,"frontmatter":34710,"imagePaths":34711},[],[],[],{"title":34699,"description":34700,"summary":34700,"pubDate":32084,"source":25191,"url":34702,"thumbnail":25193},[],"2025-06-25-privacyxray-detecting-privacy-breaches-in-llms-through-semantic-consistency-and-probability-certainty.md","2025-06-25-private-model-personalization-revisited",{"id":34713,"data":34715,"filePath":34720,"digest":34721,"rendered":34722,"legacyId":34729},{"title":34716,"description":34717,"summary":34717,"pubDate":34718,"source":25191,"url":34719,"thumbnail":25193},"Private Model Personalization Revisited","arXiv:2506.19220v1 Announce Type: cross Abstract: We study model personalization under user-level differential privacy (DP) in the shared representation framework. In this problem, there are $n$ users whose data is statistically heterogeneous, and their optimal parameters share an unknown embedding $U^* inmathbb{R}^{dtimes k}$ that maps the user parameters in $mathbb{R}^d$ to low-dimensional representations in $mathbb{R}^k$, where $kll d$. Our goal is to privately recover the shared embedding and the local low-dimensional representations with small excess risk in the federated setting. We propose a private, efficient federated learning algorithm to learn the shared embedding based on the FedRep algorithm in [CHM+21]. Unlike [CHM+21], our algorithm satisfies differential privacy, and our results hold for the case of noisy labels. In contrast to prior work on private model personalization [JRS+21], our utility guarantees hold under a larger class of users' distributions (sub-Gaussian instead of Gaussian distributions). Additionally, in natural parameter regimes, we improve the privacy error term in [JRS+21] by a factor of $widetilde{O}(dk)$. Next, we consider the binary classification setting. We present an information-theoretic construction to privately learn the shared embedding and derive a margin-based accuracy guarantee that is independent of $d$. Our method utilizes the Johnson-Lindenstrauss transform to reduce the effective dimensions of the shared embedding and the users' data. This result shows that dimension-independent risk bounds are possible in this setting under a margin loss.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19220","src/content/posts/2025-06-25-private-model-personalization-revisited.md","b276dbd54e878b48",{"html":25,"metadata":34723},{"headings":34724,"localImagePaths":34725,"remoteImagePaths":34726,"frontmatter":34727,"imagePaths":34728},[],[],[],{"title":34716,"description":34717,"summary":34717,"pubDate":32084,"source":25191,"url":34719,"thumbnail":25193},[],"2025-06-25-private-model-personalization-revisited.md","2025-06-25-process-reward-models-that-think",{"id":34730,"data":34732,"filePath":34737,"digest":34738,"rendered":34739,"legacyId":34746},{"title":34733,"description":34734,"summary":34734,"pubDate":34735,"source":25191,"url":34736,"thumbnail":25193},"Process Reward Models That Think","arXiv:2504.16828v3 Announce Type: replace-cross Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2504.16828","src/content/posts/2025-06-25-process-reward-models-that-think.md","729a1fcce58eefec",{"html":25,"metadata":34740},{"headings":34741,"localImagePaths":34742,"remoteImagePaths":34743,"frontmatter":34744,"imagePaths":34745},[],[],[],{"title":34733,"description":34734,"summary":34734,"pubDate":32084,"source":25191,"url":34736,"thumbnail":25193},[],"2025-06-25-process-reward-models-that-think.md","2025-06-25-quantifying-fairness-in-llms-beyond-tokens-a-semantic-and-statistical-perspective",{"id":34747,"data":34749,"filePath":34754,"digest":34755,"rendered":34756,"legacyId":34763},{"title":34750,"description":34751,"summary":34751,"pubDate":34752,"source":25191,"url":34753,"thumbnail":25193},"Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective","arXiv:2506.19028v1 Announce Type: cross Abstract: Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo(Fine-grained Semantic Computation), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSco more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19028","src/content/posts/2025-06-25-quantifying-fairness-in-llms-beyond-tokens-a-semantic-and-statistical-perspective.md","266fd24babddf02f",{"html":25,"metadata":34757},{"headings":34758,"localImagePaths":34759,"remoteImagePaths":34760,"frontmatter":34761,"imagePaths":34762},[],[],[],{"title":34750,"description":34751,"summary":34751,"pubDate":32084,"source":25191,"url":34753,"thumbnail":25193},[],"2025-06-25-quantifying-fairness-in-llms-beyond-tokens-a-semantic-and-statistical-perspective.md","2025-06-25-radial-attention-onlog-n-sparse-attention-with-energy-decay-for-long-video-generation",{"id":34764,"data":34766,"filePath":34771,"digest":34772,"rendered":34773,"legacyId":34780},{"title":34767,"description":34768,"summary":34768,"pubDate":34769,"source":25191,"url":34770,"thumbnail":25193},"Radial Attention: $O(nlog n)$ Sparse Attention with Energy Decay for Long Video Generation","arXiv:2506.19852v1 Announce Type: cross Abstract: Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $O(n log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $O(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$times$ longer while reducing training costs by up to 4.4$times$ compared to direct fine-tuning and accelerating inference by up to 3.7$times$ compared to dense attention inference.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19852","src/content/posts/2025-06-25-radial-attention-onlog-n-sparse-attention-with-energy-decay-for-long-video-generation.md","f3c3eb6fc8119e48",{"html":25,"metadata":34774},{"headings":34775,"localImagePaths":34776,"remoteImagePaths":34777,"frontmatter":34778,"imagePaths":34779},[],[],[],{"title":34767,"description":34768,"summary":34768,"pubDate":32084,"source":25191,"url":34770,"thumbnail":25193},[],"2025-06-25-radial-attention-onlog-n-sparse-attention-with-energy-decay-for-long-video-generation.md","2025-06-25-rag-enhancing-retrieval-augmented-generation-with-application-aware-reasoning",{"id":34781,"data":34783,"filePath":34788,"digest":34789,"rendered":34790,"legacyId":34797},{"title":34784,"description":34785,"summary":34785,"pubDate":34786,"source":25191,"url":34787,"thumbnail":25193},"RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning","arXiv:2506.11555v2 Announce Type: replace Abstract: The integration of external knowledge through Retrieval-Augmented Generation (RAG) has become foundational in enhancing large language models (LLMs) for knowledge-intensive tasks. However, existing RAG paradigms often overlook the cognitive step of applying knowledge, leaving a gap between retrieved facts and task-specific reasoning. In this work, we introduce RAG+, a principled and modular extension that explicitly incorporates application-aware reasoning into the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and aligned application examples, created either manually or automatically, and retrieves both jointly during inference. This design enables LLMs not only to access relevant information but also to apply it within structured, goal-oriented reasoning processes. Experiments across mathematical, legal, and medical domains, conducted on multiple models, demonstrate that RAG+ consistently outperforms standard RAG variants, achieving average improvements of 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval with actionable application, RAG+ advances a more cognitively grounded framework for knowledge integration, representing a step toward more interpretable and capable LLMs.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.11555","src/content/posts/2025-06-25-rag-enhancing-retrieval-augmented-generation-with-application-aware-reasoning.md","08eadfc86dc543fa",{"html":25,"metadata":34791},{"headings":34792,"localImagePaths":34793,"remoteImagePaths":34794,"frontmatter":34795,"imagePaths":34796},[],[],[],{"title":34784,"description":34785,"summary":34785,"pubDate":32084,"source":25191,"url":34787,"thumbnail":25193},[],"2025-06-25-rag-enhancing-retrieval-augmented-generation-with-application-aware-reasoning.md","2025-06-25-rarespot-spotting-small-and-rare-wildlife-in-aerial-imagery-with-multi-scale-consistency-and-context-aware-augmentation",{"id":34798,"data":34800,"filePath":34805,"digest":34806,"rendered":34807,"legacyId":34814},{"title":34801,"description":34802,"summary":34802,"pubDate":34803,"source":25191,"url":34804,"thumbnail":25193},"RareSpot: Spotting Small and Rare Wildlife in Aerial Imagery with Multi-Scale Consistency and Context-Aware Augmentation","arXiv:2506.19087v1 Announce Type: cross Abstract: Automated detection of small and rare wildlife in aerial imagery is crucial for effective conservation, yet remains a significant technical challenge. Prairie dogs exemplify this issue: their ecological importance as keystone species contrasts sharply with their elusive presence--marked by small size, sparse distribution, and subtle visual features--which undermines existing detection approaches. To address these challenges, we propose RareSpot, a robust detection framework integrating multi-scale consistency learning and context-aware augmentation. Our multi-scale consistency approach leverages structured alignment across feature pyramids, enhancing fine-grained object representation and mitigating scale-related feature loss. Complementarily, context-aware augmentation strategically synthesizes challenging training instances by embedding difficult-to-detect samples into realistic environmental contexts, significantly boosting model precision and recall. Evaluated on an expert-annotated prairie dog drone imagery benchmark, our method achieves state-of-the-art performance, improving detection accuracy by over 35% compared to baseline methods. Importantly, it generalizes effectively across additional wildlife datasets, demonstrating broad applicability. The RareSpot benchmark and approach not only support critical ecological monitoring but also establish a new foundation for detecting small, rare species in complex aerial scenes.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19087","src/content/posts/2025-06-25-rarespot-spotting-small-and-rare-wildlife-in-aerial-imagery-with-multi-scale-consistency-and-context-aware-augmentation.md","098572ffae63fbb0",{"html":25,"metadata":34808},{"headings":34809,"localImagePaths":34810,"remoteImagePaths":34811,"frontmatter":34812,"imagePaths":34813},[],[],[],{"title":34801,"description":34802,"summary":34802,"pubDate":32084,"source":25191,"url":34804,"thumbnail":25193},[],"2025-06-25-rarespot-spotting-small-and-rare-wildlife-in-aerial-imagery-with-multi-scale-consistency-and-context-aware-augmentation.md","2025-06-25-rational-metareasoning-for-large-language-models",{"id":34815,"data":34817,"filePath":34822,"digest":34823,"rendered":34824,"legacyId":34831},{"title":34818,"description":34819,"summary":34819,"pubDate":34820,"source":25191,"url":34821,"thumbnail":25193},"Rational Metareasoning for Large Language Models","arXiv:2410.05563v3 Announce Type: replace-cross Abstract: Being prompted to engage in reasoning has emerged as a core technique for using large language models (LLMs), deploying additional inference-time compute to improve task performance. However, as LLMs increase in both size and adoption, inference costs are correspondingly becoming increasingly burdensome. How, then, might we optimize reasoning's cost-performance tradeoff? This work introduces a novel approach based on computational models of metareasoning used in cognitive science, training LLMs to selectively use intermediate reasoning steps only when necessary. We first develop a reward function that incorporates the Value of Computation by penalizing unnecessary reasoning, then use this reward function with Expert Iteration to train the LLM. Compared to few-shot chain-of-thought prompting and STaR, our method significantly reduces inference costs (20-37% fewer tokens generated across three models) while maintaining task performance across diverse datasets.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2410.05563","src/content/posts/2025-06-25-rational-metareasoning-for-large-language-models.md","ce38d55b106bc67f",{"html":25,"metadata":34825},{"headings":34826,"localImagePaths":34827,"remoteImagePaths":34828,"frontmatter":34829,"imagePaths":34830},[],[],[],{"title":34818,"description":34819,"summary":34819,"pubDate":32084,"source":25191,"url":34821,"thumbnail":25193},[],"2025-06-25-rational-metareasoning-for-large-language-models.md","2025-06-25-rcstat-a-statistical-framework-for-using-relative-contextualization-in-transformers",{"id":34832,"data":34834,"filePath":34839,"digest":34840,"rendered":34841,"legacyId":34848},{"title":34835,"description":34836,"summary":34836,"pubDate":34837,"source":25191,"url":34838,"thumbnail":25193},"RCStat: A Statistical Framework for using Relative Contextualization in Transformers","arXiv:2506.19549v1 Announce Type: cross Abstract: Prior work on input-token importance in auto-regressive transformers has relied on Softmax-normalized attention weights, which obscure the richer structure of pre-Softmax query-key logits. We introduce RCStat, a statistical framework that harnesses raw attention logits via Relative Contextualization (RC), a random variable measuring contextual alignment between token segments, and derive an efficient upper bound for RC. We demonstrate two applications: (i) Key-Value compression, where RC-based thresholds drive adaptive key-value eviction for substantial cache reduction with minimal quality loss; and (ii) Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level explanations than post-Softmax methods. Across question answering, summarization, and attribution benchmarks, RCStat achieves significant empirical gains, delivering state-of-the-art compression and attribution performance without any model retraining.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19549","src/content/posts/2025-06-25-rcstat-a-statistical-framework-for-using-relative-contextualization-in-transformers.md","4e6a6db0d8b66984",{"html":25,"metadata":34842},{"headings":34843,"localImagePaths":34844,"remoteImagePaths":34845,"frontmatter":34846,"imagePaths":34847},[],[],[],{"title":34835,"description":34836,"summary":34836,"pubDate":32084,"source":25191,"url":34838,"thumbnail":25193},[],"2025-06-25-rcstat-a-statistical-framework-for-using-relative-contextualization-in-transformers.md","2025-06-25-reading-smiles-proxy-bias-in-foundation-models-for-facial-emotion-recognition",{"id":34849,"data":34851,"filePath":34856,"digest":34857,"rendered":34858,"legacyId":34865},{"title":34852,"description":34853,"summary":34853,"pubDate":34854,"source":25191,"url":34855,"thumbnail":25193},"Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition","arXiv:2506.19079v1 Announce Type: cross Abstract: Foundation Models (FMs) are rapidly transforming Affective Computing (AC), with Vision Language Models (VLMs) now capable of recognising emotions in zero shot settings. This paper probes a critical but underexplored question: what visual cues do these models rely on to infer affect, and are these cues psychologically grounded or superficially learnt? We benchmark varying scale VLMs on a teeth annotated subset of AffectNet dataset and find consistent performance shifts depending on the presence of visible teeth. Through structured introspection of, the best-performing model, i.e., GPT-4o, we show that facial attributes like eyebrow position drive much of its affective reasoning, revealing a high degree of internal consistency in its valence-arousal predictions. These patterns highlight the emergent nature of FMs behaviour, but also reveal risks: shortcut learning, bias, and fairness issues especially in sensitive domains like mental health and education.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19079","src/content/posts/2025-06-25-reading-smiles-proxy-bias-in-foundation-models-for-facial-emotion-recognition.md","e81e4b156f636c7d",{"html":25,"metadata":34859},{"headings":34860,"localImagePaths":34861,"remoteImagePaths":34862,"frontmatter":34863,"imagePaths":34864},[],[],[],{"title":34852,"description":34853,"summary":34853,"pubDate":32084,"source":25191,"url":34855,"thumbnail":25193},[],"2025-06-25-reading-smiles-proxy-bias-in-foundation-models-for-facial-emotion-recognition.md","2025-06-25-recalling-the-forgotten-class-memberships-unlearned-models-can-be-noisy-labelers-to-leak-privacy",{"id":34866,"data":34868,"filePath":34873,"digest":34874,"rendered":34875,"legacyId":34882},{"title":34869,"description":34870,"summary":34870,"pubDate":34871,"source":25191,"url":34872,"thumbnail":25193},"Recalling The Forgotten Class Memberships: Unlearned Models Can Be Noisy Labelers to Leak Privacy","arXiv:2506.19486v1 Announce Type: cross Abstract: Machine Unlearning (MU) technology facilitates the removal of the influence of specific data instances from trained models on request. Despite rapid advancements in MU technology, its vulnerabilities are still underexplored, posing potential risks of privacy breaches through leaks of ostensibly unlearned information. Current limited research on MU attacks requires access to original models containing privacy data, which violates the critical privacy-preserving objective of MU. To address this gap, we initiate an innovative study on recalling the forgotten class memberships from unlearned models (ULMs) without requiring access to the original one. Specifically, we implement a Membership Recall Attack (MRA) framework with a teacher-student knowledge distillation architecture, where ULMs serve as noisy labelers to transfer knowledge to student models. Then, it is translated into a Learning with Noisy Labels (LNL) problem for inferring the correct labels of the forgetting instances. Extensive experiments on state-of-the-art MU methods with multiple real datasets demonstrate that the proposed MRA strategy exhibits high efficacy in recovering class memberships of unlearned instances. As a result, our study and evaluation have established a benchmark for future research on MU vulnerabilities.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19486","src/content/posts/2025-06-25-recalling-the-forgotten-class-memberships-unlearned-models-can-be-noisy-labelers-to-leak-privacy.md","6ae73803899d7b4d",{"html":25,"metadata":34876},{"headings":34877,"localImagePaths":34878,"remoteImagePaths":34879,"frontmatter":34880,"imagePaths":34881},[],[],[],{"title":34869,"description":34870,"summary":34870,"pubDate":32084,"source":25191,"url":34872,"thumbnail":25193},[],"2025-06-25-recalling-the-forgotten-class-memberships-unlearned-models-can-be-noisy-labelers-to-leak-privacy.md","2025-06-25-recllm-r1-a-two-stage-training-paradigm-with-reinforcement-learning-and-chain-of-thought-v1",{"id":34883,"data":34885,"filePath":34890,"digest":34891,"rendered":34892,"legacyId":34899},{"title":34886,"description":34887,"summary":34887,"pubDate":34888,"source":25191,"url":34889,"thumbnail":25193},"RecLLM-R1: A Two-Stage Training Paradigm with Reinforcement Learning and Chain-of-Thought v1","arXiv:2506.19235v1 Announce Type: new Abstract: Traditional recommendation systems often grapple with 'filter bubbles', underutilization of external knowledge, and a disconnect between model optimization and business policy iteration. To address these limitations, this paper introduces RecLLM-R1, a novel recommendation framework leveraging Large Language Models (LLMs) and drawing inspiration from the DeepSeek R1 methodology. The framework initiates by transforming user profiles, historical interactions, and multi-faceted item attributes into LLM-interpretable natural language prompts through a carefully engineered data construction process. Subsequently, a two-stage training paradigm is employed: the initial stage involves Supervised Fine-Tuning (SFT) to imbue the LLM with fundamental recommendation capabilities. The subsequent stage utilizes Group Relative Policy Optimization (GRPO), a reinforcement learning technique, augmented with a Chain-of-Thought (CoT) mechanism. This stage guides the model through multi-step reasoning and holistic decision-making via a flexibly defined reward function, aiming to concurrently optimize recommendation accuracy, diversity, and other bespoke business objectives. Empirical evaluations on a real-world user behavior dataset from a large-scale social media platform demonstrate that RecLLM-R1 significantly surpasses existing baseline methods across a spectrum of evaluation metrics, including accuracy, diversity, and novelty. It effectively mitigates the filter bubble effect and presents a promising avenue for the integrated optimization of recommendation models and policies under intricate business goals.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19235","src/content/posts/2025-06-25-recllm-r1-a-two-stage-training-paradigm-with-reinforcement-learning-and-chain-of-thought-v1.md","d955c78ab81e524d",{"html":25,"metadata":34893},{"headings":34894,"localImagePaths":34895,"remoteImagePaths":34896,"frontmatter":34897,"imagePaths":34898},[],[],[],{"title":34886,"description":34887,"summary":34887,"pubDate":32084,"source":25191,"url":34889,"thumbnail":25193},[],"2025-06-25-recllm-r1-a-two-stage-training-paradigm-with-reinforcement-learning-and-chain-of-thought-v1.md","2025-06-25-reconx-reconstruct-any-scene-from-sparse-views-with-video-diffusion-model",{"id":34900,"data":34902,"filePath":34907,"digest":34908,"rendered":34909,"legacyId":34916},{"title":34903,"description":34904,"summary":34904,"pubDate":34905,"source":25191,"url":34906,"thumbnail":25193},"ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model","arXiv:2408.16767v3 Announce Type: replace-cross Abstract: Advancements in 3D scene reconstruction have transformed 2D images from the real world into 3D models, producing realistic 3D results from hundreds of input photos. Despite great success in dense-view reconstruction scenarios, rendering a detailed scene from insufficient captured views is still an ill-posed optimization problem, often resulting in artifacts and distortions in unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction challenge as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. However, 3D view consistency struggles to be accurately preserved in directly generated video frames from pre-trained models. To address this, given limited input views, the proposed ReconX first constructs a global point cloud and encodes it into a contextual space as the 3D structure condition. Guided by the condition, the video diffusion model then synthesizes video frames that are both detail-preserved and exhibit a high degree of 3D consistency, ensuring the coherence of the scene from various perspectives. Finally, we recover the 3D scene from the generated video through a confidence-aware 3D Gaussian Splatting optimization scheme. Extensive experiments on various real-world datasets show the superiority of our ReconX over state-of-the-art methods in terms of quality and generalizability.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2408.16767","src/content/posts/2025-06-25-reconx-reconstruct-any-scene-from-sparse-views-with-video-diffusion-model.md","ac4d9477768462d2",{"html":25,"metadata":34910},{"headings":34911,"localImagePaths":34912,"remoteImagePaths":34913,"frontmatter":34914,"imagePaths":34915},[],[],[],{"title":34903,"description":34904,"summary":34904,"pubDate":32084,"source":25191,"url":34906,"thumbnail":25193},[],"2025-06-25-reconx-reconstruct-any-scene-from-sparse-views-with-video-diffusion-model.md","2025-06-25-recycling-the-web-a-method-to-enhance-pre-training-data-quality-and-quantity-for-language-models",{"id":34917,"data":34919,"filePath":34924,"digest":34925,"rendered":34926,"legacyId":34933},{"title":34920,"description":34921,"summary":34921,"pubDate":34922,"source":25191,"url":34923,"thumbnail":25193},"Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models","arXiv:2506.04689v1 Announce Type: cross Abstract: Scaling laws predict that the performance of large language models improves with increasing model size and data size. In practice, pre-training has been relying on massive web crawls, using almost all data sources publicly available on the internet so far. However, this pool of natural data does not grow at the same rate as the compute supply. Furthermore, the availability of high-quality texts is even more limited: data filtering pipelines often remove up to 99% of the initial web scrapes to achieve state-of-the-art. To address the 'data wall' of pre-training scaling, our work explores ways to transform and recycle data discarded in existing filtering processes. We propose REWIRE, REcycling the Web with guIded REwrite, a method to enrich low-quality documents so that they could become useful for training. This in turn allows us to increase the representation of synthetic data in the final pre-training set. Experiments at 1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points improvement respectively across 22 diverse tasks, compared to training on only filtered web data. Training on the raw-synthetic data mix is also more effective than having access to 2x web data. Through further analysis, we demonstrate that about 82% of the mixed in texts come from transforming lower-quality documents that would otherwise be discarded. REWIRE also outperforms related approaches of generating synthetic data, including Wikipedia-style paraphrasing, question-answer synthesizing and knowledge extraction. These results suggest that recycling web texts holds the potential for being a simple and effective approach for scaling pre-training data.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.04689","src/content/posts/2025-06-25-recycling-the-web-a-method-to-enhance-pre-training-data-quality-and-quantity-for-language-models.md","28f9c95c5aee327e",{"html":25,"metadata":34927},{"headings":34928,"localImagePaths":34929,"remoteImagePaths":34930,"frontmatter":34931,"imagePaths":34932},[],[],[],{"title":34920,"description":34921,"summary":34921,"pubDate":32084,"source":25191,"url":34923,"thumbnail":25193},[],"2025-06-25-recycling-the-web-a-method-to-enhance-pre-training-data-quality-and-quantity-for-language-models.md","2025-06-25-redit-reward-dithering-for-improved-llm-policy-optimization",{"id":34934,"data":34936,"filePath":34941,"digest":34942,"rendered":34943,"legacyId":34950},{"title":34937,"description":34938,"summary":34938,"pubDate":34939,"source":25191,"url":34940,"thumbnail":25193},"ReDit: Reward Dithering for Improved LLM Policy Optimization","arXiv:2506.18631v2 Announce Type: replace-cross Abstract: DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18631","src/content/posts/2025-06-25-redit-reward-dithering-for-improved-llm-policy-optimization.md","32d8aaea0e0f17b4",{"html":25,"metadata":34944},{"headings":34945,"localImagePaths":34946,"remoteImagePaths":34947,"frontmatter":34948,"imagePaths":34949},[],[],[],{"title":34937,"description":34938,"summary":34938,"pubDate":32084,"source":25191,"url":34940,"thumbnail":25193},[],"2025-06-25-redit-reward-dithering-for-improved-llm-policy-optimization.md","2025-06-25-reinforcement-learning-based-dynamic-grouping-for-tubular-structure-tracking",{"id":34951,"data":34953,"filePath":34958,"digest":34959,"rendered":34960,"legacyId":34967},{"title":34954,"description":34955,"summary":34955,"pubDate":34956,"source":25191,"url":34957,"thumbnail":25193},"Reinforcement Learning-Based Dynamic Grouping for Tubular Structure Tracking","arXiv:2506.18930v1 Announce Type: cross Abstract: The computation of minimal paths for the applications in tracking tubular structures such as blood vessels and roads is challenged by complex morphologies and environmental variations. Existing approaches can be roughly categorized into two research lines: the point-wise based models and the segment-wise based models. Although segment-wise approaches have obtained promising results in many scenarios, they often suffer from computational inefficiency and heavily rely on a prescribed prior to fit the target elongated shapes. We propose a novel framework that casts segment-wise tracking as a Markov Decision Process (MDP), enabling a reinforcement learning approach. Our method leverages Q-Learning to dynamically explore a graph of segments, computing edge weights on-demand and adaptively expanding the search space. This strategy avoids the high cost of a pre-computed graph and proves robust to incomplete initial information. Experimental reuslts on typical tubular structure datasets demonstrate that our method significantly outperforms state-of-the-art point-wise and segment-wise approaches. The proposed method effectively handles complex topologies and maintains global path coherence without depending on extensive prior structural knowledge.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18930","src/content/posts/2025-06-25-reinforcement-learning-based-dynamic-grouping-for-tubular-structure-tracking.md","036c4a5cb72aca8b",{"html":25,"metadata":34961},{"headings":34962,"localImagePaths":34963,"remoteImagePaths":34964,"frontmatter":34965,"imagePaths":34966},[],[],[],{"title":34954,"description":34955,"summary":34955,"pubDate":32084,"source":25191,"url":34957,"thumbnail":25193},[],"2025-06-25-reinforcement-learning-based-dynamic-grouping-for-tubular-structure-tracking.md","2025-06-25-remar-ds-recalibrated-feature-learning-for-metal-artifact-reduction-and-ct-domain-transformation",{"id":34968,"data":34970,"filePath":34975,"digest":34976,"rendered":34977,"legacyId":34984},{"title":34971,"description":34972,"summary":34972,"pubDate":34973,"source":25191,"url":34974,"thumbnail":25193},"ReMAR-DS: Recalibrated Feature Learning for Metal Artifact Reduction and CT Domain Transformation","arXiv:2506.19531v1 Announce Type: cross Abstract: Artifacts in kilo-Voltage CT (kVCT) imaging degrade image quality, impacting clinical decisions. We propose a deep learning framework for metal artifact reduction (MAR) and domain transformation from kVCT to Mega-Voltage CT (MVCT). The proposed framework, ReMAR-DS, utilizes an encoder-decoder architecture with enhanced feature recalibration, effectively reducing artifacts while preserving anatomical structures. This ensures that only relevant information is utilized in the reconstruction process. By infusing recalibrated features from the encoder block, the model focuses on relevant spatial regions (e.g., areas with artifacts) and highlights key features across channels (e.g., anatomical structures), leading to improved reconstruction of artifact-corrupted regions. Unlike traditional MAR methods, our approach bridges the gap between high-resolution kVCT and artifact-resistant MVCT, enhancing radiotherapy planning. It produces high-quality MVCT-like reconstructions, validated through qualitative and quantitative evaluations. Clinically, this enables oncologists to rely on kVCT alone, reducing repeated high-dose MVCT scans and lowering radiation exposure for cancer patients.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19531","src/content/posts/2025-06-25-remar-ds-recalibrated-feature-learning-for-metal-artifact-reduction-and-ct-domain-transformation.md","2b34d75cd918f668",{"html":25,"metadata":34978},{"headings":34979,"localImagePaths":34980,"remoteImagePaths":34981,"frontmatter":34982,"imagePaths":34983},[],[],[],{"title":34971,"description":34972,"summary":34972,"pubDate":32084,"source":25191,"url":34974,"thumbnail":25193},[],"2025-06-25-remar-ds-recalibrated-feature-learning-for-metal-artifact-reduction-and-ct-domain-transformation.md","2025-06-25-research-on-model-parallelism-and-data-parallelism-optimization-methods-in-large-language-model-based-recommendation-systems",{"id":34985,"data":34987,"filePath":34992,"digest":34993,"rendered":34994,"legacyId":35001},{"title":34988,"description":34989,"summary":34989,"pubDate":34990,"source":25191,"url":34991,"thumbnail":25193},"Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems","arXiv:2506.17551v2 Announce Type: replace-cross Abstract: With the rapid adoption of large language models (LLMs) in recommendation systems, the computational and communication bottlenecks caused by their massive parameter sizes and large data volumes have become increasingly prominent. This paper systematically investigates two classes of optimization methods-model parallelism and data parallelism-for distributed training of LLMs in recommendation scenarios. For model parallelism, we implement both tensor parallelism and pipeline parallelism, and introduce an adaptive load-balancing mechanism to reduce cross-device communication overhead. For data parallelism, we compare synchronous and asynchronous modes, combining gradient compression and sparsification techniques with an efficient aggregation communication framework to significantly improve bandwidth utilization. Experiments conducted on a real-world recommendation dataset in a simulated service environment demonstrate that our proposed hybrid parallelism scheme increases training throughput by over 30% and improves resource utilization by approximately 20% compared to traditional single-mode parallelism, while maintaining strong scalability and robustness. Finally, we discuss trade-offs among different parallel strategies in online deployment and outline future directions involving heterogeneous hardware integration and automated scheduling technologies.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.17551","src/content/posts/2025-06-25-research-on-model-parallelism-and-data-parallelism-optimization-methods-in-large-language-model-based-recommendation-systems.md","163102890977dda9",{"html":25,"metadata":34995},{"headings":34996,"localImagePaths":34997,"remoteImagePaths":34998,"frontmatter":34999,"imagePaths":35000},[],[],[],{"title":34988,"description":34989,"summary":34989,"pubDate":32084,"source":25191,"url":34991,"thumbnail":25193},[],"2025-06-25-research-on-model-parallelism-and-data-parallelism-optimization-methods-in-large-language-model-based-recommendation-systems.md","2025-06-25-rich-interoperable-metadata-for-cultural-heritage-projects-at-jagiellonian-university",{"id":35002,"data":35004,"filePath":35009,"digest":35010,"rendered":35011,"legacyId":35018},{"title":35005,"description":35006,"summary":35006,"pubDate":35007,"source":25191,"url":35008,"thumbnail":25193},"Rich Interoperable Metadata for Cultural Heritage Projects at Jagiellonian University","arXiv:2407.06976v3 Announce Type: replace-cross Abstract: The rich metadata created nowadays for objects stored in libraries has nowhere to be stored, because core standards, namely MARC 21 and Dublin Core, are not flexible enough. The aim of this paper is to summarize our work-in-progress on tackling this problem in research on cultural heritage objects at the Jagiellonian University (JU). We compared the objects' metadata currently being collected at the JU (with examples of manuscript, placard, and obituary) with five widespread metadata standards used by the cultural heritage community: Dublin Core, EAD, MODS, EDM and Digital Scriptorium. Our preliminary results showed that mapping between them is indeed problematic, but we identified requirements that should be followed in further work on the JU cultural heritage metadata schema in order to achieve maximum interoperability. As we move forward, based on the successive versions of the conceptual model, we will conduct experiments to validate the practical feasibility of these mappings and the degree to which the proposed model will actually enable integration with data in these various metadata formats.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2407.06976","src/content/posts/2025-06-25-rich-interoperable-metadata-for-cultural-heritage-projects-at-jagiellonian-university.md","52da3bd660fe8b85",{"html":25,"metadata":35012},{"headings":35013,"localImagePaths":35014,"remoteImagePaths":35015,"frontmatter":35016,"imagePaths":35017},[],[],[],{"title":35005,"description":35006,"summary":35006,"pubDate":32084,"source":25191,"url":35008,"thumbnail":25193},[],"2025-06-25-rich-interoperable-metadata-for-cultural-heritage-projects-at-jagiellonian-university.md","2025-06-25-robotics-under-construction-challenges-on-job-sites",{"id":35019,"data":35021,"filePath":35026,"digest":35027,"rendered":35028,"legacyId":35035},{"title":35022,"description":35023,"summary":35023,"pubDate":35024,"source":25191,"url":35025,"thumbnail":25193},"Robotics Under Construction: Challenges on Job Sites","arXiv:2506.19597v1 Announce Type: cross Abstract: As labor shortages and productivity stagnation increasingly challenge the construction industry, automation has become essential for sustainable infrastructure development. This paper presents an autonomous payload transportation system as an initial step toward fully unmanned construction sites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous navigation, fleet management, and GNSS-based localization to facilitate material transport in construction site environments. While the current system does not yet incorporate dynamic environment adaptation algorithms, we have begun fundamental investigations into external-sensor based perception and mapping system. Preliminary results highlight the potential challenges, including navigation in evolving terrain, environmental perception under construction-specific conditions, and sensor placement optimization for improving autonomy and efficiency. Looking forward, we envision a construction ecosystem where collaborative autonomous agents dynamically adapt to site conditions, optimizing workflow and reducing human intervention. This paper provides foundational insights into the future of robotics-driven construction automation and identifies critical areas for further technological development.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19597","src/content/posts/2025-06-25-robotics-under-construction-challenges-on-job-sites.md","a7545273a2e43254",{"html":25,"metadata":35029},{"headings":35030,"localImagePaths":35031,"remoteImagePaths":35032,"frontmatter":35033,"imagePaths":35034},[],[],[],{"title":35022,"description":35023,"summary":35023,"pubDate":32084,"source":25191,"url":35025,"thumbnail":25193},[],"2025-06-25-robotics-under-construction-challenges-on-job-sites.md","2025-06-25-robust-behavior-cloning-via-global-lipschitz-regularization",{"id":35036,"data":35038,"filePath":35043,"digest":35044,"rendered":35045,"legacyId":35052},{"title":35039,"description":35040,"summary":35040,"pubDate":35041,"source":25191,"url":35042,"thumbnail":25193},"Robust Behavior Cloning Via Global Lipschitz Regularization","arXiv:2506.19250v1 Announce Type: cross Abstract: Behavior Cloning (BC) is an effective imitation learning technique and has even been adopted in some safety-critical domains such as autonomous vehicles. BC trains a policy to mimic the behavior of an expert by using a dataset composed of only state-action pairs demonstrated by the expert, without any additional interaction with the environment. However, During deployment, the policy observations may contain measurement errors or adversarial disturbances. Since the observations may deviate from the true states, they can mislead the agent into making sub-optimal actions. In this work, we use a global Lipschitz regularization approach to enhance the robustness of the learned policy network. We then show that the resulting global Lipschitz property provides a robustness certificate to the policy with respect to different bounded norm perturbations. Then, we propose a way to construct a Lipschitz neural network that ensures the policy robustness. We empirically validate our theory across various environments in Gymnasium. Keywords: Robust Reinforcement Learning; Behavior Cloning; Lipschitz Neural Network",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19250","src/content/posts/2025-06-25-robust-behavior-cloning-via-global-lipschitz-regularization.md","077b531535a6246a",{"html":25,"metadata":35046},{"headings":35047,"localImagePaths":35048,"remoteImagePaths":35049,"frontmatter":35050,"imagePaths":35051},[],[],[],{"title":35039,"description":35040,"summary":35040,"pubDate":32084,"source":25191,"url":35042,"thumbnail":25193},[],"2025-06-25-robust-behavior-cloning-via-global-lipschitz-regularization.md","2025-06-25-robust-optimization-with-diffusion-models-for-green-security",{"id":35053,"data":35055,"filePath":35060,"digest":35061,"rendered":35062,"legacyId":35069},{"title":35056,"description":35057,"summary":35057,"pubDate":35058,"source":25191,"url":35059,"thumbnail":25193},"Robust Optimization with Diffusion Models for Green Security","arXiv:2503.05730v2 Announce Type: replace-cross Abstract: In green security, defenders must forecast adversarial behavior, such as poaching, illegal logging, and illegal fishing, to plan effective patrols. These behavior are often highly uncertain and complex. Prior work has leveraged game theory to design robust patrol strategies to handle uncertainty, but existing adversarial behavior models primarily rely on Gaussian processes or linear models, which lack the expressiveness needed to capture intricate behavioral patterns. To address this limitation, we propose a conditional diffusion model for adversary behavior modeling, leveraging its strong distribution-fitting capabilities. To the best of our knowledge, this is the first application of diffusion models in the green security domain. Integrating diffusion models into game-theoretic optimization, however, presents new challenges, including a constrained mixed strategy space and the need to sample from an unnormalized distribution to estimate utilities. To tackle these challenges, we introduce a mixed strategy of mixed strategies and employ a twisted Sequential Monte Carlo (SMC) sampler for accurate sampling. Theoretically, our algorithm is guaranteed to converge to an epsilon equilibrium with high probability using a finite number of iterations and samples. Empirically, we evaluate our approach on both synthetic and real-world poaching datasets, demonstrating its effectiveness.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2503.05730","src/content/posts/2025-06-25-robust-optimization-with-diffusion-models-for-green-security.md","46d07800d2e9cfbe",{"html":25,"metadata":35063},{"headings":35064,"localImagePaths":35065,"remoteImagePaths":35066,"frontmatter":35067,"imagePaths":35068},[],[],[],{"title":35056,"description":35057,"summary":35057,"pubDate":32084,"source":25191,"url":35059,"thumbnail":25193},[],"2025-06-25-robust-optimization-with-diffusion-models-for-green-security.md","2025-06-25-robust-reinforcement-learning-from-human-feedback-for-large-language-models-fine-tuning",{"id":35070,"data":35072,"filePath":35077,"digest":35078,"rendered":35079,"legacyId":35086},{"title":35073,"description":35074,"summary":35074,"pubDate":35075,"source":25191,"url":35076,"thumbnail":25193},"Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning","arXiv:2504.03784v4 Announce Type: replace-cross Abstract: Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2504.03784","src/content/posts/2025-06-25-robust-reinforcement-learning-from-human-feedback-for-large-language-models-fine-tuning.md","fca5dc7956b26ee8",{"html":25,"metadata":35080},{"headings":35081,"localImagePaths":35082,"remoteImagePaths":35083,"frontmatter":35084,"imagePaths":35085},[],[],[],{"title":35073,"description":35074,"summary":35074,"pubDate":32084,"source":25191,"url":35076,"thumbnail":25193},[],"2025-06-25-robust-reinforcement-learning-from-human-feedback-for-large-language-models-fine-tuning.md","2025-06-25-saasが終わる-興味ない-ラクス社長が語るaiの真の脅威",{"id":35087,"data":35089,"filePath":35095,"digest":35096,"rendered":35097,"legacyId":35105},{"title":35090,"description":35091,"summary":35091,"pubDate":35092,"source":24944,"url":35093,"thumbnail":35094},"「SaaSが終わる？　興味ない」　ラクス社長が語るAIの「真の脅威」","国内SaaS業界トップランナーのラクス。「SaaSが死ぬかどうかってそんなに興味ない」と明かす中村崇則社長が、本当に恐れているものとは何なのか。業界トップが明かすAI時代の生存戦略を聞いた。",["Date","2025-06-24T23:00:00.000Z"],"https://www.itmedia.co.jp/business/articles/2506/25/news038.html","https://image.itmedia.co.jp/business/articles/2506/25/cover_news038.jpg","src/content/posts/2025-06-25-saasが終わる-興味ない-ラクス社長が語るaiの真の脅威.md","3602bd56fb3a5592",{"html":25,"metadata":35098},{"headings":35099,"localImagePaths":35100,"remoteImagePaths":35101,"frontmatter":35102,"imagePaths":35104},[],[],[],{"title":35090,"description":35091,"summary":35091,"pubDate":35103,"source":24944,"url":35093,"thumbnail":35094},"Wed, 25 Jun 2025 08:00:00 +0900",[],"2025-06-25-saasが終わる-興味ない-ラクス社長が語るaiの真の脅威.md","2025-06-25-safe-pruning-lora-robust-distance-guided-pruning-for-safety-alignment-in-adaptation-of-llms",{"id":35106,"data":35108,"filePath":35113,"digest":35114,"rendered":35115,"legacyId":35122},{"title":35109,"description":35110,"summary":35110,"pubDate":35111,"source":25191,"url":35112,"thumbnail":25193},"Safe Pruning LoRA: Robust Distance-Guided Pruning for Safety Alignment in Adaptation of LLMs","arXiv:2506.18931v1 Announce Type: cross Abstract: Fine-tuning Large Language Models (LLMs) with Low-Rank Adaptation (LoRA) enhances adaptability while reducing computational costs. However, fine-tuning can compromise safety alignment, even with benign data, increasing susceptibility to harmful outputs. Existing safety alignment methods struggle to capture complex parameter shifts, leading to suboptimal safety-utility trade-offs. To address this issue, we propose Safe Pruning LoRA (SPLoRA), a novel pruning-based approach that selectively removes LoRA layers that weaken safety alignment, improving safety while preserving performance. At its core, we introduce Empirical-DIEM (E-DIEM), a dimension-insensitive similarity metric that effectively detects safety misalignment in LoRA-adapted models. We conduct extensive experiments on LLMs fine-tuned with mixed of benign and malicious data, and purely benign datasets, evaluating SPLoRA across utility, safety, and reliability metrics. Results demonstrate that SPLoRA outperforms state-of-the-art safety alignment techniques, significantly reducing safety risks while maintaining or improving model performance and reliability. Additionally, SPLoRA reduces inference overhead, making it a scalable and efficient solution for deploying safer and more reliable LLMs. The code is available at https://github.com/AoShuang92/SPLoRA.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18931","src/content/posts/2025-06-25-safe-pruning-lora-robust-distance-guided-pruning-for-safety-alignment-in-adaptation-of-llms.md","95d73742d85566e2",{"html":25,"metadata":35116},{"headings":35117,"localImagePaths":35118,"remoteImagePaths":35119,"frontmatter":35120,"imagePaths":35121},[],[],[],{"title":35109,"description":35110,"summary":35110,"pubDate":32084,"source":25191,"url":35112,"thumbnail":25193},[],"2025-06-25-safe-pruning-lora-robust-distance-guided-pruning-for-safety-alignment-in-adaptation-of-llms.md","2025-06-25-sage-strategy-adaptive-generation-engine-for-query-rewriting",{"id":35123,"data":35125,"filePath":35130,"digest":35131,"rendered":35132,"legacyId":35139},{"title":35126,"description":35127,"summary":35127,"pubDate":35128,"source":25191,"url":35129,"thumbnail":25193},"SAGE: Strategy-Adaptive Generation Engine for Query Rewriting","arXiv:2506.19783v1 Announce Type: new Abstract: Query rewriting is pivotal for enhancing dense retrieval, yet current methods demand large-scale supervised data or suffer from inefficient reinforcement learning (RL) exploration. In this work, we first establish that guiding Large Language Models (LLMs) with a concise set of expert-crafted strategies, such as semantic expansion and entity disambiguation, substantially improves retrieval effectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus, and SciFact. Building on this insight, we introduce the Strategy-Adaptive Generation Engine (SAGE), which operationalizes these strategies in an RL framework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit Shaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative learning signals. This strategy-guided approach not only achieves new state-of-the-art NDCG@10 results, but also uncovers a compelling emergent behavior: the agent learns to select optimal strategies, reduces unnecessary exploration, and generates concise rewrites, lowering inference cost without sacrificing performance. Our findings demonstrate that strategy-guided RL, enhanced with nuanced reward shaping, offers a scalable, efficient, and more interpretable paradigm for developing the next generation of robust information retrieval systems.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19783","src/content/posts/2025-06-25-sage-strategy-adaptive-generation-engine-for-query-rewriting.md","8f727b701a13aac8",{"html":25,"metadata":35133},{"headings":35134,"localImagePaths":35135,"remoteImagePaths":35136,"frontmatter":35137,"imagePaths":35138},[],[],[],{"title":35126,"description":35127,"summary":35127,"pubDate":32084,"source":25191,"url":35129,"thumbnail":25193},[],"2025-06-25-sage-strategy-adaptive-generation-engine-for-query-rewriting.md","2025-06-25-sassha-sharpness-aware-adaptive-second-order-optimization-with-stable-hessian-approximation",{"id":35140,"data":35142,"filePath":35147,"digest":35148,"rendered":35149,"legacyId":35156},{"title":35143,"description":35144,"summary":35144,"pubDate":35145,"source":25191,"url":35146,"thumbnail":25193},"SASSHA: Sharpness-aware Adaptive Second-order Optimization with Stable Hessian Approximation","arXiv:2502.18153v2 Announce Type: replace-cross Abstract: Approximate second-order optimization methods often exhibit poorer generalization compared to first-order approaches. In this work, we look into this issue through the lens of the loss landscape and find that existing second-order methods tend to converge to sharper minima compared to SGD. In response, we propose Sassha, a novel second-order method designed to enhance generalization by explicitly reducing sharpness of the solution, while stabilizing the computation of approximate Hessians along the optimization trajectory. In fact, this sharpness minimization scheme is crafted also to accommodate lazy Hessian updates, so as to secure efficiency besides flatness. To validate its effectiveness, we conduct a wide range of standard deep learning experiments where Sassha demonstrates its outstanding generalization performance that is comparable to, and mostly better than, other methods. We provide a comprehensive set of analyses including convergence, robustness, stability, efficiency, and cost.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2502.18153","src/content/posts/2025-06-25-sassha-sharpness-aware-adaptive-second-order-optimization-with-stable-hessian-approximation.md","783309c54bdd7d27",{"html":25,"metadata":35150},{"headings":35151,"localImagePaths":35152,"remoteImagePaths":35153,"frontmatter":35154,"imagePaths":35155},[],[],[],{"title":35143,"description":35144,"summary":35144,"pubDate":32084,"source":25191,"url":35146,"thumbnail":25193},[],"2025-06-25-sassha-sharpness-aware-adaptive-second-order-optimization-with-stable-hessian-approximation.md","2025-06-25-semantic-scene-graph-for-ultrasound-image-explanation-and-scanning-guidance",{"id":35157,"data":35159,"filePath":35164,"digest":35165,"rendered":35166,"legacyId":35173},{"title":35160,"description":35161,"summary":35161,"pubDate":35162,"source":25191,"url":35163,"thumbnail":25193},"Semantic Scene Graph for Ultrasound Image Explanation and Scanning Guidance","arXiv:2506.19683v1 Announce Type: cross Abstract: Understanding medical ultrasound imaging remains a long-standing challenge due to significant visual variability caused by differences in imaging and acquisition parameters. Recent advancements in large language models (LLMs) have been used to automatically generate terminology-rich summaries orientated to clinicians with sufficient physiological knowledge. Nevertheless, the increasing demand for improved ultrasound interpretability and basic scanning guidance among non-expert users, e.g., in point-of-care settings, has not yet been explored. In this study, we first introduce the scene graph (SG) for ultrasound images to explain image content to ordinary and provide guidance for ultrasound scanning. The ultrasound SG is first computed using a transformer-based one-stage method, eliminating the need for explicit object detection. To generate a graspable image explanation for ordinary, the user query is then used to further refine the abstract SG representation through LLMs. Additionally, the predicted SG is explored for its potential in guiding ultrasound scanning toward missing anatomies within the current imaging view, assisting ordinary users in achieving more standardized and complete anatomical exploration. The effectiveness of this SG-based image explanation and scanning guidance has been validated on images from the left and right neck regions, including the carotid and thyroid, across five volunteers. The results demonstrate the potential of the method to maximally democratize ultrasound by enhancing its interpretability and usability for ordinaries.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19683","src/content/posts/2025-06-25-semantic-scene-graph-for-ultrasound-image-explanation-and-scanning-guidance.md","6b2a2885331cd906",{"html":25,"metadata":35167},{"headings":35168,"localImagePaths":35169,"remoteImagePaths":35170,"frontmatter":35171,"imagePaths":35172},[],[],[],{"title":35160,"description":35161,"summary":35161,"pubDate":32084,"source":25191,"url":35163,"thumbnail":25193},[],"2025-06-25-semantic-scene-graph-for-ultrasound-image-explanation-and-scanning-guidance.md","2025-06-25-shamans-sound-localization-with-hybrid-alpha-stable-spatial-measure-and-neural-steerer",{"id":35174,"data":35176,"filePath":35181,"digest":35182,"rendered":35183,"legacyId":35190},{"title":35177,"description":35178,"summary":35178,"pubDate":35179,"source":25191,"url":35180,"thumbnail":25193},"SHAMaNS: Sound Localization with Hybrid Alpha-Stable Spatial Measure and Neural Steerer","arXiv:2506.18954v1 Announce Type: cross Abstract: This paper describes a sound source localization (SSL) technique that combines an $alpha$-stable model for the observed signal with a neural network-based approach for modeling steering vectors. Specifically, a physics-informed neural network, referred to as Neural Steerer, is used to interpolate measured steering vectors (SVs) on a fixed microphone array. This allows for a more robust estimation of the so-called $alpha$-stable spatial measure, which represents the most plausible direction of arrival (DOA) of a target signal. As an $alpha$-stable model for the non-Gaussian case ($alpha$ $in$ (0, 2)) theoretically defines a unique spatial measure, we choose to leverage it to account for residual reconstruction error of the Neural Steerer in the downstream tasks. The objective scores indicate that our proposed technique outperforms state-of-the-art methods in the case of multiple sound sources.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18954","src/content/posts/2025-06-25-shamans-sound-localization-with-hybrid-alpha-stable-spatial-measure-and-neural-steerer.md","d3a611488b997f07",{"html":25,"metadata":35184},{"headings":35185,"localImagePaths":35186,"remoteImagePaths":35187,"frontmatter":35188,"imagePaths":35189},[],[],[],{"title":35177,"description":35178,"summary":35178,"pubDate":32084,"source":25191,"url":35180,"thumbnail":25193},[],"2025-06-25-shamans-sound-localization-with-hybrid-alpha-stable-spatial-measure-and-neural-steerer.md","2025-06-25-signal-use-and-emergent-cooperation",{"id":35191,"data":35193,"filePath":35198,"digest":35199,"rendered":35200,"legacyId":35207},{"title":35194,"description":35195,"summary":35195,"pubDate":35196,"source":25191,"url":35197,"thumbnail":25193},"Signal Use and Emergent Cooperation","arXiv:2506.18920v1 Announce Type: new Abstract: In this work, we investigate how autonomous agents, organized into tribes, learn to use communication signals to coordinate their activities and enhance their collective efficiency. Using the NEC-DAC (Neurally Encoded Culture - Distributed Autonomous Communicators) system, where each agent is equipped with its own neural network for decision-making, we demonstrate how these agents develop a shared behavioral system -- akin to a culture -- through learning and signalling. Our research focuses on the self-organization of culture within these tribes of agents and how varying communication strategies impact their fitness and cooperation. By analyzing different social structures, such as authority hierarchies, we show that the culture of cooperation significantly influences the tribe's performance. Furthermore, we explore how signals not only facilitate the emergence of culture but also enable its transmission across generations of agents. Additionally, we examine the benefits of coordinating behavior and signaling within individual agents' neural networks.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18920","src/content/posts/2025-06-25-signal-use-and-emergent-cooperation.md","75e7e40baca89e78",{"html":25,"metadata":35201},{"headings":35202,"localImagePaths":35203,"remoteImagePaths":35204,"frontmatter":35205,"imagePaths":35206},[],[],[],{"title":35194,"description":35195,"summary":35195,"pubDate":32084,"source":25191,"url":35197,"thumbnail":25193},[],"2025-06-25-signal-use-and-emergent-cooperation.md","2025-06-25-skywork-swe-unveiling-data-scaling-laws-for-software-engineering-in-llms",{"id":35208,"data":35210,"filePath":35215,"digest":35216,"rendered":35217,"legacyId":35224},{"title":35211,"description":35212,"summary":35212,"pubDate":35213,"source":25191,"url":35214,"thumbnail":25193},"Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs","arXiv:2506.19290v1 Announce Type: new Abstract: Software engineering (SWE) has recently emerged as a crucial testbed for next-generation LLM agents, demanding inherent capabilities in two critical dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds) and long-context dependency resolution (e.g., >32k tokens). However, the data curation process in SWE remains notoriously time-consuming, as it heavily relies on manual annotation for code file filtering and the setup of dedicated runtime environments to execute and validate unit tests. Consequently, most existing datasets are limited to only a few thousand GitHub-sourced instances. To this end, we propose an incremental, automated data-curation pipeline that systematically scales both the volume and diversity of SWE datasets. Our dataset comprises 10,169 real-world Python task instances from 2,531 distinct GitHub repositories, each accompanied by a task specified in natural language and a dedicated runtime-environment image for automated unit-test validation. We have carefully curated over 8,000 successfully runtime-validated training trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE model on these trajectories, we uncover a striking data scaling phenomenon: the trained model's performance for software engineering capabilities in LLMs continues to improve as the data size increases, showing no signs of saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark without using verifiers or multiple rollouts, establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Furthermore, with the incorporation of test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter models. We release the Skywork-SWE-32B model checkpoint to accelerate future research.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19290","src/content/posts/2025-06-25-skywork-swe-unveiling-data-scaling-laws-for-software-engineering-in-llms.md","67e4b97a8e51f533",{"html":25,"metadata":35218},{"headings":35219,"localImagePaths":35220,"remoteImagePaths":35221,"frontmatter":35222,"imagePaths":35223},[],[],[],{"title":35211,"description":35212,"summary":35212,"pubDate":32084,"source":25191,"url":35214,"thumbnail":25193},[],"2025-06-25-skywork-swe-unveiling-data-scaling-laws-for-software-engineering-in-llms.md","2025-06-25-smart-traffic-signals-comparing-marl-and-fixed-time-strategies",{"id":35225,"data":35227,"filePath":35232,"digest":35233,"rendered":35234,"legacyId":35241},{"title":35228,"description":35229,"summary":35229,"pubDate":35230,"source":25191,"url":35231,"thumbnail":25193},"Smart Traffic Signals: Comparing MARL and Fixed-Time Strategies","arXiv:2505.14544v2 Announce Type: replace Abstract: Urban traffic congestion, particularly at intersections, significantly impacts travel time, fuel consumption, and emissions. Traditional fixed-time signal control systems often lack the adaptability to manage dynamic traffic patterns effectively. This study explores the application of multi-agent reinforcement learning (MARL) to optimize traffic signal coordination across multiple intersections within a simulated environment. Utilizing Pygame, a simulation was developed to model a network of interconnected intersections with randomly generated vehicle flows to reflect realistic traffic variability. A decentralized MARL controller was implemented, in which each traffic signal operates as an autonomous agent, making decisions based on local observations and information from neighboring agents. Performance was evaluated against a baseline fixed-time controller using metrics such as average vehicle wait time and overall throughput. The MARL approach demonstrated statistically significant improvements, including reduced average waiting times and improved throughput. These findings suggest that MARL-based dynamic control strategies hold substantial promise for improving urban traffic management efficiency. More research is recommended to address scalability and real-world implementation challenges.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2505.14544","src/content/posts/2025-06-25-smart-traffic-signals-comparing-marl-and-fixed-time-strategies.md","d46cf216b359fe2c",{"html":25,"metadata":35235},{"headings":35236,"localImagePaths":35237,"remoteImagePaths":35238,"frontmatter":35239,"imagePaths":35240},[],[],[],{"title":35228,"description":35229,"summary":35229,"pubDate":32084,"source":25191,"url":35231,"thumbnail":25193},[],"2025-06-25-smart-traffic-signals-comparing-marl-and-fixed-time-strategies.md","2025-06-25-spiritual-llm-gita-inspired-mental-health-therapy-in-the-era-of-llms",{"id":35242,"data":35244,"filePath":35249,"digest":35250,"rendered":35251,"legacyId":35258},{"title":35245,"description":35246,"summary":35246,"pubDate":35247,"source":25191,"url":35248,"thumbnail":25193},"Spiritual-LLM : Gita Inspired Mental Health Therapy In the Era of LLMs","arXiv:2506.19185v1 Announce Type: new Abstract: Traditional mental health support systems often generate responses based solely on the user's current emotion and situations, resulting in superficial interventions that fail to address deeper emotional needs. This study introduces a novel framework by integrating spiritual wisdom from the Bhagavad Gita with advanced large language model GPT-4o to enhance emotional well-being. We present the GITes (Gita Integrated Therapy for Emotional Support) dataset, which enhances the existing ExTES mental health dataset by including 10,729 spiritually guided responses generated by GPT-4o and evaluated by domain experts. We benchmark GITes against 12 state-of-the-art LLMs, including both mental health specific and general purpose models. To evaluate spiritual relevance in generated responses beyond what conventional n-gram based metrics capture, we propose a novel Spiritual Insight metric and automate assessment via an LLM as jury framework using chain-of-thought prompting. Integrating spiritual guidance into AI driven support enhances both NLP and spiritual metrics for the best performing LLM Phi3-Mini 3.2B Instruct, achieving improvements of 122.71% in ROUGE, 126.53% in METEOR, 8.15% in BERT score, 15.92% in Spiritual Insight, 18.61% in Sufficiency and 13.22% in Relevance compared to its zero-shot counterpart. While these results reflect substantial improvements across automated empathy and spirituality metrics, further validation in real world patient populations remains a necessary step. Our findings indicate a strong potential for AI systems enriched with spiritual guidance to enhance user satisfaction and perceived support outcomes. The code and dataset will be publicly available to advance further research in this emerging area.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19185","src/content/posts/2025-06-25-spiritual-llm-gita-inspired-mental-health-therapy-in-the-era-of-llms.md","077c17599a9f251b",{"html":25,"metadata":35252},{"headings":35253,"localImagePaths":35254,"remoteImagePaths":35255,"frontmatter":35256,"imagePaths":35257},[],[],[],{"title":35245,"description":35246,"summary":35246,"pubDate":32084,"source":25191,"url":35248,"thumbnail":25193},[],"2025-06-25-spiritual-llm-gita-inspired-mental-health-therapy-in-the-era-of-llms.md","2025-06-25-spotting-out-of-character-behavior-atomic-level-evaluation-of-persona-fidelity-in-open-ended-generation",{"id":35259,"data":35261,"filePath":35266,"digest":35267,"rendered":35268,"legacyId":35275},{"title":35262,"description":35263,"summary":35263,"pubDate":35264,"source":25191,"url":35265,"thumbnail":25193},"Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation","arXiv:2506.19352v1 Announce Type: cross Abstract: Ensuring persona fidelity in large language models (LLMs) is essential for maintaining coherent and engaging human-AI interactions. However, LLMs often exhibit Out-of-Character (OOC) behavior, where generated responses deviate from an assigned persona, leading to inconsistencies that affect model reliability. Existing evaluation methods typically assign single scores to entire responses, struggling to capture subtle persona misalignment, particularly in long-form text generation. To address this limitation, we propose an atomic-level evaluation framework that quantifies persona fidelity at a finer granularity. Our three key metrics measure the degree of persona alignment and consistency within and across generations. Our approach enables a more precise and realistic assessment of persona fidelity by identifying subtle deviations that real users would encounter. Through our experiments, we demonstrate that our framework effectively detects persona inconsistencies that prior methods overlook. By analyzing persona fidelity across diverse tasks and personality types, we reveal how task structure and persona desirability influence model adaptability, highlighting challenges in maintaining consistent persona expression.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19352","src/content/posts/2025-06-25-spotting-out-of-character-behavior-atomic-level-evaluation-of-persona-fidelity-in-open-ended-generation.md","7d047b0bbf83f132",{"html":25,"metadata":35269},{"headings":35270,"localImagePaths":35271,"remoteImagePaths":35272,"frontmatter":35273,"imagePaths":35274},[],[],[],{"title":35262,"description":35263,"summary":35263,"pubDate":32084,"source":25191,"url":35265,"thumbnail":25193},[],"2025-06-25-spotting-out-of-character-behavior-atomic-level-evaluation-of-persona-fidelity-in-open-ended-generation.md","2025-06-25-srft-a-single-stage-method-with-supervised-and-reinforcement-fine-tuning-for-reasoning",{"id":35276,"data":35278,"filePath":35283,"digest":35284,"rendered":35285,"legacyId":35292},{"title":35279,"description":35280,"summary":35280,"pubDate":35281,"source":25191,"url":35282,"thumbnail":25193},"SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning","arXiv:2506.19767v1 Announce Type: cross Abstract: Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19767","src/content/posts/2025-06-25-srft-a-single-stage-method-with-supervised-and-reinforcement-fine-tuning-for-reasoning.md","5741b78216dcefdb",{"html":25,"metadata":35286},{"headings":35287,"localImagePaths":35288,"remoteImagePaths":35289,"frontmatter":35290,"imagePaths":35291},[],[],[],{"title":35279,"description":35280,"summary":35280,"pubDate":32084,"source":25191,"url":35282,"thumbnail":25193},[],"2025-06-25-srft-a-single-stage-method-with-supervised-and-reinforcement-fine-tuning-for-reasoning.md","2025-06-25-ssps-self-supervised-positive-sampling-for-robust-self-supervised-speaker-verification",{"id":35293,"data":35295,"filePath":35300,"digest":35301,"rendered":35302,"legacyId":35309},{"title":35296,"description":35297,"summary":35297,"pubDate":35298,"source":25191,"url":35299,"thumbnail":25193},"SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification","arXiv:2505.14561v2 Announce Type: replace-cross Abstract: Self-Supervised Learning (SSL) has led to considerable progress in Speaker Verification (SV). The standard framework uses same-utterance positive sampling and data-augmentation to generate anchor-positive pairs of the same speaker. This is a major limitation, as this strategy primarily encodes channel information from the recording condition, shared by the anchor and positive. We propose a new positive sampling technique to address this bottleneck: Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find an appropriate positive, i.e., of the same speaker identity but a different recording condition, in the latent space using clustering assignments and a memory queue of positive embeddings. SSPS improves SV performance for both SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by lowering intra-speaker variance, providing comparable performance to DINO-SSPS.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2505.14561","src/content/posts/2025-06-25-ssps-self-supervised-positive-sampling-for-robust-self-supervised-speaker-verification.md","75ce6c2862cd7877",{"html":25,"metadata":35303},{"headings":35304,"localImagePaths":35305,"remoteImagePaths":35306,"frontmatter":35307,"imagePaths":35308},[],[],[],{"title":35296,"description":35297,"summary":35297,"pubDate":32084,"source":25191,"url":35299,"thumbnail":25193},[],"2025-06-25-ssps-self-supervised-positive-sampling-for-robust-self-supervised-speaker-verification.md","2025-06-25-statistical-inference-for-optimal-transport-maps-recent-advances-and-perspectives",{"id":35310,"data":35312,"filePath":35317,"digest":35318,"rendered":35319,"legacyId":35326},{"title":35313,"description":35314,"summary":35314,"pubDate":35315,"source":25191,"url":35316,"thumbnail":25193},"Statistical Inference for Optimal Transport Maps: Recent Advances and Perspectives","arXiv:2506.19025v1 Announce Type: cross Abstract: In many applications of optimal transport (OT), the object of primary interest is the optimal transport map. This map rearranges mass from one probability distribution to another in the most efficient way possible by minimizing a specified cost. In this paper we review recent advances in estimating and developing limit theorems for the OT map, using samples from the underlying distributions. We also review parallel lines of work that establish similar results for special cases and variants of the basic OT setup. We conclude with a discussion of key directions for future research with the goal of providing practitioners with reliable inferential tools.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19025","src/content/posts/2025-06-25-statistical-inference-for-optimal-transport-maps-recent-advances-and-perspectives.md","324ecacacf494d32",{"html":25,"metadata":35320},{"headings":35321,"localImagePaths":35322,"remoteImagePaths":35323,"frontmatter":35324,"imagePaths":35325},[],[],[],{"title":35313,"description":35314,"summary":35314,"pubDate":32084,"source":25191,"url":35316,"thumbnail":25193},[],"2025-06-25-statistical-inference-for-optimal-transport-maps-recent-advances-and-perspectives.md","2025-06-25-stylized-structural-patterns-for-improved-neural-network-pre-training",{"id":35327,"data":35329,"filePath":35334,"digest":35335,"rendered":35336,"legacyId":35343},{"title":35330,"description":35331,"summary":35331,"pubDate":35332,"source":25191,"url":35333,"thumbnail":25193},"Stylized Structural Patterns for Improved Neural Network Pre-training","arXiv:2506.19465v1 Announce Type: cross Abstract: Modern deep learning models in computer vision require large datasets of real images, which are difficult to curate and pose privacy and legal concerns, limiting their commercial use. Recent works suggest synthetic data as an alternative, yet models trained with it often underperform. This paper proposes a two-step approach to bridge this gap. First, we propose an improved neural fractal formulation through which we introduce a new class of synthetic data. Second, we propose reverse stylization, a technique that transfers visual features from a small, license-free set of real images onto synthetic datasets, enhancing their effectiveness. We analyze the domain gap between our synthetic datasets and real images using Kernel Inception Distance (KID) and show that our method achieves a significantly lower distributional gap compared to existing synthetic datasets. Furthermore, our experiments across different tasks demonstrate the practical impact of this reduced gap. We show that pretraining the EDM2 diffusion model on our synthetic dataset leads to an 11% reduction in FID during image generation, compared to models trained on existing synthetic datasets, and a 20% decrease in autoencoder reconstruction error, indicating improved performance in data representation. Furthermore, a ViT-S model trained for classification on this synthetic data achieves over a 10% improvement in ImageNet-100 accuracy. Our work opens up exciting possibilities for training practical models when sufficiently large real training sets are not available.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19465","src/content/posts/2025-06-25-stylized-structural-patterns-for-improved-neural-network-pre-training.md","bee3b5dd07801535",{"html":25,"metadata":35337},{"headings":35338,"localImagePaths":35339,"remoteImagePaths":35340,"frontmatter":35341,"imagePaths":35342},[],[],[],{"title":35330,"description":35331,"summary":35331,"pubDate":32084,"source":25191,"url":35333,"thumbnail":25193},[],"2025-06-25-stylized-structural-patterns-for-improved-neural-network-pre-training.md","2025-06-25-sum-of-parts-self-attributing-neural-networks-with-end-to-end-learning-of-feature-groups",{"id":35344,"data":35346,"filePath":35351,"digest":35352,"rendered":35353,"legacyId":35360},{"title":35347,"description":35348,"summary":35348,"pubDate":35349,"source":25191,"url":35350,"thumbnail":25193},"Sum-of-Parts: Self-Attributing Neural Networks with End-to-End Learning of Feature Groups","arXiv:2310.16316v4 Announce Type: replace-cross Abstract: Self-attributing neural networks (SANNs) present a potential path towards interpretable models for high-dimensional problems, but often face significant trade-offs in performance. In this work, we formally prove a lower bound on errors of per-feature SANNs, whereas group-based SANNs can achieve zero error and thus high performance. Motivated by these insights, we propose Sum-of-Parts (SOP), a framework that transforms any differentiable model into a group-based SANN, where feature groups are learned end-to-end without group supervision. SOP achieves state-of-the-art performance for SANNs on vision and language tasks, and we validate that the groups are interpretable on a range of quantitative and semantic metrics. We further validate the utility of SOP explanations in model debugging and cosmological scientific discovery. Our code is available at https://github.com/BrachioLab/sop",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2310.16316","src/content/posts/2025-06-25-sum-of-parts-self-attributing-neural-networks-with-end-to-end-learning-of-feature-groups.md","6ee19667edb858f2",{"html":25,"metadata":35354},{"headings":35355,"localImagePaths":35356,"remoteImagePaths":35357,"frontmatter":35358,"imagePaths":35359},[],[],[],{"title":35347,"description":35348,"summary":35348,"pubDate":32084,"source":25191,"url":35350,"thumbnail":25193},[],"2025-06-25-sum-of-parts-self-attributing-neural-networks-with-end-to-end-learning-of-feature-groups.md","2025-06-25-surgery-r1-advancing-surgical-vqla-with-reasoning-multimodal-large-language-model-via-reinforcement-learning",{"id":35361,"data":35363,"filePath":35368,"digest":35369,"rendered":35370,"legacyId":35377},{"title":35364,"description":35365,"summary":35365,"pubDate":35366,"source":25191,"url":35367,"thumbnail":25193},"Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large Language Model via Reinforcement Learning","arXiv:2506.19469v1 Announce Type: cross Abstract: In recent years, significant progress has been made in the field of surgical scene understanding, particularly in the task of Visual Question Localized-Answering in robotic surgery (Surgical-VQLA). However, existing Surgical-VQLA models lack deep reasoning capabilities and interpretability in surgical scenes, which limits their reliability and potential for development in clinical applications. To address this issue, inspired by the development of Reasoning Multimodal Large Language Models (MLLMs), we first build the Surgery-R1-54k dataset, including paired data for Visual-QA, Grounding-QA, and Chain-of-Thought (CoT). Then, we propose the first Reasoning MLLM for Surgical-VQLA (Surgery-R1). In our Surgery-R1, we design a two-stage fine-tuning mechanism to enable the basic MLLM with complex reasoning abilities by utilizing supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). Furthermore, for an efficient and high-quality rule-based reward system in our RFT, we design a Multimodal Coherence reward mechanism to mitigate positional illusions that may arise in surgical scenarios. Experiment results demonstrate that Surgery-R1 outperforms other existing state-of-the-art (SOTA) models in the Surgical-VQLA task and widely-used MLLMs, while also validating its reasoning capabilities and the effectiveness of our approach. The code and dataset will be organized in https://github.com/FiFi-HAO467/Surgery-R1.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19469","src/content/posts/2025-06-25-surgery-r1-advancing-surgical-vqla-with-reasoning-multimodal-large-language-model-via-reinforcement-learning.md","f7178b8c6af4eae4",{"html":25,"metadata":35371},{"headings":35372,"localImagePaths":35373,"remoteImagePaths":35374,"frontmatter":35375,"imagePaths":35376},[],[],[],{"title":35364,"description":35365,"summary":35365,"pubDate":32084,"source":25191,"url":35367,"thumbnail":25193},[],"2025-06-25-surgery-r1-advancing-surgical-vqla-with-reasoning-multimodal-large-language-model-via-reinforcement-learning.md","2025-06-25-survey-of-hpc-in-us-research-institutions",{"id":35378,"data":35380,"filePath":35385,"digest":35386,"rendered":35387,"legacyId":35394},{"title":35381,"description":35382,"summary":35382,"pubDate":35383,"source":25191,"url":35384,"thumbnail":25193},"Survey of HPC in US Research Institutions","arXiv:2506.19019v1 Announce Type: cross Abstract: The rapid growth of AI, data-intensive science, and digital twin technologies has driven an unprecedented demand for high-performance computing (HPC) across the research ecosystem. While national laboratories and industrial hyperscalers have invested heavily in exascale and GPU-centric architectures, university-operated HPC systems remain comparatively under-resourced. This survey presents a comprehensive assessment of the HPC landscape across U.S. universities, benchmarking their capabilities against Department of Energy (DOE) leadership-class systems and industrial AI infrastructures. We examine over 50 premier research institutions, analyzing compute capacity, architectural design, governance models, and energy efficiency. Our findings reveal that university clusters, though vital for academic research, exhibit significantly lower growth trajectories (CAGR $approx$ 18%) than their national ($approx$ 43%) and industrial ($approx$ 78%) counterparts. The increasing skew toward GPU-dense AI workloads has widened the capability gap, highlighting the need for federated computing, idle-GPU harvesting, and cost-sharing models. We also identify emerging paradigms, such as decentralized reinforcement learning, as promising opportunities for democratizing AI training within campus environments. Ultimately, this work provides actionable insights for academic leaders, funding agencies, and technology partners to ensure more equitable and sustainable HPC access in support of national research priorities.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19019","src/content/posts/2025-06-25-survey-of-hpc-in-us-research-institutions.md","ef54dace4fcd2d43",{"html":25,"metadata":35388},{"headings":35389,"localImagePaths":35390,"remoteImagePaths":35391,"frontmatter":35392,"imagePaths":35393},[],[],[],{"title":35381,"description":35382,"summary":35382,"pubDate":32084,"source":25191,"url":35384,"thumbnail":25193},[],"2025-06-25-survey-of-hpc-in-us-research-institutions.md","2025-06-25-swe-sql-illuminating-llm-pathways-to-solve-user-sql-issues-in-real-world-applications",{"id":35395,"data":35397,"filePath":35402,"digest":35403,"rendered":35404,"legacyId":35411},{"title":35398,"description":35399,"summary":35399,"pubDate":35400,"source":25191,"url":35401,"thumbnail":25193},"SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications","arXiv:2506.18951v1 Announce Type: cross Abstract: Resolution of complex SQL issues persists as a significant bottleneck in real-world database applications. Current Large Language Models (LLMs), while adept at text-to-SQL translation, have not been rigorously evaluated on the more challenging task of debugging SQL issues. To address this gap, we introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530 PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks (BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within new environments to facilitate rigorous evaluation. Baseline evaluations underscore the task's complexity, with the leading reasoning model O3-Mini achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks is crucial for empowering local development while safeguarding data privacy. Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for elevating open-source model capabilities for SQL issue debugging. This environment leverages SQL-Rewind strategy, which automatically generates executable issue-solution datasets by reverse-engineering issues from verified SQLs. However, popular trajectory-based fine-tuning methods do not explore substantial supervisory signals. We further propose f-Plan Boosting, which extracts high-level debugging plans from SQL solutions, enabling teacher LLMs to produce 73.7% more successful trajectories for training. We integrate these components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B, Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on BIRD-CRITIC-Multi, surpassing leading proprietary models such as Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing sophisticated SQL-debugging capabilities. The leaderboard and source code are available: https://bird-critic.github.io/",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18951","src/content/posts/2025-06-25-swe-sql-illuminating-llm-pathways-to-solve-user-sql-issues-in-real-world-applications.md","d85a36741f23f506",{"html":25,"metadata":35405},{"headings":35406,"localImagePaths":35407,"remoteImagePaths":35408,"frontmatter":35409,"imagePaths":35410},[],[],[],{"title":35398,"description":35399,"summary":35399,"pubDate":32084,"source":25191,"url":35401,"thumbnail":25193},[],"2025-06-25-swe-sql-illuminating-llm-pathways-to-solve-user-sql-issues-in-real-world-applications.md","2025-06-25-sycnmapv2-robust-and-adaptive-unsupervised-segmentation",{"id":35412,"data":35414,"filePath":35417,"digest":35418,"rendered":35419,"legacyId":35426},{"title":30937,"description":35415,"summary":35415,"pubDate":35416,"source":25191,"url":30940,"thumbnail":25193},"arXiv:2506.16297v2 Announce Type: replace-cross Abstract: Human vision excels at segmenting visual cues without the need for explicit training, and it remains remarkably robust even as noise severity increases. In contrast, existing AI algorithms struggle to maintain accuracy under similar conditions. Here, we present SyncMapV2, the first to solve unsupervised segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop observed in SOTA methods. This superior performance extends across various types of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training, supervision, or loss functions. It is based on a learning paradigm that uses self-organizing dynamical equations combined with concepts from random networks. Moreover, unlike conventional methods that require re-initialization for each new input, SyncMapV2 adapts online, mimicking the continuous adaptability of human vision. Thus, we go beyond the accurate and robust results, and present the first algorithm that can do all the above online, adapting to input rather than re-initializing. In adaptability tests, SyncMapV2 demonstrates near-zero performance degradation, which motivates and fosters a new generation of robust and adaptive intelligence in the near future.",["Date","2025-06-25T04:00:00.000Z"],"src/content/posts/2025-06-25-sycnmapv2-robust-and-adaptive-unsupervised-segmentation.md","a24ab44362d99499",{"html":25,"metadata":35420},{"headings":35421,"localImagePaths":35422,"remoteImagePaths":35423,"frontmatter":35424,"imagePaths":35425},[],[],[],{"title":30937,"description":35415,"summary":35415,"pubDate":32084,"source":25191,"url":30940,"thumbnail":25193},[],"2025-06-25-sycnmapv2-robust-and-adaptive-unsupervised-segmentation.md","2025-06-25-tagged-for-direction-pinning-down-causal-edge-directions-with-precision",{"id":35427,"data":35429,"filePath":35434,"digest":35435,"rendered":35436,"legacyId":35443},{"title":35430,"description":35431,"summary":35431,"pubDate":35432,"source":25191,"url":35433,"thumbnail":25193},"Tagged for Direction: Pinning Down Causal Edge Directions with Precision","arXiv:2506.19459v1 Announce Type: cross Abstract: Not every causal relation between variables is equal, and this can be leveraged for the task of causal discovery. Recent research shows that pairs of variables with particular type assignments induce a preference on the causal direction of other pairs of variables with the same type. Although useful, this assignment of a specific type to a variable can be tricky in practice. We propose a tag-based causal discovery approach where multiple tags are assigned to each variable in a causal graph. Existing causal discovery approaches are first applied to direct some edges, which are then used to determine edge relations between tags. Then, these edge relations are used to direct the undirected edges. Doing so improves upon purely type-based relations, where the assumption of type consistency lacks robustness and flexibility due to being restricted to single types for each variable. Our experimental evaluations show that this boosts causal discovery and that these high-level tag relations fit common knowledge.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19459","src/content/posts/2025-06-25-tagged-for-direction-pinning-down-causal-edge-directions-with-precision.md","d2fffb1dadd37c0b",{"html":25,"metadata":35437},{"headings":35438,"localImagePaths":35439,"remoteImagePaths":35440,"frontmatter":35441,"imagePaths":35442},[],[],[],{"title":35430,"description":35431,"summary":35431,"pubDate":32084,"source":25191,"url":35433,"thumbnail":25193},[],"2025-06-25-tagged-for-direction-pinning-down-causal-edge-directions-with-precision.md","2025-06-25-tailored-conversations-beyond-llms-a-rl-based-dialogue-manager",{"id":35444,"data":35446,"filePath":35451,"digest":35452,"rendered":35453,"legacyId":35460},{"title":35447,"description":35448,"summary":35448,"pubDate":35449,"source":25191,"url":35450,"thumbnail":25193},"Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager","arXiv:2506.19652v1 Announce Type: cross Abstract: In this work, we propose a novel framework that integrates large language models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a specific goal. By leveraging hierarchical reinforcement learning to model the structured phases of dialogue and employ meta-learning to enhance adaptability across diverse user profiles, our approach enhances adaptability and efficiency, enabling the system to learn from limited data, transition fluidly between dialogue phases, and personalize responses to heterogeneous patient needs. We apply our framework to Motivational Interviews, aiming to foster behavior change, and demonstrate that the proposed dialogue manager outperforms a state-of-the-art LLM baseline in terms of reward, showing a potential benefit of conditioning LLMs to create open-ended dialogue systems with specific goals.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19652","src/content/posts/2025-06-25-tailored-conversations-beyond-llms-a-rl-based-dialogue-manager.md","08b76b2fdc01f113",{"html":25,"metadata":35454},{"headings":35455,"localImagePaths":35456,"remoteImagePaths":35457,"frontmatter":35458,"imagePaths":35459},[],[],[],{"title":35447,"description":35448,"summary":35448,"pubDate":32084,"source":25191,"url":35450,"thumbnail":25193},[],"2025-06-25-tailored-conversations-beyond-llms-a-rl-based-dialogue-manager.md","2025-06-25-temporal-irl-modeling-port-congestion-and-berth-scheduling-with-inverse-reinforcement-learning",{"id":35461,"data":35463,"filePath":35468,"digest":35469,"rendered":35470,"legacyId":35477},{"title":35464,"description":35465,"summary":35465,"pubDate":35466,"source":25191,"url":35467,"thumbnail":25193},"Temporal-IRL: Modeling Port Congestion and Berth Scheduling with Inverse Reinforcement Learning","arXiv:2506.19843v1 Announce Type: new Abstract: Predicting port congestion is crucial for maintaining reliable global supply chains. Accurate forecasts enableimprovedshipment planning, reducedelaysand costs, and optimizeinventoryanddistributionstrategies, thereby ensuring timely deliveries and enhancing supply chain resilience. To achieve accurate predictions, analyzing vessel behavior and their stay times at specific port terminals is essential, focusing particularly on berth scheduling under various conditions. Crucially, the model must capture and learn the underlying priorities and patterns of berth scheduling. Berth scheduling and planning are influenced by a range of factors, including incoming vessel size, waiting times, and the status of vessels within the port terminal. By observing historical Automatic Identification System (AIS) positions of vessels, we reconstruct berth schedules, which are subsequently utilized to determine the reward function via Inverse Reinforcement Learning (IRL). For this purpose, we modeled a specific terminal at the Port of New York/New Jersey and developed Temporal-IRL. This Temporal-IRL model learns berth scheduling to predict vessel sequencing at the terminal and estimate vessel port stay, encompassing both waiting and berthing times, to forecast port congestion. Utilizing data from Maher Terminal spanning January 2015 to September 2023, we trained and tested the model, achieving demonstrably excellent results.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19843","src/content/posts/2025-06-25-temporal-irl-modeling-port-congestion-and-berth-scheduling-with-inverse-reinforcement-learning.md","f7f40743caf91435",{"html":25,"metadata":35471},{"headings":35472,"localImagePaths":35473,"remoteImagePaths":35474,"frontmatter":35475,"imagePaths":35476},[],[],[],{"title":35464,"description":35465,"summary":35465,"pubDate":32084,"source":25191,"url":35467,"thumbnail":25193},[],"2025-06-25-temporal-irl-modeling-port-congestion-and-berth-scheduling-with-inverse-reinforcement-learning.md","2025-06-25-tevir-text-to-video-reward-with-diffusion-models-for-efficient-reinforcement-learning",{"id":35478,"data":35480,"filePath":35485,"digest":35486,"rendered":35487,"legacyId":35494},{"title":35481,"description":35482,"summary":35482,"pubDate":35483,"source":25191,"url":35484,"thumbnail":25193},"TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning","arXiv:2505.19769v2 Announce Type: replace-cross Abstract: Developing scalable and generalizable reward engineering for reinforcement learning (RL) is crucial for creating general-purpose agents, especially in the challenging domain of robotic manipulation. While recent advances in reward engineering with Vision-Language Models (VLMs) have shown promise, their sparse reward nature significantly limits sample efficiency. This paper introduces TeViR, a novel method that leverages a pre-trained text-to-video diffusion model to generate dense rewards by comparing the predicted image sequence with current observations. Experimental results across 11 complex robotic tasks demonstrate that TeViR outperforms traditional methods leveraging sparse rewards and other state-of-the-art (SOTA) methods, achieving better sample efficiency and performance without ground truth environmental rewards. TeViR's ability to efficiently guide agents in complex environments highlights its potential to advance reinforcement learning applications in robotic manipulation.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2505.19769","src/content/posts/2025-06-25-tevir-text-to-video-reward-with-diffusion-models-for-efficient-reinforcement-learning.md","278e2e50fa1e22b0",{"html":25,"metadata":35488},{"headings":35489,"localImagePaths":35490,"remoteImagePaths":35491,"frontmatter":35492,"imagePaths":35493},[],[],[],{"title":35481,"description":35482,"summary":35482,"pubDate":32084,"source":25191,"url":35484,"thumbnail":25193},[],"2025-06-25-tevir-text-to-video-reward-with-diffusion-models-for-efficient-reinforcement-learning.md","2025-06-25-the-elements-of-differentiable-programming",{"id":35495,"data":35497,"filePath":35502,"digest":35503,"rendered":35504,"legacyId":35511},{"title":35498,"description":35499,"summary":35499,"pubDate":35500,"source":25191,"url":35501,"thumbnail":25193},"The Elements of Differentiable Programming","arXiv:2403.14606v3 Announce Type: replace-cross Abstract: Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the thoughtful design of programs intended for differentiation. By making programs differentiable, we inherently introduce probability distributions over their execution, providing a means to quantify the uncertainty associated with program outputs.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2403.14606","src/content/posts/2025-06-25-the-elements-of-differentiable-programming.md","dbb958852c057c48",{"html":25,"metadata":35505},{"headings":35506,"localImagePaths":35507,"remoteImagePaths":35508,"frontmatter":35509,"imagePaths":35510},[],[],[],{"title":35498,"description":35499,"summary":35499,"pubDate":32084,"source":25191,"url":35501,"thumbnail":25193},[],"2025-06-25-the-elements-of-differentiable-programming.md","2025-06-25-the-receptron-is-a-nonlinear-threshold-logic-gate-with-intrinsic-multi-dimensional-selective-capabilities-for-analog-inputs",{"id":35512,"data":35514,"filePath":35519,"digest":35520,"rendered":35521,"legacyId":35528},{"title":35515,"description":35516,"summary":35516,"pubDate":35517,"source":25191,"url":35518,"thumbnail":25193},"The receptron is a nonlinear threshold logic gate with intrinsic multi-dimensional selective capabilities for analog inputs","arXiv:2506.19642v1 Announce Type: cross Abstract: Threshold logic gates (TLGs) have been proposed as artificial counterparts of biological neurons with classification capabilities based on a linear predictor function combining a set of weights with the feature vector. The linearity of TLGs limits their classification capabilities requiring the use of networks for the accomplishment of complex tasks. A generalization of the TLG model called receptron, characterized by input-dependent weight functions allows for a significant enhancement of classification performances even with the use of a single unit. Here we formally demonstrate that a receptron, characterized by nonlinear input-dependent weight functions, exhibit intrinsic selective activation properties for analog inputs, when the input vector is within cubic domains in a 3D space. The proposed model can be extended to the n-dimensional case for multidimensional applications. Our results suggest that receptron-based networks can represent a new class of devices capable to manage a large number of analog inputs, for edge applications requiring high selectivity and classification capabilities without the burden of complex training.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19642","src/content/posts/2025-06-25-the-receptron-is-a-nonlinear-threshold-logic-gate-with-intrinsic-multi-dimensional-selective-capabilities-for-analog-inputs.md","bf1efdc79735cac9",{"html":25,"metadata":35522},{"headings":35523,"localImagePaths":35524,"remoteImagePaths":35525,"frontmatter":35526,"imagePaths":35527},[],[],[],{"title":35515,"description":35516,"summary":35516,"pubDate":32084,"source":25191,"url":35518,"thumbnail":25193},[],"2025-06-25-the-receptron-is-a-nonlinear-threshold-logic-gate-with-intrinsic-multi-dimensional-selective-capabilities-for-analog-inputs.md","2025-06-25-thought-anchors-which-llm-reasoning-steps-matter",{"id":35529,"data":35531,"filePath":35536,"digest":35537,"rendered":35538,"legacyId":35545},{"title":35532,"description":35533,"summary":35533,"pubDate":35534,"source":25191,"url":35535,"thumbnail":25193},"Thought Anchors: Which LLM Reasoning Steps Matter?","arXiv:2506.19143v1 Announce Type: cross Abstract: Reasoning large language models have recently achieved state-of-the-art performance in many fields. However, their long-form chain-of-thought reasoning creates interpretability challenges as each generated token depends on all previous ones, making the computation harder to decompose. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We present three complementary attribution methods: (1) a black-box method measuring each sentence's counterfactual importance by comparing final answers across 100 rollouts conditioned on the model generating that sentence or one with a different meaning; (2) a white-box method of aggregating attention patterns between pairs of sentences, which identified ``broadcasting'' sentences that receive disproportionate attention from all future sentences via ``receiver'' attention heads; (3) a causal attribution method measuring logical connections between sentences by suppressing attention toward one sentence and measuring the effect on each future sentence's tokens. Each method provides evidence for the existence of thought anchors, reasoning steps that have outsized importance and that disproportionately influence the subsequent reasoning process. These thought anchors are typically planning or backtracking sentences. We provide an open-source tool (www.thought-anchors.com) for visualizing the outputs of our methods, and present a case study showing converging patterns across methods that map how a model performs multi-step reasoning. The consistency across methods demonstrates the potential of sentence-level analysis for a deeper understanding of reasoning models.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19143","src/content/posts/2025-06-25-thought-anchors-which-llm-reasoning-steps-matter.md","265fca605e016820",{"html":25,"metadata":35539},{"headings":35540,"localImagePaths":35541,"remoteImagePaths":35542,"frontmatter":35543,"imagePaths":35544},[],[],[],{"title":35532,"description":35533,"summary":35533,"pubDate":32084,"source":25191,"url":35535,"thumbnail":25193},[],"2025-06-25-thought-anchors-which-llm-reasoning-steps-matter.md","2025-06-25-time-imm-a-dataset-and-benchmark-for-irregular-multimodal-multivariate-time-series",{"id":35546,"data":35548,"filePath":35553,"digest":35554,"rendered":35555,"legacyId":35562},{"title":35549,"description":35550,"summary":35550,"pubDate":35551,"source":25191,"url":35552,"thumbnail":25193},"Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series","arXiv:2506.10412v2 Announce Type: replace-cross Abstract: Time series data in real-world applications such as healthcare, climate modeling, and finance are often irregular, multimodal, and messy, with varying sampling rates, asynchronous modalities, and pervasive missingness. However, existing benchmarks typically assume clean, regularly sampled, unimodal data, creating a significant gap between research and real-world deployment. We introduce Time-IMM, a dataset specifically designed to capture cause-driven irregularity in multimodal multivariate time series. Time-IMM represents nine distinct types of time series irregularity, categorized into trigger-based, constraint-based, and artifact-based mechanisms. Complementing the dataset, we introduce IMM-TSF, a benchmark library for forecasting on irregular multimodal time series, enabling asynchronous integration and realistic evaluation. IMM-TSF includes specialized fusion modules, including a timestamp-to-text fusion module and a multimodality fusion module, which support both recency-aware averaging and attention-based integration strategies. Empirical results demonstrate that explicitly modeling multimodality on irregular time series data leads to substantial gains in forecasting performance. Time-IMM and IMM-TSF provide a foundation for advancing time series analysis under real-world conditions. The dataset is publicly available at https://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the benchmark library can be accessed at https://anonymous.4open.science/r/IMMTSF_NeurIPS2025.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.10412","src/content/posts/2025-06-25-time-imm-a-dataset-and-benchmark-for-irregular-multimodal-multivariate-time-series.md","df5067fb757e09f2",{"html":25,"metadata":35556},{"headings":35557,"localImagePaths":35558,"remoteImagePaths":35559,"frontmatter":35560,"imagePaths":35561},[],[],[],{"title":35549,"description":35550,"summary":35550,"pubDate":32084,"source":25191,"url":35552,"thumbnail":25193},[],"2025-06-25-time-imm-a-dataset-and-benchmark-for-irregular-multimodal-multivariate-time-series.md","2025-06-25-toward-decision-oriented-prognostics-an-integrated-estimate-optimize-framework-for-predictive-maintenance",{"id":35563,"data":35565,"filePath":35570,"digest":35571,"rendered":35572,"legacyId":35579},{"title":35566,"description":35567,"summary":35567,"pubDate":35568,"source":25191,"url":35569,"thumbnail":25193},"Toward Decision-Oriented Prognostics: An Integrated Estimate-Optimize Framework for Predictive Maintenance","arXiv:2506.19698v1 Announce Type: new Abstract: Recent research increasingly integrates machine learning (ML) into predictive maintenance (PdM) to reduce operational and maintenance costs in data-rich operational settings. However, uncertainty due to model misspecification continues to limit widespread industrial adoption. This paper proposes a PdM framework in which sensor-driven prognostics inform decision-making under economic trade-offs within a finite decision space. We investigate two key questions: (1) Does higher predictive accuracy necessarily lead to better maintenance decisions? (2) If not, how can the impact of prediction errors on downstream maintenance decisions be mitigated? We first demonstrate that in the traditional estimate-then-optimize (ETO) framework, errors in probabilistic prediction can result in inconsistent and suboptimal maintenance decisions. To address this, we propose an integrated estimate-optimize (IEO) framework that jointly tunes predictive models while directly optimizing for maintenance outcomes. We establish theoretical finite-sample guarantees on decision consistency under standard assumptions. Specifically, we develop a stochastic perturbation gradient descent algorithm suitable for small run-to-failure datasets. Empirical evaluations on a turbofan maintenance case study show that the IEO framework reduces average maintenance regret up to 22% compared to ETO. This study provides a principled approach to managing prediction errors in data-driven PdM. By aligning prognostic model training with maintenance objectives, the IEO framework improves robustness under model misspecification and improves decision quality. The improvement is particularly pronounced when the decision-making policy is misaligned with the decision-maker's target. These findings support more reliable maintenance planning in uncertain operational environments.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19698","src/content/posts/2025-06-25-toward-decision-oriented-prognostics-an-integrated-estimate-optimize-framework-for-predictive-maintenance.md","7b1fb54f6565b213",{"html":25,"metadata":35573},{"headings":35574,"localImagePaths":35575,"remoteImagePaths":35576,"frontmatter":35577,"imagePaths":35578},[],[],[],{"title":35566,"description":35567,"summary":35567,"pubDate":32084,"source":25191,"url":35569,"thumbnail":25193},[],"2025-06-25-toward-decision-oriented-prognostics-an-integrated-estimate-optimize-framework-for-predictive-maintenance.md","2025-06-25-towards-an-introspective-dynamic-model-of-globally-distributed-computing-infrastructures",{"id":35580,"data":35582,"filePath":35587,"digest":35588,"rendered":35589,"legacyId":35596},{"title":35583,"description":35584,"summary":35584,"pubDate":35585,"source":25191,"url":35586,"thumbnail":25193},"Towards an Introspective Dynamic Model of Globally Distributed Computing Infrastructures","arXiv:2506.19578v1 Announce Type: cross Abstract: Large-scale scientific collaborations like ATLAS, Belle II, CMS, DUNE, and others involve hundreds of research institutes and thousands of researchers spread across the globe. These experiments generate petabytes of data, with volumes soon expected to reach exabytes. Consequently, there is a growing need for computation, including structured data processing from raw data to consumer-ready derived data, extensive Monte Carlo simulation campaigns, and a wide range of end-user analysis. To manage these computational and storage demands, centralized workflow and data management systems are implemented. However, decisions regarding data placement and payload allocation are often made disjointly and via heuristic means. A significant obstacle in adopting more effective heuristic or AI-driven solutions is the absence of a quick and reliable introspective dynamic model to evaluate and refine alternative approaches. In this study, we aim to develop such an interactive system using real-world data. By examining job execution records from the PanDA workflow management system, we have pinpointed key performance indicators such as queuing time, error rate, and the extent of remote data access. The dataset includes five months of activity. Additionally, we are creating a generative AI model to simulate time series of payloads, which incorporate visible features like category, event count, and submitting group, as well as hidden features like the total computational load-derived from existing PanDA records and computing site capabilities. These hidden features, which are not visible to job allocators, whether heuristic or AI-driven, influence factors such as queuing times and data movement.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19578","src/content/posts/2025-06-25-towards-an-introspective-dynamic-model-of-globally-distributed-computing-infrastructures.md","9959d529b36d0a4f",{"html":25,"metadata":35590},{"headings":35591,"localImagePaths":35592,"remoteImagePaths":35593,"frontmatter":35594,"imagePaths":35595},[],[],[],{"title":35583,"description":35584,"summary":35584,"pubDate":32084,"source":25191,"url":35586,"thumbnail":25193},[],"2025-06-25-towards-an-introspective-dynamic-model-of-globally-distributed-computing-infrastructures.md","2025-06-25-towards-robust-stability-prediction-in-smart-grids-gan-based-approach-under-data-constraints-and-adversarial-challenges",{"id":35597,"data":35599,"filePath":35604,"digest":35605,"rendered":35606,"legacyId":35613},{"title":35600,"description":35601,"summary":35601,"pubDate":35602,"source":25191,"url":35603,"thumbnail":25193},"Towards Robust Stability Prediction in Smart Grids: GAN-based Approach under Data Constraints and Adversarial Challenges","arXiv:2501.16490v2 Announce Type: replace-cross Abstract: Smart grids are crucial for meeting rising energy demands driven by global population growth and urbanization. By integrating renewable energy sources, they enhance efficiency, reliability, and sustainability. However, ensuring their availability and security requires advanced operational control and safety measures. Although artificial intelligence and machine learning can help assess grid stability, challenges such as data scarcity and cybersecurity threats, particularly adversarial attacks, remain. Data scarcity is a major issue, as obtaining real-world instances of grid instability requires significant expertise, resources, and time. Yet, these instances are critical for testing new research advancements and security mitigations. This paper introduces a novel framework for detecting instability in smart grids using only stable data. It employs a Generative Adversarial Network (GAN) where the generator is designed not to produce near-realistic data but instead to generate Out-Of-Distribution (OOD) samples with respect to the stable class. These OOD samples represent unstable behavior, anomalies, or disturbances that deviate from the stable data distribution. By training exclusively on stable data and exposing the discriminator to OOD samples, our framework learns a robust decision boundary to distinguish stable conditions from any unstable behavior, without requiring unstable data during training. Furthermore, we incorporate an adversarial training layer to enhance resilience against attacks. Evaluated on a real-world dataset, our solution achieves up to 98.1% accuracy in predicting grid stability and 98.9% in detecting adversarial attacks. Implemented on a single-board computer, it enables real-time decision-making with an average response time of under 7ms.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2501.16490","src/content/posts/2025-06-25-towards-robust-stability-prediction-in-smart-grids-gan-based-approach-under-data-constraints-and-adversarial-challenges.md","924c9c0b3a2b65ac",{"html":25,"metadata":35607},{"headings":35608,"localImagePaths":35609,"remoteImagePaths":35610,"frontmatter":35611,"imagePaths":35612},[],[],[],{"title":35600,"description":35601,"summary":35601,"pubDate":32084,"source":25191,"url":35603,"thumbnail":25193},[],"2025-06-25-towards-robust-stability-prediction-in-smart-grids-gan-based-approach-under-data-constraints-and-adversarial-challenges.md","2025-06-25-towards-unsupervised-multi-agent-reinforcement-learning-via-task-agnostic-exploration",{"id":35614,"data":35616,"filePath":35621,"digest":35622,"rendered":35623,"legacyId":35630},{"title":35617,"description":35618,"summary":35618,"pubDate":35619,"source":25191,"url":35620,"thumbnail":25193},"Towards Unsupervised Multi-Agent Reinforcement Learning via Task-Agnostic Exploration","arXiv:2502.08365v3 Announce Type: replace-cross Abstract: In reinforcement learning, we typically refer to unsupervised pre-training when we aim to pre-train a policy without a priori access to the task specification, i.e. rewards, to be later employed for efficient learning of downstream tasks. In single-agent settings, the problem has been extensively studied and mostly understood. A popular approach, called task-agnostic exploration, casts the unsupervised objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follow. In contrast, little is known about it in multi-agent settings, which are ubiquitous in the real world. What are the pros and cons of alternative problem formulations in this setting? How hard is the problem in theory, how can we solve it in practice? In this paper, we address these questions by first characterizing those alternative formulations and highlighting how the problem, even when tractable in theory, is non-trivial in practice. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide numerical validations to both corroborate the theoretical findings and pave the way for unsupervised multi-agent reinforcement learning via task-agnostic exploration in challenging domains, showing that optimizing for a specific objective, namely mixture entropy, provides an excellent trade-off between tractability and performances.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2502.08365","src/content/posts/2025-06-25-towards-unsupervised-multi-agent-reinforcement-learning-via-task-agnostic-exploration.md","34e3d02587e7bba4",{"html":25,"metadata":35624},{"headings":35625,"localImagePaths":35626,"remoteImagePaths":35627,"frontmatter":35628,"imagePaths":35629},[],[],[],{"title":35617,"description":35618,"summary":35618,"pubDate":32084,"source":25191,"url":35620,"thumbnail":25193},[],"2025-06-25-towards-unsupervised-multi-agent-reinforcement-learning-via-task-agnostic-exploration.md","2025-06-25-trail-trace-reasoning-and-agentic-issue-localization",{"id":35631,"data":35633,"filePath":35638,"digest":35639,"rendered":35640,"legacyId":35647},{"title":35634,"description":35635,"summary":35635,"pubDate":35636,"source":25191,"url":35637,"thumbnail":25193},"TRAIL: Trace Reasoning and Agentic Issue Localization","arXiv:2505.08638v3 Announce Type: replace Abstract: The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2505.08638","src/content/posts/2025-06-25-trail-trace-reasoning-and-agentic-issue-localization.md","3acf0bac4ff149ef",{"html":25,"metadata":35641},{"headings":35642,"localImagePaths":35643,"remoteImagePaths":35644,"frontmatter":35645,"imagePaths":35646},[],[],[],{"title":35634,"description":35635,"summary":35635,"pubDate":32084,"source":25191,"url":35637,"thumbnail":25193},[],"2025-06-25-trail-trace-reasoning-and-agentic-issue-localization.md","2025-06-25-trainverify-equivalence-based-verification-for-distributed-llm-training",{"id":35648,"data":35650,"filePath":35653,"digest":35654,"rendered":35655,"legacyId":35662},{"title":31328,"description":35651,"summary":35651,"pubDate":35652,"source":25191,"url":31331,"thumbnail":25193},"arXiv:2506.15961v2 Announce Type: replace-cross Abstract: Training large language models (LLMs) at scale requires parallel execution across thousands of devices, incurring enormous computational costs. Yet, these costly distributed trainings are rarely verified, leaving them prone to silent errors and potentially wasting millions of GPU hours. We introduce TrainVerify, a system for verifiable distributed training of LLMs. Given a deep learning model's logical specification as the ground truth, TrainVerify formally verifies that a distributed parallel execution plan is mathematically equivalent to it. Direct verification is notoriously difficult due to the sheer scale of LLMs which often involves billions of variables and highly intricate computation graphs. Therefore, TrainVerify introduces shape-reduction techniques and a stage-wise parallel verification algorithm that significantly reduces complexity while preserving formal correctness. TrainVerify scales to frontier LLMs, including the successful verification of the Llama3 (405B) and DeepSeek-V3 (671B) training plans.",["Date","2025-06-25T04:00:00.000Z"],"src/content/posts/2025-06-25-trainverify-equivalence-based-verification-for-distributed-llm-training.md","a239f30c775b0e43",{"html":25,"metadata":35656},{"headings":35657,"localImagePaths":35658,"remoteImagePaths":35659,"frontmatter":35660,"imagePaths":35661},[],[],[],{"title":31328,"description":35651,"summary":35651,"pubDate":32084,"source":25191,"url":31331,"thumbnail":25193},[],"2025-06-25-trainverify-equivalence-based-verification-for-distributed-llm-training.md","2025-06-25-uncovering-conceptual-blindspots-in-generative-image-models-using-sparse-autoencoders",{"id":35663,"data":35665,"filePath":35670,"digest":35671,"rendered":35672,"legacyId":35679},{"title":35666,"description":35667,"summary":35667,"pubDate":35668,"source":25191,"url":35669,"thumbnail":25193},"Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders","arXiv:2506.19708v1 Announce Type: cross Abstract: Despite their impressive performance, generative image models trained on large-scale datasets frequently fail to produce images with seemingly simple concepts -- e.g., human hands or objects appearing in groups of four -- that are reasonably expected to appear in the training data. These failure modes have largely been documented anecdotally, leaving open the question of whether they reflect idiosyncratic anomalies or more structural limitations of these models. To address this, we introduce a systematic approach for identifying and characterizing 'conceptual blindspots' -- concepts present in the training data but absent or misrepresented in a model's generations. Our method leverages sparse autoencoders (SAEs) to extract interpretable concept embeddings, enabling a quantitative comparison of concept prevalence between real and generated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with 32,000 concepts -- the largest such SAE to date -- enabling fine-grained analysis of conceptual disparities. Applied to four popular generative models (Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals specific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces on documents) and exaggerated blindspots (e.g., wood background texture and palm trees). At the individual datapoint level, we further isolate memorization artifacts -- instances where models reproduce highly specific visual templates seen during training. Overall, we propose a theoretically grounded framework for systematically identifying conceptual blindspots in generative models by assessing their conceptual fidelity with respect to the underlying data-generating process.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19708","src/content/posts/2025-06-25-uncovering-conceptual-blindspots-in-generative-image-models-using-sparse-autoencoders.md","3b187b1525ab4190",{"html":25,"metadata":35673},{"headings":35674,"localImagePaths":35675,"remoteImagePaths":35676,"frontmatter":35677,"imagePaths":35678},[],[],[],{"title":35666,"description":35667,"summary":35667,"pubDate":32084,"source":25191,"url":35669,"thumbnail":25193},[],"2025-06-25-uncovering-conceptual-blindspots-in-generative-image-models-using-sparse-autoencoders.md","2025-06-25-understanding-human-ai-trust-in-education",{"id":35680,"data":35682,"filePath":35687,"digest":35688,"rendered":35689,"legacyId":35696},{"title":35683,"description":35684,"summary":35684,"pubDate":35685,"source":25191,"url":35686,"thumbnail":25193},"Understanding Human-AI Trust in Education","arXiv:2506.09160v3 Announce Type: replace-cross Abstract: As AI chatbots become increasingly integrated in education, students are turning to these systems for guidance, feedback, and information. However, the anthropomorphic characteristics of these chatbots create ambiguity regarding whether students develop trust toward them as they would a human peer or instructor, based in interpersonal trust, or as they would any other piece of technology, based in technology trust. This ambiguity presents theoretical challenges, as interpersonal trust models may inappropriately ascribe human intentionality and morality to AI, while technology trust models were developed for non-social technologies, leaving their applicability to anthropomorphic systems unclear. To address this gap, we investigate how human-like and system-like trusting beliefs comparatively influence students' perceived enjoyment, trusting intention, behavioral intention to use, and perceived usefulness of an AI chatbot - factors associated with students' engagement and learning outcomes. Through partial least squares structural equation modeling, we found that human-like and system-like trust significantly influenced student perceptions, with varied effects. Human-like trust more strongly predicted trusting intention, while system-like trust better predicted behavioral intention and perceived usefulness. Both had similar effects on perceived enjoyment. Given the partial explanatory power of each type of trust, we propose that students develop a distinct form of trust with AI chatbots (human-AI trust) that differs from human-human and human-technology models of trust. Our findings highlight the need for new theoretical frameworks specific to human-AI trust and offer practical insights for fostering appropriately calibrated trust, which is critical for the effective adoption and pedagogical impact of AI in education.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.09160","src/content/posts/2025-06-25-understanding-human-ai-trust-in-education.md","9389156bca71b6fe",{"html":25,"metadata":35690},{"headings":35691,"localImagePaths":35692,"remoteImagePaths":35693,"frontmatter":35694,"imagePaths":35695},[],[],[],{"title":35683,"description":35684,"summary":35684,"pubDate":32084,"source":25191,"url":35686,"thumbnail":25193},[],"2025-06-25-understanding-human-ai-trust-in-education.md","2025-06-25-understanding-reasoning-in-thinking-language-models-via-steering-vectors",{"id":35697,"data":35699,"filePath":35704,"digest":35705,"rendered":35706,"legacyId":35713},{"title":35700,"description":35701,"summary":35701,"pubDate":35702,"source":25191,"url":35703,"thumbnail":25193},"Understanding Reasoning in Thinking Language Models via Steering Vectors","arXiv:2506.18167v2 Announce Type: replace-cross Abstract: Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressing uncertainty, generating examples for hypothesis validation, and backtracking in reasoning chains. We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our approach offers practical tools for steering reasoning processes in thinking models in a controlled and interpretable manner. We validate our steering method using three DeepSeek-R1-Distill models, demonstrating consistent control across different model architectures.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18167","src/content/posts/2025-06-25-understanding-reasoning-in-thinking-language-models-via-steering-vectors.md","b9154ebecdb1522a",{"html":25,"metadata":35707},{"headings":35708,"localImagePaths":35709,"remoteImagePaths":35710,"frontmatter":35711,"imagePaths":35712},[],[],[],{"title":35700,"description":35701,"summary":35701,"pubDate":32084,"source":25191,"url":35703,"thumbnail":25193},[],"2025-06-25-understanding-reasoning-in-thinking-language-models-via-steering-vectors.md","2025-06-25-unified-neural-backdoor-removal-with-only-few-clean-samples-through-unlearning-and-relearning",{"id":35714,"data":35716,"filePath":35721,"digest":35722,"rendered":35723,"legacyId":35730},{"title":35717,"description":35718,"summary":35718,"pubDate":35719,"source":25191,"url":35720,"thumbnail":25193},"Unified Neural Backdoor Removal with Only Few Clean Samples through Unlearning and Relearning","arXiv:2405.14781v2 Announce Type: replace-cross Abstract: Deep neural networks have achieved remarkable success across various applications; however, their vulnerability to backdoor attacks poses severe security risks -- especially in situations where only a limited set of clean samples is available for defense. In this work, we address this critical challenge by proposing ULRL (UnLearn and ReLearn for backdoor removal), a novel two-phase approach for comprehensive backdoor removal. Our method first employs an unlearning phase, in which the network's loss is intentionally maximized on a small clean dataset to expose neurons that are excessively sensitive to backdoor triggers. Subsequently, in the relearning phase, these suspicious neurons are recalibrated using targeted reinitialization and cosine similarity regularization, effectively neutralizing backdoor influences while preserving the model's performance on benign data. Extensive experiments with 12 backdoor types on multiple datasets (CIFAR-10, CIFAR-100, GTSRB, and Tiny-ImageNet) and architectures (PreAct-ResNet18, VGG19-BN, and ViT-B-16) demonstrate that ULRL significantly reduces the attack success rate without compromising clean accuracy -- even when only 1% of clean data is used for defense.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2405.14781","src/content/posts/2025-06-25-unified-neural-backdoor-removal-with-only-few-clean-samples-through-unlearning-and-relearning.md","fb887221f6b0e997",{"html":25,"metadata":35724},{"headings":35725,"localImagePaths":35726,"remoteImagePaths":35727,"frontmatter":35728,"imagePaths":35729},[],[],[],{"title":35717,"description":35718,"summary":35718,"pubDate":32084,"source":25191,"url":35720,"thumbnail":25193},[],"2025-06-25-unified-neural-backdoor-removal-with-only-few-clean-samples-through-unlearning-and-relearning.md","2025-06-25-unlocking-insights-addressing-alcohol-inference-mismatch-through-database-narrative-alignment",{"id":35731,"data":35733,"filePath":35738,"digest":35739,"rendered":35740,"legacyId":35747},{"title":35734,"description":35735,"summary":35735,"pubDate":35736,"source":25191,"url":35737,"thumbnail":25193},"Unlocking Insights Addressing Alcohol Inference Mismatch through Database-Narrative Alignment","arXiv:2506.19342v1 Announce Type: cross Abstract: Road traffic crashes are a significant global cause of fatalities, emphasizing the urgent need for accurate crash data to enhance prevention strategies and inform policy development. This study addresses the challenge of alcohol inference mismatch (AIM) by employing database narrative alignment to identify AIM in crash data. A framework was developed to improve data quality in crash management systems and reduce the percentage of AIM crashes. Utilizing the BERT model, the analysis of 371,062 crash records from Iowa (2016-2022) revealed 2,767 AIM incidents, resulting in an overall AIM percentage of 24.03%. Statistical tools, including the Probit Logit model, were used to explore the crash characteristics affecting AIM patterns. The findings indicate that alcohol-related fatal crashes and nighttime incidents have a lower percentage of the mismatch, while crashes involving unknown vehicle types and older drivers are more susceptible to mismatch. The geospatial cluster as part of this study can identify the regions which have an increased need for education and training. These insights highlight the necessity for targeted training programs and data management teams to improve the accuracy of crash reporting and support evidence-based policymaking.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19342","src/content/posts/2025-06-25-unlocking-insights-addressing-alcohol-inference-mismatch-through-database-narrative-alignment.md","8166f1c50631e261",{"html":25,"metadata":35741},{"headings":35742,"localImagePaths":35743,"remoteImagePaths":35744,"frontmatter":35745,"imagePaths":35746},[],[],[],{"title":35734,"description":35735,"summary":35735,"pubDate":32084,"source":25191,"url":35737,"thumbnail":25193},[],"2025-06-25-unlocking-insights-addressing-alcohol-inference-mismatch-through-database-narrative-alignment.md","2025-06-25-unsupervised-dataset-dictionary-learning-for-domain-shift-robust-clustering-application-to-sitting-posture-identification",{"id":35748,"data":35750,"filePath":35755,"digest":35756,"rendered":35757,"legacyId":35764},{"title":35751,"description":35752,"summary":35752,"pubDate":35753,"source":25191,"url":35754,"thumbnail":25193},"Unsupervised Dataset Dictionary Learning for domain shift robust clustering: application to sitting posture identification","arXiv:2506.19410v1 Announce Type: new Abstract: This paper introduces a novel approach, Unsupervised Dataset Dictionary Learning (U-DaDiL), for totally unsupervised robust clustering applied to sitting posture identification. Traditional methods often lack adaptability to diverse datasets and suffer from domain shift issues. U-DaDiL addresses these challenges by aligning distributions from different datasets using Wasserstein barycenter based representation. Experimental evaluations on the Office31 dataset demonstrate significant improvements in cluster alignment accuracy. This work also presents a promising step for addressing domain shift and robust clustering for unsupervised sitting posture identification",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19410","src/content/posts/2025-06-25-unsupervised-dataset-dictionary-learning-for-domain-shift-robust-clustering-application-to-sitting-posture-identification.md","3832e2078ff5eef9",{"html":25,"metadata":35758},{"headings":35759,"localImagePaths":35760,"remoteImagePaths":35761,"frontmatter":35762,"imagePaths":35763},[],[],[],{"title":35751,"description":35752,"summary":35752,"pubDate":32084,"source":25191,"url":35754,"thumbnail":25193},[],"2025-06-25-unsupervised-dataset-dictionary-learning-for-domain-shift-robust-clustering-application-to-sitting-posture-identification.md","2025-06-25-vesselsam-leveraging-sam-for-aortic-vessel-segmentation-with-atrouslora",{"id":35765,"data":35767,"filePath":35772,"digest":35773,"rendered":35774,"legacyId":35781},{"title":35768,"description":35769,"summary":35769,"pubDate":35770,"source":25191,"url":35771,"thumbnail":25193},"VesselSAM: Leveraging SAM for Aortic Vessel Segmentation with AtrousLoRA","arXiv:2502.18185v4 Announce Type: replace-cross Abstract: Medical image segmentation is crucial for clinical diagnosis and treatment planning, especially when dealing with complex anatomical structures such as vessels. However, accurately segmenting vessels remains challenging due to their small size, intricate edge structures, and susceptibility to artifacts and imaging noise. In this work, we propose VesselSAM, an enhanced version of the Segment Anything Model (SAM), specifically tailored for aortic vessel segmentation. VesselSAM incorporates AtrousLoRA, a novel module integrating Atrous Attention and Low-Rank Adaptation (LoRA), to enhance segmentation performance. Atrous Attention enables the model to capture multi-scale contextual information, preserving both fine-grained local details and broader global context. Additionally, LoRA facilitates efficient fine-tuning of the frozen SAM image encoder, reducing the number of trainable parameters and thereby enhancing computational efficiency. We evaluate VesselSAM using two challenging datasets: the Aortic Vessel Tree (AVT) dataset and the Type-B Aortic Dissection (TBAD) dataset. VesselSAM achieves state-of-the-art performance, attaining DSC scores of 93.50%, 93.25%, 93.02%, and 93.26% across multi-center datasets. Our results demonstrate that VesselSAM delivers high segmentation accuracy while significantly reducing computational overhead compared to existing large-scale models. This development paves the way for enhanced AI-based aortic vessel segmentation in clinical environments. The code and models will be released at https://github.com/Adnan-CAS/AtrousLora.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2502.18185","src/content/posts/2025-06-25-vesselsam-leveraging-sam-for-aortic-vessel-segmentation-with-atrouslora.md","9106560baa526eed",{"html":25,"metadata":35775},{"headings":35776,"localImagePaths":35777,"remoteImagePaths":35778,"frontmatter":35779,"imagePaths":35780},[],[],[],{"title":35768,"description":35769,"summary":35769,"pubDate":32084,"source":25191,"url":35771,"thumbnail":25193},[],"2025-06-25-vesselsam-leveraging-sam-for-aortic-vessel-segmentation-with-atrouslora.md","2025-06-25-video-xl-2-towards-very-long-video-understanding-through-task-aware-kv-sparsification",{"id":35782,"data":35784,"filePath":35789,"digest":35790,"rendered":35791,"legacyId":35798},{"title":35785,"description":35786,"summary":35786,"pubDate":35787,"source":25191,"url":35788,"thumbnail":25193},"Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV Sparsification","arXiv:2506.19225v1 Announce Type: cross Abstract: Multi-modal large language models (MLLMs) models have made significant progress in video understanding over the past few years. However, processing long video inputs remains a major challenge due to high memory and computational costs. This makes it difficult for current models to achieve both strong performance and high efficiency in long video understanding. To address this challenge, we propose Video-XL-2, a novel MLLM that delivers superior cost-effectiveness for long-video understanding based on task-aware KV sparsification. The proposed framework operates with two key steps: chunk-based pre-filling and bi-level key-value decoding. Chunk-based pre-filling divides the visual token sequence into chunks, applying full attention within each chunk and sparse attention across chunks. This significantly reduces computational and memory overhead. During decoding, bi-level key-value decoding selectively reloads either dense or sparse key-values for each chunk based on its relevance to the task. This approach further improves memory efficiency and enhances the model's ability to capture fine-grained information. Video-XL-2 achieves state-of-the-art performance on various long video understanding benchmarks, outperforming existing open-source lightweight models. It also demonstrates exceptional efficiency, capable of processing over 10,000 frames on a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few seconds.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19225","src/content/posts/2025-06-25-video-xl-2-towards-very-long-video-understanding-through-task-aware-kv-sparsification.md","27cb8487e4b8a177",{"html":25,"metadata":35792},{"headings":35793,"localImagePaths":35794,"remoteImagePaths":35795,"frontmatter":35796,"imagePaths":35797},[],[],[],{"title":35785,"description":35786,"summary":35786,"pubDate":32084,"source":25191,"url":35788,"thumbnail":25193},[],"2025-06-25-video-xl-2-towards-very-long-video-understanding-through-task-aware-kv-sparsification.md","2025-06-25-videopcdnet-video-parsing-and-prediction-with-phase-correlation-networks",{"id":35799,"data":35801,"filePath":35806,"digest":35807,"rendered":35808,"legacyId":35815},{"title":35802,"description":35803,"summary":35803,"pubDate":35804,"source":25191,"url":35805,"thumbnail":25193},"VideoPCDNet: Video Parsing and Prediction with Phase Correlation Networks","arXiv:2506.19621v1 Announce Type: cross Abstract: Understanding and predicting video content is essential for planning and reasoning in dynamic environments. Despite advancements, unsupervised learning of object representations and dynamics remains challenging. We present VideoPCDNet, an unsupervised framework for object-centric video decomposition and prediction. Our model uses frequency-domain phase correlation techniques to recursively parse videos into object components, which are represented as transformed versions of learned object prototypes, enabling accurate and interpretable tracking. By explicitly modeling object motion through a combination of frequency domain operations and lightweight learned modules, VideoPCDNet enables accurate unsupervised object tracking and prediction of future video frames. In our experiments, we demonstrate that VideoPCDNet outperforms multiple object-centric baseline models for unsupervised tracking and prediction on several synthetic datasets, while learning interpretable object and motion representations.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19621","src/content/posts/2025-06-25-videopcdnet-video-parsing-and-prediction-with-phase-correlation-networks.md","3044fcd5c01935a6",{"html":25,"metadata":35809},{"headings":35810,"localImagePaths":35811,"remoteImagePaths":35812,"frontmatter":35813,"imagePaths":35814},[],[],[],{"title":35802,"description":35803,"summary":35803,"pubDate":32084,"source":25191,"url":35805,"thumbnail":25193},[],"2025-06-25-videopcdnet-video-parsing-and-prediction-with-phase-correlation-networks.md","2025-06-25-vision-transformer-based-time-series-image-reconstruction-for-cloud-filling-applications",{"id":35816,"data":35818,"filePath":35823,"digest":35824,"rendered":35825,"legacyId":35832},{"title":35819,"description":35820,"summary":35820,"pubDate":35821,"source":25191,"url":35822,"thumbnail":25193},"Vision Transformer-Based Time-Series Image Reconstruction for Cloud-Filling Applications","arXiv:2506.19591v1 Announce Type: cross Abstract: Cloud cover in multispectral imagery (MSI) poses significant challenges for early season crop mapping, as it leads to missing or corrupted spectral information. Synthetic aperture radar (SAR) data, which is not affected by cloud interference, offers a complementary solution, but lack sufficient spectral detail for precise crop mapping. To address this, we propose a novel framework, Time-series MSI Image Reconstruction using Vision Transformer (ViT), to reconstruct MSI data in cloud-covered regions by leveraging the temporal coherence of MSI and the complementary information from SAR from the attention mechanism. Comprehensive experiments, using rigorous reconstruction evaluation metrics, demonstrate that Time-series ViT framework significantly outperforms baselines that use non-time-series MSI and SAR or time-series MSI without SAR, effectively enhancing MSI image reconstruction in cloud-covered regions.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19591","src/content/posts/2025-06-25-vision-transformer-based-time-series-image-reconstruction-for-cloud-filling-applications.md","57003d38898e20f9",{"html":25,"metadata":35826},{"headings":35827,"localImagePaths":35828,"remoteImagePaths":35829,"frontmatter":35830,"imagePaths":35831},[],[],[],{"title":35819,"description":35820,"summary":35820,"pubDate":32084,"source":25191,"url":35822,"thumbnail":25193},[],"2025-06-25-vision-transformer-based-time-series-image-reconstruction-for-cloud-filling-applications.md","2025-06-25-what-do-professional-software-developers-need-to-know-to-succeed-in-an-age-of-artificial-intelligence",{"id":35833,"data":35835,"filePath":35840,"digest":35841,"rendered":35842,"legacyId":35849},{"title":35836,"description":35837,"summary":35837,"pubDate":35838,"source":25191,"url":35839,"thumbnail":25193},"What do professional software developers need to know to succeed in an age of Artificial Intelligence?","arXiv:2506.00202v3 Announce Type: replace Abstract: Generative AI is showing early evidence of productivity gains for software developers, but concerns persist regarding workforce disruption and deskilling. We describe our research with 21 developers at the cutting edge of using AI, summarizing 12 of their work goals we uncovered, together with 75 associated tasks and the skills & knowledge for each, illustrating how developers use AI at work. From all of these, we distilled our findings in the form of 5 insights. We found that the skills & knowledge to be a successful AI-enhanced developer are organized into four domains (using Generative AI effectively, core software engineering, adjacent engineering, and adjacent non-engineering) deployed at critical junctures throughout a 6-step task workflow. In order to 'future proof' developers for this age of AI, on-the-job learning initiatives and computer science degree programs will need to target both 'soft' skills and the technical skills & knowledge in all four domains to reskill, upskill and safeguard against deskilling.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.00202","src/content/posts/2025-06-25-what-do-professional-software-developers-need-to-know-to-succeed-in-an-age-of-artificial-intelligence.md","eeb24f5f5441a748",{"html":25,"metadata":35843},{"headings":35844,"localImagePaths":35845,"remoteImagePaths":35846,"frontmatter":35847,"imagePaths":35848},[],[],[],{"title":35836,"description":35837,"summary":35837,"pubDate":32084,"source":25191,"url":35839,"thumbnail":25193},[],"2025-06-25-what-do-professional-software-developers-need-to-know-to-succeed-in-an-age-of-artificial-intelligence.md","2025-06-25-when-can-we-reuse-a-calibration-set-for-multiple-conformal-predictions",{"id":35850,"data":35852,"filePath":35857,"digest":35858,"rendered":35859,"legacyId":35866},{"title":35853,"description":35854,"summary":35854,"pubDate":35855,"source":25191,"url":35856,"thumbnail":25193},"When Can We Reuse a Calibration Set for Multiple Conformal Predictions?","arXiv:2506.19689v1 Announce Type: cross Abstract: Reliable uncertainty quantification is crucial for the trustworthiness of machine learning applications. Inductive Conformal Prediction (ICP) offers a distribution-free framework for generating prediction sets or intervals with user-specified confidence. However, standard ICP guarantees are marginal and typically require a fresh calibration set for each new prediction to maintain their validity. This paper addresses this practical limitation by demonstrating how e-conformal prediction, in conjunction with Hoeffding's inequality, can enable the repeated use of a single calibration set with a high probability of preserving the desired coverage. Through a case study on the CIFAR-10 dataset, we train a deep neural network and utilise a calibration set to estimate a Hoeffding correction. This correction allows us to apply a modified Markov's inequality, leading to the construction of prediction sets with quantifiable confidence. Our results illustrate the feasibility of maintaining provable performance in conformal prediction while enhancing its practicality by reducing the need for repeated calibration. The code for this work is publicly available.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19689","src/content/posts/2025-06-25-when-can-we-reuse-a-calibration-set-for-multiple-conformal-predictions.md","c7081ce224046b37",{"html":25,"metadata":35860},{"headings":35861,"localImagePaths":35862,"remoteImagePaths":35863,"frontmatter":35864,"imagePaths":35865},[],[],[],{"title":35853,"description":35854,"summary":35854,"pubDate":32084,"source":25191,"url":35856,"thumbnail":25193},[],"2025-06-25-when-can-we-reuse-a-calibration-set-for-multiple-conformal-predictions.md","2025-06-25-which-consciousness-can-be-artificialized-local-percept-perceiver-phenomenon-for-the-existence-of-machine-consciousness",{"id":35867,"data":35869,"filePath":35874,"digest":35875,"rendered":35876,"legacyId":35883},{"title":35870,"description":35871,"summary":35871,"pubDate":35872,"source":25191,"url":35873,"thumbnail":25193},"Which Consciousness Can Be Artificialized? Local Percept-Perceiver Phenomenon for the Existence of Machine Consciousness","arXiv:2506.18935v1 Announce Type: cross Abstract: This paper presents a novel paradigm of the local percept-perceiver phenomenon to formalize certain observations in neuroscientific theories of consciousness. Using this model, a set-theoretic formalism is developed for artificial systems, and the existence of machine consciousness is proved by invoking Zermelo-Fraenkel set theory. The article argues for the possibility of a reductionist form of epistemic consciousness within machines.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.18935","src/content/posts/2025-06-25-which-consciousness-can-be-artificialized-local-percept-perceiver-phenomenon-for-the-existence-of-machine-consciousness.md","a9e03e09d6ce7f1e",{"html":25,"metadata":35877},{"headings":35878,"localImagePaths":35879,"remoteImagePaths":35880,"frontmatter":35881,"imagePaths":35882},[],[],[],{"title":35870,"description":35871,"summary":35871,"pubDate":32084,"source":25191,"url":35873,"thumbnail":25193},[],"2025-06-25-which-consciousness-can-be-artificialized-local-percept-perceiver-phenomenon-for-the-existence-of-machine-consciousness.md","2025-06-25-who-does-what-in-deep-learning-multidimensional-game-theoretic-attribution-of-function-of-neural-units",{"id":35884,"data":35886,"filePath":35891,"digest":35892,"rendered":35893,"legacyId":35900},{"title":35887,"description":35888,"summary":35888,"pubDate":35889,"source":25191,"url":35890,"thumbnail":25193},"Who Does What in Deep Learning? Multidimensional Game-Theoretic Attribution of Function of Neural Units","arXiv:2506.19732v1 Announce Type: cross Abstract: Neural networks now generate text, images, and speech with billions of parameters, producing a need to know how each neural unit contributes to these high-dimensional outputs. Existing explainable-AI methods, such as SHAP, attribute importance to inputs, but cannot quantify the contributions of neural units across thousands of output pixels, tokens, or logits. Here we close that gap with Multiperturbation Shapley-value Analysis (MSA), a model-agnostic game-theoretic framework. By systematically lesioning combinations of units, MSA yields Shapley Modes, unit-wise contribution maps that share the exact dimensionality of the model's output. We apply MSA across scales, from multi-layer perceptrons to the 56-billion-parameter Mixtral-8x7B and Generative Adversarial Networks (GAN). The approach demonstrates how regularisation concentrates computation in a few hubs, exposes language-specific experts inside the LLM, and reveals an inverted pixel-generation hierarchy in GANs. Together, these results showcase MSA as a powerful approach for interpreting, editing, and compressing deep neural networks.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19732","src/content/posts/2025-06-25-who-does-what-in-deep-learning-multidimensional-game-theoretic-attribution-of-function-of-neural-units.md","e6459a1a162396c9",{"html":25,"metadata":35894},{"headings":35895,"localImagePaths":35896,"remoteImagePaths":35897,"frontmatter":35898,"imagePaths":35899},[],[],[],{"title":35887,"description":35888,"summary":35888,"pubDate":32084,"source":25191,"url":35890,"thumbnail":25193},[],"2025-06-25-who-does-what-in-deep-learning-multidimensional-game-theoretic-attribution-of-function-of-neural-units.md","2025-06-25-why-do-open-source-llms-struggle-with-data-analysis-a-systematic-empirical-study",{"id":35901,"data":35903,"filePath":35908,"digest":35909,"rendered":35910,"legacyId":35917},{"title":35904,"description":35905,"summary":35905,"pubDate":35906,"source":25191,"url":35907,"thumbnail":25193},"Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study","arXiv:2506.19794v1 Announce Type: cross Abstract: Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19794","src/content/posts/2025-06-25-why-do-open-source-llms-struggle-with-data-analysis-a-systematic-empirical-study.md","abf33de1497653f3",{"html":25,"metadata":35911},{"headings":35912,"localImagePaths":35913,"remoteImagePaths":35914,"frontmatter":35915,"imagePaths":35916},[],[],[],{"title":35904,"description":35905,"summary":35905,"pubDate":32084,"source":25191,"url":35907,"thumbnail":25193},[],"2025-06-25-why-do-open-source-llms-struggle-with-data-analysis-a-systematic-empirical-study.md","2025-06-25-why-uncertainty-calibration-matters-for-reliable-perturbation-based-explanations",{"id":35918,"data":35920,"filePath":35925,"digest":35926,"rendered":35927,"legacyId":35934},{"title":35921,"description":35922,"summary":35922,"pubDate":35923,"source":25191,"url":35924,"thumbnail":25193},"Why Uncertainty Calibration Matters for Reliable Perturbation-based Explanations","arXiv:2506.19630v1 Announce Type: cross Abstract: Perturbation-based explanations are widely utilized to enhance the transparency of modern machine-learning models. However, their reliability is often compromised by the unknown model behavior under the specific perturbations used. This paper investigates the relationship between uncertainty calibration - the alignment of model confidence with actual accuracy - and perturbation-based explanations. We show that models frequently produce unreliable probability estimates when subjected to explainability-specific perturbations and theoretically prove that this directly undermines explanation quality. To address this, we introduce ReCalX, a novel approach to recalibrate models for improved perturbation-based explanations while preserving their original predictions. Experiments on popular computer vision models demonstrate that our calibration strategy produces explanations that are more aligned with human perception and actual object locations.",["Date","2025-06-25T04:00:00.000Z"],"https://arxiv.org/abs/2506.19630","src/content/posts/2025-06-25-why-uncertainty-calibration-matters-for-reliable-perturbation-based-explanations.md","eed00993379a6a8f",{"html":25,"metadata":35928},{"headings":35929,"localImagePaths":35930,"remoteImagePaths":35931,"frontmatter":35932,"imagePaths":35933},[],[],[],{"title":35921,"description":35922,"summary":35922,"pubDate":32084,"source":25191,"url":35924,"thumbnail":25193},[],"2025-06-25-why-uncertainty-calibration-matters-for-reliable-perturbation-based-explanations.md","2025-06-25-日本の私立大学初東京工科大学がnvidia-dgxのaiスパコンを構築ai大学構想を加速",{"id":35935,"data":35937,"filePath":35943,"digest":35944,"rendered":35945,"legacyId":35953},{"title":35938,"description":35939,"summary":35939,"pubDate":35940,"source":24252,"url":35941,"thumbnail":35942},"日本の私立大学初、東京工科大学がNVIDIA DGXのAIスパコンを構築。「AI大学」構想を加速","\u003Cp>東京工科大学は、AI教育と研究を加速させるため、NVIDIA DGX B200システムを用いた日本の私立大学最大のAIスーパーコンピューターを構築し、2025年10月に本格稼働予定です。 このニュースのポイント 東京工科 [&#8230;]\u003C/p> \u003Cp>The post \u003Ca href='https://aismiley.co.jp/ai_news/teu-ac-nvidia-dgx/'>日本の私立大学初、東京工科大学がNVIDIA DGXのAIスパコンを構築。「AI大学」構想を加速\u003C/a> first appeared on \u003Ca href='https://aismiley.co.jp'>AIポータルメディアAIsmiley\u003C/a>.\u003C/p>",["Date","2025-06-25T09:24:03.000Z"],"https://aismiley.co.jp/ai_news/teu-ac-nvidia-dgx/","https://aismiley.co.jp/wp-content/uploads/2025/06/teu0.png","src/content/posts/2025-06-25-日本の私立大学初東京工科大学がnvidia-dgxのaiスパコンを構築ai大学構想を加速.md","b017d80a7da92471",{"html":25,"metadata":35946},{"headings":35947,"localImagePaths":35948,"remoteImagePaths":35949,"frontmatter":35950,"imagePaths":35952},[],[],[],{"title":35938,"description":35939,"summary":35939,"pubDate":35951,"source":24252,"url":35941,"thumbnail":35942},"Wed, 25 Jun 2025 09:24:03 +0000",[],"2025-06-25-日本の私立大学初東京工科大学がnvidia-dgxのaiスパコンを構築ai大学構想を加速.md","2025-06-25-生成aiのここが難しい-プロンプト設計を超えた1位は-フリーランスに聞く",{"id":35954,"data":35956,"filePath":35962,"digest":35963,"rendered":35964,"legacyId":35971},{"title":35957,"description":35958,"summary":35958,"pubDate":35959,"source":24944,"url":35960,"thumbnail":35961},"生成AIのここが難しい　「プロンプト設計」を超えた1位は？　フリーランスに聞く","リモラボ（東京都渋谷区）によると、フリーランスの3人に1人が生成AIの有料版を利用し、AIを本格的な業務ツールとして取り入れている実態が明らかになった。実際に使ってみたフリーランスたちは、有料版の生成AIをどのように評価しているのか。また、どのようなポイントでつまづいているのか？",["Date","2025-06-24T23:00:00.000Z"],"https://www.itmedia.co.jp/business/articles/2506/25/news020.html","https://image.itmedia.co.jp/business/articles/2506/25/cover_news020.jpg","src/content/posts/2025-06-25-生成aiのここが難しい-プロンプト設計を超えた1位は-フリーランスに聞く.md","138e3b01ec97a897",{"html":25,"metadata":35965},{"headings":35966,"localImagePaths":35967,"remoteImagePaths":35968,"frontmatter":35969,"imagePaths":35970},[],[],[],{"title":35957,"description":35958,"summary":35958,"pubDate":35103,"source":24944,"url":35960,"thumbnail":35961},[],"2025-06-25-生成aiのここが難しい-プロンプト設計を超えた1位は-フリーランスに聞く.md","2025-06-25-生成aiの学習に書籍を無断使用は合法米地裁フェアユース-anthropicへの訴訟巡り",{"id":35972,"data":35974,"filePath":35980,"digest":35981,"rendered":35982,"legacyId":35990},{"title":35975,"description":35976,"summary":35976,"pubDate":35977,"source":24944,"url":35978,"thumbnail":35979},"「生成AIの学習に書籍を無断使用」は合法───米地裁「フェアユース」　Anthropicへの訴訟巡り","米サンフランシスコの連邦地裁は6月23日、米新興AnthropicがAIの学習に著者の許可なく書籍を利用したことは米著作権法上合法との判断を示した。同社の行為は「フェアユース」（公正利用）に当たるとした。",["Date","2025-06-25T05:03:00.000Z"],"https://www.itmedia.co.jp/news/articles/2506/25/news082.html","https://image.itmedia.co.jp/news/articles/2506/25/cover_news082.jpg","src/content/posts/2025-06-25-生成aiの学習に書籍を無断使用は合法米地裁フェアユース-anthropicへの訴訟巡り.md","318471dc6109d27f",{"html":25,"metadata":35983},{"headings":35984,"localImagePaths":35985,"remoteImagePaths":35986,"frontmatter":35987,"imagePaths":35989},[],[],[],{"title":35975,"description":35976,"summary":35976,"pubDate":35988,"source":24944,"url":35978,"thumbnail":35979},"Wed, 25 Jun 2025 14:03:00 +0900",[],"2025-06-25-生成aiの学習に書籍を無断使用は合法米地裁フェアユース-anthropicへの訴訟巡り.md","2025-06-25-私の顔型に合うフレームはaiが提案-独自開発jins-ai実証実験拡大",{"id":35991,"data":35993,"filePath":35999,"digest":36000,"rendered":36001,"legacyId":36009},{"title":35994,"description":35995,"summary":35995,"pubDate":35996,"source":24944,"url":35997,"thumbnail":35998},"「私の顔型に合うフレームは？」AIが提案　独自開発「JINS AI」実証実験拡大","生成AIを活用し、顧客のメガネ購入に関する疑問や悩みに瞬時に回答・提案をするサービス「JINS AI」の実証実験の対象店舗が拡大へ。",["Date","2025-06-25T07:58:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/25/news104.html","https://image.itmedia.co.jp/aiplus/articles/2506/25/cover_news104.jpg","src/content/posts/2025-06-25-私の顔型に合うフレームはaiが提案-独自開発jins-ai実証実験拡大.md","00e367b08a1acc85",{"html":25,"metadata":36002},{"headings":36003,"localImagePaths":36004,"remoteImagePaths":36005,"frontmatter":36006,"imagePaths":36008},[],[],[],{"title":35994,"description":35995,"summary":35995,"pubDate":36007,"source":24944,"url":35997,"thumbnail":35998},"Wed, 25 Jun 2025 16:58:00 +0900",[],"2025-06-25-私の顔型に合うフレームはaiが提案-独自開発jins-ai実証実験拡大.md","2025-06-25-米ai企業のanthropic東京に拠点開設へ-claude日本語版もリリース予定",{"id":36010,"data":36012,"filePath":36018,"digest":36019,"rendered":36020,"legacyId":36028},{"title":36013,"description":36014,"summary":36014,"pubDate":36015,"source":24944,"url":36016,"thumbnail":36017},"米AI企業のAnthropic、東京に拠点開設へ　「Claude」日本語版もリリース予定","米Anthropicは、秋ごろに東京都に拠点を開設すると発表した。併せて、同社のAIサービス「Claude」の日本語版をリリースする。",["Date","2025-06-25T04:15:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/25/news076.html","https://image.itmedia.co.jp/aiplus/articles/2506/25/cover_news076.jpg","src/content/posts/2025-06-25-米ai企業のanthropic東京に拠点開設へ-claude日本語版もリリース予定.md","d2ffe37b3a7197a7",{"html":25,"metadata":36021},{"headings":36022,"localImagePaths":36023,"remoteImagePaths":36024,"frontmatter":36025,"imagePaths":36027},[],[],[],{"title":36013,"description":36014,"summary":36014,"pubDate":36026,"source":24944,"url":36016,"thumbnail":36017},"Wed, 25 Jun 2025 13:15:00 +0900",[],"2025-06-25-米ai企業のanthropic東京に拠点開設へ-claude日本語版もリリース予定.md","2025-06-25-複数の質問で毒を仕込む-新型aiジェイルブレークecho-chamber-attackメカニズム",{"id":36029,"data":36031,"filePath":36037,"digest":36038,"rendered":36039,"legacyId":36047},{"title":36032,"description":36033,"summary":36033,"pubDate":36034,"source":24944,"url":36035,"thumbnail":36036},"複数の質問で毒を仕込む　新型AIジェイルブレーク「Echo Chamber Attack」メカニズム","NeuralTrustは新たなLLMジェイルブレーク手法「Echo Chamber Attack」を発表した。複数ターンの無害なやりとりを通じてモデルの内部文脈を誘導し、有害出力を引き出す技術とされ、多くのAIモデルに通用するという。",["Date","2025-06-25T00:00:00.000Z"],"https://www.itmedia.co.jp/enterprise/articles/2506/25/news036.html","https://image.itmedia.co.jp/enterprise/articles/2506/25/cover_news036.jpg","src/content/posts/2025-06-25-複数の質問で毒を仕込む-新型aiジェイルブレークecho-chamber-attackメカニズム.md","9b8121b0eb486d2e",{"html":25,"metadata":36040},{"headings":36041,"localImagePaths":36042,"remoteImagePaths":36043,"frontmatter":36044,"imagePaths":36046},[],[],[],{"title":36032,"description":36033,"summary":36033,"pubDate":36045,"source":24944,"url":36035,"thumbnail":36036},"Wed, 25 Jun 2025 09:00:00 +0900",[],"2025-06-25-複数の質問で毒を仕込む-新型aiジェイルブレークecho-chamber-attackメカニズム.md","2025-06-26-ai時代の安全基準owasp-ai-testing-guideが始動-その中身とは",{"id":36048,"data":36050,"filePath":36056,"digest":36057,"rendered":36058,"legacyId":36066},{"title":36051,"description":36052,"summary":36052,"pubDate":36053,"source":24944,"url":36054,"thumbnail":36055},"AI時代の安全基準、「OWASP AI Testing Guide」が始動　その中身とは？","OWASPはAI技術の特異性に対応するための「AI Testing Guide」初期ドラフトを公開した。同ガイドは技術・業界を問わず適用可能な試験方法論を提示するもので、AIセキュリティや倫理、信頼性確保を目的としている。",["Date","2025-06-25T23:30:00.000Z"],"https://www.itmedia.co.jp/enterprise/articles/2506/26/news031.html","https://image.itmedia.co.jp/enterprise/articles/2506/26/cover_news031.jpg","src/content/posts/2025-06-26-ai時代の安全基準owasp-ai-testing-guideが始動-その中身とは.md","f4254d61adfb376f",{"html":25,"metadata":36059},{"headings":36060,"localImagePaths":36061,"remoteImagePaths":36062,"frontmatter":36063,"imagePaths":36065},[],[],[],{"title":36051,"description":36052,"summary":36052,"pubDate":36064,"source":24944,"url":36054,"thumbnail":36055},"Thu, 26 Jun 2025 08:30:00 +0900",[],"2025-06-26-ai時代の安全基準owasp-ai-testing-guideが始動-その中身とは.md","2025-06-26-google-deepmindオープンソースaiエージェントgemini-cli提供開始",{"id":36067,"data":36069,"filePath":36075,"digest":36076,"rendered":36077,"legacyId":36085},{"title":36070,"description":36071,"summary":36071,"pubDate":36072,"source":24944,"url":36073,"thumbnail":36074},"Google DeepMind、オープンソースAIエージェント「Gemini CLI」提供開始","Google DeepMindは、ターミナルから直接「Gemini 2.5 Pro」の機能を利用できるオープンソースAIエージェント「Gemini CLI」を発表した。個人利用の場合は、Googleアカウントでログインすることで、無料の「Gemini Code Assist」ライセンスが付与される。",["Date","2025-06-25T21:59:00.000Z"],"https://www.itmedia.co.jp/aiplus/articles/2506/26/news052.html","https://image.itmedia.co.jp/aiplus/articles/2506/26/cover_news052.jpg","src/content/posts/2025-06-26-google-deepmindオープンソースaiエージェントgemini-cli提供開始.md","d72b66dd5ccc5b5c",{"html":25,"metadata":36078},{"headings":36079,"localImagePaths":36080,"remoteImagePaths":36081,"frontmatter":36082,"imagePaths":36084},[],[],[],{"title":36070,"description":36071,"summary":36071,"pubDate":36083,"source":24944,"url":36073,"thumbnail":36074},"Thu, 26 Jun 2025 06:59:00 +0900",[],"2025-06-26-google-deepmindオープンソースaiエージェントgemini-cli提供開始.md","2025-06-26-google-deepmindゲノム理解のためのaialphagenomeを非営利研究向けに提供開始",{"id":36086,"data":36088,"filePath":36094,"digest":36095,"rendered":36096,"legacyId":36104},{"title":36089,"description":36090,"summary":36090,"pubDate":36091,"source":24944,"url":36092,"thumbnail":36093},"Google DeepMind、ゲノム理解のためのAI「AlphaGenome」を非営利研究向けに提供開始","Google DeepMindは、ゲノム理解を深めるAI「AlphaGenome」を発表した。DNA配列の変異が遺伝子制御に与える影響を包括的かつ正確に予測することを目指す。既存モデルを上回る性能を示し、非営利の研究向けにAPIのプレビュー版が提供される。",["Date","2025-06-26T00:00:00.000Z"],"https://www.itmedia.co.jp/news/articles/2506/26/news056.html","https://image.itmedia.co.jp/news/articles/2506/26/cover_news056.jpg","src/content/posts/2025-06-26-google-deepmindゲノム理解のためのaialphagenomeを非営利研究向けに提供開始.md","18c86711a4489b50",{"html":25,"metadata":36097},{"headings":36098,"localImagePaths":36099,"remoteImagePaths":36100,"frontmatter":36101,"imagePaths":36103},[],[],[],{"title":36089,"description":36090,"summary":36090,"pubDate":36102,"source":24944,"url":36092,"thumbnail":36093},"Thu, 26 Jun 2025 09:00:00 +0900",[],"2025-06-26-google-deepmindゲノム理解のためのaialphagenomeを非営利研究向けに提供開始.md","2025-06-26-サムアルトマン氏も認める緊張状態microsoftopenaiへの出資比率見直しか",{"id":36105,"data":36107,"filePath":36113,"digest":36114,"rendered":36115,"legacyId":36123},{"title":36108,"description":36109,"summary":36109,"pubDate":36110,"source":24944,"url":36111,"thumbnail":36112},"サム・アルトマン氏も認める「緊張状態」──Microsoft、OpenAIへの出資比率見直しか","米OpenAIのサム・アルトマンCEOは、米Microsoftのサティア・ナデラCEOと7月23日に電話で会談し、将来的な協業関係について話し合ったと、24日に公開されたニューヨーク・タイムズのポッドキャスト番組で語った。",["Date","2025-06-25T22:00:00.000Z"],"https://www.itmedia.co.jp/business/articles/2506/25/news114.html","https://image.itmedia.co.jp/business/articles/2506/25/cover_news114.jpg","src/content/posts/2025-06-26-サムアルトマン氏も認める緊張状態microsoftopenaiへの出資比率見直しか.md","961c61ba6b3f22ec",{"html":25,"metadata":36116},{"headings":36117,"localImagePaths":36118,"remoteImagePaths":36119,"frontmatter":36120,"imagePaths":36122},[],[],[],{"title":36108,"description":36109,"summary":36109,"pubDate":36121,"source":24944,"url":36111,"thumbnail":36112},"Thu, 26 Jun 2025 07:00:00 +0900",[],"2025-06-26-サムアルトマン氏も認める緊張状態microsoftopenaiへの出資比率見直しか.md"]
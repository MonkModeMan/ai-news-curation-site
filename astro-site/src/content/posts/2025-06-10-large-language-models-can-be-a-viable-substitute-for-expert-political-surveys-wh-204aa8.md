---
title: Large Language Models Can Be a Viable Substitute for Expert Political Surveys
  When a Shock Disrupts Traditional Measurement Approaches
description: "arXiv:2506.06540v1 Announce Type: cross \nAbstract: After a disruptive\
  \ event or shock, such as the Department of Government Efficiency (DOGE) federal\
  \ layoffs of 2025, expert judgments are colored by knowledge of the outcome. This\
  \ can make it difficult or impossible to reconstruct the pre-event perceptions needed\
  \ to study the factors associated with the event. This position paper argues that\
  \ large language models (LLMs), trained on vast amounts of digital media data, can\
  \ be a viable substitute for expert political surveys when a shock disrupts traditional\
  \ measurement. We analyze the DOGE layoffs as a specific case study for this position.\
  \ We use pairwise comparison prompts with LLMs and derive ideology scores for federal\
  \ executive agencies. These scores replicate pre-layoff expert measures and predict\
  \ which agencies were targeted by DOGE. We also use this same approach and find\
  \ that the perceptions of certain federal agencies as knowledge institutions predict\
  \ which agencies were targeted by DOGE, even when controlling for ideology. This\
  \ case study demonstrates that using LLMs allows us to rapidly and easily test the\
  \ associated factors hypothesized behind the shock. More broadly, our case study\
  \ of this recent event exemplifies how LLMs offer insights into the correlational\
  \ factors of the shock when traditional measurement techniques fail. We conclude\
  \ by proposing a two-part criterion for when researchers can turn to LLMs as a substitute\
  \ for expert political surveys."
pubDate: Tue, 10 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.06540
---

Computer Science > Computers and Society
[Submitted on 6 Jun 2025]
Title:Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches
View PDF HTML (experimental)Abstract:After a disruptive event or shock, such as the Department of Government Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by knowledge of the outcome. This can make it difficult or impossible to reconstruct the pre-event perceptions needed to study the factors associated with the event. This position paper argues that large language models (LLMs), trained on vast amounts of digital media data, can be a viable substitute for expert political surveys when a shock disrupts traditional measurement. We analyze the DOGE layoffs as a specific case study for this position. We use pairwise comparison prompts with LLMs and derive ideology scores for federal executive agencies. These scores replicate pre-layoff expert measures and predict which agencies were targeted by DOGE. We also use this same approach and find that the perceptions of certain federal agencies as knowledge institutions predict which agencies were targeted by DOGE, even when controlling for ideology. This case study demonstrates that using LLMs allows us to rapidly and easily test the associated factors hypothesized behind the shock. More broadly, our case study of this recent event exemplifies how LLMs offer insights into the correlational factors of the shock when traditional measurement techniques fail. We conclude by proposing a two-part criterion for when researchers can turn to LLMs as a substitute for expert political surveys.
Current browse context:
cs.CY
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
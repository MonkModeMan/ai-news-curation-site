---
title: Dynamic Policy Fusion for User Alignment Without Re-Interaction
description: "arXiv:2409.20016v3 Announce Type: replace \nAbstract: Deep reinforcement\
  \ learning (RL) policies, although optimal in terms of task rewards, may not align\
  \ with the personal preferences of human users. To ensure this alignment, a naive\
  \ solution would be to retrain the agent using a reward function that encodes the\
  \ user's specific preferences. However, such a reward function is typically not\
  \ readily available, and as such, retraining the agent from scratch can be prohibitively\
  \ expensive. We propose a more practical approach - to adapt the already trained\
  \ policy to user-specific needs with the help of human feedback. To this end, we\
  \ infer the user's intent through trajectory-level feedback and combine it with\
  \ the trained task policy via a theoretically grounded dynamic policy fusion approach.\
  \ As our approach collects human feedback on the very same trajectories used to\
  \ learn the task policy, it does not require any additional interactions with the\
  \ environment, making it a zero-shot approach. We empirically demonstrate in a number\
  \ of environments that our proposed dynamic policy fusion approach consistently\
  \ achieves the intended task while simultaneously adhering to user-specific needs."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2409.20016
---

Computer Science > Artificial Intelligence
[Submitted on 30 Sep 2024 (v1), last revised 13 Jun 2025 (this version, v3)]
Title:Dynamic Policy Fusion for User Alignment Without Re-Interaction
View PDF HTML (experimental)Abstract:Deep reinforcement learning (RL) policies, although optimal in terms of task rewards, may not align with the personal preferences of human users. To ensure this alignment, a naive solution would be to retrain the agent using a reward function that encodes the user's specific preferences. However, such a reward function is typically not readily available, and as such, retraining the agent from scratch can be prohibitively expensive. We propose a more practical approach - to adapt the already trained policy to user-specific needs with the help of human feedback. To this end, we infer the user's intent through trajectory-level feedback and combine it with the trained task policy via a theoretically grounded dynamic policy fusion approach. As our approach collects human feedback on the very same trajectories used to learn the task policy, it does not require any additional interactions with the environment, making it a zero-shot approach. We empirically demonstrate in a number of environments that our proposed dynamic policy fusion approach consistently achieves the intended task while simultaneously adhering to user-specific needs.
Submission history
From: Ajsal Shereef Palattuparambil Mr [view email][v1] Mon, 30 Sep 2024 07:23:47 UTC (2,831 KB)
[v2] Thu, 3 Oct 2024 03:15:28 UTC (2,831 KB)
[v3] Fri, 13 Jun 2025 01:54:04 UTC (2,404 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
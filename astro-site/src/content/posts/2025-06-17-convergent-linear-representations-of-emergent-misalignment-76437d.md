---
title: Convergent Linear Representations of Emergent Misalignment
description: "arXiv:2506.11618v1 Announce Type: cross \nAbstract: Fine-tuning large\
  \ language models on narrow datasets can cause them to develop broadly misaligned\
  \ behaviours: a phenomena known as emergent misalignment. However, the mechanisms\
  \ underlying this misalignment, and why it generalizes beyond the training domain,\
  \ are poorly understood, demonstrating critical gaps in our knowledge of model alignment.\
  \ In this work, we train and study a minimal model organism which uses just 9 rank-1\
  \ adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that\
  \ different emergently misaligned models converge to similar representations of\
  \ misalignment. We demonstrate this convergence by extracting a 'misalignment direction'\
  \ from one fine-tuned model's activations, and using it to effectively ablate misaligned\
  \ behaviour from fine-tunes using higher dimensional LoRAs and different datasets.\
  \ Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of\
  \ experiments for directly interpreting the fine-tuning adapters, showing that six\
  \ contribute to general misalignment, while two specialise for misalignment in just\
  \ the fine-tuning domain. Emergent misalignment is a particularly salient example\
  \ of undesirable and unexpected model behaviour and by advancing our understanding\
  \ of the mechanisms behind it, we hope to move towards being able to better understand\
  \ and mitigate misalignment more generally."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11618
---

Computer Science > Machine Learning
[Submitted on 13 Jun 2025]
Title:Convergent Linear Representations of Emergent Misalignment
View PDF HTML (experimental)Abstract:Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a 'misalignment direction' from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
---
title: 'MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space'
description: "arXiv:2506.11684v1 Announce Type: cross \nAbstract: Vision-Language\
  \ Models (VLMs) have demonstrated remarkable capabilities in interpreting visual\
  \ layouts and text. However, a significant challenge remains in their ability to\
  \ interpret robustly and reason over multi-tabular data presented as images, a common\
  \ occurrence in real-world scenarios like web pages and digital documents. Existing\
  \ benchmarks typically address single tables or non-visual data (text/structured).\
  \ This leaves a critical gap: they don't assess the ability to parse diverse table\
  \ images, correlate information across them, and perform multi-hop reasoning on\
  \ the combined visual data. We introduce MTabVQA, a novel benchmark specifically\
  \ designed for multi-tabular visual question answering to bridge that gap. MTabVQA\
  \ comprises 3,745 complex question-answer pairs that necessitate multi-hop reasoning\
  \ across several visually rendered table images. We provide extensive benchmark\
  \ results for state-of-the-art VLMs on MTabVQA, revealing significant performance\
  \ limitations. We further investigate post-training techniques to enhance these\
  \ reasoning abilities and release MTabVQA-Instruct, a large-scale instruction-tuning\
  \ dataset. Our experiments show that fine-tuning VLMs with MTabVQA-Instruct substantially\
  \ improves their performance on visual multi-tabular reasoning. Code and dataset\
  \ (https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval) are available online (https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E)."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11684
---

Computer Science > Computer Vision and Pattern Recognition
[Submitted on 13 Jun 2025]
Title:MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space
View PDF HTML (experimental)Abstract:Vision-Language Models (VLMs) have demonstrated remarkable capabilities in interpreting visual layouts and text. However, a significant challenge remains in their ability to interpret robustly and reason over multi-tabular data presented as images, a common occurrence in real-world scenarios like web pages and digital documents. Existing benchmarks typically address single tables or non-visual data (text/structured). This leaves a critical gap: they don't assess the ability to parse diverse table images, correlate information across them, and perform multi-hop reasoning on the combined visual data. We introduce MTabVQA, a novel benchmark specifically designed for multi-tabular visual question answering to bridge that gap. MTabVQA comprises 3,745 complex question-answer pairs that necessitate multi-hop reasoning across several visually rendered table images. We provide extensive benchmark results for state-of-the-art VLMs on MTabVQA, revealing significant performance limitations. We further investigate post-training techniques to enhance these reasoning abilities and release MTabVQA-Instruct, a large-scale instruction-tuning dataset. Our experiments show that fine-tuning VLMs with MTabVQA-Instruct substantially improves their performance on visual multi-tabular reasoning. Code and dataset (this https URL) are available online (this https URL).
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
---
title: 'Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding
  and Reasoning'
description: "arXiv:2506.07044v4 Announce Type: replace-cross \nAbstract: Multimodal\
  \ Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding\
  \ common visual elements, largely due to their large-scale datasets and advanced\
  \ training strategies. However, their effectiveness in medical applications remains\
  \ limited due to the inherent discrepancies between data and tasks in medical scenarios\
  \ and those in the general domain. Concretely, existing medical MLLMs face the following\
  \ critical limitations: (1) limited coverage of medical knowledge beyond imaging,\
  \ (2) heightened susceptibility to hallucinations due to suboptimal data curation\
  \ processes, (3) lack of reasoning capabilities tailored for complex medical scenarios.\
  \ To address these challenges, we first propose a comprehensive data curation procedure\
  \ that (1) efficiently acquires rich medical knowledge data not only from medical\
  \ imaging but also from extensive medical texts and general-domain data; and (2)\
  \ synthesizes accurate medical captions, visual question answering (VQA), and reasoning\
  \ samples. As a result, we build a multimodal dataset enriched with extensive medical\
  \ knowledge. Building on the curated data, we introduce our medical-specialized\
  \ MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise\
  \ and enhance its task-solving capabilities progressively. Besides, we preliminarily\
  \ explore the potential of applying reinforcement learning with verifiable rewards\
  \ paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop\
  \ MedEvalKit, a unified evaluation framework that consolidates leading multimodal\
  \ and textual medical benchmarks for standardized, fair, and efficient model assessment.\
  \ We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal\
  \ QA, text-based QA, and medical report generation. The results show that Lingshu\
  \ consistently outperforms the existing open-source multimodal models on most tasks\
  \ ..."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.07044
---

Computer Science > Computation and Language
[Submitted on 8 Jun 2025 (v1), last revised 13 Jun 2025 (this version, v4)]
Title:Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning
View PDF HTML (experimental)Abstract:Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ...
Submission history
From: Weiwen Xu [view email][v1] Sun, 8 Jun 2025 08:47:30 UTC (5,004 KB)
[v2] Tue, 10 Jun 2025 07:38:24 UTC (5,003 KB)
[v3] Wed, 11 Jun 2025 09:12:18 UTC (4,851 KB)
[v4] Fri, 13 Jun 2025 04:22:02 UTC (4,907 KB)
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
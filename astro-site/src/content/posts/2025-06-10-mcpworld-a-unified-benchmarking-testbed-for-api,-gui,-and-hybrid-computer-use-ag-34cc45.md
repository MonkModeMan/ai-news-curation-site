---
title: 'MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer
  Use Agents'
description: "arXiv:2506.07672v1 Announce Type: new \nAbstract: (M)LLM-powered computer\
  \ use agents (CUA) are emerging as a transformative technique to automate human-computer\
  \ interaction. However, existing CUA benchmarks predominantly target GUI agents,\
  \ whose evaluation methods are susceptible to UI changes and ignore function interactions\
  \ exposed by application APIs, e.g., Model Context Protocol (MCP). To this end,\
  \ we propose MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI\
  \ hybrid agents. A key principle of MCPWorld is the use of \"white-box apps\", i.e.,\
  \ those with source code availability and can be revised/re-compiled as needed (e.g.,\
  \ adding MCP support), with two notable advantages:\n  (1) It greatly broadens the\
  \ design space of CUA, such as what and how the app features to be exposed/extracted\
  \ as CUA-callable APIs.\n  (2) It allows MCPWorld to programmatically verify task\
  \ completion by directly monitoring application behavior through techniques like\
  \ dynamic code instrumentation, offering robust, accurate CUA evaluation decoupled\
  \ from specific agent implementations or UI states.\n  Currently, MCPWorld includes\
  \ 201 well curated and annotated user tasks, covering diversified use cases and\
  \ difficulty levels. MCPWorld is also fully containerized with GPU acceleration\
  \ support for flexible adoption on different OS/hardware environments. Our preliminary\
  \ experiments, using a representative LLM-powered CUA framework, achieve 75.12%\
  \ task completion accuracy, simultaneously providing initial evidence on the practical\
  \ effectiveness of agent automation leveraging MCP. Overall, we anticipate MCPWorld\
  \ to facilitate and standardize the benchmarking of next-generation computer use\
  \ agents that can leverage rich external tools. Our code and dataset are publicly\
  \ available at https://github.com/SAAgent/MCPWorld."
pubDate: Tue, 10 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.07672
---

Computer Science > Artificial Intelligence
[Submitted on 9 Jun 2025]
Title:MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents
View PDF HTML (experimental)Abstract:(M)LLM-powered computer use agents (CUA) are emerging as a transformative technique to automate human-computer interaction. However, existing CUA benchmarks predominantly target GUI agents, whose evaluation methods are susceptible to UI changes and ignore function interactions exposed by application APIs, e.g., Model Context Protocol (MCP). To this end, we propose MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid agents. A key principle of MCPWorld is the use of "white-box apps", i.e., those with source code availability and can be revised/re-compiled as needed (e.g., adding MCP support), with two notable advantages:
(1) It greatly broadens the design space of CUA, such as what and how the app features to be exposed/extracted as CUA-callable APIs.
(2) It allows MCPWorld to programmatically verify task completion by directly monitoring application behavior through techniques like dynamic code instrumentation, offering robust, accurate CUA evaluation decoupled from specific agent implementations or UI states.
Currently, MCPWorld includes 201 well curated and annotated user tasks, covering diversified use cases and difficulty levels. MCPWorld is also fully containerized with GPU acceleration support for flexible adoption on different OS/hardware environments. Our preliminary experiments, using a representative LLM-powered CUA framework, achieve 75.12% task completion accuracy, simultaneously providing initial evidence on the practical effectiveness of agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate and standardize the benchmarking of next-generation computer use agents that can leverage rich external tools. Our code and dataset are publicly available at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
---
title: "Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens tokens and 11 languages"
description: ""
summary: "Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 la..."
pubDate: "Fri, 24 May 2024 00:00:00 GMT"
source: "Hugging Face Blog"
url: "https://huggingface.co/blog/falcon2-11b"
thumbnail: "https://huggingface.co/blog/assets/179_falcon2-11b/thumbnail.jpg"
---


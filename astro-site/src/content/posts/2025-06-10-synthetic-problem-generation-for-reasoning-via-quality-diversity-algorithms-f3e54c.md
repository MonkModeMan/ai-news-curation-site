---
title: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms
description: "arXiv:2506.06499v1 Announce Type: cross \nAbstract: Large language model\
  \ (LLM) driven synthetic data generation has emerged as a powerful method for improving\
  \ model reasoning capabilities. However, most methods either distill large state-of-the-art\
  \ models into small students or use natural ground-truth problem statements to guarantee\
  \ problem statement quality. This limits the scalability of these approaches to\
  \ more complex and diverse problem domains. To address this, we present SPARQ: Synthetic\
  \ Problem Generation for Reasoning via Quality-Diversity Algorithms, a novel approach\
  \ for generating high-quality and diverse synthetic math problem and solution pairs\
  \ using only a single model by measuring a problem's solve-rate: a proxy for problem\
  \ difficulty. Starting from a seed dataset of 7.5K samples, we generate over 20\
  \ million new problem-solution pairs. We show that filtering the generated data\
  \ by difficulty and then fine-tuning the same model on the resulting data improves\
  \ relative model performance by up to 24\\%. Additionally, we conduct ablations\
  \ studying the impact of synthetic data quantity, quality and diversity on model\
  \ generalization. We find that higher quality, as measured by problem difficulty,\
  \ facilitates better in-distribution performance. Further, while generating diverse\
  \ synthetic data does not as strongly benefit in-distribution performance, filtering\
  \ for more diverse data facilitates more robust OOD generalization. We also confirm\
  \ the existence of model and data scaling laws for synthetically generated problems,\
  \ which positively benefit downstream model generalization."
pubDate: Tue, 10 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.06499
---

Computer Science > Machine Learning
[Submitted on 6 Jun 2025]
Title:Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms
View PDF HTML (experimental)Abstract:Large language model (LLM) driven synthetic data generation has emerged as a powerful method for improving model reasoning capabilities. However, most methods either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality. This limits the scalability of these approaches to more complex and diverse problem domains. To address this, we present SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for generating high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate: a proxy for problem difficulty. Starting from a seed dataset of 7.5K samples, we generate over 20 million new problem-solution pairs. We show that filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24\%. Additionally, we conduct ablations studying the impact of synthetic data quantity, quality and diversity on model generalization. We find that higher quality, as measured by problem difficulty, facilitates better in-distribution performance. Further, while generating diverse synthetic data does not as strongly benefit in-distribution performance, filtering for more diverse data facilitates more robust OOD generalization. We also confirm the existence of model and data scaling laws for synthetically generated problems, which positively benefit downstream model generalization.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
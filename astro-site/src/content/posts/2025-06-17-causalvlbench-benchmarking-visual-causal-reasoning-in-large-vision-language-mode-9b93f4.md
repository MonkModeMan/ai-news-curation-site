---
title: 'CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language
  Models'
description: "arXiv:2506.11034v1 Announce Type: cross \nAbstract: Large language models\
  \ (LLMs) have shown remarkable ability in various language tasks, especially with\
  \ their emergent in-context learning capability. Extending LLMs to incorporate visual\
  \ inputs, large vision-language models (LVLMs) have shown impressive performance\
  \ in tasks such as recognition and visual question answering (VQA). Despite increasing\
  \ interest in the utility of LLMs in causal reasoning tasks such as causal discovery\
  \ and counterfactual reasoning, there has been relatively little work showcasing\
  \ the abilities of LVLMs on visual causal reasoning tasks. We take this opportunity\
  \ to formally introduce a comprehensive causal reasoning benchmark for multi-modal\
  \ in-context learning from LVLMs. Our CausalVLBench encompasses three representative\
  \ tasks: causal structure inference, intervention target prediction, and counterfactual\
  \ prediction. We evaluate the ability of state-of-the-art open-source LVLMs on our\
  \ causal reasoning tasks across three causal representation learning datasets and\
  \ demonstrate their fundamental strengths and weaknesses. We hope that our benchmark\
  \ elucidates the drawbacks of existing vision-language models and motivates new\
  \ directions and paradigms in improving the visual causal reasoning abilities of\
  \ LVLMs."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11034
---

Computer Science > Machine Learning
[Submitted on 21 May 2025]
Title:CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models
View PDF HTML (experimental)Abstract:Large language models (LLMs) have shown remarkable ability in various language tasks, especially with their emergent in-context learning capability. Extending LLMs to incorporate visual inputs, large vision-language models (LVLMs) have shown impressive performance in tasks such as recognition and visual question answering (VQA). Despite increasing interest in the utility of LLMs in causal reasoning tasks such as causal discovery and counterfactual reasoning, there has been relatively little work showcasing the abilities of LVLMs on visual causal reasoning tasks. We take this opportunity to formally introduce a comprehensive causal reasoning benchmark for multi-modal in-context learning from LVLMs. Our CausalVLBench encompasses three representative tasks: causal structure inference, intervention target prediction, and counterfactual prediction. We evaluate the ability of state-of-the-art open-source LVLMs on our causal reasoning tasks across three causal representation learning datasets and demonstrate their fundamental strengths and weaknesses. We hope that our benchmark elucidates the drawbacks of existing vision-language models and motivates new directions and paradigms in improving the visual causal reasoning abilities of LVLMs.
Current browse context:
cs.LG
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
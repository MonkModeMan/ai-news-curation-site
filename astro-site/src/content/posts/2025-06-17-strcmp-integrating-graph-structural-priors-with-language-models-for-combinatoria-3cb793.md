---
title: 'STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial
  Optimization'
description: "arXiv:2506.11057v1 Announce Type: cross \nAbstract: Combinatorial optimization\
  \ (CO) problems, central to operation research and theoretical computer science,\
  \ present significant computational challenges due to their NP-hard nature. While\
  \ large language models (LLMs) have emerged as promising tools for CO--either by\
  \ directly generating solutions or synthesizing solver-specific codes--existing\
  \ approaches often neglect critical structural priors inherent to CO problems, leading\
  \ to suboptimality and iterative inefficiency. Inspired by human experts' success\
  \ in leveraging CO structures for algorithm design, we propose STRCMP, a novel structure-aware\
  \ LLM-based algorithm discovery framework that systematically integrates structure\
  \ priors to enhance solution quality and solving efficiency. Our framework combines\
  \ a graph neural network (GNN) for extracting structural embeddings from CO instances\
  \ with an LLM conditioned on these embeddings to identify high-performing algorithms\
  \ in the form of solver-specific codes. This composite architecture ensures syntactic\
  \ correctness, preserves problem topology, and aligns with natural language objectives,\
  \ while an evolutionary refinement process iteratively optimizes generated algorithm.\
  \ Extensive evaluations across Mixed Integer Linear Programming and Boolean Satisfiability\
  \ problems, using nine benchmark datasets, demonstrate that our proposed STRCMP\
  \ outperforms five strong neural and LLM-based methods by a large margin, in terms\
  \ of both solution optimality and computational efficiency. The code and learned\
  \ model will be publicly available upon the acceptance of the paper."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11057
---

Computer Science > Machine Learning
[Submitted on 22 May 2025]
Title:STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization
View PDF HTML (experimental)Abstract:Combinatorial optimization (CO) problems, central to operation research and theoretical computer science, present significant computational challenges due to their NP-hard nature. While large language models (LLMs) have emerged as promising tools for CO--either by directly generating solutions or synthesizing solver-specific codes--existing approaches often neglect critical structural priors inherent to CO problems, leading to suboptimality and iterative inefficiency. Inspired by human experts' success in leveraging CO structures for algorithm design, we propose STRCMP, a novel structure-aware LLM-based algorithm discovery framework that systematically integrates structure priors to enhance solution quality and solving efficiency. Our framework combines a graph neural network (GNN) for extracting structural embeddings from CO instances with an LLM conditioned on these embeddings to identify high-performing algorithms in the form of solver-specific codes. This composite architecture ensures syntactic correctness, preserves problem topology, and aligns with natural language objectives, while an evolutionary refinement process iteratively optimizes generated algorithm. Extensive evaluations across Mixed Integer Linear Programming and Boolean Satisfiability problems, using nine benchmark datasets, demonstrate that our proposed STRCMP outperforms five strong neural and LLM-based methods by a large margin, in terms of both solution optimality and computational efficiency. The code and learned model will be publicly available upon the acceptance of the paper.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
---
title: 'LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?'
description: "arXiv:2506.11928v1 Announce Type: cross \nAbstract: Recent reports claim\
  \ that large language models (LLMs) now outperform elite humans in competitive programming.\
  \ Drawing on knowledge from a group of medalists in international algorithmic contests,\
  \ we revisit this claim, examining how LLMs differ from human experts and where\
  \ limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed\
  \ of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce\
  \ the likelihood of data contamination. A team of Olympiad medalists annotates every\
  \ problem for algorithmic categories and conducts a line-by-line analysis of failed\
  \ model-generated submissions. Using this new data and benchmark, we find that frontier\
  \ models still have significant limitations: without external tools, the best model\
  \ achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems,\
  \ domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy\
  \ problems but struggle with nuanced algorithmic reasoning and complex case analysis,\
  \ often generating confidently incorrect justifications. High performance appears\
  \ largely driven by implementation precision and tool augmentation, not superior\
  \ reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster\
  \ levels, while offering fine-grained diagnostics to steer future improvements in\
  \ code-centric LLM reasoning."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11928
---

Computer Science > Software Engineering
[Submitted on 13 Jun 2025]
Title:LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?
View PDFAbstract:Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.
Current browse context:
cs.SE
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
---
title: 'CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval'
description: "arXiv:2506.11066v1 Announce Type: cross \nAbstract: Code retrieval is\
  \ essential in modern software development, as it boosts code reuse and accelerates\
  \ debugging. However, current benchmarks primarily emphasize functional relevance\
  \ while neglecting critical dimensions of software quality. Motivated by this gap,\
  \ we introduce CoQuIR, the first large-scale, multilingual benchmark specifically\
  \ designed to evaluate quality-aware code retrieval across four key dimensions:\
  \ correctness, efficiency, security, and maintainability. CoQuIR provides fine-grained\
  \ quality annotations for 42,725 queries and 134,907 code snippets in 11 programming\
  \ languages, and is accompanied by two quality-centric evaluation metrics: Pairwise\
  \ Preference Accuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark\
  \ 23 retrieval models, covering both open-source and proprietary systems, and find\
  \ that even top-performing models frequently fail to distinguish buggy or insecure\
  \ code from their more robust counterparts. Furthermore, we conduct preliminary\
  \ investigations into training methods that explicitly encourage retrievers to recognize\
  \ code quality. Using synthetic datasets, we demonstrate promising improvements\
  \ in quality-aware metrics across various models, without sacrificing semantic relevance.\
  \ Downstream code generation experiments further validate the effectiveness of our\
  \ approach. Overall, our work highlights the importance of integrating quality signals\
  \ into code retrieval systems, laying the groundwork for more trustworthy and robust\
  \ software development tools."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11066
---

Computer Science > Software Engineering
[Submitted on 31 May 2025]
Title:CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval
View PDF HTML (experimental)Abstract:Code retrieval is essential in modern software development, as it boosts code reuse and accelerates debugging. However, current benchmarks primarily emphasize functional relevance while neglecting critical dimensions of software quality. Motivated by this gap, we introduce CoQuIR, the first large-scale, multilingual benchmark specifically designed to evaluate quality-aware code retrieval across four key dimensions: correctness, efficiency, security, and maintainability. CoQuIR provides fine-grained quality annotations for 42,725 queries and 134,907 code snippets in 11 programming languages, and is accompanied by two quality-centric evaluation metrics: Pairwise Preference Accuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23 retrieval models, covering both open-source and proprietary systems, and find that even top-performing models frequently fail to distinguish buggy or insecure code from their more robust counterparts. Furthermore, we conduct preliminary investigations into training methods that explicitly encourage retrievers to recognize code quality. Using synthetic datasets, we demonstrate promising improvements in quality-aware metrics across various models, without sacrificing semantic relevance. Downstream code generation experiments further validate the effectiveness of our approach. Overall, our work highlights the importance of integrating quality signals into code retrieval systems, laying the groundwork for more trustworthy and robust software development tools.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
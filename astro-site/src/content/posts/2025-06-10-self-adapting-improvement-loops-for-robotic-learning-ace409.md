---
title: Self-Adapting Improvement Loops for Robotic Learning
description: "arXiv:2506.06658v1 Announce Type: cross \nAbstract: Video generative\
  \ models trained on expert demonstrations have been utilized as performant text-conditioned\
  \ visual planners for solving robotic tasks. However, generalization to unseen tasks\
  \ remains a challenge. Whereas improved generalization may be facilitated by leveraging\
  \ learned prior knowledge from additional pre-collected offline data sources, such\
  \ as web-scale video datasets, in the era of experience we aim to design agents\
  \ that can continuously improve in an online manner from self-collected behaviors.\
  \ In this work we thus propose the Self-Adapting Improvement Loop (SAIL), where\
  \ an in-domain video model iteratively updates itself on self-produced trajectories,\
  \ collected through adaptation with an internet-scale pretrained video model, and\
  \ steadily improves its performance for a specified task of interest. We apply SAIL\
  \ to a diverse suite of MetaWorld tasks, as well as two manipulation tasks on a\
  \ real robot arm, and find that performance improvements continuously emerge over\
  \ multiple iterations for novel tasks initially unseen during original in-domain\
  \ video model training. Furthermore, we discover that SAIL is surprisingly robust\
  \ regarding if and how the self-collected experience is filtered, and the quality\
  \ of the initial in-domain demonstrations. Through adaptation with summarized internet-scale\
  \ data, and learning through online experience, we thus demonstrate a way to iteratively\
  \ bootstrap a high-performance video model for solving novel robotic tasks through\
  \ self-improvement."
pubDate: Tue, 10 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.06658
---

Computer Science > Robotics
[Submitted on 7 Jun 2025]
Title:Self-Adapting Improvement Loops for Robotic Learning
View PDF HTML (experimental)Abstract:Video generative models trained on expert demonstrations have been utilized as performant text-conditioned visual planners for solving robotic tasks. However, generalization to unseen tasks remains a challenge. Whereas improved generalization may be facilitated by leveraging learned prior knowledge from additional pre-collected offline data sources, such as web-scale video datasets, in the era of experience we aim to design agents that can continuously improve in an online manner from self-collected behaviors. In this work we thus propose the Self-Adapting Improvement Loop (SAIL), where an in-domain video model iteratively updates itself on self-produced trajectories, collected through adaptation with an internet-scale pretrained video model, and steadily improves its performance for a specified task of interest. We apply SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks on a real robot arm, and find that performance improvements continuously emerge over multiple iterations for novel tasks initially unseen during original in-domain video model training. Furthermore, we discover that SAIL is surprisingly robust regarding if and how the self-collected experience is filtered, and the quality of the initial in-domain demonstrations. Through adaptation with summarized internet-scale data, and learning through online experience, we thus demonstrate a way to iteratively bootstrap a high-performance video model for solving novel robotic tasks through self-improvement.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
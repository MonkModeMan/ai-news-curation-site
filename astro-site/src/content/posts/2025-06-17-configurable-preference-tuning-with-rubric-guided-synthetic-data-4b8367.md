---
title: Configurable Preference Tuning with Rubric-Guided Synthetic Data
description: "arXiv:2506.11702v1 Announce Type: cross \nAbstract: Models of human\
  \ feedback for AI alignment, such as those underpinning Direct Preference Optimization\
  \ (DPO), often bake in a singular, static set of preferences, limiting adaptability.\
  \ This paper challenges the assumption of monolithic preferences by introducing\
  \ Configurable Preference Tuning (CPT), a novel framework for endowing language\
  \ models with the ability to dynamically adjust their behavior based on explicit,\
  \ human-interpretable directives. CPT leverages synthetically generated preference\
  \ data, conditioned on system prompts derived from structured, fine-grained rubrics\
  \ that define desired attributes like writing style. By fine-tuning with these rubric-guided\
  \ preferences, the LLM learns to modulate its outputs at inference time in response\
  \ to the system prompt, without retraining. This approach not only offers fine-grained\
  \ control but also provides a mechanism for modeling more nuanced and context-dependent\
  \ human feedback. Several experimental artifacts, such as training code, generated\
  \ datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning"
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11702
---

Computer Science > Computation and Language
[Submitted on 13 Jun 2025]
Title:Configurable Preference Tuning with Rubric-Guided Synthetic Data
View PDF HTML (experimental)Abstract:Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at this https URL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
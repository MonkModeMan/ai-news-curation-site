---
title: 'TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and
  Structure Attacks'
description: "arXiv:2506.11844v1 Announce Type: cross \nAbstract: Inspired by the\
  \ success of large language models (LLMs), there is a significant research shift\
  \ from traditional graph learning methods to LLM-based graph frameworks, formally\
  \ known as GraphLLMs. GraphLLMs leverage the reasoning power of LLMs by integrating\
  \ three key components: the textual attributes of input nodes, the structural information\
  \ of node neighborhoods, and task-specific prompts that guide decision-making. Despite\
  \ their promise, the robustness of GraphLLMs against adversarial perturbations remains\
  \ largely unexplored-a critical concern for deploying these models in high-stakes\
  \ scenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study evaluating\
  \ the vulnerability of GraphLLMs to adversarial attacks across three dimensions:\
  \ text, graph structure, and prompt manipulations. We implement state-of-the-art\
  \ attack algorithms from each perspective to rigorously assess model resilience.\
  \ Through extensive experiments on six benchmark datasets from diverse domains,\
  \ our findings reveal that GraphLLMs are highly susceptible to text attacks that\
  \ merely replace a few semantically similar words in a node's textual attribute.\
  \ We also find that standard graph structure attack methods can significantly degrade\
  \ model performance, while random shuffling of the candidate label set in prompt\
  \ templates leads to substantial performance drops. Beyond characterizing these\
  \ vulnerabilities, we investigate defense techniques tailored to each attack vector\
  \ through data-augmented training and adversarial training, which show promising\
  \ potential to enhance the robustness of GraphLLMs. We hope that our open-sourced\
  \ library will facilitate rapid, equitable evaluation and inspire further innovative\
  \ research in this field."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11844
---

Computer Science > Machine Learning
[Submitted on 13 Jun 2025]
Title:TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks
View PDF HTML (experimental)Abstract:Inspired by the success of large language models (LLMs), there is a significant research shift from traditional graph learning methods to LLM-based graph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning power of LLMs by integrating three key components: the textual attributes of input nodes, the structural information of node neighborhoods, and task-specific prompts that guide decision-making. Despite their promise, the robustness of GraphLLMs against adversarial perturbations remains largely unexplored-a critical concern for deploying these models in high-stakes scenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study evaluating the vulnerability of GraphLLMs to adversarial attacks across three dimensions: text, graph structure, and prompt manipulations. We implement state-of-the-art attack algorithms from each perspective to rigorously assess model resilience. Through extensive experiments on six benchmark datasets from diverse domains, our findings reveal that GraphLLMs are highly susceptible to text attacks that merely replace a few semantically similar words in a node's textual attribute. We also find that standard graph structure attack methods can significantly degrade model performance, while random shuffling of the candidate label set in prompt templates leads to substantial performance drops. Beyond characterizing these vulnerabilities, we investigate defense techniques tailored to each attack vector through data-augmented training and adversarial training, which show promising potential to enhance the robustness of GraphLLMs. We hope that our open-sourced library will facilitate rapid, equitable evaluation and inspire further innovative research in this field.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
---
title: 'Beyond the Visible: Multispectral Vision-Language Learning for Earth Observation'
description: "arXiv:2503.15969v2 Announce Type: replace-cross \nAbstract: Vision-language\
  \ models for Earth observation (EO) typically rely on the visual spectrum of data\
  \ as the only model input, thus failing to leverage the rich spectral information\
  \ available in the multispectral channels recorded by satellites. Therefore, we\
  \ introduce Llama3-MS-CLIP, the first vision-language model pre-trained with contrastive\
  \ learning on a large-scale multispectral dataset and report on the performance\
  \ gains due to the extended spectral range. Furthermore, we present the largest-to-date\
  \ image-caption dataset for multispectral data, consisting of one million Sentinel-2\
  \ samples and corresponding textual descriptions generated using Llama3-LLaVA-Next\
  \ and Overture Maps data. We develop a scalable captioning pipeline, which is validated\
  \ by domain experts. We evaluate Llama3-MS-CLIP on multispectral zero-shot image\
  \ classification and retrieval using three datasets of varying complexity. Our results\
  \ demonstrate that Llama3-MS-CLIP significantly outperforms other RGB-based approaches,\
  \ improving classification accuracy by +6.77% on average and retrieval performance\
  \ by +4.63% mAP compared to the second-best model. Our results emphasize the relevance\
  \ of multispectral vision-language learning. The image-caption dataset, code, and\
  \ model weights are available at https://github.com/IBM/MS-CLIP."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2503.15969
---

Computer Science > Computer Vision and Pattern Recognition
[Submitted on 20 Mar 2025 (v1), last revised 13 Jun 2025 (this version, v2)]
Title:Beyond the Visible: Multispectral Vision-Language Learning for Earth Observation
View PDF HTML (experimental)Abstract:Vision-language models for Earth observation (EO) typically rely on the visual spectrum of data as the only model input, thus failing to leverage the rich spectral information available in the multispectral channels recorded by satellites. Therefore, we introduce Llama3-MS-CLIP, the first vision-language model pre-trained with contrastive learning on a large-scale multispectral dataset and report on the performance gains due to the extended spectral range. Furthermore, we present the largest-to-date image-caption dataset for multispectral data, consisting of one million Sentinel-2 samples and corresponding textual descriptions generated using Llama3-LLaVA-Next and Overture Maps data. We develop a scalable captioning pipeline, which is validated by domain experts. We evaluate Llama3-MS-CLIP on multispectral zero-shot image classification and retrieval using three datasets of varying complexity. Our results demonstrate that Llama3-MS-CLIP significantly outperforms other RGB-based approaches, improving classification accuracy by +6.77% on average and retrieval performance by +4.63% mAP compared to the second-best model. Our results emphasize the relevance of multispectral vision-language learning. The image-caption dataset, code, and model weights are available at this https URL.
Submission history
From: Benedikt Blumenstiel [view email][v1] Thu, 20 Mar 2025 09:13:31 UTC (1,560 KB)
[v2] Fri, 13 Jun 2025 11:24:09 UTC (1,200 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
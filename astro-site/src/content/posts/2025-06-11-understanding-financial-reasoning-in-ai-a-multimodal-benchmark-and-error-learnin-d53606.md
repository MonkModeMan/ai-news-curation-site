---
title: 'Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error
  Learning Approach'
description: "arXiv:2506.06282v1 Announce Type: new \nAbstract: Effective financial\
  \ reasoning demands not only textual understanding but also the ability to interpret\
  \ complex visual data such as charts, tables, and trend graphs. This paper introduces\
  \ a new benchmark designed to evaluate how well AI models - especially large language\
  \ and multimodal models - reason in finance-specific contexts. Covering 3,200 expert-level\
  \ question-answer pairs across 15 core financial topics, the benchmark integrates\
  \ both textual and visual modalities to reflect authentic analytical challenges\
  \ in finance. To address limitations in current reasoning approaches, we propose\
  \ an error-aware learning framework that leverages historical model mistakes and\
  \ feedback to guide inference, without requiring fine-tuning. Our experiments across\
  \ state-of-the-art models show that multimodal inputs significantly enhance performance\
  \ and that incorporating error feedback leads to consistent and measurable improvements.\
  \ The results highlight persistent challenges in visual understanding and mathematical\
  \ logic, while also demonstrating the promise of self-reflective reasoning in financial\
  \ AI systems. Our code and data can be found at https://anonymous/FinMR/CodeData."
pubDate: Tue, 10 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.06282
---

Computer Science > Artificial Intelligence
[Submitted on 22 Apr 2025]
Title:Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach
View PDF HTML (experimental)Abstract:Effective financial reasoning demands not only textual understanding but also the ability to interpret complex visual data such as charts, tables, and trend graphs. This paper introduces a new benchmark designed to evaluate how well AI models - especially large language and multimodal models - reason in finance-specific contexts. Covering 3,200 expert-level question-answer pairs across 15 core financial topics, the benchmark integrates both textual and visual modalities to reflect authentic analytical challenges in finance. To address limitations in current reasoning approaches, we propose an error-aware learning framework that leverages historical model mistakes and feedback to guide inference, without requiring fine-tuning. Our experiments across state-of-the-art models show that multimodal inputs significantly enhance performance and that incorporating error feedback leads to consistent and measurable improvements. The results highlight persistent challenges in visual understanding and mathematical logic, while also demonstrating the promise of self-reflective reasoning in financial AI systems. Our code and data can be found at https://anonymous/FinMR/CodeData.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
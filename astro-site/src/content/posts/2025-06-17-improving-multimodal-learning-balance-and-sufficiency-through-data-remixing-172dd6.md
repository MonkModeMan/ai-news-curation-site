---
title: Improving Multimodal Learning Balance and Sufficiency through Data Remixing
description: "arXiv:2506.11550v1 Announce Type: cross \nAbstract: Different modalities\
  \ hold considerable gaps in optimization trajectories, including speeds and paths,\
  \ which lead to modality laziness and modality clash when jointly training multimodal\
  \ models, resulting in insufficient and imbalanced multimodal learning. Existing\
  \ methods focus on enforcing the weak modality by adding modality-specific optimization\
  \ objectives, aligning their optimization speeds, or decomposing multimodal learning\
  \ to enhance unimodal learning. These methods fail to achieve both unimodal sufficiency\
  \ and multimodal balance. In this paper, we, for the first time, address both concerns\
  \ by proposing multimodal Data Remixing, including decoupling multimodal data and\
  \ filtering hard samples for each modality to mitigate modality imbalance; and then\
  \ batch-level reassembling to align the gradient directions and avoid cross-modal\
  \ interference, thus enhancing unimodal learning sufficiency. Experimental results\
  \ demonstrate that our method can be seamlessly integrated with existing approaches,\
  \ improving accuracy by approximately 6.50%$\\uparrow$ on CREMAD and 3.41%$\\uparrow$\
  \ on Kinetic-Sounds, without training set expansion or additional computational\
  \ overhead during inference. The source code is available at \\href{https://github.com/MatthewMaxy/Remix_ICML2025}{Data\
  \ Remixing}."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11550
---

Computer Science > Machine Learning
[Submitted on 13 Jun 2025 (v1), last revised 16 Jun 2025 (this version, v2)]
Title:Improving Multimodal Learning Balance and Sufficiency through Data Remixing
View PDF HTML (experimental)Abstract:Different modalities hold considerable gaps in optimization trajectories, including speeds and paths, which lead to modality laziness and modality clash when jointly training multimodal models, resulting in insufficient and imbalanced multimodal learning. Existing methods focus on enforcing the weak modality by adding modality-specific optimization objectives, aligning their optimization speeds, or decomposing multimodal learning to enhance unimodal learning. These methods fail to achieve both unimodal sufficiency and multimodal balance. In this paper, we, for the first time, address both concerns by proposing multimodal Data Remixing, including decoupling multimodal data and filtering hard samples for each modality to mitigate modality imbalance; and then batch-level reassembling to align the gradient directions and avoid cross-modal interference, thus enhancing unimodal learning sufficiency. Experimental results demonstrate that our method can be seamlessly integrated with existing approaches, improving accuracy by approximately 6.50%$\uparrow$ on CREMAD and 3.41%$\uparrow$ on Kinetic-Sounds, without training set expansion or additional computational overhead during inference. The source code is available at this https URL.
Submission history
From: Xiaoyu Ma [view email][v1] Fri, 13 Jun 2025 08:01:29 UTC (1,005 KB)
[v2] Mon, 16 Jun 2025 02:50:29 UTC (1,005 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
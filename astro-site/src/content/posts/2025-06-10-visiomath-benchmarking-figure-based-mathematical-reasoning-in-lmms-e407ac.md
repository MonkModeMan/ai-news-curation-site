---
title: 'VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs'
description: "arXiv:2506.06727v1 Announce Type: new \nAbstract: Large Multimodal Models\
  \ (LMMs) have demonstrated remarkable problem-solving capabilities across various\
  \ domains. However, their ability to perform mathematical reasoning when answer\
  \ options are represented as images--an essential aspect of multi-image comprehension--remains\
  \ underexplored. To bridge this gap, we introduce VisioMath, a benchmark designed\
  \ to evaluate mathematical reasoning in multimodal contexts involving image-based\
  \ answer choices. VisioMath comprises 8,070 images and 1,800 multiple-choice questions,\
  \ where each answer option is an image, presenting unique challenges to existing\
  \ LMMs. To the best of our knowledge, VisioMath is the first dataset specifically\
  \ tailored for mathematical reasoning in image-based-option scenarios, where fine-grained\
  \ distinctions between answer choices are critical for accurate problem-solving.\
  \ We systematically evaluate state-of-the-art LMMs on VisioMath and find that even\
  \ the most advanced models struggle with this task. Notably, GPT-4o achieves only\
  \ 45.9% accuracy, underscoring the limitations of current models in reasoning over\
  \ visually similar answer choices. By addressing a crucial gap in existing benchmarks,\
  \ VisioMath establishes a rigorous testbed for future research, driving advancements\
  \ in multimodal reasoning."
pubDate: Tue, 10 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.06727
---

Computer Science > Artificial Intelligence
[Submitted on 7 Jun 2025]
Title:VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs
View PDF HTML (experimental)Abstract:Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving capabilities across various domains. However, their ability to perform mathematical reasoning when answer options are represented as images--an essential aspect of multi-image comprehension--remains underexplored. To bridge this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical reasoning in multimodal contexts involving image-based answer choices. VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where each answer option is an image, presenting unique challenges to existing LMMs. To the best of our knowledge, VisioMath is the first dataset specifically tailored for mathematical reasoning in image-based-option scenarios, where fine-grained distinctions between answer choices are critical for accurate problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath and find that even the most advanced models struggle with this task. Notably, GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current models in reasoning over visually similar answer choices. By addressing a crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed for future research, driving advancements in multimodal reasoning.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
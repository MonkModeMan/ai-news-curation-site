---
title: 'The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning
  Models via the Lens of Problem Complexity'
description: "arXiv:2506.06941v1 Announce Type: new \nAbstract: Recent generations\
  \ of language models have introduced Large Reasoning Models (LRMs) that generate\
  \ detailed thinking processes before providing answers. While these models demonstrate\
  \ improved performance on reasoning benchmarks, their fundamental capabilities,\
  \ scaling properties, and limitations remain insufficiently understood. Current\
  \ evaluations primarily focus on established math and coding benchmarks, emphasizing\
  \ final answer accuracy. However, this evaluation paradigm often suffers from contamination\
  \ and does not provide insights into the reasoning traces. In this work, we systematically\
  \ investigate these gaps with the help of controllable puzzle environments that\
  \ allow precise manipulation of complexity while maintaining consistent logical\
  \ structures. This setup enables the analysis of not only final answers but also\
  \ the internal reasoning traces, offering insights into how LRMs think. Through\
  \ extensive experiments, we show that LRMs face a complete accuracy collapse beyond\
  \ certain complexities. Moreover, they exhibit a counterintuitive scaling limit:\
  \ their reasoning effort increases with problem complexity up to a point, then declines\
  \ despite having remaining token budget. By comparing LRMs with their standard LLM\
  \ counterparts under same inference compute, we identify three performance regimes:\
  \ (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity\
  \ tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both\
  \ models face complete collapse. We found that LRMs have limitations in exact computation:\
  \ they fail to use explicit algorithms and reason inconsistently across scales.\
  \ We also investigate the reasoning traces in more depth, studying the patterns\
  \ of explored solutions and analyzing the models' computational behavior, shedding\
  \ light on their strengths, limitations, and raising questions about their reasoning\
  \ capabilities."
pubDate: Tue, 10 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.06941
---

Computer Science > Artificial Intelligence
[Submitted on 7 Jun 2025]
Title:The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity
View PDFAbstract:Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.
Current browse context:
cs.AI
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
---
title: How Malicious AI Swarms Can Threaten Democracy
description: "arXiv:2506.06299v1 Announce Type: cross \nAbstract: Advances in AI portend\
  \ a new era of sophisticated disinformation operations. While individual AI systems\
  \ already create convincing -- and at times misleading -- information, an imminent\
  \ development is the emergence of malicious AI swarms. These systems can coordinate\
  \ covertly, infiltrate communities, evade traditional detectors, and run continuous\
  \ A/B tests, with round-the-clock persistence. The result can include fabricated\
  \ grassroots consensus, fragmented shared reality, mass harassment, voter micro-suppression\
  \ or mobilization, contamination of AI training data, and erosion of institutional\
  \ trust. With democratic processes worldwide increasingly vulnerable, we urge a\
  \ three-pronged response: (1) platform-side defenses -- always-on swarm-detection\
  \ dashboards, pre-election high-fidelity swarm-simulation stress-tests, transparency\
  \ audits, and optional client-side \"AI shields\" for users; (2) model-side safeguards\
  \ -- standardized persuasion-risk tests, provenance-authenticating passkeys, and\
  \ watermarking; and (3) system-level oversight -- a UN-backed AI Influence Observatory."
pubDate: Tue, 10 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.06299
---

Computer Science > Computers and Society
[Submitted on 18 May 2025]
Title:How Malicious AI Swarms Can Threaten Democracy
View PDF HTML (experimental)Abstract:Advances in AI portend a new era of sophisticated disinformation operations. While individual AI systems already create convincing -- and at times misleading -- information, an imminent development is the emergence of malicious AI swarms. These systems can coordinate covertly, infiltrate communities, evade traditional detectors, and run continuous A/B tests, with round-the-clock persistence. The result can include fabricated grassroots consensus, fragmented shared reality, mass harassment, voter micro-suppression or mobilization, contamination of AI training data, and erosion of institutional trust. With democratic processes worldwide increasingly vulnerable, we urge a three-pronged response: (1) platform-side defenses -- always-on swarm-detection dashboards, pre-election high-fidelity swarm-simulation stress-tests, transparency audits, and optional client-side "AI shields" for users; (2) model-side safeguards -- standardized persuasion-risk tests, provenance-authenticating passkeys, and watermarking; and (3) system-level oversight -- a UN-backed AI Influence Observatory.
Current browse context:
cs.CY
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
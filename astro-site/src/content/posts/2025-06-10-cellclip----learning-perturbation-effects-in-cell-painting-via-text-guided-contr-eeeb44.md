---
title: CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided
  Contrastive Learning
description: "arXiv:2506.06290v1 Announce Type: cross \nAbstract: High-content screening\
  \ (HCS) assays based on high-throughput microscopy techniques such as Cell Painting\
  \ have enabled the interrogation of cells' morphological responses to perturbations\
  \ at an unprecedented scale. The collection of such data promises to facilitate\
  \ a better understanding of the relationships between different perturbations and\
  \ their effects on cellular state. Towards achieving this goal, recent advances\
  \ in cross-modal contrastive learning could, in theory, be leveraged to learn a\
  \ unified latent space that aligns perturbations with their corresponding morphological\
  \ effects. However, the application of such methods to HCS data is not straightforward\
  \ due to substantial differences in the semantics of Cell Painting images compared\
  \ to natural images, and the difficulty of representing different classes of perturbations\
  \ (e.g., small molecule vs CRISPR gene knockout) in a single latent space. In response\
  \ to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning\
  \ framework for HCS data. CellCLIP leverages pre-trained image encoders coupled\
  \ with a novel channel encoding scheme to better capture relationships between different\
  \ microscopy channels in image embeddings, along with natural language encoders\
  \ for representing perturbations. Our framework outperforms current open-source\
  \ models, demonstrating the best performance in both cross-modal retrieval and biologically\
  \ meaningful downstream tasks while also achieving significant reductions in computation\
  \ time."
pubDate: Tue, 10 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.06290
---

Computer Science > Machine Learning
[Submitted on 16 May 2025]
Title:CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning
View PDF HTML (experimental)Abstract:High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells' morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.
Current browse context:
cs.LG
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
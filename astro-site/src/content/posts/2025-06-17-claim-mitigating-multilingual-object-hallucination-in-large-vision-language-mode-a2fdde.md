---
title: 'CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language
  Models with Cross-Lingual Attention Intervention'
description: "arXiv:2506.11073v1 Announce Type: cross \nAbstract: Large Vision-Language\
  \ Models (LVLMs) have demonstrated impressive multimodal abilities but remain prone\
  \ to multilingual object hallucination, with a higher likelihood of generating responses\
  \ inconsistent with the visual input when utilizing queries in non-English languages\
  \ compared to English. Most existing approaches to address these rely on pretraining\
  \ or fine-tuning, which are resource-intensive. In this paper, inspired by observing\
  \ the disparities in cross-modal attention patterns across languages, we propose\
  \ Cross-Lingual Attention Intervention for Mitigating multilingual object hallucination\
  \ (CLAIM) in LVLMs, a novel near training-free method by aligning attention patterns.\
  \ CLAIM first identifies language-specific cross-modal attention heads, then estimates\
  \ language shift vectors from English to the target language, and finally intervenes\
  \ in the attention outputs during inference to facilitate cross-lingual visual perception\
  \ capability alignment. Extensive experiments demonstrate that CLAIM achieves an\
  \ average improvement of 13.56% (up to 30% in Spanish) on the POPE and 21.75% on\
  \ the hallucination subsets of the MME benchmark across various languages. Further\
  \ analysis reveals that multilingual attention divergence is most prominent in intermediate\
  \ layers, highlighting their critical role in multilingual scenarios."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11073
---

Computer Science > Computation and Language
[Submitted on 3 Jun 2025]
Title:CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention
View PDF HTML (experimental)Abstract:Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal abilities but remain prone to multilingual object hallucination, with a higher likelihood of generating responses inconsistent with the visual input when utilizing queries in non-English languages compared to English. Most existing approaches to address these rely on pretraining or fine-tuning, which are resource-intensive. In this paper, inspired by observing the disparities in cross-modal attention patterns across languages, we propose Cross-Lingual Attention Intervention for Mitigating multilingual object hallucination (CLAIM) in LVLMs, a novel near training-free method by aligning attention patterns. CLAIM first identifies language-specific cross-modal attention heads, then estimates language shift vectors from English to the target language, and finally intervenes in the attention outputs during inference to facilitate cross-lingual visual perception capability alignment. Extensive experiments demonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in Spanish) on the POPE and 21.75% on the hallucination subsets of the MME benchmark across various languages. Further analysis reveals that multilingual attention divergence is most prominent in intermediate layers, highlighting their critical role in multilingual scenarios.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
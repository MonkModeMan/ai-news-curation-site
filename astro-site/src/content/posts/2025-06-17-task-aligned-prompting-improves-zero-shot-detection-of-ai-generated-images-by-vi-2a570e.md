---
title: Task-aligned prompting improves zero-shot detection of AI-generated images
  by Vision-Language Models
description: "arXiv:2506.11031v1 Announce Type: cross \nAbstract: As image generators\
  \ produce increasingly realistic images, concerns about potential misuse continue\
  \ to grow. Supervised detection relies on large, curated datasets and struggles\
  \ to generalize across diverse generators. In this work, we investigate the use\
  \ of pre-trained Vision-Language Models (VLMs) for zero-shot detection of AI-generated\
  \ images. While off-the-shelf VLMs exhibit some task-specific reasoning and chain-of-thought\
  \ prompting offers gains, we show that task-aligned prompting elicits more focused\
  \ reasoning and significantly improves performance without fine-tuning. Specifically,\
  \ prefixing the model's response with the phrase ``Let's examine the style and the\
  \ synthesis artifacts'' -- a method we call zero-shot-s$^2$ -- boosts Macro F1 scores\
  \ by 8%-29% for two widely used open-source models. These gains are consistent across\
  \ three recent, diverse datasets spanning human faces, objects, and animals with\
  \ images generated by 16 different models -- demonstrating strong generalization.\
  \ We further evaluate the approach across three additional model sizes and observe\
  \ improvements in most dataset-model combinations -- suggesting robustness to model\
  \ scale. Surprisingly, self-consistency, a behavior previously observed in language\
  \ reasoning, where aggregating answers from diverse reasoning paths improves performance,\
  \ also holds in this setting. Even here, zero-shot-s$^2$ scales better than chain-of-thought\
  \ in most cases -- indicating that it elicits more useful diversity. Our findings\
  \ show that task-aligned prompts elicit more focused reasoning and enhance latent\
  \ capabilities in VLMs, like the detection of AI-generated images -- offering a\
  \ simple, generalizable, and explainable alternative to supervised methods. Our\
  \ code is publicly available on github: https://github.com/osome-iu/Zero-shot-s2.git."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11031
---

Computer Science > Machine Learning
[Submitted on 20 May 2025]
Title:Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models
View PDFAbstract:As image generators produce increasingly realistic images, concerns about potential misuse continue to grow. Supervised detection relies on large, curated datasets and struggles to generalize across diverse generators. In this work, we investigate the use of pre-trained Vision-Language Models (VLMs) for zero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit some task-specific reasoning and chain-of-thought prompting offers gains, we show that task-aligned prompting elicits more focused reasoning and significantly improves performance without fine-tuning. Specifically, prefixing the model's response with the phrase ``Let's examine the style and the synthesis artifacts'' -- a method we call zero-shot-s$^2$ -- boosts Macro F1 scores by 8%-29% for two widely used open-source models. These gains are consistent across three recent, diverse datasets spanning human faces, objects, and animals with images generated by 16 different models -- demonstrating strong generalization. We further evaluate the approach across three additional model sizes and observe improvements in most dataset-model combinations -- suggesting robustness to model scale. Surprisingly, self-consistency, a behavior previously observed in language reasoning, where aggregating answers from diverse reasoning paths improves performance, also holds in this setting. Even here, zero-shot-s$^2$ scales better than chain-of-thought in most cases -- indicating that it elicits more useful diversity. Our findings show that task-aligned prompts elicit more focused reasoning and enhance latent capabilities in VLMs, like the detection of AI-generated images -- offering a simple, generalizable, and explainable alternative to supervised methods. Our code is publicly available on github: this https URL.
Current browse context:
cs.LG
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
---
title: Deep Learning Model Acceleration and Optimization Strategies for Real-Time
  Recommendation Systems
description: "arXiv:2506.11421v1 Announce Type: cross \nAbstract: With the rapid growth\
  \ of Internet services, recommendation systems play a central role in delivering\
  \ personalized content. Faced with massive user requests and complex model architectures,\
  \ the key challenge for real-time recommendation systems is how to reduce inference\
  \ latency and increase system throughput without sacrificing recommendation quality.\
  \ This paper addresses the high computational cost and resource bottlenecks of deep\
  \ learning models in real-time settings by proposing a combined set of modeling-\
  \ and system-level acceleration and optimization strategies. At the model level,\
  \ we dramatically reduce parameter counts and compute requirements through lightweight\
  \ network design, structured pruning, and weight quantization. At the system level,\
  \ we integrate multiple heterogeneous compute platforms and high-performance inference\
  \ libraries, and we design elastic inference scheduling and load-balancing mechanisms\
  \ based on real-time load characteristics. Experiments show that, while maintaining\
  \ the original recommendation accuracy, our methods cut latency to less than 30%\
  \ of the baseline and more than double system throughput, offering a practical solution\
  \ for deploying large-scale online recommendation services."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11421
---

Computer Science > Information Retrieval
[Submitted on 13 Jun 2025]
Title:Deep Learning Model Acceleration and Optimization Strategies for Real-Time Recommendation Systems
View PDFAbstract:With the rapid growth of Internet services, recommendation systems play a central role in delivering personalized content. Faced with massive user requests and complex model architectures, the key challenge for real-time recommendation systems is how to reduce inference latency and increase system throughput without sacrificing recommendation quality. This paper addresses the high computational cost and resource bottlenecks of deep learning models in real-time settings by proposing a combined set of modeling- and system-level acceleration and optimization strategies. At the model level, we dramatically reduce parameter counts and compute requirements through lightweight network design, structured pruning, and weight quantization. At the system level, we integrate multiple heterogeneous compute platforms and high-performance inference libraries, and we design elastic inference scheduling and load-balancing mechanisms based on real-time load characteristics. Experiments show that, while maintaining the original recommendation accuracy, our methods cut latency to less than 30% of the baseline and more than double system throughput, offering a practical solution for deploying large-scale online recommendation services.
Current browse context:
cs.IR
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
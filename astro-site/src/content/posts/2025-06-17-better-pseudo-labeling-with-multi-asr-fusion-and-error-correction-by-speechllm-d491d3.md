---
title: Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM
description: "arXiv:2506.11089v1 Announce Type: cross \nAbstract: Automatic speech\
  \ recognition (ASR) models rely on high-quality transcribed data for effective training.\
  \ Generating pseudo-labels for large unlabeled audio datasets often relies on complex\
  \ pipelines that combine multiple ASR outputs through multi-stage processing, leading\
  \ to error propagation, information loss and disjoint optimization. We propose a\
  \ unified multi-ASR prompt-driven framework using postprocessing by either textual\
  \ or speech-based large language models (LLMs), replacing voting or other arbitration\
  \ logic for reconciling the ensemble outputs. We perform a comparative study of\
  \ multiple architectures with and without LLMs, showing significant improvements\
  \ in transcription accuracy compared to traditional methods. Furthermore, we use\
  \ the pseudo-labels generated by the various approaches to train semi-supervised\
  \ ASR models for different datasets, again showing improved performance with textual\
  \ and speechLLM transcriptions compared to baselines."
pubDate: Mon, 16 Jun 2025 00:00:00 -0400
source: arXiv AI
tags:
- arxiv
- ai
- research
url: https://arxiv.org/abs/2506.11089
---

Electrical Engineering and Systems Science > Audio and Speech Processing
[Submitted on 5 Jun 2025]
Title:Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM
View PDF HTML (experimental)Abstract:Automatic speech recognition (ASR) models rely on high-quality transcribed data for effective training. Generating pseudo-labels for large unlabeled audio datasets often relies on complex pipelines that combine multiple ASR outputs through multi-stage processing, leading to error propagation, information loss and disjoint optimization. We propose a unified multi-ASR prompt-driven framework using postprocessing by either textual or speech-based large language models (LLMs), replacing voting or other arbitration logic for reconciling the ensemble outputs. We perform a comparative study of multiple architectures with and without LLMs, showing significant improvements in transcription accuracy compared to traditional methods. Furthermore, we use the pseudo-labels generated by the various approaches to train semi-supervised ASR models for different datasets, again showing improved performance with textual and speechLLM transcriptions compared to baselines.
Current browse context:
eess.AS
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.
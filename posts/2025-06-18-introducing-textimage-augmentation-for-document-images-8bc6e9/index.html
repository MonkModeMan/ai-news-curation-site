<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Introducing TextImage Augmentation for Document Images</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.Ba3Vwl3b.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Introducing TextImage Augmentation for Document Images</h1> <p class="text-sm text-gray-500"> 2024/8/6 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/doc_aug_hf_alb" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Introducing Multimodal TextImage Augmentation for Document Images
In this blog post, we provide a tutorial on how to use a new data augmentation technique for document images, developed in collaboration with Albumentations AI.
Motivation
Vision Language Models (VLMs) have an immense range of applications, but they often need to be fine-tuned to specific use-cases, particularly for datasets containing document images, i.e., images with high textual content. In these cases, it is crucial for text and image to interact with each other at all stages of model training, and applying augmentation to both modalities ensures this interaction. Essentially, we want a model to learn to read properly, which is challenging in the most common cases where data is missing.
Hence, the need for effective data augmentation techniques for document images became evident when addressing challenges in fine-tuning models with limited datasets. A common concern is that typical image transformations, such as resizing, blurring, or changing background colors, can negatively impact text extraction accuracy.
We recognized the need for data augmentation techniques that preserve the integrity of the text while augmenting the dataset. Such data augmentation can facilitate generation of new documents or modification of existing ones, while preserving their text quality.
Introduction
To address this need, we introduce a new data augmentation pipeline developed in collaboration with Albumentations AI. This pipeline handles both images and text within them, providing a comprehensive solution for document images. This class of data augmentation is multimodal as it modifies both the image content and the text annotations simultaneously.
As discussed in a previous blog post, our goal is to test the hypothesis that integrating augmentations on both text and images during pretraining of VLMs is effective. Detailed parameters and use case illustrations can be found on the Albumentations AI Documentation. Albumentations AI enables the dynamic design of these augmentations and their integration with other types of augmentations.
Method
To augment document images, we begin by randomly selecting lines within the document. A hyperparameter fraction_range
controls the bounding box fraction to be modified.
Next, we apply one of several text augmentation methods to the corresponding lines of text, which are commonly utilized in text generation tasks. These methods include Random Insertion, Deletion, and Swap, and Stopword Replacement.
After modifying the text, we black out parts of the image where the text is inserted and inpaint them, using the original bounding box size as a proxy for the new text&#39;s font size. The font size can be specified with the parameter font_size_fraction_range
, which determines the range for selecting the font size as a fraction of the bounding box height. Note that the modified text and corresponding bounding box can be retrieved and used for training. This process results in a dataset with semantically similar textual content and visually distorted images.
Main Features of the TextImage Augmentation
The library can be used for two main purposes:
Inserting any text on the image: This feature allows you to overlay text on document images, effectively generating synthetic data. By using any random image as a background and rendering completely new text, you can create diverse training samples. A similar technique, called SynthDOG, was introduced in the OCR-free document understanding transformer.
Inserting augmented text on the image: This includes the following text augmentations:
- Random deletion: Randomly removes words from the text.
- Random swapping: Swaps words within the text.
- Stop words insertion: Inserts common stop words into the text.
Combining these augmentations with other image transformations from Albumentations allows for simultaneous modification of images and text. You can retrieve the augmented text as well.
Note: The initial version of the data augmentation pipeline presented in this repo, included synonym replacement. It was removed in this version because it caused significant time overhead.
Installation
!pip install -U pillow
!pip install albumentations
!pip install nltk
import albumentations as A
import cv2
from matplotlib import pyplot as plt
import json
import nltk
nltk.download(&#39;stopwords&#39;)
from nltk.corpus import stopwords
Visualization
def visualize(image):
plt.figure(figsize=(20, 15))
plt.axis(&#39;off&#39;)
plt.imshow(image)
Load data
Note that for this type of augmentation you can use the IDL and PDFA datasets. They provide the bounding boxes of the lines that you want to modify. For this tutorial, we will focus on the sample from IDL dataset.
bgr_image = cv2.imread(&quot;examples/original/fkhy0236.tif&quot;)
image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)
with open(&quot;examples/original/fkhy0236.json&quot;) as f:
labels = json.load(f)
font_path = &quot;/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf&quot;
visualize(image)
We need to correctly preprocess the data, as the input format for the bounding boxes is the normalized Pascal VOC. Hence, we build the metadata as follows:
page = labels[&#39;pages&#39;][0]
def prepare_metadata(page: dict, image_height: int, image_width: int) -&gt; list:
metadata = []
for text, box in zip(page[&#39;text&#39;], page[&#39;bbox&#39;]):
left, top, width_norm, height_norm = box
metadata.append({
&quot;bbox&quot;: [left, top, left + width_norm, top + height_norm],
&quot;text&quot;: text
})
return metadata
image_height, image_width = image.shape[:2]
metadata = prepare_metadata(page, image_height, image_width)
Random Swap
transform = A.Compose([A.TextImage(font_path=font_path, p=1, augmentations=[&quot;swap&quot;], clear_bg=True, font_color = &#39;red&#39;, fraction_range = (0.5,0.8), font_size_fraction_range=(0.8, 0.9))])
transformed = transform(image=image, textimage_metadata=metadata)
visualize(transformed[&quot;image&quot;])
Random Deletion
transform = A.Compose([A.TextImage(font_path=font_path, p=1, augmentations=[&quot;deletion&quot;], clear_bg=True, font_color = &#39;red&#39;, fraction_range = (0.5,0.8), font_size_fraction_range=(0.8, 0.9))])
transformed = transform(image=image, textimage_metadata=metadata)
visualize(transformed[&#39;image&#39;])
Random Insertion
In random insertion we insert random words or phrases into the text. In this case, we use stop words, common words in a language that are often ignored or filtered out during natural language processing (NLP) tasks because they carry less meaningful information compared to other words. Examples of stop words include &quot;is,&quot; &quot;the,&quot; &quot;in,&quot; &quot;and,&quot; &quot;of,&quot; etc.
stops = stopwords.words(&#39;english&#39;)
transform = A.Compose([A.TextImage(font_path=font_path, p=1, augmentations=[&quot;insertion&quot;], stopwords = stops, clear_bg=True, font_color = &#39;red&#39;, fraction_range = (0.5,0.8), font_size_fraction_range=(0.8, 0.9))])
transformed = transform(image=image, textimage_metadata=metadata)
visualize(transformed[&#39;image&#39;])
Can we combine with other transformations?
Let&#39;s define a complex transformation pipeline using A.Compose
, which includes text insertion with specified font properties and stopwords, Planckian jitter, and affine transformations. Firstly, with A.TextImage
we insert text into the image using specified font properties, with a clear background and red font color. The fraction and size of the text to be inserted are also specified.
Then with A.PlanckianJitter
we alter the color balance of the image. Finally, using A.Affine
we apply affine transformations, which can include scaling, rotating, and translating the image.
transform_complex = A.Compose([A.TextImage(font_path=font_path, p=1, augmentations=[&quot;insertion&quot;], stopwords = stops, clear_bg=True, font_color = &#39;red&#39;, fraction_range = (0.5,0.8), font_size_fraction_range=(0.8, 0.9)),
A.PlanckianJitter(p=1),
A.Affine(p=1)
])
transformed = transform_complex(image=image, textimage_metadata=metadata)
visualize(transformed[&quot;image&quot;])
How to get the altered text?
To extract the information on the bounding box indices where text was altered, along with the corresponding transformed text data run the following cell. This data can be used effectively for training models to recognize and process text changes in images.
transformed[&#39;overlay_data&#39;]
[{&#39;bbox_coords&#39;: (375, 1149, 2174, 1196),
&#39;text&#39;: &quot;Lionberger, Ph.D., (Title: if Introduction to won i FDA&#39;s yourselves Draft Guidance once of the wasn&#39;t General Principles&quot;,
&#39;original_text&#39;: &quot;Lionberger, Ph.D., (Title: Introduction to FDA&#39;s Draft Guidance of the General Principles&quot;,
&#39;bbox_index&#39;: 12,
&#39;font_color&#39;: &#39;red&#39;},
{&#39;bbox_coords&#39;: (373, 1677, 2174, 1724),
&#39;text&#39;: &quot;After off needn&#39;t were a brief break, ADC member mustn Jeffrey that Dayno, MD, Chief Medical Officer for at their Egalet&quot;,
&#39;original_text&#39;: &#39;After a brief break, ADC member Jeffrey Dayno, MD, Chief Medical Officer at Egalet&#39;,
&#39;bbox_index&#39;: 19,
&#39;font_color&#39;: &#39;red&#39;},
{&#39;bbox_coords&#39;: (525, 2109, 2172, 2156),
&#39;text&#39;: &#39;ll Brands recognize the has importance and of a generics ADF guidance to ensure which after&#39;,
&#39;original_text&#39;: &#39;Brands recognize the importance of a generics ADF guidance to ensure&#39;,
&#39;bbox_index&#39;: 23,
&#39;font_color&#39;: &#39;red&#39;}]
Synthetic Data Generation
This augmentation method can be extended to the generation of synthetic data, as it enables the rendering of text on any background or template.
template = cv2.imread(&#39;template.png&#39;)
image_template = cv2.cvtColor(template, cv2.COLOR_BGR2RGB)
transform = A.Compose([A.TextImage(font_path=font_path, p=1, clear_bg=True, font_color = &#39;red&#39;, font_size_fraction_range=(0.5, 0.7))])
metadata = [{
&quot;bbox&quot;: [0.1, 0.4, 0.5, 0.48],
&quot;text&quot;: &quot;Some smart text goes here.&quot;,
}, {
&quot;bbox&quot;: [0.1, 0.5, 0.5, 0.58],
&quot;text&quot;: &quot;Hope you find it helpful.&quot;,
}]
transformed = transform(image=image_template, textimage_metadata=metadata)
visualize(transformed[&#39;image&#39;])
Conclusion
In collaboration with Albumentations AI, we introduced TextImage Augmentation, a multimodal technique that modifies document images along with the text. By combining text augmentations such as Random Insertion, Deletion, Swap, and Stopword Replacement with image modifications, this pipeline allows for the generation of diverse training samples.
For detailed parameters and use case illustrations, refer to the Albumentations AI Documentation. We hope you find these augmentations useful for enhancing your document image processing workflows.
References
@inproceedings{kim2022ocr,
title={Ocr-free document understanding transformer},
author={Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
booktitle={European Conference on Computer Vision},
pages={498--517},
year={2022},
organization={Springer}
} </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Trace &amp; Evaluate your Agent with Arize Phoenix</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.CO8YC2Z5.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Trace &amp; Evaluate your Agent with Arize Phoenix</h1> <p class="text-sm text-gray-500"> 2025/2/28 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/smolagents-phoenix" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Trace &amp; Evaluate your Agent with Arize Phoenix
So, you‚Äôve built your agent. It takes in inputs and tools, processes them, and generates responses. Maybe it‚Äôs making decisions, retrieving information, executing tasks autonomously, or all three. But now comes the big question ‚Äì how effectively is it performing? And more importantly, how do you know?
Building an agent is one thing; understanding its behavior is another. That‚Äôs where tracing and evaluations come in. Tracing allows you to see exactly what your agent is doing step by step‚Äîwhat inputs it receives, how it processes information, and how it arrives at its final output. Think of it like having an X-ray for your agent‚Äôs decision-making process. Meanwhile, evaluation helps you measure performance, ensuring your agent isn‚Äôt just functional, but actually effective. Is it producing the right answers? How relevant are its findings at each step? How well-crafted is the agent‚Äôs response? Does it align with your goals?
Arize Phoenix provides a centralized platform to trace, evaluate, and debug your agent&#39;s decisions in real time‚Äîall in one place. We‚Äôll dive into how you can implement them to refine and optimize your agent. Because building is just the beginning‚Äîtrue intelligence comes from knowing exactly what‚Äôs happening under the hood.
For this, let‚Äôs make sure that we have an Agent setup! You can follow along with the following steps or use your own agent.
Make An Agent
Step 1: Install the Required Libraries
pip install -q smolagents
Step 2: Import all the Essential Building Blocks
Now let‚Äôs bring in the classes and tools we‚Äôll be using:
from smolagents import (
CodeAgent,
DuckDuckGoSearchTool,
VisitWebpageTool,
HfApiModel,
)
Step 3: Set Up Our Base Models
We‚Äôll create a model instance powered by the Hugging Face Hub Serverless API:
hf_model = HfApiModel()
Step 4: Create the Tool-Calling Agent
agent = CodeAgent(
tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],
model=hf_model,
add_base_tools=True
)
Step 5: Run the Agent
Now for the magic moment‚Äîlet‚Äôs see our agent in action. The question we‚Äôre asking our agent is:
‚ÄúFetch the share price of Google from 2020 to 2024, and create a line graph from it?‚Äù
agent.run(&quot;fetch the share price of google from 2020 to 2024, and create a line graph from it?&quot;)
Your agent will now:
- Use DuckDuckGoSearchTool to search for historical share prices of Google.
- Potentially visit pages with the VisitWebpageTool to find that data.
- Attempt to gather information and generate or describe how to create the line graph.
Trace Your Agent
Once your agent is running, the next challenge is making sense of its internal workflow. Tracing helps you track each step your agent takes‚Äîfrom invoking tools to processing inputs and generating responses‚Äîallowing you to debug issues, optimize performance, and ensure it behaves as expected.
To enable tracing, we‚Äôll use Arize Phoenix for visualization, and OpenTelemetry + OpenInference for instrumentation.
Install the telemetry module from smolagents:
pip install -q &#39;smolagents[telemetry]&#39;
You can run Phoenix in a bunch of different ways. This command will run a local instance of Phoenix on your machine:
python -m phoenix.server.main serve
For other hosting options of Phoenix, you can create a free online instance of Phoenix, self-host the application locally, or host the application on Hugging Face Spaces.
After launching, we register a tracer provider, pointing to our Phoenix instance.
from phoenix.otel import register
from openinference.instrumentation.smolagents import SmolagentsInstrumentor
tracer_provider = register(project_name=&quot;my-smolagents-app&quot;) # creates a tracer provider to capture OTEL traces
SmolagentsInstrumentor().instrument(tracer_provider=tracer_provider) # automatically captures any smolagents calls as traces
Now any calls made to smolagents will be sent through to our Phoenix instance.
Now that tracing is enabled, let‚Äôs test it with a simple query:
agent.run(&quot;What time is it in Tokyo right now?&quot;)
Once OpenInference is set up with SmolAgents, every agent invocation will be automatically traced in Phoenix.
Evaluate Your Agent
Once your agent is up and its run is monitored, the next step is to assess its performance. Evaluations (evals) help determine how well your agent is retrieving, processing, and presenting information.
There are many types of evals you can run, such as response relevance, factual accuracy, latency, and more. Check out the Phoenix documentation for a deeper dive into different evaluation techniques.
In this example, we‚Äôll focus on evaluating the DuckDuckGo search tool used by our agent. We‚Äôll measure the relevance of its search results using a Large Language Model (LLM) as a judge‚Äîspecifically, OpenAI&#39;s GPT-4o.
Step 1: Install OpenAI
First, install the necessary packages:
pip install -q openai
We‚Äôll be using GPT-4o to evaluate whether the search tool‚Äôs responses are relevant.
This method, known as LLM-as-a-judge, leverages language models to classify and score responses.
Step 2: Retrieve Tool Execution Spans
To evaluate how well DuckDuckGo is retrieving information, we first need to extract the execution traces where the tool was called.
from phoenix.trace.dsl import SpanQuery
import phoenix as px
import json
query = SpanQuery().where(
&quot;name == &#39;DuckDuckGoSearchTool&#39;&quot;,
).select(
input=&quot;input.value&quot;, # this parameter must be named input to work with the RAG_RELEVANCY_PROMPT_TEMPLATE
reference=&quot;output.value&quot;, # this parameter must be named reference to work with the RAG_RELEVANCY_PROMPT_TEMPLATE
)
# The Phoenix Client can take this query and return the dataframe.
tool_spans = px.Client().query_spans(query, project_name=&quot;my-smolagents-app&quot;)
tool_spans[&quot;input&quot;] = tool_spans[&quot;input&quot;].apply(lambda x: json.loads(x).get(&quot;kwargs&quot;, {}).get(&quot;query&quot;, &quot;&quot;))
tool_spans.head()
Step 3: Import Prompt Template
Next, we load the RAG Relevancy Prompt Template, which will help the LLM classify whether the search results are relevant or not.
from phoenix.evals import (
RAG_RELEVANCY_PROMPT_RAILS_MAP,
RAG_RELEVANCY_PROMPT_TEMPLATE,
OpenAIModel,
llm_classify
)
import nest_asyncio
nest_asyncio.apply()
print(RAG_RELEVANCY_PROMPT_TEMPLATE)
Step 4: Run the Evaluation
Now, we run the evaluation using GPT-4o as the judge:
from phoenix.evals import (
llm_classify,
OpenAIModel,
RAG_RELEVANCY_PROMPT_TEMPLATE,
)
eval_model = OpenAIModel(model=&quot;gpt-4o&quot;)
eval_results = llm_classify(
dataframe=tool_spans,
model=eval_model,
template=RAG_RELEVANCY_PROMPT_TEMPLATE,
rails=[&quot;relevant&quot;, &quot;unrelated&quot;],
concurrency=10,
provide_explanation=True,
)
eval_results[&quot;score&quot;] = eval_results[&quot;explanation&quot;].apply(lambda x: 1 if &quot;relevant&quot; in x else 0)
What‚Äôs happening here?
- We use GPT-4o to analyze the search query (input) and search result (output).
- The LLM classifies whether the result is relevant or unrelated based on the prompt.
- We assign a binary score (1 = relevant, 0 = unrelated) for further analysis.
To see your results:
eval_results.head()
Step 5: Send Evaluation Results to Phoenix
from phoenix.trace import SpanEvaluations
px.Client().log_evaluations(SpanEvaluations(eval_name=&quot;DuckDuckGoSearchTool Relevancy&quot;, dataframe=eval_results))
With this setup, we can now systematically evaluate the effectiveness of the DuckDuckGo search tool within our agent. Using LLM-as-a-judge, we can ensure our agent retrieves accurate and relevant information, leading to better performance.
Any evaluation is easy to set up using this tutorial‚Äîjust swap out the RAG_RELEVANCY_PROMPT_TEMPLATE for a different prompt template that fits your needs. Phoenix provides a variety of pre-written and pre-tested evaluation templates, covering areas like faithfulness, response coherence, factual accuracy, and more. Check out the Phoenix docs to explore the full list and find the best fit for your agent!
| Evaluation Template | Applicable Agent Type |
|---|---|
| Hallucination Detection | RAG agents General chatbots Knowledge-based assistants |
| Q&amp;A on Retrieved Data | RAG agents Research Assistants Document Search Tools |
| RAG Relevance | RAG agents Search-based AI assistants |
| Summarization | Summarization tools Document digesters Meeting note generators |
| Code Generation | Code assistants AI programming bots |
| Toxicity Detection | Moderation bots Content filtering AI |
| AI vs Human (Ground Truth) | Evaluation &amp; benchmarking tools AI-generated content validators |
| Reference (Citation) Link | Research assistants Citation tools Academic writing aids |
| SQL Generation Evaluation | Database query agents SQL automation tools |
| Agent Function Calling Evaluation | Multi-step reasoning agents API-calling AI Task automation bots | </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
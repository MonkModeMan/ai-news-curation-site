<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Designing a new way to optimize complex coordinated systems</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/index.BoXAN-Xr.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- âœ… ã‚¿ã‚¤ãƒˆãƒ« --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">ğŸ“° Designing a new way to optimize complex coordinated systems</h1> <p class="text-sm text-gray-500"> 2025/4/24 â€“ MIT  <a href="https://news.mit.edu/2025/designing-new-way-optimize-complex-coordinated-systems-0424" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
å…ƒè¨˜äº‹
</a>  </p> </header> <!-- âœ… æœ¬æ–‡ --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Coordinating complicated interactive systems, whether itâ€™s the different modes of transportation in a city or the various components that must work together to make an effective and efficient robot, is an increasingly important subject for software designers to tackle. Now, researchers at MIT have developed an entirely new way of approaching these complex problems, using simple diagrams as a tool to reveal better approaches to software optimization in deep-learning models.
They say the new method makes addressing these complex tasks so simple that it can be reduced to a drawing that would fit on the back of a napkin.
The new approach is described in the journal Transactions of Machine Learning Research, in a paper by incoming doctoral student Vincent Abbott and Professor Gioele Zardini of MITâ€™s Laboratory for Information and Decision Systems (LIDS).
â€œWe designed a new language to talk about these new systems,â€ Zardini says. This new diagram-based â€œlanguageâ€ is heavily based on something called category theory, he explains.
It all has to do with designing the underlying architecture of computer algorithms â€” the programs that will actually end up sensing and controlling the various different parts of the system thatâ€™s being optimized. â€œThe components are different pieces of an algorithm, and they have to talk to each other, exchange information, but also account for energy usage, memory consumption, and so on.â€ Such optimizations are notoriously difficult because each change in one part of the system can in turn cause changes in other parts, which can further affect other parts, and so on.
The researchers decided to focus on the particular class of deep-learning algorithms, which are currently a hot topic of research. Deep learning is the basis of the large artificial intelligence models, including large language models such as ChatGPT and image-generation models such as Midjourney. These models manipulate data by a â€œdeepâ€ series of matrix multiplications interspersed with other operations. The numbers within matrices are parameters, and are updated during long training runs, allowing for complex patterns to be found. Models consist of billions of parameters, making computation expensive, and hence improved resource usage and optimization invaluable.
Diagrams can represent details of the parallelized operations that deep-learning models consist of, revealing the relationships between algorithms and the parallelized graphics processing unit (GPU) hardware they run on, supplied by companies such as NVIDIA. â€œIâ€™m very excited about this,â€ says Zardini, because â€œwe seem to have found a language that very nicely describes deep learning algorithms, explicitly representing all the important things, which is the operators you use,â€ for example the energy consumption, the memory allocation, and any other parameter that youâ€™re trying to optimize for.
Much of the progress within deep learning has stemmed from resource efficiency optimizations. The latest DeepSeek model showed that a small team can compete with top models from OpenAI and other major labs by focusing on resource efficiency and the relationship between software and hardware. Typically, in deriving these optimizations, he says, â€œpeople need a lot of trial and error to discover new architectures.â€ For example, a widely used optimization program called FlashAttention took more than four years to develop, he says. But with the new framework they developed, â€œwe can really approach this problem in a more formal way.â€ And all of this is represented visually in a precisely defined graphical language.
But the methods that have been used to find these improvements â€œare very limited,â€ he says. â€œI think this shows that thereâ€™s a major gap, in that we donâ€™t have a formal systematic method of relating an algorithm to either its optimal execution, or even really understanding how many resources it will take to run.â€ But now, with the new diagram-based method they devised, such a system exists.
Category theory, which underlies this approach, is a way of mathematically describing the different components of a system and how they interact in a generalized, abstract manner. Different perspectives can be related. For example, mathematical formulas can be related to algorithms that implement them and use resources, or descriptions of systems can be related to robust â€œmonoidal string diagrams.â€ These visualizations allow you to directly play around and experiment with how the different parts connect and interact. What they developed, he says, amounts to â€œstring diagrams on steroids,â€ which incorporates many more graphical conventions and many more properties.
â€œCategory theory can be thought of as the mathematics of abstraction and composition,â€ Abbott says. â€œAny compositional system can be described using category theory, and the relationship between compositional systems can then also be studied.â€ Algebraic rules that are typically associated with functions can also be represented as diagrams, he says. â€œThen, a lot of the visual tricks we can do with diagrams, we can relate to algebraic tricks and functions. So, it creates this correspondence between these different systems.â€
As a result, he says, â€œthis solves a very important problem, which is that we have these deep-learning algorithms, but theyâ€™re not clearly understood as mathematical models.â€ But by representing them as diagrams, it becomes possible to approach them formally and systematically, he says.
One thing this enables is a clear visual understanding of the way parallel real-world processes can be represented by parallel processing in multicore computer GPUs. â€œIn this way,â€ Abbott says, â€œdiagrams can both represent a function, and then reveal how to optimally execute it on a GPU.â€
The â€œattentionâ€ algorithm is used by deep-learning algorithms that require general, contextual information, and is a key phase of the serialized blocks that constitute large language models such as ChatGPT. FlashAttention is an optimization that took years to develop, but resulted in a sixfold improvement in the speed of attention algorithms.
Applying their method to the well-established FlashAttention algorithm, Zardini says that â€œhere we are able to derive it, literally, on a napkin.â€ He then adds, â€œOK, maybe itâ€™s a large napkin.â€ But to drive home the point about how much their new approach can simplify dealing with these complex algorithms, they titled their formal research paper on the work â€œFlashAttention on a Napkin.â€
This method, Abbott says, â€œallows for optimization to be really quickly derived, in contrast to prevailing methods.â€ While they initially applied this approach to the already existing FlashAttention algorithm, thus verifying its effectiveness, â€œwe hope to now use this language to automate the detection of improvements,â€ says Zardini, who in addition to being a principal investigator in LIDS, is the Rudge and Nancy Allen Assistant Professor of Civil and Environmental Engineering, and an affiliate faculty with the Institute for Data, Systems, and Society.
The plan is that ultimately, he says, they will develop the software to the point that â€œthe researcher uploads their code, and with the new algorithm you automatically detect what can be improved, what can be optimized, and you return an optimized version of the algorithm to the user.â€
In addition to automating algorithm optimization, Zardini notes that a robust analysis of how deep-learning algorithms relate to hardware resource usage allows for systematic co-design of hardware and software. This line of work integrates with Zardiniâ€™s focus on categorical co-design, which uses the tools of category theory to simultaneously optimize various components of engineered systems.
Abbott says that â€œthis whole field of optimized deep learning models, I believe, is quite critically unaddressed, and thatâ€™s why these diagrams are so exciting. They open the doors to a systematic approach to this problem.â€
â€œIâ€™m very impressed by the quality of this research. ... The new approach to diagramming deep-learning algorithms used by this paper could be a very significant step,â€ says Jeremy Howard, founder and CEO of Answers.ai, who was not associated with this work. â€œThis paper is the first time Iâ€™ve seen such a notation used to deeply analyze the performance of a deep-learning algorithm on real-world hardware. ... The next step will be to see whether real-world performance gains can be achieved.â€
â€œThis is a beautifully executed piece of theoretical research, which also aims for high accessibility to uninitiated readers â€” a trait rarely seen in papers of this kind,â€ says Petar Velickovic, a senior research scientist at Google DeepMind and a lecturer at Cambridge University, who was not associated with this work. These researchers, he says, â€œare clearly excellent communicators, and I cannot wait to see what they come up with next!â€
The new diagram-based language, having been posted online, has already attracted great attention and interest from software developers. A reviewer from Abbottâ€™s prior paper introducing the diagrams noted that â€œThe proposed neural circuit diagrams look great from an artistic standpoint (as far as I am able to judge this).â€ â€œItâ€™s technical research, but itâ€™s also flashy!â€ Zardini says. </article> <!-- âœ… æˆ»ã‚‹ãƒœã‚¿ãƒ³ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
â† ä¸€è¦§ã¸æˆ»ã‚‹
</a> </div> </div> <!-- âœ… base ã‚’æ­£ã—ãåŸ‹ã‚è¾¼ã‚€ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- âœ… æˆ»ã‚‹ãƒªãƒ³ã‚¯ã‚’æ­£ã—ãæ§‹ç¯‰ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("âœ… base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("âœ… backLink.href:", backLink.href);
      } else {
        console.warn("âš ï¸ backLink not found");
      }
    </script> </body> </html>
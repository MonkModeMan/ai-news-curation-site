<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Announcing New Hugging Face and KerasHub integration</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.CO8YC2Z5.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Announcing New Hugging Face and KerasHub integration</h1> <p class="text-sm text-gray-500"> 2024/7/10 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/keras-hub-integration" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Announcing New Hugging Face and KerasHub integration
The Hugging Face Hub is a vast repository, currently hosting 750K+ public models, offering a diverse range of pre-trained models for various machine learning frameworks. Among these, 346,268 (as of the time of writing) models are built using the popular Transformers library. The KerasHub library recently added an integration with the Hub compatible with a first batch of 33 models.
In this first version, users of KerasHub were limited to only the KerasHub-based models available on the Hugging Face Hub.
from keras_hub.models import GemmaCausalLM
gemma_lm = GemmaCausalLM.from_preset(
&quot;hf://google/gemma-2b-keras&quot;
)
They were able to train/fine-tune the model and upload it back to the Hub (notice that the model is still a Keras model).
model.save_to_preset(&quot;./gemma-2b-finetune&quot;)
keras_hub.upload_preset(
&quot;hf://username/gemma-2b-finetune&quot;,
&quot;./gemma-2b-finetune&quot;
)
They were missing out on the extensive collection of over 300K models created with the transformers library. Figure 1 shows 4k Gemma models in the Hub.
| Figure 1: Gemma Models in the Hugging Face Hub (Source:https://huggingface.co/models?other=gemma) |
However, what if we told you that you can now access and use these 300K+ models with KerasHub, significantly expanding your model selection and capabilities?
from keras_hub.models import GemmaCausalLM
gemma_lm = GemmaCausalLM.from_preset(
&quot;hf://google/gemma-2b&quot; # this is not a keras model!
)
We&#39;re thrilled to announce a significant step forward for the Hub community: Transformers and KerasHub now have a shared model save format. This means that models of the transformers library on the Hugging Face Hub can now also be loaded directly into KerasHub - immediately making a huge range of fine-tuned models available to KerasHub users. Initially, this integration focuses on enabling the use of Gemma (1 and 2), Llama 3, and PaliGemma models, with plans to expand compatibility to a wider range of architectures in the near future.
Use a wider range of frameworks
Because KerasHub models can seamlessly use TensorFlow, JAX, or PyTorch backends, this means that a huge range of model checkpoints can now be loaded into any of these frameworks in a single line of code. Found a great checkpoint on Hugging Face, but you wish you could deploy it to TFLite for serving or port it into JAX to do research? Now you can!
How to use it
Using the integration requires updating your Keras versions
$ pip install -U -q keras-hub
$ pip install -U keras&gt;=3.3.3
Once updated, trying out the integration is as simple as:
from keras_hub.models import Llama3CausalLM
# this model was not fine-tuned with Keras but can still be loaded
causal_lm = Llama3CausalLM.from_preset(
&quot;hf://NousResearch/Hermes-2-Pro-Llama-3-8B&quot;
)
causal_lm.summary()
Under the Hood: How It Works
Transformers models are stored as a set of config files in JSON format, a tokenizer (usually also a .JSON file), and a set of safetensors weights files. The actual modeling code is contained in the Transformers library itself. This means that cross-loading a Transformers checkpoint into KerasHub is relatively straightforward as long as both libraries have modeling code for the relevant architecture. All we need to do is map config variables, weight names, and tokenizer vocabularies from one format to the other, and we create a KerasHub checkpoint from a Transformers checkpoint, or vice-versa.
All of this is handled internally for you, so you can focus on trying out the models rather than converting them!
Common Use Cases
Generation
A first use case of language models is to generate text. Here is an
example to load a transformers model and generate new tokens using
the .generate
method from KerasHub.
from keras_hub.models import Llama3CausalLM
# Get the model
causal_lm = Llama3CausalLM.from_preset(
&quot;hf://NousResearch/Hermes-2-Pro-Llama-3-8B&quot;
)
prompts = [
&quot;&quot;&quot;&lt;|im_start|&gt;system
You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.&lt;|im_end|&gt;
&lt;|im_start|&gt;user
Write a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant&quot;&quot;&quot;,
]
# Generate from the model
causal_lm.generate(prompts, max_length=200)[0]
Changing precision
You can change the precision of your model using keras.config
like so
import keras
keras.config.set_dtype_policy(&quot;bfloat16&quot;)
from keras_hub.models import Llama3CausalLM
causal_lm = Llama3CausalLM.from_preset(
&quot;hf://NousResearch/Hermes-2-Pro-Llama-3-8B&quot;
)
Using the checkpoint with JAX backend
To test drive a model using JAX, you can leverage Keras to run the model with the JAX backend. This can be achieved by simply switching Keras&#39;s backend to JAX. Here‚Äôs how you can use the model within the JAX environment.
import os
os.environ[&quot;KERAS_BACKEND&quot;] = &quot;jax&quot;
from keras_hub.models import Llama3CausalLM
causal_lm = Llama3CausalLM.from_preset(
&quot;hf://NousResearch/Hermes-2-Pro-Llama-3-8B&quot;
)
Gemma 2
We are pleased to inform you that the Gemma 2 models are also compatible with this integration.
from keras_hub.models import GemmaCausalLM
causal_lm = keras_hub.models.GemmaCausalLM.from_preset(
&quot;hf://google/gemma-2-9b&quot; # This is Gemma 2!
)
PaliGemma
You can also use any PaliGemma safetensor checkpoint in your KerasHub pipeline.
from keras_hub.models import PaliGemmaCausalLM
pali_gemma_lm = PaliGemmaCausalLM.from_preset(
&quot;hf://gokaygokay/sd3-long-captioner&quot; # A finetuned version of PaliGemma
)
What&#39;s Next?
This is just the beginning. We envision expanding this integration to encompass an even wider range of Hugging Face models and architectures. Stay tuned for updates and be sure to explore the incredible potential that this collaboration unlocks!
I would like to take this opportunity to thank Matthew Carrigan and Matthew Watson for their help in the entire process. </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
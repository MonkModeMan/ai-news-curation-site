<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Hugging Face + PyCharm</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/index.BoXAN-Xr.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Hugging Face + PyCharm</h1> <p class="text-sm text-gray-500"> 2024/11/5 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/pycharm-integration" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Hugging Face + PyCharm
It‚Äôs a Tuesday morning. As a Transformers maintainer, I‚Äôm doing the same thing I do most weekday mornings: Opening PyCharm, loading up the Transformers codebase and gazing lovingly at the chat template documentation while ignoring the 50 user issues I was pinged on that day. But this time, something feels different:
Something is‚Ä¶ wait! Computer! Enhance!
Is that..?
Those user issues are definitely not getting responses today. Let‚Äôs talk about the Hugging Face integration in PyCharm.
The Hugging Face Is Inside Your House
I could introduce this integration by just listing features, but that‚Äôs boring and there‚Äôs documentation for that. Instead, let‚Äôs walk through how we‚Äôd use it all in practice. Let‚Äôs say I‚Äôm writing a Python app, and I decide I want the app to be able to chat with users. Not just text chat, though ‚Äì we want the users to be able to paste in images too, and for the app to naturally chat about them as well.
If you‚Äôre not super-familiar with the current state-of-the-art in machine learning, this might seem like a terrifying demand, but don‚Äôt fear. Simply right click in your code, and select ‚ÄúInsert HF Model‚Äù. You‚Äôll get a dialog box:
Chatting with both images and text is called ‚Äúimage-text-to-text‚Äù: the user can supply images and text, and the model outputs text. Scroll down on the left until you find it. By default, the model list will be sorted by Likes ‚Äì but remember, older models often have a lot of likes built up even if they‚Äôre not really the state of the art anymore. We can check how old models are by seeing the date they were last updated, just under the model name. Let‚Äôs pick something that‚Äôs both recent and popular: microsoft/Phi-3.5-vision-instruct
.
You can select ‚ÄúUse Model‚Äù for some model categories to automatically paste some basic code into your notebook, but what often works better is to scroll through the Model Card on the right and grab any sample code. You can see the full model card to the right of the dialog box, exactly as it&#39;s shown on Hugging Face Hub. Let‚Äôs do that and paste it into our code!
Your office cybersecurity person might complain about you copying a chunk of random text from the internet and running it without even reading all of it, but if that happens just call them a nerd and continue regardless. And behold: We now have a working model that can happily chat about images - in this case, it reads and comments on screenshots of a Microsoft slide deck. Feel free to play around with this example. Try your own chat, or your own images. Once you get it working, simply wrap this code into a class and it‚Äôs ready to go in your app. We just got state of the art open-source machine learning in ten minutes without even opening a web browser.
These models can be large! If you‚Äôre getting memory errors, try using a GPU with more memory, or try reducing the 20 in the sample code. You can also remove device_map=&quot;cuda&quot; to put the model in CPU memory instead, at the cost of speed.
Instant Model Cards
Next, let‚Äôs change perspective in our little scenario. Now let‚Äôs say you‚Äôre not the author of this code - you‚Äôre a coworker who has to review it. Maybe you‚Äôre the cybersecurity person from earlier, who‚Äôs still upset about the ‚Äúnerd‚Äù comment. You look at this code snippet, and you have no idea what you‚Äôre seeing. Don‚Äôt panic - just hover over the model name, and the entire model card instantly appears. You can quickly verify the origin of this model, and what its intended uses are.
(This is also extremely helpful if you work on something else and completely forget everything about the code you wrote two weeks ago)
The Local Model Cache
You might notice that the model has to be downloaded the first time you run this code, but after that, it‚Äôs loaded much more quickly. The model has been stored in your local cache. Remember the mysterious little ü§ó icon from earlier? Simply click it, and you‚Äôll get a listing of everything in your cache:
This is a neat way to find the models you‚Äôre working with right now, and also to clear them out and save some disk space once you don‚Äôt need them anymore. It‚Äôs also very helpful for the two-week amnesia scenario - if you can‚Äôt remember the model you were using back then, it‚Äôs probably in here. Remember, though, that most useful, production-ready models in 2024 are going to be &gt;1GB, so your cache can fill up fast!
Python in the age of AI
At Hugging Face, we tend to think of open-source AI as being a natural extension of the open-source philosophy: Open software solves problems for developers and users, creating new abilities for them to integrate into their code, and open models do the same. There is a tendency to be blinded by complexity, and to focus too much on the implementation details because they‚Äôre all so novel and exciting, but models exist to do stuff for you. If you abstract away the details of architecture and training, they‚Äôre fundamentally functions - tools in your code that will transform a certain kind of input into a certain kind of output.
These features are thus a natural fit. Just as IDEs already pull up function signatures and docstrings for you, they can also pull up sample code and model cards for trained models. Integrations like these make it easy to reach over and import a chat or image recognition model as conveniently as you would import any other library. We think it‚Äôs obvious that this is what the future of code will look like, and we hope that you find these features useful!
Download PyCharm to give the Hugging Face integration a try.
[HF integration is a Pycharm Professional feature.]
Get a free 3-month PyCharm subscription using the PyCharm4HF code here. </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
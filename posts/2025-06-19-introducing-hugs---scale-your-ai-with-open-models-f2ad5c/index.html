<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Introducing HUGS - Scale your AI with Open Models</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.BNX7D8gk.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Introducing HUGS - Scale your AI with Open Models</h1> <p class="text-sm text-gray-500"> 2024/10/23 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/hugs" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Introducing HUGS - Scale your AI with Open Models
Today, we are thrilled to announce the launch of Hugging Face Generative AI Services a.k.a. HUGS: optimized, zero-configuration inference microservices designed to simplify and accelerate the development of AI applications with open models. Built on open-source Hugging Face technologies such as Text Generation Inference and Transformers, HUGS provides the best solution to efficiently build and scale Generative AI Applications in your own infrastructure. HUGS is optimized to run open models on a variety of hardware accelerators, including NVIDIA GPUs, AMD GPUs, and soon AWS Inferentia and Google TPUs.
Zero-Configuration Optimized Inference for Open Models
HUGS simplifies the optimized deployment of open models in your own infrastructure and on a wide variety of hardware. One key challenge developers and organizations face is the engineering complexity of optimizing inference workloads for LLMs on a particular GPU or AI accelerator. With HUGS, we enable maximum throughput deployments for the most popular open LLMs with zero configuration required. Each deployment configuration offered by HUGS is fully tested and maintained to work out of the box.
HUGS model deployments provide an OpenAI compatible API for a drop-in replacement of existing Generative AI applications built on top of model provider APIs. Just point your code to the HUGS deployment to power your applications with open models hosted in your own infrastructure.
Why HUGS?
HUGS offers an easy way to build AI applications with open models hosted in your own infrastructure, with the following benefits:
- In YOUR infrastructure: Deploy open models within your own secure environment. Keep your data and models off the Internet!
- Zero-configuration Deployment: HUGS reduces deployment time from weeks to minutes with zero-configuration setup, automatically optimizing the model and serving configuration for your NVIDIA, AMD GPU or AI accelerator.
- Hardware-Optimized Inference: Built on Hugging Face&#39;s Text Generation Inference (TGI), HUGS is optimized for peak performance across different hardware setups.
- Hardware Flexibility: Run HUGS on a variety of accelerators, including NVIDIA GPUs, AMD GPUs, with support for AWS Inferentia and Google TPUs coming soon.
- Model Flexibility: HUGS is compatible with a wide selection of open-source models, ensuring flexibility and choice for your AI applications.
- Industry Standard APIs: Deploy HUGS easily using Kubernetes with endpoints compatible with the OpenAI API, minimizing code changes.
- Enterprise Distribution: HUGS is an enterprise distribution of Hugging Face open source technologies, offering long-term support, rigorous testing, and SOC2 compliance.
- Enterprise Compliance: Minimizes compliance risks by including necessary licenses and terms of service.
We provided early access to HUGS to select Enterprise Hub customers:
HUGS is a huge timesaver to deploy locally ready-to-work models with good performances - before HUGS it would take us a week, now we can be done in less than 1 hour. For customers with sovereign AI requirements it&#39;s a game changer! - Henri Jouhaud, CTO at Polyconseil
We tried HUGS to deploy Gemma 2 on GCP using a L4 GPU - we didn&#39;t have to fiddle with libraries, versions and parameters, it just worked out of the box. HUGS gives us confidence we can scale our internal usage of open models! - Ghislain Putois, Research Engineer at Orange
How it Works
Using HUGS is straightforward. Here&#39;s how you can get started:
Note: You will need access to the appropriate subscription or marketplace offering depending on your chosen deployment method.
Where to find HUGS
HUGS is available through several channels:
- Cloud Service Provider (CSP) Marketplaces: You can find and deploy HUGS on Amazon Web Services (AWS) and Google Cloud Platform (GCP). Microsoft Azure support will come soon.
- DigitalOcean: HUGS is natively available within DigitalOcean as a new 1-Click Models service, powered by Hugging Face HUGS and GPU Droplets.
- Enterprise Hub: If your organization is upgraded to Enterprise Hub, contact our Sales team to get access to HUGS.
For specific deployment instructions for each platform, please refer to the relevant documentation linked above.
Pricing
HUGS offers on-demand pricing based on the uptime of each container, except for deployments on DigitalOcean.
- AWS Marketplace and Google Cloud Platform Marketplace: $1 per hour per container, no minimum fee (compute usage billed separately by CSP). On AWS you have 5 day free trial period for you to test HUGS for free.
- DigitalOcean: 1-Click Models powered by Hugging Face HUGS are available at no additional cost on DigitalOcean - regular GPU Droplets compute costs apply.
- Enterprise Hub: We offer custom HUGS access to Enterprise Hub organizations. Please contact our Sales team to learn more.
Running Inference
HUGS is based on Text Generation Inference (TGI), offering a seamless inference experience. For detailed instructions and examples, refer to the Run Inference on HUGS guide. HUGS leverages the OpenAI-compatible Messages API, allowing you to use familiar tools and libraries like cURL, the huggingface_hub
SDK, and the openai
SDK for sending requests.
from huggingface_hub import InferenceClient
ENDPOINT_URL=&quot;REPLACE&quot; # replace with your deployed url or IP
client = InferenceClient(base_url=ENDPOINT_URL, api_key=&quot;-&quot;)
chat_completion = client.chat.completions.create(
messages=[
{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;What is Deep Learning?&quot;},
],
temperature=0.7,
top_p=0.95,
max_tokens=128,
)
Supported Models and Hardware
HUGS supports a growing ecosystem of open models and hardware platforms. Refer to our Supported Models and Supported Hardware pages for the most up-to-date information.
We launch today with 13 popular open LLMs:
- meta-llama/Llama-3.1-8B-Instruct
- meta-llama/Llama-3.1-70B-Instruct
- meta-llama/Llama-3.1-405B-Instruct-FP8
- NousResearch/Hermes-3-Llama-3.1-8B
- NousResearch/Hermes-3-Llama-3.1-70B
- NousResearch/Hermes-3-Llama-3.1-405B-FP8
- NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO
- mistralai/Mixtral-8x7B-Instruct-v0.1
- mistralai/Mistral-7B-Instruct-v0.3
- mistralai/Mixtral-8x22B-Instruct-v0.1
- google/gemma-2-27b-it
- google/gemma-2-9b-it
- Qwen/Qwen2.5-7B-Instruct
For a detailed view of supported Models x Hardware, check out the documentation.
Get Started with HUGS Today
HUGS makes it easy to harness the power of open models, with zero-configuration optimized inference in your own infra. With HUGS, you can take control of your AI applications and easily transition proof of concept applications built with closed models to open models you host yourself.
Get started today and deploy HUGS on AWS, Google Cloud or DigitalOcean! </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
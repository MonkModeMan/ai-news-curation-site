<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Using LoRA for Efficient Stable Diffusion Fine-Tuning</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/index.BoXAN-Xr.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Using LoRA for Efficient Stable Diffusion Fine-Tuning</h1> <p class="text-sm text-gray-500"> 2023/1/26 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/lora" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Using LoRA for Efficient Stable Diffusion Fine-Tuning
LoRA: Low-Rank Adaptation of Large Language Models is a novel technique introduced by Microsoft researchers to deal with the problem of fine-tuning large-language models. Powerful models with billions of parameters, such as GPT-3, are prohibitively expensive to fine-tune in order to adapt them to particular tasks or domains. LoRA proposes to freeze pre-trained model weights and inject trainable layers (rank-decomposition matrices) in each transformer block. This greatly reduces the number of trainable parameters and GPU memory requirements since gradients don&#39;t need to be computed for most model weights. The researchers found that by focusing on the Transformer attention blocks of large-language models, fine-tuning quality with LoRA was on par with full model fine-tuning while being much faster and requiring less compute.
LoRA for Diffusers üß®
Even though LoRA was initially proposed for large-language models and demonstrated on transformer blocks, the technique can also be applied elsewhere. In the case of Stable Diffusion fine-tuning, LoRA can be applied to the cross-attention layers that relate the image representations with the prompts that describe them. The details of the following figure (taken from the Stable Diffusion paper) are not important, just note that the yellow blocks are the ones in charge of building the relationship between image and text representations.
To the best of our knowledge, Simo Ryu (@cloneofsimo
) was the first one to come up with a LoRA implementation adapted to Stable Diffusion. Please, do take a look at their GitHub project to see examples and lots of interesting discussions and insights.
In order to inject LoRA trainable matrices as deep in the model as in the cross-attention layers, people used to need to hack the source code of diffusers in imaginative (but fragile) ways. If Stable Diffusion has shown us one thing, it is that the community always comes up with ways to bend and adapt the models for creative purposes, and we love that! Providing the flexibility to manipulate the cross-attention layers could be beneficial for many other reasons, such as making it easier to adopt optimization techniques such as xFormers. Other creative projects such as Prompt-to-Prompt could do with some easy way to access those layers, so we decided to provide a general way for users to do it. We&#39;ve been testing that pull request since late December, and it officially launched with our diffusers release yesterday.
We&#39;ve been working with @cloneofsimo
to provide LoRA training support in diffusers, for both Dreambooth and full fine-tuning methods! These techniques provide the following benefits:
- Training is much faster, as already discussed.
- Compute requirements are lower. We could create a full fine-tuned model in a 2080 Ti with 11 GB of VRAM!
- Trained weights are much, much smaller. Because the original model is frozen and we inject new layers to be trained, we can save the weights for the new layers as a single file that weighs in at ~3 MB in size. This is about one thousand times smaller than the original size of the UNet model!
We are particularly excited about the last point. In order for users to share their awesome fine-tuned or dreamboothed models, they had to share a full copy of the final model. Other users that want to try them out have to download the fine-tuned weights in their favorite UI, adding up to combined massive storage and download costs. As of today, there are about 1,000 Dreambooth models registered in the Dreambooth Concepts Library, and probably many more not registered in the library.
With LoRA, it is now possible to publish a single 3.29 MB file to allow others to use your fine-tuned model.
(h/t to @mishig25
, the first person I heard use dreamboothing as a verb in a normal conversation).
LoRA fine-tuning
Full model fine-tuning of Stable Diffusion used to be slow and difficult, and that&#39;s part of the reason why lighter-weight methods such as Dreambooth or Textual Inversion have become so popular. With LoRA, it is much easier to fine-tune a model on a custom dataset.
Diffusers now provides a LoRA fine-tuning script that can run in as low as 11 GB of GPU RAM without resorting to tricks such as 8-bit optimizers. This is how you&#39;d use it to fine-tune a model using Lambda Labs Pok√©mon dataset:
export MODEL_NAME=&quot;runwayml/stable-diffusion-v1-5&quot;
export OUTPUT_DIR=&quot;/sddata/finetune/lora/pokemon&quot;
export HUB_MODEL_ID=&quot;pokemon-lora&quot;
export DATASET_NAME=&quot;lambdalabs/pokemon-blip-captions&quot;
accelerate launch --mixed_precision=&quot;fp16&quot; train_text_to_image_lora.py \
--pretrained_model_name_or_path=$MODEL_NAME \
--dataset_name=$DATASET_NAME \
--dataloader_num_workers=8 \
--resolution=512 --center_crop --random_flip \
--train_batch_size=1 \
--gradient_accumulation_steps=4 \
--max_train_steps=15000 \
--learning_rate=1e-04 \
--max_grad_norm=1 \
--lr_scheduler=&quot;cosine&quot; --lr_warmup_steps=0 \
--output_dir=${OUTPUT_DIR} \
--push_to_hub \
--hub_model_id=${HUB_MODEL_ID} \
--report_to=wandb \
--checkpointing_steps=500 \
--validation_prompt=&quot;Totoro&quot; \
--seed=1337
One thing of notice is that the learning rate is 1e-4
, much larger than the usual learning rates for regular fine-tuning (in the order of ~1e-6
, typically). This is a W&amp;B dashboard of the previous run, which took about 5 hours in a 2080 Ti GPU (11 GB of RAM). I did not attempt to optimize the hyperparameters, so feel free to try it out yourself! Sayak did another run on a T4 (16 GB of RAM), here&#39;s his final model, and here&#39;s a demo Space that uses it.
For additional details on LoRA support in diffusers, please refer to our documentation ‚Äì it will be always kept up to date with the implementation.
Inference
As we&#39;ve discussed, one of the major advantages of LoRA is that you get excellent results by training orders of magnitude less weights than the original model size. We designed an inference process that allows loading the additional weights on top of the unmodified Stable Diffusion model weights. Let&#39;s see how it works.
First, we&#39;ll use the Hub API to automatically determine what was the base model that was used to fine-tune a LoRA model. Starting from Sayak&#39;s model, we can use this code:
from huggingface_hub import model_info
# LoRA weights ~3 MB
model_path = &quot;sayakpaul/sd-model-finetuned-lora-t4&quot;
info = model_info(model_path)
model_base = info.cardData[&quot;base_model&quot;]
print(model_base) # CompVis/stable-diffusion-v1-4
This snippet will print the model he used for fine-tuning, which is CompVis/stable-diffusion-v1-4
. In my case, I trained my model starting from version 1.5 of Stable Diffusion, so if you run the same code with my LoRA model you&#39;ll see that the output is runwayml/stable-diffusion-v1-5
.
The information about the base model is automatically populated by the fine-tuning script we saw in the previous section, if you use the --push_to_hub
option. This is recorded as a metadata tag in the README
file of the model&#39;s repo, as you can see here.
After we determine the base model we used to fine-tune with LoRA, we load a normal Stable Diffusion pipeline. We&#39;ll customize it with the DPMSolverMultistepScheduler
for very fast inference:
import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
pipe = StableDiffusionPipeline.from_pretrained(model_base, torch_dtype=torch.float16)
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
And here&#39;s where the magic comes. We load the LoRA weights from the Hub on top of the regular model weights, move the pipeline to the cuda device and run inference:
pipe.unet.load_attn_procs(model_path)
pipe.to(&quot;cuda&quot;)
image = pipe(&quot;Green pokemon with menacing face&quot;, num_inference_steps=25).images[0]
image.save(&quot;green_pokemon.png&quot;)
Dreamboothing with LoRA
Dreambooth allows you to &quot;teach&quot; new concepts to a Stable Diffusion model. LoRA is compatible with Dreambooth and the process is similar to fine-tuning, with a couple of advantages:
- Training is faster.
- We only need a few images of the subject we want to train (5 or 10 are usually enough).
- We can tweak the text encoder, if we want, for additional fidelity to the subject.
To train Dreambooth with LoRA you need to use this diffusers script. Please, take a look at the README, the documentation and our hyperparameter exploration blog post for details.
For a quick, cheap and easy way to train your Dreambooth models with LoRA, please check this Space by hysts
. You need to duplicate it and assign a GPU so it runs fast. This process will save you from having to set up your own training environment and you&#39;ll be able to train your models in minutes!
Other Methods
The quest for easy fine-tuning is not new. In addition to Dreambooth, textual inversion is another popular method that attempts to teach new concepts to a trained Stable Diffusion Model. One of the main reasons for using Textual Inversion is that trained weights are also small and easy to share. However, they only work for a single subject (or a small handful of them), whereas LoRA can be used for general-purpose fine-tuning, meaning that it can be adapted to new domains or datasets.
Pivotal Tuning is a method that tries to combine Textual Inversion with LoRA. First, you teach the model a new concept using Textual Inversion techniques, obtaining a new token embedding to represent it. Then, you train that token embedding using LoRA to get the best of both worlds.
We haven&#39;t explored Pivotal Tuning with LoRA yet. Who&#39;s up for the challenge? ü§ó </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
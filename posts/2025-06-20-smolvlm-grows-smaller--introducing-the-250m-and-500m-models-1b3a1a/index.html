<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>SmolVLM Grows Smaller ‚Äì Introducing the 250M &amp; 500M Models!</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/index.BoXAN-Xr.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ SmolVLM Grows Smaller ‚Äì Introducing the 250M &amp; 500M Models!</h1> <p class="text-sm text-gray-500"> 2025/1/23 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/smolervlm" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> SmolVLM Grows Smaller ‚Äì Introducing the 250M &amp; 500M Models!
TLDR
We‚Äôre excited to announce two new additions to the SmolVLM family: SmolVLM-256M and SmolVLM-500M. That‚Äôs right‚Äî256M parameters, making it the smallest Vision Language Model in the world!
We built on everything we learned from SmolVLM 2B while focusing on efficiency, data mixtures, and new design trade-offs. We are excited to introduce a pair of models that preserve strong multimodal performance in a fraction of the footprint.
This release comes with four checkpoints: two base models and two instruction fine-tuned models with sizes 256M and 500M parameters. These models can be loadable directly to transformers, MLX and ONNX, and we have demos for transformers and WebGPU (with ONNX). You can find all the models and the demo for this release here.
Table of Contents
- Overview
- Why Go Smaller?
- What Changed Since SmolVLM 2B?
- Smaller Multimodal Retrieval: ColSmolVLM 256M &amp; 500M
- Using Smaller SmolVLM
- Citation information
- Next Steps
Overview
- SmolVLM-256M ‚Äì The world‚Äôs smallest VLM!
- SmolVLM-500M ‚Äì A half-billion-parameter sibling that offers a significant performance bump while still remaining super lightweight.
- New Vision Encoder Choices ‚Äì We compared SigLIP 400M SO (used in SmolVLM 2B and many other large VLMs) against a smaller SigLIP base patch-16/512. Surprisingly, the bigger encoder offered only marginally better results, so we opted for the 93M-parameter SigLIP base patch-16/512 in these new releases.
- Larger Image Resolution ‚Äì Our smaller vision encoder processes images at a larger resolution (inspired by Apple‚Äôs VLM research and Google‚Äôs PaliGemma). This yields sharper image understanding with minimal overhead.
- Training Optimization ‚Äì A new tokenization trick significantly boosted real-world benchmarks, even though it made the training loss look worse on paper.
We&#39;re now reaching model parity with the SmolLM2 family (135M, 360M, 1.7B), so you have a complete set of smaller LLM + VLM combos to play with.
Why Go Smaller?
When we released SmolVLM 2B, the community response was fantastic: The model is very light weight, open-source and permissive, and easy to integrate into existing workflows. But we wanted to push this approach even further for people working with constrained devices, consumer laptops, or even potentially browser-based inference. That‚Äôs where our new 256M and 500M models come in. On the other side, for people trying to process huge amounts of data, these models can run at a fraction of the cost of the 2B model.
In the last year, we trained two 80B VLMs and reduced them to 8B. Then for SmolVLM we took the challenge of reducing that 2B. And what we learned was that we could push the frontier way further! We are excited to show that at 256M and 500M we can still get great performance. Our new 256M model is the smallest VLM ever released, yet it surpasses the performance of our Idefics 80B model from just 17 months ago.
Meet the 256M Parameter Giant
With just 256 million parameters, this model stands as the tiniest VLM ever. Despite its small size, it packs a surprising punch. It‚Äôs more than capable on many multimodal tasks, including:
- Captioning: Describing images or short videos.
- Document Q&amp;A: Answering questions about PDFs or scanned text.
- Basic Visual Reasoning: Answering questions about charts or diagrams.
A Step Up: 500M
If you need more performance headroom while still keeping the memory usage low, SmolVLM-500M is our half-billion-parameter compromise. It‚Äôs significantly smaller than the previous 2B release yet manages to push scores on tasks like DocVQA and MMMU closer to the bigger models. We also found this model to be more robust to prompting, which makes it out-of-the-box better fitted for production. But both models do great when fine-tuned.
We have visualized the throughput gains across different batch sizes in below graph. Below numbers are throughput benchmarks ran on A100.
What Changed Since SmolVLM 2B?
1. Vision Encoder Choices Previously, we used the standard SigLIP 400M SO vision backbone, the same one found in many VLM architectures. For these smaller models, we experimented with two setups:
- SigLIP 400M SO: Higher capacity, great performance.
- SigLIP base patch-16/512 (93M): Much smaller, surprisingly close performance.
We found the performance gap wasn‚Äôt big enough to justify the heavier encoder for our 256M and 500M models. So, we decided to go small on the vision encoder, too. As a bonus, the smaller encoder processes images at a larger resolution, which (per research from Apple and Google) can often yield better visual understanding without ballooning parameter counts.
2. Data mixture update
Similarly to our previous release, we rely on The Cauldron and Docmatix with the addition of MathWriting to the mix.
The proportions of the datasets were adjusted to place a stronger emphasis on document understanding (41%) and image captioning (14%), while still maintaining a balanced focus on other essential areas such as visual reasoning, chart comprehension, and general instruction following. With this update the model is built on a strong document understanding basis and lets the door open to fine-tunes that will adjust its understanding of specific tasks.
3. Tokenization optimizations
We increased the pixel shuffle even more! Our new models encode images at a rate of 4096 pixels per token, compared to 1820 pixels per token in the 2B model.
To optimize the tokenization even more, we added special tokens to represent our sub-image separators in a more efficient way. This means that now instead of a string like &lt;row_1_col_1&gt;
being mapped to 7 tokens, it is mapped to a single token. We did the same for strings up to &lt;row_6_col_6&gt;
. This led to a sizeable improvement in the model&#39;s stability during training and quality of the results. More details were documented in this LinkedIn post.
4. Completing the SmolLM2-SmolVLM family
SmolLM2 came in three sizes: 135M, 360M, and 1.7B. With the two models we are releasing today, we now have a complete set of smaller LLM + VLM combos to play with.
Smaller Multimodal Retrieval: ColSmolVLM 256M &amp; 500M
We also found that it&#39;s surprisingly easy to fine-tune and experiment. The team behind the ColBERT-like retrieval models have trained ColSmolVLM, delivering SOTA multimodal retrieval speeds with performance rivaling models 10x their size. SmolVLM makes it faster and cheaper to build searchable databases. We think the 256M model can become a great specialized model for many tasks. Find the link on how to use the new ColSmolVLM with the new SmolVLM models in Next Steps.
SmolDocling
We partnered with IBM to build models for Docling. Their early results with the 256M model are impressive. Below are some early examples they shared with us. Stay tuned for more updates on this!
Using Smaller SmolVLM
Newer SmolVLMs are working out-of-the-box with the old SmolVLM code, so you can use transformers and MLX for inference and fine-tuning, and TRL for alignment üöÄ Moreover, this release also comes with ONNX checkpoints.
Get started with SmolVLM using transformers like below.
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
# Initialize processor and model
processor = AutoProcessor.from_pretrained(&quot;HuggingFaceTB/SmolVLM-500M-Instruct&quot;)
model = AutoModelForVision2Seq.from_pretrained(
&quot;HuggingFaceTB/SmolVLM-500M-Instruct&quot;,
torch_dtype=torch.bfloat16,
_attn_implementation=&quot;flash_attention_2&quot; if DEVICE == &quot;cuda&quot; else &quot;eager&quot;,
)
# Create input messages
messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{&quot;type&quot;: &quot;image&quot;},
{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Can you describe this image?&quot;}
]
},
]
# Preprocess
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, images=[image], return_tensors=&quot;pt&quot;)
# Generate
generated_ids = model.generate(**inputs, max_new_tokens=500)
generated_texts = processor.batch_decode(
generated_ids,
skip_special_tokens=True,
)
Use SmolVLM with MLX by running the following CLI command:
python3 -m mlx_vlm.generate --model HuggingfaceTB/SmolVLM-500M-Instruct --max-tokens 400 --temp 0.0 --image https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vlm_example.jpg --prompt &quot;What is in this image?&quot;
You can play with the WebGPU demos for the SmolVLM-256M-Instruct and SmolVLM-500M-Instruct.
Find links to fine-tuning and multimodal RAG with ColSmolVLM on the Next Steps.
Citation information
You can cite us in the following way:
@article{marafioti2025smolvlm,
title={SmolVLM: Redefining small and efficient multimodal models},
author={Andr√©s Marafioti and Orr Zohar and Miquel Farr√© and Merve Noyan and Elie Bakouch and Pedro Cuenca and Cyril Zakka and Loubna Ben Allal and Anton Lozhkov and Nouamane Tazi and Vaibhav Srivastav and Joshua Lochner and Hugo Larcher and Mathieu Morlon and Lewis Tunstall and Leandro von Werra and Thomas Wolf},
journal={arXiv preprint arXiv:2504.05299},
year={2025}
}
Next Steps
- We are looking forward to ways you will be using SmollerVLMs! Get started here.
- Learn more in-depth about SmolVLM here.
- Fine-tuning and QLoRA SmolVLM using transformers
- [Direct Preference Optimization on SmolVLM using TRL](Fine-tuning SmolVLM using direct preference optimization (DPO) with TRL on a consumer GPU)
- Smol Multimodal RAG: Building with ColSmolVLM and SmolVLM on Colab‚Äôs Free-Tier GPU
We would like to thank ViDoRe team for training ColSmolVLM: Tony Wu, Manuel Faysse, and Joshua Lochner for the ONNX conversion and WebGPU demo and Vaibhav Srivastav for his help on this release. </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Music AI Sandbox, now with new features and broader access</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.CO8YC2Z5.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Music AI Sandbox, now with new features and broader access</h1> <p class="text-sm text-gray-500"> 2025/4/24 ‚Äì DeepMind Blog  <a href="https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Models
Music AI Sandbox, now with new features and broader access
Musicians today are drawing inspiration and crafting their sound using a broad ecosystem of tools ‚Äî from mobile apps to traditional Digital Audio Workstations, specialized plug-ins and hardware. Now, artificial intelligence (AI) is emerging as a powerful new part of this creative toolkit, opening doors to novel workflows and sonic possibilities.
Google has long collaborated with musicians, producers, and artists in the research and development of music AI tools. Ever since launching the Magenta project, in 2016, we‚Äôve been exploring how AI can enhance creativity ‚Äî sparking inspiration, facilitating exploration and enabling new forms of expression, always hand-in-hand with the music community.
Our ongoing collaborations led to the creation of Music AI Sandbox, in 2023, which we‚Äôve shared with musicians, producers and songwriters through YouTube‚Äôs Music AI Incubator.
Building upon the work we&#39;ve done to date, today, we&#39;re introducing new features and improvements to Music AI Sandbox, including Lyria 2, our latest music generation model. We&#39;re giving more musicians, producers and songwriters in the U.S. access to experiment with these tools, and are gathering feedback to inform their development.
We&#39;re excited to see what this growing community creates with Music AI Sandbox and encourage interested musicians, songwriters, and producers to sign up here.
Music AI Sandbox
We created Music AI Sandbox in close collaboration with musicians. Their input guided our development and experiments, resulting in a set of responsibly created tools that are practical, useful and can open doors to new forms of music creation.
The Music AI Sandbox is a set of experimental tools, which can spark new creative possibilities and help artists explore unique musical ideas. Artists can generate fresh instrumental ideas, craft vocal arrangements or simply break through a creative block.
With these tools, musicians can discover new sounds, experiment with different genres, expand and enhance their musical libraries, or develop entirely new styles. They can also push further into unexplored territories ‚Äî from unique soundscapes to their next creative breakthrough.
Create new musical parts
Quickly try out music ideas by describing what kind of sound you want ‚Äî the Music AI Sandbox understands genres, moods, vocal styles and instruments. The Create tool helps generate many different music samples to spark the imagination or for use in a track. Artists can also place their own lyrics on a timeline and specify musical characteristics, like tempo and key.
Explore new directions with Extend
Need inspiration for where to take an existing musical piece? The Extend feature generates musical continuations based on uploaded or generated audio clips. It‚Äôs a way to hear potential developments for your ideas, reimagine your own work, or overcome writer&#39;s block.
Reimagine music with Edit
Reshape music with fine-grained control. The Edit feature makes it possible to transform the mood, genre or style of an entire clip, or make targeted modifications to specific parts. Intuitive controls enable subtle tweaks or dramatic shifts. Now, users can also transform audio using text prompts, experiment with preset transformations to fill gaps or blend clips and build transitions between different musical sections.
What artists are creating with the Music AI Sandbox
See how musicians are leveraging this tool to fuel their creativity and generate fresh musical concepts.
Listen to these demo tracks that artists are bringing to life using the Music AI Sandbox:
High-fidelity and real-time music with Lyria
Since introducing Lyria, we‚Äôve continued to innovate with input and insights from music industry professionals. Our latest music generation model, Lyria 2, delivers high-fidelity music and professional-grade audio outputs that capture subtle nuances across a range of genres and intricate compositions.
We‚Äôve also developed Lyria RealTime, which allows users to interactively create, perform and control music in real-time, mixing genres, blending styles and shaping audio moment by moment. Lyria RealTime can help users create continuous streams of music, forge sonic connections and quickly explore ideas on the fly.
Responsibly deploying generative technologies is core to our values, so all music generated by Lyria 2 and Lyria RealTime models is watermarked using our SynthID technology.
Building AI for musicians, with musicians
Through collaborations like Music AI Sandbox, we aim to build trust with musicians, the industry and artists. Their expertise and valuable feedback help us ensure our tools empower creators, enabling them to realize the possibilities of AI in their art and explore new ways to express themselves. We‚Äôre excited to see what artists create with our tools and look forward to sharing more later this year.
Music AI Sandbox was developed by Adam Roberts, Amy Stuart, Ari Troper, Beat Gfeller, Chris Deaner, Chris Reardon, Colin McArdell, DY Kim, Ethan Manilow, Felix Riedel, George Brower, Hema Manickavasagam, Jeff Chang, Jesse Engel, Michael Chang, Moon Park, Pawel Wluka, Reed Enger, Ross Cairns, Sage Stevens, Tom Jenkins, Tom Hume and Yotam Mann. Additional contributions provided by Arathi Sethumadhavan, Brian McWilliams, CƒÉtƒÉlina Cangea, Doug Fritz, Drew Jaegle, Eleni Shaw, Jessi Liang, Kazuya Kawakami, Kehang Han, and Veronika Goldberg.
Lyria 2 was developed by Asahi Ushio, Beat Gfeller, Brian McWilliams, Kazuya Kawakami, Keyang Xu, Matej Kastelic, Mauro Verzetti, Myriam Hamed Torres, Ondrej Skopek, Pavel Khrushkov, Pen Li, Tobenna Peter Igwe and Zalan Borsos. Additional contributions provided by Adam Roberts, Andrea Agostinelli, Benigno Uria, Carrie Zhang, Chris Deaner, Colin McArdell, DY Kim, Eleni Shaw, Ethan Manilow, Hongliang Fei, Jason Baldridge, Jesse Engel, Li Li, Luyu Wang, Mauricio Zuluaga, Nemanja Spasojevic, Noah Constant, Ruba Haroun, Tayniat Khan, Volodymyr Mnih, Yan Wu and Zoe Ashwood.
Special thanks to A√§ron van den Oord, Mahyar Bordbar, Douglas Eck, Eli Collins, Mira Lane, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the development process.
We also acknowledge the many other individuals who contributed across Google DeepMind and Alphabet, including our colleagues at YouTube (a particular shout out to the YouTube Artist Partnerships team led by Vivien Lewit for their support partnering with the music industry). </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
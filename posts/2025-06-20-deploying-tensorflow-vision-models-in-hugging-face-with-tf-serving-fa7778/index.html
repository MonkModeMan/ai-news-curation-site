<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Deploying TensorFlow Vision Models in Hugging Face with TF Serving</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/index.BoXAN-Xr.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Deploying TensorFlow Vision Models in Hugging Face with TF Serving</h1> <p class="text-sm text-gray-500"> 2022/7/25 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/tf-serving-vision" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Deploying TensorFlow Vision Models in Hugging Face with TF Serving
In the past few months, the Hugging Face team and external contributors added a variety of vision models in TensorFlow to Transformers. This list is growing comprehensively and already includes state-of-the-art pre-trained models like Vision Transformer, Masked Autoencoders, RegNet, ConvNeXt, and many others!
When it comes to deploying TensorFlow models, you have got a variety of options. Depending on your use case, you may want to expose your model as an endpoint or package it in an application itself. TensorFlow provides tools that cater to each of these different scenarios.
In this post, you&#39;ll see how to deploy a Vision Transformer (ViT) model (for image classification) locally using TensorFlow Serving (TF Serving). This will allow developers to expose the model either as a REST or gRPC endpoint. Moreover, TF Serving supports many deployment-specific features off-the-shelf such as model warmup, server-side batching, etc.
To get the complete working code shown throughout this post, refer to the Colab Notebook shown at the beginning.
Saving the Model
All TensorFlow models in ü§ó Transformers have a method named
save_pretrained()
. With it, you can serialize the model weights in
the h5 format as well as in the standalone SavedModel format.
TF Serving needs a model to be present in the SavedModel format. So, let&#39;s first
load a Vision Transformer model and save it:
from transformers import TFViTForImageClassification
temp_model_dir = &quot;vit&quot;
ckpt = &quot;google/vit-base-patch16-224&quot;
model = TFViTForImageClassification.from_pretrained(ckpt)
model.save_pretrained(temp_model_dir, saved_model=True)
By default, save_pretrained()
will first create a version directory
inside the path we provide to it. So, the path ultimately becomes:
{temp_model_dir}/saved_model/{version}
.
We can inspect the serving signature of the SavedModel like so:
saved_model_cli show --dir {temp_model_dir}/saved_model/1 --tag_set serve --signature_def serving_default
This should output:
The given SavedModel SignatureDef contains the following input(s):
inputs[&#39;pixel_values&#39;] tensor_info:
dtype: DT_FLOAT
shape: (-1, -1, -1, -1)
name: serving_default_pixel_values:0
The given SavedModel SignatureDef contains the following output(s):
outputs[&#39;logits&#39;] tensor_info:
dtype: DT_FLOAT
shape: (-1, 1000)
name: StatefulPartitionedCall:0
Method name is: tensorflow/serving/predict
As can be noticed the model accepts single 4-d inputs (namely
pixel_values
) which has the following axes: (batch_size, num_channels, height, width)
. For this model, the acceptable height
and width are set to 224, and the number of channels is 3. You can verify
this by inspecting the config argument of the model (model.config
).
The model yields a 1000-d vector of logits
.
Model Surgery
Usually, every ML model has certain preprocessing and postprocessing steps. The ViT model is no exception to this. The major preprocessing steps include:
Scaling the image pixel values to [0, 1] range.
Normalizing the scaled pixel values to [-1, 1].
Resizing the image so that it has a spatial resolution of (224, 224).
You can confirm these by investigating the image processor associated with the model:
from transformers import AutoImageProcessor
processor = AutoImageProcessor.from_pretrained(ckpt)
print(processor)
This should print:
ViTImageProcessor {
&quot;do_normalize&quot;: true,
&quot;do_resize&quot;: true,
&quot;image_mean&quot;: [
0.5,
0.5,
0.5
],
&quot;image_std&quot;: [
0.5,
0.5,
0.5
],
&quot;resample&quot;: 2,
&quot;size&quot;: 224
}
Since this is an image classification model pre-trained on the ImageNet-1k dataset, the model outputs need to be mapped to the ImageNet-1k classes as the post-processing step.
To reduce the developers&#39; cognitive load and training-serving skew, it&#39;s often a good idea to ship a model that has most of the preprocessing and postprocessing steps in built. Therefore, you should serialize the model as a SavedModel such that the above-mentioned processing ops get embedded into its computation graph.
Preprocessing
For preprocessing, image normalization is one of the most essential components:
def normalize_img(
img, mean=processor.image_mean, std=processor.image_std
):
# Scale to the value range of [0, 1] first and then normalize.
img = img / 255
mean = tf.constant(mean)
std = tf.constant(std)
return (img - mean) / std
You also need to resize the image and transpose it so that it has leading channel dimensions since following the standard format of ü§ó Transformers. The below code snippet shows all the preprocessing steps:
CONCRETE_INPUT = &quot;pixel_values&quot; # Which is what we investigated via the SavedModel CLI.
SIZE = processor.size[&quot;height&quot;]
def normalize_img(
img, mean=processor.image_mean, std=processor.image_std
):
# Scale to the value range of [0, 1] first and then normalize.
img = img / 255
mean = tf.constant(mean)
std = tf.constant(std)
return (img - mean) / std
def preprocess(string_input):
decoded_input = tf.io.decode_base64(string_input)
decoded = tf.io.decode_jpeg(decoded_input, channels=3)
resized = tf.image.resize(decoded, size=(SIZE, SIZE))
normalized = normalize_img(resized)
normalized = tf.transpose(
normalized, (2, 0, 1)
) # Since HF models are channel-first.
return normalized
@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def preprocess_fn(string_input):
decoded_images = tf.map_fn(
preprocess, string_input, dtype=tf.float32, back_prop=False
)
return {CONCRETE_INPUT: decoded_images}
Note on making the model accept string inputs:
When dealing with images via REST or gRPC requests the size of the request payload can easily spiral up depending on the resolution of the images being passed. This is why it is a good practice to compress them reliably and then prepare the request payload.
Postprocessing and Model Export
You&#39;re now equipped with the preprocessing operations that you can inject into the model&#39;s existing computation graph. In this section, you&#39;ll also inject the post-processing operations into the graph and export the model!
def model_exporter(model: tf.keras.Model):
m_call = tf.function(model.call).get_concrete_function(
tf.TensorSpec(
shape=[None, 3, SIZE, SIZE], dtype=tf.float32, name=CONCRETE_INPUT
)
)
@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])
def serving_fn(string_input):
labels = tf.constant(list(model.config.id2label.values()), dtype=tf.string)
images = preprocess_fn(string_input)
predictions = m_call(**images)
indices = tf.argmax(predictions.logits, axis=1)
pred_source = tf.gather(params=labels, indices=indices)
probs = tf.nn.softmax(predictions.logits, axis=1)
pred_confidence = tf.reduce_max(probs, axis=1)
return {&quot;label&quot;: pred_source, &quot;confidence&quot;: pred_confidence}
return serving_fn
You can first derive the concrete function
from the model&#39;s forward pass method (call()
) so the model is nicely compiled
into a graph. After that, you can apply the following steps in order:
Pass the inputs through the preprocessing operations.
Pass the preprocessing inputs through the derived concrete function.
Post-process the outputs and return them in a nicely formatted dictionary.
Now it&#39;s time to export the model!
MODEL_DIR = tempfile.gettempdir()
VERSION = 1
tf.saved_model.save(
model,
os.path.join(MODEL_DIR, str(VERSION)),
signatures={&quot;serving_default&quot;: model_exporter(model)},
)
os.environ[&quot;MODEL_DIR&quot;] = MODEL_DIR
After exporting, let&#39;s inspect the model signatures again:
saved_model_cli show --dir {MODEL_DIR}/1 --tag_set serve --signature_def serving_default
The given SavedModel SignatureDef contains the following input(s):
inputs[&#39;string_input&#39;] tensor_info:
dtype: DT_STRING
shape: (-1)
name: serving_default_string_input:0
The given SavedModel SignatureDef contains the following output(s):
outputs[&#39;confidence&#39;] tensor_info:
dtype: DT_FLOAT
shape: (-1)
name: StatefulPartitionedCall:0
outputs[&#39;label&#39;] tensor_info:
dtype: DT_STRING
shape: (-1)
name: StatefulPartitionedCall:1
Method name is: tensorflow/serving/predict
You can notice that the model&#39;s signature has now changed. Specifically, the input type is now a string and the model returns two things: a confidence score and the string label.
Provided you&#39;ve already installed TF Serving (covered in the Colab Notebook), you&#39;re now ready to deploy this model!
Deployment with TensorFlow Serving
It just takes a single command to do this:
nohup tensorflow_model_server \
--rest_api_port=8501 \
--model_name=vit \
--model_base_path=$MODEL_DIR &gt;server.log 2&gt;&amp;1
From the above command, the important parameters are:
rest_api_port
denotes the port number that TF Serving will use deploying the REST endpoint of your model. By default, TF Serving uses the 8500 port for the gRPC endpoint.model_name
specifies the model name (can be anything) that will used for calling the APIs.model_base_path
denotes the base model path that TF Serving will use to load the latest version of the model.
(The complete list of supported parameters is here.)
And voila! Within minutes, you should be up and running with a deployed model having two endpoints - REST and gRPC.
Querying the REST Endpoint
Recall that you exported the model such that it accepts string inputs encoded with the base64 format. So, to craft the request payload you can do something like this:
# Get image of a cute cat.
image_path = tf.keras.utils.get_file(
&quot;image.jpg&quot;, &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;
)
# Read the image from disk as raw bytes and then encode it.
bytes_inputs = tf.io.read_file(image_path)
b64str = base64.urlsafe_b64encode(bytes_inputs.numpy()).decode(&quot;utf-8&quot;)
# Create the request payload.
data = json.dumps({&quot;signature_name&quot;: &quot;serving_default&quot;, &quot;instances&quot;: [b64str]})
TF Serving&#39;s request payload format specification for the REST endpoint
is available here.
Within the instances
you can pass multiple encoded images. This kind
of endpoints are meant to be consumed for online prediction scenarios.
For inputs having more than a single data point, you would to want to
enable batching
to get performance optimization benefits.
Now you can call the API:
headers = {&quot;content-type&quot;: &quot;application/json&quot;}
json_response = requests.post(
&quot;http://localhost:8501/v1/models/vit:predict&quot;, data=data, headers=headers
)
print(json.loads(json_response.text))
# {&#39;predictions&#39;: [{&#39;label&#39;: &#39;Egyptian cat&#39;, &#39;confidence&#39;: 0.896659195}]}
The REST API is -
http://localhost:8501/v1/models/vit:predict
following the specification from
here. By default,
this always picks up the latest version of the model. But if you wanted a
specific version you can do: http://localhost:8501/v1/models/vit/versions/1:predict
.
Querying the gRPC Endpoint
While REST is quite popular in the API world, many applications often benefit from gRPC. This post does a good job comparing the two ways of deployment. gRPC is usually preferred for low-latency, highly scalable, and distributed systems.
There are a couple of steps are. First, you need to open a communication channel:
import grpc
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2_grpc
channel = grpc.insecure_channel(&quot;localhost:8500&quot;)
stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)
Then, create the request payload:
request = predict_pb2.PredictRequest()
request.model_spec.name = &quot;vit&quot;
request.model_spec.signature_name = &quot;serving_default&quot;
request.inputs[serving_input].CopyFrom(tf.make_tensor_proto([b64str]))
You can determine the serving_input
key programmatically like so:
loaded = tf.saved_model.load(f&quot;{MODEL_DIR}/{VERSION}&quot;)
serving_input = list(
loaded.signatures[&quot;serving_default&quot;].structured_input_signature[1].keys()
)[0]
print(&quot;Serving function input:&quot;, serving_input)
# Serving function input: string_input
Now, you can get some predictions:
grpc_predictions = stub.Predict(request, 10.0) # 10 secs timeout
print(grpc_predictions)
outputs {
key: &quot;confidence&quot;
value {
dtype: DT_FLOAT
tensor_shape {
dim {
size: 1
}
}
float_val: 0.8966591954231262
}
}
outputs {
key: &quot;label&quot;
value {
dtype: DT_STRING
tensor_shape {
dim {
size: 1
}
}
string_val: &quot;Egyptian cat&quot;
}
}
model_spec {
name: &quot;resnet&quot;
version {
value: 1
}
signature_name: &quot;serving_default&quot;
}
You can also fetch the key-values of our interest from the above results like so:
grpc_predictions.outputs[&quot;label&quot;].string_val, grpc_predictions.outputs[
&quot;confidence&quot;
].float_val
# ([b&#39;Egyptian cat&#39;], [0.8966591954231262])
Wrapping Up
In this post, we learned how to deploy a TensorFlow vision model from Transformers with TF Serving. While local deployments are great for weekend projects, we would want to be able to scale these deployments to serve many users. In the next series of posts, you&#39;ll see how to scale up these deployments with Kubernetes and Vertex AI. </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>FastRTC: The Real-Time Communication Library for Python</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.CO8YC2Z5.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ FastRTC: The Real-Time Communication Library for Python</h1> <p class="text-sm text-gray-500"> 2025/2/25 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/fastrtc" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> FastRTC: The Real-Time Communication Library for Python
In the last few months, many new real-time speech models have been released and entire companies have been founded around both open and closed source models. To name a few milestones:
- OpenAI and Google released their live multimodal APIs for ChatGPT and Gemini. OpenAI even went so far as to release a 1-800-ChatGPT phone number!
- Kyutai released Moshi, a fully open-source audio-to-audio LLM. Alibaba released Qwen2-Audio and Fixie.ai released Ultravox - two open-source LLMs that natively understand audio.
- ElevenLabs raised $180m in their Series C.
Despite the explosion on the model and funding side, it&#39;s still difficult to build real-time AI applications that stream audio and video, especially in Python.
- ML engineers may not have experience with the technologies needed to build real-time applications, such as WebRTC.
- Even code assistant tools like Cursor and Copilot struggle to write Python code that supports real-time audio/video applications. I know from experience!
That&#39;s why we&#39;re excited to announce FastRTC
, the real-time communication library for Python. The library is designed to make it super easy to build real-time audio and video AI applications entirely in Python!
In this blog post, we&#39;ll walk through the basics of FastRTC
by building real-time audio applications. At the end, you&#39;ll understand the core features of FastRTC
:
- üó£Ô∏è Automatic Voice Detection and Turn Taking built-in, so you only need to worry about the logic for responding to the user.
- üíª Automatic UI - Built-in WebRTC-enabled Gradio UI for testing (or deploying to production!).
- üìû Call via Phone - Use fastphone() to get a FREE phone number to call into your audio stream (HF Token required. Increased limits for PRO accounts).
- ‚ö°Ô∏è WebRTC and Websocket support.
- üí™ Customizable - You can mount the stream to any FastAPI app so you can serve a custom UI or deploy beyond Gradio.
- üß∞ Lots of utilities for text-to-speech, speech-to-text, stop word detection to get you started.
Let&#39;s dive in.
Getting Started
We&#39;ll start by building the &quot;hello world&quot; of real-time audio: echoing back what the user says. In FastRTC
, this is as simple as:
from fastrtc import Stream, ReplyOnPause
import numpy as np
def echo(audio: tuple[int, np.ndarray]) -&gt; tuple[int, np.ndarray]:
yield audio
stream = Stream(ReplyOnPause(echo), modality=&quot;audio&quot;, mode=&quot;send-receive&quot;)
stream.ui.launch()
Let&#39;s break it down:
- The
ReplyOnPause
will handle the voice detection and turn taking for you. You just have to worry about the logic for responding to the user. Any generator that returns a tuple of audio, (represented as(sample_rate, audio_data)
) will work. - The
Stream
class will build a Gradio UI for you to quickly test out your stream. Once you have finished prototyping, you can deploy your Stream as a production-ready FastAPI app in a single line of code -stream.mount(app)
. Whereapp
is a FastAPI app.
Here it is in action:
Leveling-Up: LLM Voice Chat
The next level is to use an LLM to respond to the user. FastRTC
comes with built-in speech-to-text and text-to-speech capabilities, so working with LLMs is really easy. Let&#39;s change our echo
function accordingly:
import os
from fastrtc import (ReplyOnPause, Stream, get_stt_model, get_tts_model)
from openai import OpenAI
sambanova_client = OpenAI(
api_key=os.getenv(&quot;SAMBANOVA_API_KEY&quot;), base_url=&quot;https://api.sambanova.ai/v1&quot;
)
stt_model = get_stt_model()
tts_model = get_tts_model()
def echo(audio):
prompt = stt_model.stt(audio)
response = sambanova_client.chat.completions.create(
model=&quot;Meta-Llama-3.2-3B-Instruct&quot;,
messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
max_tokens=200,
)
prompt = response.choices[0].message.content
for audio_chunk in tts_model.stream_tts_sync(prompt):
yield audio_chunk
stream = Stream(ReplyOnPause(echo), modality=&quot;audio&quot;, mode=&quot;send-receive&quot;)
stream.ui.launch()
We&#39;re using the SambaNova API since it&#39;s fast. The get_stt_model()
will fetch Moonshine Base and get_tts_model()
will fetch Kokoro from the Hub, both of which have been further optimized for on-device CPU inference. But you can use any LLM/text-to-speech/speech-to-text API or even a speech-to-speech model. Bring the tools you love - FastRTC
just handles the real-time communication layer.
Bonus: Call via Phone
If instead of stream.ui.launch()
, you call stream.fastphone()
, you&#39;ll get a free phone number to call into your stream. Note, a Hugging Face token is required. Increased limits for PRO accounts.
You&#39;ll see something like this in your terminal:
INFO: Your FastPhone is now live! Call +1 877-713-4471 and use code 530574 to connect to your stream.
INFO: You have 30:00 minutes remaining in your quota (Resetting on 2025-03-23)
You can then call the number and it will connect you to your stream!
Next Steps
- Read the docs to learn more about the basics of
FastRTC
. - The best way to start building is by checking out the cookbook. Find out how to integrate with popular LLM providers (including OpenAI and Gemini&#39;s real-time APIs), integrate your stream with a FastAPI app and do a custom deployment, return additional data from your handler, do video processing, and more!
- ‚≠êÔ∏è Star the repo and file bug and issue requests!
- Follow the FastRTC Org on HuggingFace for updates and check out deployed examples!
Thank you for checking out FastRTC
! </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
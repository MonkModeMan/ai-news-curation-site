<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Accelerating LLM Inference with TGI on Intel Gaudi</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.Ba3Vwl3b.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ✅ タイトル --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">📰 Accelerating LLM Inference with TGI on Intel Gaudi</h1> <p class="text-sm text-gray-500"> 2025/3/28 – Hugging Face Blog  <a href="https://huggingface.co/blog/intel-gaudi-backend-for-tgi" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
元記事
</a>  </p> </header> <!-- ✅ 本文 --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> 🚀 Accelerating LLM Inference with TGI on Intel Gaudi
We&#39;re excited to announce the native integration of Intel Gaudi hardware support directly into Text Generation Inference (TGI), our production-ready serving solution for Large Language Models (LLMs). This integration brings the power of Intel&#39;s specialized AI accelerators to our high-performance inference stack, enabling more deployment options for the open-source AI community 🎉
✨ What&#39;s New?
We&#39;ve fully integrated Gaudi support into TGI&#39;s main codebase in PR #3091. Previously, we maintained a separate fork for Gaudi devices at tgi-gaudi. This was cumbersome for users and prevented us from supporting the latest TGI features at launch. Now using the new TGI multi-backend architecture, we support Gaudi directly on TGI – no more finicking on a custom repository 🙌
This integration supports Intel&#39;s full line of Gaudi hardware:
- Gaudi1 💻: Available on AWS EC2 DL1 instances
- Gaudi2 💻💻: Available on Intel Tiber AI Cloud and Denvr Dataworks
- Gaudi3 💻💻💻: Available on Intel Tiber AI Cloud, IBM Cloud and from OEM such as Dell, HP and Supermicro
You can also find more information on Gaudi hardware on Intel&#39;s Gaudi product page
🌟 Why This Matters
The Gaudi backend for TGI provides several key benefits:
- Hardware Diversity 🔄: More options for deploying LLMs in production beyond traditional GPUs
- Cost Efficiency 💰: Gaudi hardware often provides compelling price-performance for specific workloads
- Production-Ready ⚙️: All the robustness of TGI (dynamic batching, streamed responses, etc.) now available on Gaudi
- Model Support 🤖: Run popular models like Llama 3.1, Mixtral, Mistral, and more on Gaudi hardware
- Advanced Features 🔥: Support for multi-card inference (sharding), vision-language models, and FP8 precision
🚦 Getting Started with TGI on Gaudi
The easiest way to run TGI on Gaudi is to use our official Docker image. You need to run the image on a Gaudi hardware machine. Here&#39;s a basic example to get you started:
model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run
hf_token=YOUR_HF_ACCESS_TOKEN
docker run --runtime=habana --cap-add=sys_nice --ipc=host \
-p 8080:80 \
-v $volume:/data \
-e HF_TOKEN=$hf_token \
-e HABANA_VISIBLE_DEVICES=all \
ghcr.io/huggingface/text-generation-inference:3.2.1-gaudi \
--model-id $model
Once the server is running, you can send inference requests:
curl 127.0.0.1:8080/generate
-X POST
-d &#39;{&quot;inputs&quot;:&quot;What is Deep Learning?&quot;,&quot;parameters&quot;:{&quot;max_new_tokens&quot;:32}}&#39;
-H &#39;Content-Type: application/json&#39;
For comprehensive documentation on using TGI with Gaudi, including how-to guides and advanced configurations, refer to the new dedicated Gaudi backend documentation.
🎉 Top features
We have optimized the following models for both single and multi-card configurations. This means these models run as fast as possible on Intel Gaudi. We&#39;ve specifically optimized the modeling code to target Intel Gaudi hardware, ensuring we offer the best performance and fully utilize Gaudi&#39;s capabilities:
- Llama 3.1 (8B and 70B)
- Llama 3.3 (70B)
- Llama 3.2 Vision (11B)
- Mistral (7B)
- Mixtral (8x7B)
- CodeLlama (13B)
- Falcon (180B)
- Qwen2 (72B)
- Starcoder and Starcoder2
- Gemma (7B)
- Llava-v1.6-Mistral-7B
- Phi-2
🏃♂️ We also offer many advanced features on Gaudi hardware, such as FP8 quantization thanks to Intel Neural Compressor (INC), enabling even greater performance optimizations.
✨ Coming soon! We&#39;re excited to expand our model lineup with cutting-edge additions including DeepSeek-r1/v3, QWen-VL, and more powerful models to power your AI applications! 🚀
💪 Getting Involved
We invite the community to try out TGI on Gaudi hardware and provide feedback. The full documentation is available in the TGI Gaudi backend documentation. 📚 If you&#39;re interested in contributing, check out our contribution guidelines or open an issue with your feedback on GitHub. 🤝 By bringing Intel Gaudi support directly into TGI, we&#39;re continuing our mission to provide flexible, efficient, and production-ready tools for deploying LLMs. We&#39;re excited to see what you&#39;ll build with this new capability! 🎉 </article> <!-- ✅ 戻るボタン --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
← 一覧へ戻る
</a> </div> </div> <!-- ✅ base を正しく埋め込む --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ✅ 戻るリンクを正しく構築 --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("✅ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("✅ backLink.href:", backLink.href);
      } else {
        console.warn("⚠️ backLink not found");
      }
    </script> </body> </html>
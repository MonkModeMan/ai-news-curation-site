<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>A guide to setting up your own Hugging Face leaderboard: an end-to-end example with Vectara&#39;s hallucination leaderboard</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/index.BoXAN-Xr.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ A guide to setting up your own Hugging Face leaderboard: an end-to-end example with Vectara&#39;s hallucination leaderboard</h1> <p class="text-sm text-gray-500"> 2024/1/12 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/leaderboard-vectara" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> A guide to setting up your own Hugging Face leaderboard: an end-to-end example with Vectara&#39;s hallucination leaderboard
Hugging Face‚Äôs Open LLM Leaderboard (originally created by Ed Beeching and Lewis Tunstall, and maintained by Nathan Habib and Cl√©mentine Fourrier) is well known for tracking the performance of open source LLMs, comparing their performance in a variety of tasks, such as TruthfulQA or HellaSwag.
This has been of tremendous value to the open-source community, as it provides a way for practitioners to keep track of the best open-source models.
In late 2023, at Vectara we introduced the Hughes Hallucination Evaluation Model (HHEM), an open-source model for measuring the extent to which an LLM hallucinates (generates text that is nonsensical or unfaithful to the provided source content). Covering both open source models like Llama 2 or Mistral 7B, as well as commercial models like OpenAI‚Äôs GPT-4, Anthropic Claude, or Google‚Äôs Gemini, this model highlighted the stark differences that currently exist between models in terms of their likelihood to hallucinate.
As we continue to add new models to HHEM, we were looking for an open-source solution to manage and update the HHEM leaderboard.
Quite recently, the Hugging Face leaderboard team released leaderboard templates (here and here). These are lightweight versions of the Open LLM Leaderboard itself, which are both open-source and simpler to use than the original code.
Today we‚Äôre happy to announce the release of the new HHEM leaderboard, powered by the HF leaderboard template.
Vectara‚Äôs Hughes Hallucination Evaluation Model (HHEM)
The Hughes Hallucination Evaluation Model (HHEM) Leaderboard is dedicated to assessing the frequency of hallucinations in document summaries generated by Large Language Models (LLMs) such as GPT-4, Google Gemini or Meta‚Äôs Llama 2. To use it you can follow the instructions here.
By doing an open-source release of this model, we at Vectara aim to democratize the evaluation of LLM hallucinations, driving awareness to the differences that exist in LLM performance in terms of propensity to hallucinate.
Our initial release of HHEM was a Huggingface model alongside a Github repository, but we quickly realized that we needed a mechanism to allow new types of models to be evaluated. Using the HF leaderboard code template, we were able to quickly put together a new leaderboard that allows for dynamic updates, and we encourage the LLM community to submit new relevant models for HHEM evaluation.
Setting up HHEM with the LLM leaderboard template
To set up the Vectara HHEM leaderboard, we had to follow a few steps, adjusting the HF leaderboard template code to our needs:
- After cloning the space repository to our own organization, we created two associated datasets: ‚Äúrequests‚Äù and ‚Äúresults‚Äù; these datasets maintain the requests submitted by users for new LLMs to evaluate, and the results of such evaluations, respectively.
- We populated the results dataset with existing results from the initial launch, and updated the ‚ÄúAbout‚Äù and ‚ÄúCitations‚Äù sections.
For a simple leaderboard, where evaluations results are pushed by your backend to the results dataset, that‚Äôs all you need!
As our evaluation is more complex, we then customized the source code to fit the needs of the HHEM leaderboard - here are the details:
leaderboard/src/backend/model_operations.py
: This file contains two primary classes -SummaryGenerator
andEvaluationModel
. a. TheSummaryGenerator
generates summaries based on the HHEM private evaluation dataset and calculates metrics like Answer Rate and Average Summary Length. b. TheEvaluationModel
loads our proprietary Hughes Hallucination Evaluation Model (HHEM) to assess these summaries, yielding metrics such as Factual Consistency Rate and Hallucination Rate.leaderboard/src/backend/evaluate_model.py
: defines theEvaluator
class which utilizes bothSummaryGenerator
andEvaluationModel
to compute and return results in JSON format.leaderboard/src/backend/run_eval_suite.py
: contains a functionrun_evaluation
that leveragesEvaluator
to obtain and upload evaluation results to theresults
dataset mentioned above, causing them to appear in the leaderboard.leaderboard/main_backend.py
: Manages pending evaluation requests and executes auto evaluations using aforementioned classes and functions. It also includes an option for users to replicate our evaluation results.
The final source code is available in the Files tab of our HHEM leaderboard repository. With all these changes, we now have the evaluation pipeline ready to go, and easily deployable as a Huggingface Space.
Summary
The HHEM is a novel classification model that can be used to evaluate the extent to which LLMs hallucinate. Our use of the Hugging Face leaderboard template provided much needed support for a common need for any leaderboard: the ability to manage the submission of new model evaluation requests, and the update of the leaderboard as new results emerge.
Big kudos to the Hugging Face team for making this valuable framework open-source, and supporting the Vectara team in the implementation. We expect this code to be reused by other community members who aim to publish other types of LLM leaderboards.
If you want to contribute to the HHEM with new models, please submit it on the leaderboard - we very much appreciate any suggestions for new models to evaluate.
And if you have any questions about the Hugging Face LLM front-end or Vectara, please feel free to reach out in the Vectara or Huggingface forums. </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Nystr√∂mformer, Approximating self-attention in linear time and memory via the Nystr√∂m method</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/index.BoXAN-Xr.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Nystr√∂mformer, Approximating self-attention in linear time and memory via the Nystr√∂m method</h1> <p class="text-sm text-gray-500"> 2022/8/2 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/nystromformer" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Nystr√∂mformer: Approximating self-attention in linear time and memory via the Nystr√∂m method
Introduction
Transformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of (where is the length of the input sequence), making it expensive to train on long input sequences.
The Nystr√∂mformer is one of many efficient Transformer models that approximates standard self-attention with complexity. Nystr√∂mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr√∂m method and how it can be adapted to approximate self-attention.
Nystr√∂m method for matrix approximation
At the heart of Nystr√∂mformer is the Nystr√∂m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let&#39;s consider a matrix , which is expensive to compute in its entirety. So, instead, we approximate it using the Nystr√∂m method. We start by sampling rows and columns from . We can then arrange the sampled rows and columns as follows:
We now have four submatrices: and , with sizes and respectively. The sampled columns are contained in and , whereas the sampled rows are contained in and . So, the entries of and are known to us, and we will estimate . According to the Nystr√∂m method, is given by:
Here, denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the Nystr√∂m approximation of can be written as:
As shown in the second line, can be expressed as a product of three matrices. The reason for doing so will become clear later.
Can we approximate self-attention with the Nystr√∂m method?
Our goal is to ultimately approximate the softmax matrix in standard self attention: S = softmax
Here, and denote the queries and keys respectively. Following the procedure discussed above, we would sample rows and columns from , form four submatrices, and obtain :
But, what does it mean to sample a column from ? It means we select one element from each row. Recall how S is calculated: the final operation is a row-wise softmax. To find a single entry in a row, we must access all other entries (for the denominator in softmax). So, sampling one column requires us to know all other columns in the matrix. Therefore, we cannot directly apply the Nystr√∂m method to approximate the softmax matrix.
How can we adapt the Nystr√∂m method to approximate self-attention?
Instead of sampling from , the authors propose to sample landmarks (or Nystr√∂m points) from queries and keys. We denote the query landmarks and key landmarks as and respectively. and can be used to construct three matrices corresponding to those in the Nystr√∂m approximation of . We define the following matrices:
The sizes of , , and and respectively. We replace the three matrices in the Nystr√∂m approximation of with the new matrices we have defined to obtain an alternative Nystr√∂m approximation:
This is the Nystr√∂m approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix with the values ( ) to obtain a linear approximation of self-attention. Note that we never calculated the product , avoiding the complexity.
How do we select landmarks?
Instead of sampling rows from and , the authors propose to construct and using segment means. In this procedure, tokens are grouped into segments, and the mean of each segment is computed. Ideally, is much smaller than . According to experiments from the paper, selecting just or landmarks produces competetive performance compared to standard self-attention and other efficient attention mechanisms, even for long sequences lengths ( or ).
The overall algorithm is summarised by the following figure from the paper:
The three orange matrices above correspond to the three matrices we constructed using the key and query landmarks. Also, notice that there is a DConv box. This corresponds to a skip connection added to the values using a 1D depthwise convolution.
How is Nystr√∂mformer implemented?
The original implementation of Nystr√∂mformer can be found here and the HuggingFace implementation can be found here. Let&#39;s take a look at a few lines of code (with some comments added) from the HuggingFace implementation. Note that some details such as normalization, attention masking, and depthwise convolution are avoided for simplicity.
key_layer = self.transpose_for_scores(self.key(hidden_states)) # K
value_layer = self.transpose_for_scores(self.value(hidden_states)) # V
query_layer = self.transpose_for_scores(mixed_query_layer) # Q
q_landmarks = query_layer.reshape(
-1,
self.num_attention_heads,
self.num_landmarks,
self.seq_len // self.num_landmarks,
self.attention_head_size,
).mean(dim=-2) # \tilde{Q}
k_landmarks = key_layer.reshape(
-1,
self.num_attention_heads,
self.num_landmarks,
self.seq_len // self.num_landmarks,
self.attention_head_size,
).mean(dim=-2) # \tilde{K}
kernel_1 = torch.nn.functional.softmax(torch.matmul(query_layer, k_landmarks.transpose(-1, -2)), dim=-1) # \tilde{F}
kernel_2 = torch.nn.functional.softmax(torch.matmul(q_landmarks, k_landmarks.transpose(-1, -2)), dim=-1) # \tilde{A} before pseudo-inverse
attention_scores = torch.matmul(q_landmarks, key_layer.transpose(-1, -2)) # \tilde{B} before softmax
kernel_3 = nn.functional.softmax(attention_scores, dim=-1) # \tilde{B}
attention_probs = torch.matmul(kernel_1, self.iterative_inv(kernel_2)) # \tilde{F} * \tilde{A}
new_value_layer = torch.matmul(kernel_3, value_layer) # \tilde{B} * V
context_layer = torch.matmul(attention_probs, new_value_layer) # \tilde{F} * \tilde{A} * \tilde{B} * V
Using Nystr√∂mformer with HuggingFace
Nystr√∂mformer for Masked Language Modeling (MLM) is available on HuggingFace. Currently, there are 4 checkpoints, corresponding to various sequence lengths: nystromformer-512
, nystromformer-1024
, nystromformer-2048
, and nystromformer-4096
. The number of landmarks, , can be controlled using the num_landmarks
parameter in the NystromformerConfig
. Let&#39;s take a look at a minimal example of Nystr√∂mformer for MLM:
from transformers import AutoTokenizer, NystromformerForMaskedLM
import torch
tokenizer = AutoTokenizer.from_pretrained(&quot;uw-madison/nystromformer-512&quot;)
model = NystromformerForMaskedLM.from_pretrained(&quot;uw-madison/nystromformer-512&quot;)
inputs = tokenizer(&quot;Paris is the [MASK] of France.&quot;, return_tensors=&quot;pt&quot;)
with torch.no_grad():
logits = model(**inputs).logits
# retrieve index of [MASK]
mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]
predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
tokenizer.decode(predicted_token_id)
Output:
----------------------------------------------------------------------------------------------------
capital
Alternatively, we can use the pipeline API (which handles all the complexity for us):
from transformers import pipeline
unmasker = pipeline(&#39;fill-mask&#39;, model=&#39;uw-madison/nystromformer-512&#39;)
unmasker(&quot;Paris is the [MASK] of France.&quot;)
Output:
----------------------------------------------------------------------------------------------------
[{&#39;score&#39;: 0.829957902431488,
&#39;token&#39;: 1030,
&#39;token_str&#39;: &#39;capital&#39;,
&#39;sequence&#39;: &#39;paris is the capital of france.&#39;},
{&#39;score&#39;: 0.022157637402415276,
&#39;token&#39;: 16081,
&#39;token_str&#39;: &#39;birthplace&#39;,
&#39;sequence&#39;: &#39;paris is the birthplace of france.&#39;},
{&#39;score&#39;: 0.01904447190463543,
&#39;token&#39;: 197,
&#39;token_str&#39;: &#39;name&#39;,
&#39;sequence&#39;: &#39;paris is the name of france.&#39;},
{&#39;score&#39;: 0.017583081498742104,
&#39;token&#39;: 1107,
&#39;token_str&#39;: &#39;kingdom&#39;,
&#39;sequence&#39;: &#39;paris is the kingdom of france.&#39;},
{&#39;score&#39;: 0.005948934704065323,
&#39;token&#39;: 148,
&#39;token_str&#39;: &#39;city&#39;,
&#39;sequence&#39;: &#39;paris is the city of france.&#39;}]
Conclusion
Nystr√∂mformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes. In this blog post, we went over a high-level overview of the Nystr√∂m method and how it can be leveraged for self-attention. Readers interested in deploying or fine-tuning Nystr√∂mformer for downstream tasks can find the HuggingFace documentation here. </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Cohere on Hugging Face Inference Providers 🔥</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/index.BoXAN-Xr.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ✅ タイトル --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">📰 Cohere on Hugging Face Inference Providers 🔥</h1> <p class="text-sm text-gray-500"> 2025/4/16 – Hugging Face Blog  <a href="https://huggingface.co/blog/inference-providers-cohere" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
元記事
</a>  </p> </header> <!-- ✅ 本文 --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Cohere on Hugging Face Inference Providers 🔥
We&#39;re thrilled to share that Cohere is now a supported Inference Provider on HF Hub! This also marks the first model creator to share and serve their models directly on the Hub.
Cohere is committed to building and serving models purpose-built for enterprise use-cases. Their comprehensive suite of secure AI solutions, from cutting-edge Generative AI to powerful Embeddings and Ranking models, are designed to tackle real-world business challenges. Additionally, Cohere Labs, Cohere’s in house research lab, supports fundamental research and seeks to change the spaces where research happens.
Starting now, you can run serverless inference to the following models via Cohere and Inference Providers:
- CohereLabs/c4ai-command-r-v01
- CohereLabs/c4ai-command-r-plus
- CohereLabs/c4ai-command-r-08-2024
- CohereLabs/c4ai-command-r7b-12-2024
- CohereLabs/c4ai-command-a-03-2025
- CohereLabs/aya-expanse-8b
- CohereLabs/aya-expanse-32b
- CohereLabs/aya-vision-8b
- CohereLabs/aya-vision-32b
Light up your projects with Cohere and Cohere Labs today!
Cohere Models
Cohere and Cohere Labs bring a swathe of their models to Inference Providers that excel at specific business applications. Let’s explore some in detail.
CohereLabs/c4ai-command-a-03-2025 🔗
Optimized for demanding enterprises that require fast, secure, and high-quality AI. Its 256k context length (2x most leading models) can handle much longer enterprise documents. Other key features include Cohere’s advanced retrieval-augmented generation (RAG) with verifiable citations, agentic tool use, enterprise-grade security, and strong multilingual performance (support for 23 languages).
CohereLabs/aya-expanse-32b 🔗
Focuses on state-of-the-art multilingual support, applying the latest research on multilingual pre-training. Supports Arabic, Chinese (simplified &amp; traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian, and Vietnamese with 128K context length.
CohereLabs/c4ai-command-r7b-12-2024 🔗
Ideal for low-cost or low-latency use cases, bringing state-of-the-art performance in its class of open-weight models across real-world tasks. This model offers a context length of 128k. It delivers a powerful combination of multilingual support, citation-verified retrieval-augmented generation (RAG), reasoning, tool use, and agentic behavior. Also supports 23 languages.
CohereLabs/aya-vision-32b 🔗
32-billion parameter model with advanced capabilities optimized for a variety of vision-language use cases, including OCR, captioning, visual reasoning, summarization, question answering, code, and more. It expands multimodal capabilities to 23 languages spoken by over half the world&#39;s population.
How it works
You can use Cohere models directly on the Hub either on the website UI or via the client SDKs.
You can find all the examples mentioned in this section on the Cohere documentation page.
In the website UI
You can search for Cohere models by filtering by the inference provider in the model hub.
From the Model Card, you can select the inference provider and run inference directly in the UI.
From the client SDKs
Let’s walk through using Cohere models from client SDKs. We’ve also made a colab notebook with these snippets, in case you want to try them out right away.
from Python, using huggingface_hub
The following example shows how to use Command A using Cohere as your inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own cohere API key if you have one.
Install huggingface_hub
v0.30.0 or later:
pip install -U &quot;huggingface_hub&gt;=0.30.0&quot;
Use the huggingface_hub
python library to call Cohere endpoints by defining the provider
parameter.
from huggingface_hub import InferenceClient
client = InferenceClient(
provider=&quot;cohere&quot;,
api_key=&quot;xxxxxxxxxxxxxxxxxxxxxxxx&quot;,
)
messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: &quot;How to make extremely spicy Mayonnaise?&quot;
}
]
completion = client.chat.completions.create(
model=&quot;CohereLabs/c4ai-command-r7b-12-2024&quot;,
messages=messages,
temperature=0.7,
max_tokens=512,
)
print(completion.choices[0].message)
Aya Vision, Cohere Labs’ multilingual, multimodal model is also supported. You can include images encoded in base64 as follows:
image_path = &quot;img.jpg&quot;
with open(image_path, &quot;rb&quot;) as f:
base64_image = base64.b64encode(f.read()).decode(&quot;utf-8&quot;)
image_url = f&quot;data:image/jpeg;base64,{base64_image}&quot;
from huggingface_hub import InferenceClient
client = InferenceClient(
provider=&quot;cohere&quot;,
api_key=&quot;xxxxxxxxxxxxxxxxxxxxxxxx&quot;,
)
messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{
&quot;type&quot;: &quot;text&quot;,
&quot;text&quot;: &quot;What&#39;s in this image?&quot;
},
{
&quot;type&quot;: &quot;image_url&quot;,
&quot;image_url&quot;: {&quot;url&quot;: image_url},
},
]
}
]
completion = client.chat.completions.create(
model=&quot;CohereLabs/aya-vision-32b&quot;,
messages=messages,
temperature=0.7,
max_tokens=512,
)
print(completion.choices[0].message)
from JS using @huggingface/inference
import { HfInference } from &quot;@huggingface/inference&quot;;
const client = new HfInference(&quot;xxxxxxxxxxxxxxxxxxxxxxxx&quot;);
const chatCompletion = await client.chatCompletion({
model: &quot;CohereLabs/c4ai-command-a-03-2025&quot;,
messages: [
{
role: &quot;user&quot;,
content: &quot;How to make extremely spicy Mayonnaise?&quot;
}
],
provider: &quot;cohere&quot;,
max_tokens: 512
});
console.log(chatCompletion.choices[0].message);
From OpenAI client
Here&#39;s how you can call Command R7B using Cohere as the inference provider via the OpenAI client library.
from openai import OpenAI
client = OpenAI(
base_url=&quot;https://router.huggingface.co/cohere/compatibility/v1&quot;,
api_key=&quot;xxxxxxxxxxxxxxxxxxxxxxxx&quot;,
)
messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: &quot;How to make extremely spicy Mayonnaise?&quot;
}
]
completion = client.chat.completions.create(
model=&quot;command-a-03-2025&quot;,
messages=messages,
temperature=0.7,
)
print(completion.choices[0].message)
Tool Use with Cohere Models
Cohere’s models bring state-of-the-art agentic tool use to Inference Providers so let’s explore that in detail. Both the Hugging Face Hub client and the OpenAI client are compatible with tools via inference providers, so the above examples can be expanded.
First, we will need to define tools for the model to use. Below we define the get_flight_info
which calls an API for the latest flight information using two locations. This tool definition will be represented by the model’s chat template. Which we can also explore in the model card (🎉 open source).
tools = [
{
&quot;type&quot;: &quot;function&quot;,
&quot;function&quot;: {
&quot;name&quot;: &quot;get_flight_info&quot;,
&quot;description&quot;: &quot;Get flight information between two cities or airports&quot;,
&quot;parameters&quot;: {
&quot;type&quot;: &quot;object&quot;,
&quot;properties&quot;: {
&quot;loc_origin&quot;: {
&quot;type&quot;: &quot;string&quot;,
&quot;description&quot;: &quot;The departure airport, e.g. MIA&quot;,
},
&quot;loc_destination&quot;: {
&quot;type&quot;: &quot;string&quot;,
&quot;description&quot;: &quot;The destination airport, e.g. NYC&quot;,
},
},
&quot;required&quot;: [&quot;loc_origin&quot;, &quot;loc_destination&quot;],
},
},
}
]
Next, we’ll need to pass messages to the inference client for the model to use the tools when relevant. In the example below we define the assistant’s tool call in tool_calls,
for the sake of clarity.
messages = [
{&quot;role&quot;: &quot;developer&quot;, &quot;content&quot;: &quot;Today is April 30th&quot;},
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: &quot;When is the next flight from Miami to Seattle?&quot;,
},
{
&quot;role&quot;: &quot;assistant&quot;,
&quot;tool_calls&quot;: [
{
&quot;function&quot;: {
&quot;arguments&quot;: &#39;{ &quot;loc_destination&quot;: &quot;Seattle&quot;, &quot;loc_origin&quot;: &quot;Miami&quot; }&#39;,
&quot;name&quot;: &quot;get_flight_info&quot;,
},
&quot;id&quot;: &quot;get_flight_info0&quot;,
&quot;type&quot;: &quot;function&quot;,
}
],
},
{
&quot;role&quot;: &quot;tool&quot;,
&quot;name&quot;: &quot;get_flight_info&quot;,
&quot;tool_call_id&quot;: &quot;get_flight_info0&quot;,
&quot;content&quot;: &quot;Miami to Seattle, May 1st, 10 AM.&quot;,
},
]
Finally, the tools and messages are passed to the create method.
from huggingface_hub import InferenceClient
client = InferenceClient(
provider=&quot;cohere&quot;,
api_key=&quot;xxxxxxxxxxxxxxxxxxxxxxxx&quot;,
)
completion = client.chat.completions.create(
model=&quot;CohereLabs/c4ai-command-r7b-12-2024&quot;,
messages=messages,
tools=tools,
temperature=0.7,
max_tokens=512,
)
print(completion.choices[0].message)
Billing
For direct requests, i.e. when you use a Cohere key, you are billed directly on your Cohere account.
For routed requests, i.e. when you authenticate via the Hub, you&#39;ll only pay the standard Cohere API rates. There&#39;s no additional markup from us, we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)
Important Note ‼️ PRO users get $2 worth of Inference credits every month. You can use them across providers. 🔥
Subscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more. </article> <!-- ✅ 戻るボタン --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
← 一覧へ戻る
</a> </div> </div> <!-- ✅ base を正しく埋め込む --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ✅ 戻るリンクを正しく構築 --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("✅ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("✅ backLink.href:", backLink.href);
      } else {
        console.warn("⚠️ backLink not found");
      }
    </script> </body> </html>
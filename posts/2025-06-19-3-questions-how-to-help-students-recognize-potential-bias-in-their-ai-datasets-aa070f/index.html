<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>3 Questions: How to help students recognize potential bias in their AI datasets</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.Ba3Vwl3b.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ 3 Questions: How to help students recognize potential bias in their AI datasets</h1> <p class="text-sm text-gray-500"> 2025/6/2 ‚Äì MIT  <a href="https://news.mit.edu/2025/3-questions-recognizing-potential-bias-in-ai-datasets-0602" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Every year, thousands of students take courses that teach them how to deploy artificial intelligence models that can help doctors diagnose disease and determine appropriate treatments. However, many of these courses omit a key element: training students to detect flaws in the training data used to develop the models.
Leo Anthony Celi, a senior research scientist at MIT‚Äôs Institute for Medical Engineering and Science, a physician at Beth Israel Deaconess Medical Center, and an associate professor at Harvard Medical School, has documented these shortcomings in a new paper and hopes to persuade course developers to teach students to more thoroughly evaluate their data before incorporating it into their models. Many previous studies have found that models trained mostly on clinical data from white males don‚Äôt work well when applied to people from other groups. Here, Celi describes the impact of such bias and how educators might address it in their teachings about AI models.
Q: How does bias get into these datasets, and how can these shortcomings be addressed?
A: Any problems in the data will be baked into any modeling of the data. In the past we have described instruments and devices that don‚Äôt work well across individuals. As one example, we found that pulse oximeters overestimate oxygen levels for people of color, because there weren‚Äôt enough people of color enrolled in the clinical trials of the devices. We remind our students that medical devices and equipment are optimized on healthy young males. They were never optimized for an 80-year-old woman with heart failure, and yet we use them for those purposes. And the FDA does not require that a device work well on this diverse of a population that we will be using it on. All they need is proof that it works on healthy subjects.
Additionally, the electronic health record system is in no shape to be used as the building blocks of AI. Those records were not designed to be a learning system, and for that reason, you have to be really careful about using electronic health records. The electronic health record system is to be replaced, but that‚Äôs not going to happen anytime soon, so we need to be smarter. We need to be more creative about using the data that we have now, no matter how bad they are, in building algorithms.
One promising avenue that we are exploring is the development of a transformer model of numeric electronic health record data, including but not limited to laboratory test results. Modeling the underlying relationship between the laboratory tests, the vital signs and the treatments can mitigate the effect of missing data as a result of social determinants of health and provider implicit biases.
Q: Why is it important for courses in AI to cover the sources of potential bias? What did you find when you analyzed such courses‚Äô content?
A: Our course at MIT started in 2016, and at some point we realized that we were encouraging people to race to build models that are overfitted to some statistical measure of model performance, when in fact the data that we‚Äôre using is rife with problems that people are not aware of. At that time, we were wondering: How common is this problem?
Our suspicion was that if you looked at the courses where the syllabus is available online, or the online courses, that none of them even bothers to tell the students that they should be paranoid about the data. And true enough, when we looked at the different online courses, it‚Äôs all about building the model. How do you build the model? How do you visualize the data? We found that of 11 courses we reviewed, only five included sections on bias in datasets, and only two contained any significant discussion of bias.
That said, we cannot discount the value of these courses. I‚Äôve heard lots of stories where people self-study based on these online courses, but at the same time, given how influential they are, how impactful they are, we need to really double down on requiring them to teach the right skillsets, as more and more people are drawn to this AI multiverse. It‚Äôs important for people to really equip themselves with the agency to be able to work with AI. We‚Äôre hoping that this paper will shine a spotlight on this huge gap in the way we teach AI now to our students.
Q: What kind of content should course developers be incorporating?
A: One, giving them a checklist of questions in the beginning. Where did this data came from? Who were the observers? Who were the doctors and nurses who collected the data? And then learn a little bit about the landscape of those institutions. If it‚Äôs an ICU database, they need to ask who makes it to the ICU, and who doesn‚Äôt make it to the ICU, because that already introduces a sampling selection bias. If all the minority patients don‚Äôt even get admitted to the ICU because they cannot reach the ICU in time, then the models are not going to work for them. Truly, to me, 50 percent of the course content should really be understanding the data, if not more, because the modeling itself is easy once you understand the data.
Since 2014, the MIT Critical Data consortium has been organizing datathons (data ‚Äúhackathons‚Äù) around the world. At these gatherings, doctors, nurses, other health care workers, and data scientists get together to comb through databases and try to examine health and disease in the local context. Textbooks and journal papers present diseases based on observations and trials involving a narrow demographic typically from countries with resources for research.
Our main objective now, what we want to teach them, is critical thinking skills. And the main ingredient for critical thinking is bringing together people with different backgrounds.
You cannot teach critical thinking in a room full of CEOs or in a room full of doctors. The environment is just not there. When we have datathons, we don‚Äôt even have to teach them how do you do critical thinking. As soon as you bring the right mix of people ‚Äî and it‚Äôs not just coming from different backgrounds but from different generations ‚Äî you don‚Äôt even have to tell them how to think critically. It just happens. The environment is right for that kind of thinking. So, we now tell our participants and our students, please, please do not start building any model unless you truly understand how the data came about, which patients made it into the database, what devices were used to measure, and are those devices consistently accurate across individuals?
When we have events around the world, we encourage them to look for data sets that are local, so that they are relevant. There‚Äôs resistance because they know that they will discover how bad their data sets are. We say that that‚Äôs fine. This is how you fix that. If you don‚Äôt know how bad they are, you‚Äôre going to continue collecting them in a very bad manner and they‚Äôre useless. You have to acknowledge that you‚Äôre not going to get it right the first time, and that‚Äôs perfectly fine. MIMIC (the Medical Information Marked for Intensive Care database built at Beth Israel Deaconess Medical Center) took a decade before we had a decent schema, and we only have a decent schema because people were telling us how bad MIMIC was.
We may not have the answers to all of these questions, but we can evoke something in people that helps them realize that there are so many problems in the data. I‚Äôm always thrilled to look at the blog posts from people who attended a datathon, who say that their world has changed. Now they‚Äôre more excited about the field because they realize the immense potential, but also the immense risk of harm if they don‚Äôt do this correctly. </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
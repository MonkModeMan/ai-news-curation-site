<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Hugging Face Text Generation Inference available for AWS Inferentia2</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.Ba3Vwl3b.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ✅ タイトル --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">📰 Hugging Face Text Generation Inference available for AWS Inferentia2</h1> <p class="text-sm text-gray-500"> 2024/2/1 – Hugging Face Blog  <a href="https://huggingface.co/blog/text-generation-inference-on-inferentia2" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
元記事
</a>  </p> </header> <!-- ✅ 本文 --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Hugging Face Text Generation Inference available for AWS Inferentia2
We are excited to announce the general availability of Hugging Face Text Generation Inference (TGI) on AWS Inferentia2 and Amazon SageMaker.
Text Generation Inference (TGI), is a purpose-built solution for deploying and serving Large Language Models (LLMs) for production workloads at scale. TGI enables high-performance text generation using Tensor Parallelism and continuous batching for the most popular open LLMs, including Llama, Mistral, and more. Text Generation Inference is used in production by companies such as Grammarly, Uber, Deutsche Telekom, and many more.
The integration of TGI into Amazon SageMaker, in combination with AWS Inferentia2, presents a powerful solution and viable alternative to GPUs for building production LLM applications. The seamless integration ensures easy deployment and maintenance of models, making LLMs more accessible and scalable for a wide range of production use cases.
With the new TGI for AWS Inferentia2 on Amazon SageMaker, AWS customers can benefit from the same technologies that power highly-concurrent, low-latency LLM experiences like HuggingChat, OpenAssistant, and Serverless Endpoints for LLMs on the Hugging Face Hub.
Deploy Zephyr 7B on AWS Inferentia2 using Amazon SageMaker
This tutorial shows how easy it is to deploy a state-of-the-art LLM, such as Zephyr 7B, on AWS Inferentia2 using Amazon SageMaker. Zephyr is a 7B fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on a mix of publicly available and synthetic datasets using Direct Preference Optimization (DPO), as described in detail in the technical report. The model is released under the Apache 2.0 license, ensuring wide accessibility and use.
We are going to show you how to:
- Setup development environment
- Retrieve the TGI Neuronx Image
- Deploy Zephyr 7B to Amazon SageMaker
- Run inference and chat with the model
Let’s get started.
1. Setup development environment
We are going to use the sagemaker
python SDK to deploy Zephyr to Amazon SageMaker. We need to make sure to have an AWS account configured and the sagemaker
python SDK installed.
!pip install transformers &quot;sagemaker&gt;=2.206.0&quot; --upgrade --quiet
If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find out more about it here.
import sagemaker
import boto3
sess = sagemaker.Session()
# sagemaker session bucket -&gt; used for uploading data, models and logs
# sagemaker will automatically create this bucket if it doesn&#39;t exist
sagemaker_session_bucket=None
if sagemaker_session_bucket is None and sess is not None:
# set to default bucket if a bucket name is not given
sagemaker_session_bucket = sess.default_bucket()
try:
role = sagemaker.get_execution_role()
except ValueError:
iam = boto3.client(&#39;iam&#39;)
role = iam.get_role(RoleName=&#39;sagemaker_execution_role&#39;)[&#39;Role&#39;][&#39;Arn&#39;]
sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)
print(f&quot;sagemaker role arn: {role}&quot;)
print(f&quot;sagemaker session region: {sess.boto_region_name}&quot;)
2. Retrieve TGI Neuronx Image
The new Hugging Face TGI Neuronx DLCs can be used to run inference on AWS Inferentia2. You can use the get_huggingface_llm_image_uri
method of the sagemaker
SDK to retrieve the appropriate Hugging Face TGI Neuronx DLC URI based on your desired backend
, session
, region
, and version
. You can find all the available versions here.
Note: At the time of writing this blog post the latest version of the Hugging Face LLM DLC is not yet available via the get_huggingface_llm_image_uri
method. We are going to use the raw container uri instead.
from sagemaker.huggingface import get_huggingface_llm_image_uri
# retrieve the llm image uri
llm_image = get_huggingface_llm_image_uri(
&quot;huggingface-neuronx&quot;,
version=&quot;0.0.20&quot;
)
# print ecr image uri
print(f&quot;llm image uri: {llm_image}&quot;)
4. Deploy Zephyr 7B to Amazon SageMaker
Text Generation Inference (TGI) on Inferentia2 supports popular open LLMs, including Llama, Mistral, and more. You can check the full list of supported models (text-generation) here.
Compiling LLMs for Inferentia2
At the time of writing, AWS Inferentia2 does not support dynamic shapes for inference, which means that we need to specify our sequence length and batch size ahead of time. To make it easier for customers to utilize the full power of Inferentia2, we created a neuron model cache, which contains pre-compiled configurations for the most popular LLMs. A cached configuration is defined through a model architecture (Mistral), model size (7B), neuron version (2.16), number of inferentia cores (2), batch size (2), and sequence length (2048).
This means we don&#39;t need to compile the model ourselves, but we can use the pre-compiled model from the cache. Examples of this are mistralai/Mistral-7B-v0.1 and HuggingFaceH4/zephyr-7b-beta. You can find compiled/cached configurations on the Hugging Face Hub. If your desired configuration is not yet cached, you can compile it yourself using the Optimum CLI or open a request at the Cache repository
For this post we re-compiled HuggingFaceH4/zephyr-7b-beta
using the following command and parameters on a inf2.8xlarge
instance, and pushed it to the Hub at aws-neuron/zephyr-7b-seqlen-2048-bs-4-cores-2
# compile model with optimum for batch size 4 and sequence length 2048
optimum-cli export neuron -m HuggingFaceH4/zephyr-7b-beta --batch_size 4 --sequence_length 2048 --num_cores 2 --auto_cast_type bf16 ./zephyr-7b-beta-neuron
# push model to hub [repo_id] [local_path] [path_in_repo]
huggingface-cli upload aws-neuron/zephyr-7b-seqlen-2048-bs-4 ./zephyr-7b-beta-neuron ./ --exclude &quot;checkpoint/**&quot;
# Move tokenizer to neuron model repository
python -c &quot;from transformers import AutoTokenizer; AutoTokenizer.from_pretrained(&#39;HuggingFaceH4/zephyr-7b-beta&#39;).push_to_hub(&#39;aws-neuron/zephyr-7b-seqlen-2048-bs-4&#39;)&quot;
If you are trying to compile an LLM with a configuration that is not yet cached, it can take up to 45 minutes.
Deploying TGI Neuronx Endpoint
Before deploying the model to Amazon SageMaker, we must define the TGI Neuronx endpoint configuration. We need to make sure the following additional parameters are defined:
HF_NUM_CORES
: Number of Neuron Cores used for the compilation.HF_BATCH_SIZE
: The batch size that was used to compile the model.HF_SEQUENCE_LENGTH
: The sequence length that was used to compile the model.HF_AUTO_CAST_TYPE
: The auto cast type that was used to compile the model.
We still need to define traditional TGI parameters with:
HF_MODEL_ID
: The Hugging Face model ID.HF_TOKEN
: The Hugging Face API token to access gated models.MAX_BATCH_SIZE
: The maximum batch size that the model can handle, equal to the batch size used for compilation.MAX_INPUT_LENGTH
: The maximum input length that the model can handle.MAX_TOTAL_TOKENS
: The maximum total tokens the model can generate, equal to the sequence length used for compilation.
import json
from sagemaker.huggingface import HuggingFaceModel
# sagemaker config &amp; model config
instance_type = &quot;ml.inf2.8xlarge&quot;
health_check_timeout = 1800
# Define Model and Endpoint configuration parameter
config = {
&quot;HF_MODEL_ID&quot;: &quot;HuggingFaceH4/zephyr-7b-beta&quot;,
&quot;HF_NUM_CORES&quot;: &quot;2&quot;,
&quot;HF_BATCH_SIZE&quot;: &quot;4&quot;,
&quot;HF_SEQUENCE_LENGTH&quot;: &quot;2048&quot;,
&quot;HF_AUTO_CAST_TYPE&quot;: &quot;bf16&quot;,
&quot;MAX_BATCH_SIZE&quot;: &quot;4&quot;,
&quot;MAX_INPUT_LENGTH&quot;: &quot;1512&quot;,
&quot;MAX_TOTAL_TOKENS&quot;: &quot;2048&quot;,
}
# create HuggingFaceModel with the image uri
llm_model = HuggingFaceModel(
role=role,
image_uri=llm_image,
env=config
)
After we have created the HuggingFaceModel
we can deploy it to Amazon SageMaker using the deploy
method. We will deploy the model with the ml.inf2.8xlarge
instance type.
# Deploy model to an endpoint
llm = llm_model.deploy(
initial_instance_count=1,
instance_type=instance_type,
container_startup_health_check_timeout=health_check_timeout,
)
SageMaker will create our endpoint and deploy the model to it. This can take 10-15 minutes.
5. Run inference and chat with the model
After our endpoint is deployed, we can run inference on it, using the predict
method from predictor
. We can provide different parameters to impact the generation, adding them to the parameters
attribute of the payload. You can find the supported parameters here, or in the open API specification of TGI in the swagger documentation
The HuggingFaceH4/zephyr-7b-beta
is a conversational chat model, meaning we can chat with it using a prompt structure like the following:
&lt;|system|&gt;\nYou are a friendly.&lt;/s&gt;\n&lt;|user|&gt;\nInstruction&lt;/s&gt;\n&lt;|assistant|&gt;\n
Manually preparing the prompt is error prone, so we can use the apply_chat_template
method from the tokenizer to help with it. It expects a messages
dictionary in the well-known OpenAI format, and converts it into the correct format for the model. Let&#39;s see if Zephyr knows some facts about AWS.
from transformers import AutoTokenizer
# load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;aws-neuron/zephyr-7b-seqlen-2048-bs-4-cores-2&quot;)
# Prompt to generate
messages = [
{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are the AWS expert&quot;},
{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Can you tell me an interesting fact about AWS?&quot;},
]
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
# Generation arguments
payload = {
&quot;do_sample&quot;: True,
&quot;top_p&quot;: 0.6,
&quot;temperature&quot;: 0.9,
&quot;top_k&quot;: 50,
&quot;max_new_tokens&quot;: 256,
&quot;repetition_penalty&quot;: 1.03,
&quot;return_full_text&quot;: False,
&quot;stop&quot;: [&quot;&lt;/s&gt;&quot;]
}
chat = llm.predict({&quot;inputs&quot;:prompt, &quot;parameters&quot;:payload})
print(chat[0][&quot;generated_text&quot;][len(prompt):])
# Sure, here&#39;s an interesting fact about AWS: As of 2021, AWS has more than 200 services in its portfolio, ranging from compute power and storage to databases,
Awesome, we have successfully deployed Zephyr to Amazon SageMaker on Inferentia2 and chatted with it.
6. Clean up
To clean up, we can delete the model and endpoint.
llm.delete_model()
llm.delete_endpoint()
Conclusion
The integration of Hugging Face Text Generation Inference (TGI) with AWS Inferentia2 and Amazon SageMaker provides a cost-effective alternative solution for deploying Large Language Models (LLMs).
We&#39;re actively working on supporting more models, streamlining the compilation process, and refining the caching system.
Thanks for reading! If you have any questions, feel free to contact me on Twitter or LinkedIn. </article> <!-- ✅ 戻るボタン --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
← 一覧へ戻る
</a> </div> </div> <!-- ✅ base を正しく埋め込む --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ✅ 戻るリンクを正しく構築 --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("✅ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("✅ backLink.href:", backLink.href);
      } else {
        console.warn("⚠️ backLink not found");
      }
    </script> </body> </html>
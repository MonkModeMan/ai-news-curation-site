<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Hybrid AI model crafts smooth, high-quality videos in seconds</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.CO8YC2Z5.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- âœ… ã‚¿ã‚¤ãƒˆãƒ« --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">ğŸ“° Hybrid AI model crafts smooth, high-quality videos in seconds</h1> <p class="text-sm text-gray-500"> 2025/5/6 â€“ MIT  <a href="https://news.mit.edu/2025/causevid-hybrid-ai-model-crafts-smooth-high-quality-videos-in-seconds-0506" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
å…ƒè¨˜äº‹
</a>  </p> </header> <!-- âœ… æœ¬æ–‡ --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> What would a behind-the-scenes look at a video generated by an artificial intelligence model be like? You might think the process is similar to stop-motion animation, where many images are created and stitched together, but thatâ€™s not quite the case for â€œdiffusion modelsâ€ like OpenAl&#39;s SORA and Google&#39;s VEO 2.
Instead of producing a video frame-by-frame (or â€œautoregressivelyâ€), these systems process the entire sequence at once. The resulting clip is often photorealistic, but the process is slow and doesnâ€™t allow for on-the-fly changes.
Scientists from MITâ€™s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Adobe Research have now developed a hybrid approach, called â€œCausVid,â€ to create videos in seconds. Much like a quick-witted student learning from a well-versed teacher, a full-sequence diffusion model trains an autoregressive system to swiftly predict the next frame while ensuring high quality and consistency. CausVidâ€™s student model can then generate clips from a simple text prompt, turning a photo into a moving scene, extending a video, or altering its creations with new inputs mid-generation.
This dynamic tool enables fast, interactive content creation, cutting a 50-step process into just a few actions. It can craft many imaginative and artistic scenes, such as a paper airplane morphing into a swan, woolly mammoths venturing through snow, or a child jumping in a puddle. Users can also make an initial prompt, like â€œgenerate a man crossing the street,â€ and then make follow-up inputs to add new elements to the scene, like â€œhe writes in his notebook when he gets to the opposite sidewalk.â€
The CSAIL researchers say that the model could be used for different video editing tasks, like helping viewers understand a livestream in a different language by generating a video that syncs with an audio translation. It could also help render new content in a video game or quickly produce training simulations to teach robots new tasks.
Tianwei Yin SM â€™25, PhD â€™25, a recently graduated student in electrical engineering and computer science and CSAIL affiliate, attributes the modelâ€™s strength to its mixed approach.
â€œCausVid combines a pre-trained diffusion-based model with autoregressive architecture thatâ€™s typically found in text generation models,â€ says Yin, co-lead author of a new paper about the tool. â€œThis AI-powered teacher model can envision future steps to train a frame-by-frame system to avoid making rendering errors.â€
Yinâ€™s co-lead author, Qiang Zhang, is a research scientist at xAI and a former CSAIL visiting researcher. They worked on the project with Adobe Research scientists Richard Zhang, Eli Shechtman, and Xun Huang, and two CSAIL principal investigators: MIT professors Bill Freeman and FrÃ©do Durand.
Caus(Vid) and effect
Many autoregressive models can create a video thatâ€™s initially smooth, but the quality tends to drop off later in the sequence. A clip of a person running might seem lifelike at first, but their legs begin to flail in unnatural directions, indicating frame-to-frame inconsistencies (also called â€œerror accumulationâ€).
Error-prone video generation was common in prior causal approaches, which learned to predict frames one by one on their own. CausVid instead uses a high-powered diffusion model to teach a simpler system its general video expertise, enabling it to create smooth visuals, but much faster.
CausVid displayed its video-making aptitude when researchers tested its ability to make high-resolution, 10-second-long videos. It outperformed baselines like â€œOpenSORAâ€ and â€œMovieGen,â€ working up to 100 times faster than its competition while producing the most stable, high-quality clips.
Then, Yin and his colleagues tested CausVidâ€™s ability to put out stable 30-second videos, where it also topped comparable models on quality and consistency. These results indicate that CausVid may eventually produce stable, hours-long videos, or even an indefinite duration.
A subsequent study revealed that users preferred the videos generated by CausVidâ€™s student model over its diffusion-based teacher.
â€œThe speed of the autoregressive model really makes a difference,â€ says Yin. â€œIts videos look just as good as the teacherâ€™s ones, but with less time to produce, the trade-off is that its visuals are less diverse.â€
CausVid also excelled when tested on over 900 prompts using a text-to-video dataset, receiving the top overall score of 84.27. It boasted the best metrics in categories like imaging quality and realistic human actions, eclipsing state-of-the-art video generation models like â€œVchitectâ€ and â€œGen-3.â€
While an efficient step forward in AI video generation, CausVid may soon be able to design visuals even faster â€” perhaps instantly â€” with a smaller causal architecture. Yin says that if the model is trained on domain-specific datasets, it will likely create higher-quality clips for robotics and gaming.
Experts say that this hybrid system is a promising upgrade from diffusion models, which are currently bogged down by processing speeds. â€œThese models are way slower than LLMs [large language models] or generative image models,â€ says Carnegie Mellon University Assistant Professor Jun-Yan Zhu, who was not involved in the paper. â€œThis new work changes that, making video generation much more efficient. That means better streaming speed, more interactive applications, and lower carbon footprints.â€
The teamâ€™s work was supported, in part, by the Amazon Science Hub, the Gwangju Institute of Science and Technology, Adobe, Google, the U.S. Air Force Research Laboratory, and the U.S. Air Force Artificial Intelligence Accelerator. CausVid will be presented at the Conference on Computer Vision and Pattern Recognition in June. </article> <!-- âœ… æˆ»ã‚‹ãƒœã‚¿ãƒ³ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
â† ä¸€è¦§ã¸æˆ»ã‚‹
</a> </div> </div> <!-- âœ… base ã‚’æ­£ã—ãåŸ‹ã‚è¾¼ã‚€ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- âœ… æˆ»ã‚‹ãƒªãƒ³ã‚¯ã‚’æ­£ã—ãæ§‹ç¯‰ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("âœ… base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("âœ… backLink.href:", backLink.href);
      } else {
        console.warn("âš ï¸ backLink not found");
      }
    </script> </body> </html>
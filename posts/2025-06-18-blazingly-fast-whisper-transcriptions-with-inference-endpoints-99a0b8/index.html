<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Blazingly fast whisper transcriptions with Inference Endpoints</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.CO8YC2Z5.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Blazingly fast whisper transcriptions with Inference Endpoints</h1> <p class="text-sm text-gray-500"> 2025/5/13 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/fast-whisper-endpoints" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Blazingly fast whisper transcriptions with Inference Endpoints
Today we are happy to introduce a new blazing fast OpenAI Whisper deployment option on Inference Endpoints. It provides up to 8x performance improvements compared to the previous version, and makes everyone one click away from deploying dedicated, powerful transcription models in a cost-effective way, leveraging the amazing work done by the AI community.
Through this release, we would like to make Inference Endpoints more community-centric and allow anyone to come and contribute to create incredible inference deployments on the Hugging Face Platform. Along with the community, we would like to propose optimized deployments for a wide range of tasks through the use of awesome and available open-source technologies.
The unique position of Hugging Face, at the heart of the Open-Source AI Community, working hand-in-hand with individuals, institutions and industrial partners, makes it the most heterogeneous platform when it comes to deploying AI models for inference on a wide variety of hardware and software.
Inference Stack
The new Whisper endpoint leverages amazing open-source community projects. Inference is powered by the vLLM project, which provides efficient ways of running AI models on various hardware families ‚Äì especially, but not limited to, NVIDIA GPUs. We use the vLLM implementation of OpenAI&#39;s Whisper model, allowing us to enable further, lower-level optimizations down the software stack.
In this initial release, we are targeting NVIDIA GPUs with compute capabilities 8.9 or better (Ada Lovelace), like L4 &amp; L40s, which unlocks a wide range of software optimizations:
- PyTorch compilation (torch.compile)
- CUDA graphs
- float8 KV cache
Compilation with
torch.compile
generates optimized kernels in a Just-In-Time (JIT) fashion, which can modify the computational graph, reorder operations, call specialized methods, and more.
CUDA graphs record the flow of sequential operations, or kernels, happening on the GPU, and attempts to group them as bigger chunks of work units to execute on the GPU. This grouping operation reduces data movements, synchronizations, and GPU scheduling overhead by executing a single, much larger work unit, rather than multiple smaller ones.
Last but not least, we are dynamically quantizing activations to reduce the memory requirement incurred by the KV cache(s). The computations are done in half precision, bfloat16 in this case, and the outputs are being stored in reduced precision (1 byte for float8 vs 2 bytes for bfloat16) which allows us to store more elements in the KV cache, increasing the cache hit rate.
There are many ways to continue pushing this and we are gearing up to work hand in hand with the community to improve it!
Benchmarks
Whisper Large V3 shows a nearly 8x improvement in RTFx, enabling much faster inference with no loss in transcription quality.
We evaluated the transcription quality and runtime efficiency of several Whisper-based models‚ÄîWhisper Large V3, Whisper Large V3-Turbo, and Distil-Whisper Large V3.5‚Äîand compared them against their implementations on the Transformers library to assess both accuracy and decoding speed under identical conditions.
We computed Word Error Rate (WER) across 8 standard datasets from the Open ASR Leaderboard, including AMI, GigaSpeech, LibriSpeech (Clean and Other), SPGISpeech, Tedlium, VoxPopuli, and Earnings22. These datasets span diverse domains and recording conditions, ensuring a robust evaluation of generalization and real-world transcription quality. WER measures transcription accuracy by calculating the percentage of words that are incorrectly predicted (via insertions, deletions, or substitutions); lower WER indicates better performance. All three Whisper variants maintain WER performance comparable to their Transformer baselines.
To assess inference efficiency, we sampled from the rev16 long-form dataset, which contains audio segments over 45 minutes in length‚Äîrepresentative of real transcription workloads such as meetings, podcasts, or interviews. We measured the Real-Time Factor (RTFx), defined as the ratio of audio duration to transcription time, and averaged it across samples. All models were evaluated in bfloat16
on a single L4 GPU, using consistent decoding settings (language, beam size, and batch size).
How to deploy
You can deploy your own ASR inference pipeline via Hugging Face Endpoints. Endpoints allows anyone willing to deploy AI models into production ready environments to do so by filling in a few parameters. It also features the most complete fleet of AI hardware available on the market to suit your need for cost and performance. All of this directly from where the AI community is being built. To get started, nothing easier, simply choose the model you want to deploy:
Inference
Running inference on the deployed model endpoint can be done in just a few lines of code in Python, you can also use the same structure in Javascript or any other language you&#39;re comfortable with.
Here&#39;s a small snippet to test the deployed checkpoint quickly.
import requests
ENDPOINT_URL = &quot;https://&lt;your‚Äëhf‚Äëendpoint&gt;.cloud/api/v1/audio/transcriptions&quot; # üåê replace with your URL endpoint
HF_TOKEN = &quot;hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot; # üîë replace with your HF token
AUDIO_FILE = &quot;sample.wav&quot; # üîä path to your local audio file
headers = {&quot;Authorization&quot;: f&quot;Bearer {HF_TOKEN}&quot;}
with open(AUDIO_FILE, &quot;rb&quot;) as f:
files = {&quot;file&quot;: f.read()}
response = requests.post(ENDPOINT_URL, headers=headers, files=files)
response.raise_for_status()
print(&quot;Transcript:&quot;, response.json()[&quot;text&quot;])
FastRTC Demo
With this blazing fast endpoint, it‚Äôs possible to build real-time transcription apps. Try out this example built with FastRTC. Simply speak into your microphone and see your speech transcribed in real time!
Spaces can easily be duplicated so please feel free to duplicate away. All of the above is made available for community use on the Hugging Face Hub in our dedicated HF Endpoints organization. Open issues, suggest use-cases and contribute here: hfendpoints-images (Inference Endpoints Images) üöÄ </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
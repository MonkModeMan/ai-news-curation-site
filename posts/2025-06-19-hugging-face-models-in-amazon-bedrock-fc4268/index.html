<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Hugging Face models in Amazon Bedrock</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.BNX7D8gk.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Hugging Face models in Amazon Bedrock</h1> <p class="text-sm text-gray-500"> 2024/12/9 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/bedrock-marketplace" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Use Hugging Face models with Amazon Bedrock
We are excited to announce that popular open models from Hugging Face are now available on Amazon Bedrock in the new Bedrock Marketplace! AWS customers can now deploy 83 open models with Bedrock Marketplace to build their Generative AI applications.
Under the hood, Bedrock Marketplace model endpoints are managed by Amazon Sagemaker Jumpstart. With Bedrock Marketplace, you can now combine the ease of use of SageMaker JumpStart with the fully managed infrastructure of Amazon Bedrock, including compatibility with high-level APIs such as Agents, Knowledge Bases, Guardrails and Model Evaluations.
When registering your Sagemaker Jumpstart endpoints in Amazon Bedrock, you only pay for the Sagemaker compute resources and regular Amazon Bedrock APIs prices are applicable.
In this blog we will show you how to deploy Gemma 2 27B Instruct and use the model with Amazon Bedrock APIs. Learn how to:
- Deploy Google Gemma 2 27B Instruct
- Send requests using the Amazon Bedrock APIs
- Clean Up
Deploy Google Gemma 2 27B Instruct
There are two ways to deploy an open model to be used with Amazon Bedrock:
- You can deploy your open model from the Bedrock Model Catalog.
- You can deploy your open model with Amazon Jumpstart and register it with Bedrock.
Both ways are similar, so we will guide you through the Bedrock Model catalog.
To get started, in the Amazon Bedrock console, make sure you are in one of the 14 regions where the Bedrock Marketplace is available. Then, you choose ‚ÄúModel catalog‚Äù in the ‚ÄúFoundation models‚Äù section of the navigation pane. Here, you can search for both serverless models and models available in Amazon Bedrock Marketplace. You filter results by ‚ÄúHugging Face‚Äù provider and you can browse through the 83 open models available.
For example, let‚Äôs search and select Google Gemma 2 27B Instruct.
Choosing the model opens the model detail page where you can see more information from the model provider such as highlights about the model, and usage including sample API calls.
On the top right, let‚Äôs click on Deploy.
It brings you to the deployment page where you can select the endpoint name, the instance configuration and advanced settings related to networking configuration and service role used to perform the deployment in Sagemaker. Let‚Äôs use the default advanced settings and the recommended instance type.
You are also required to accept the End User License Agreement of the model provider.
On the bottom right, let‚Äôs click on Deploy.
We just launched the deployment of GoogleGemma 2 27B Instruct model on a ml.g5.48xlarge instance, hosted in your Amazon Sagemaker tenancy, compatible with Amazon Bedrock APIs!
The endpoint deployment can take several minutes. It will appear in the ‚ÄúMarketplace deployments‚Äù page, which you can find in the ‚ÄúFoundation models‚Äù section of the navigation pane.
Use the model with Amazon Bedrock APIs
You can quickly test the model in the Playground through the UI. However, to invoke the deployed model programmatically with any Amazon Bedrock APIs, you need to get the endpoint ARN.
From the list of managed deployments, choose your model deployment to copy its endpoint ARN.
You can query your endpoint using the AWS SDK in your preferred language or with the AWS CLI.
Here is an example using Bedrock Converse API through the AWS SDK for Python (boto3):
import boto3
bedrock_runtime = boto3.client(&quot;bedrock-runtime&quot;)
# Add your bedrock endpoint arn here.
endpoint_arn = &quot;arn:aws:sagemaker:&lt;AWS::REGION&gt;:&lt;AWS::AccountId&gt;:endpoint/&lt;Endpoint_Name&gt;&quot;
# Base inference parameters to use.
inference_config = {
&quot;maxTokens&quot;: 256,
&quot;temperature&quot;: 0.1,
&quot;topP&quot;: 0.999,
}
# Additional inference parameters to use.
additional_model_fields = {&quot;parameters&quot;: {&quot;repetition_penalty&quot;: 0.9, &quot;top_k&quot;: 250, &quot;do_sample&quot;: True}}
response = bedrock_runtime.converse(
modelId=endpoint_arn,
messages=[
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{
&quot;text&quot;: &quot;What is Amazon doing in the field of generative AI?&quot;,
},
]
},
],
inferenceConfig=inference_config,
additionalModelRequestFields=additional_model_fields,
)
print(response[&quot;output&quot;][&quot;message&quot;][&quot;content&quot;][0][&quot;text&quot;])
&quot;Amazon is making significant strides in the field of generative AI, applying it across various products and services. Here&#39;s a breakdown of their key initiatives:\n\n**1. Amazon Bedrock:**\n\n* This is their **fully managed service** that allows developers to build and scale generative AI applications using models from Amazon and other leading AI companies. \n* It offers access to foundational models like **Amazon Titan**, a family of large language models (LLMs) for text generation, and models from Cohere&quot;
That‚Äôs it! If you want to go further, have a look at the Bedrock documentation.
Clean up
Don‚Äôt forget to delete your endpoint at the end of your experiment to stop incurring costs! At the top right of the page where you grab the endpoint ARN, you can delete your endpoint by clicking on ‚ÄúDelete‚Äù. </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
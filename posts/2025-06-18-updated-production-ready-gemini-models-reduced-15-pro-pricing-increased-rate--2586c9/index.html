<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Updated production-ready Gemini models, reduced 1.5 Pro pricing, increased rate limits, and more</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.CO8YC2Z5.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Updated production-ready Gemini models, reduced 1.5 Pro pricing, increased rate limits, and more</h1> <p class="text-sm text-gray-500"> 2024/9/24 ‚Äì DeepMind Blog  <a href="https://deepmind.google/discover/blog/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Today, we‚Äôre releasing two updated production-ready Gemini models: Gemini-1.5-Pro-002 and Gemini-1.5-Flash-002 along with:
These new models build on our latest experimental model releases and include meaningful improvements to the Gemini 1.5 models released at Google I/O in May. Developers can access our latest models for free via Google AI Studio and the Gemini API. For larger organizations and Google Cloud customers, the models are also available on Vertex AI.
The Gemini 1.5 series are models that are designed for general performance across a wide range of text, code, and multimodal tasks. For example, Gemini models can be used to synthesize information from 1000 page PDFs, answer questions about repos containing more than 10 thousand lines of code, take in hour long videos and create useful content from them, and more.
With the latest updates, 1.5 Pro and Flash are now better, faster, and more cost-efficient to build with in production. We see a ~7% increase in MMLU-Pro, a more challenging version of the popular MMLU benchmark. On MATH and HiddenMath (an internal holdout set of competition math problems) benchmarks, both models have made a considerable ~20% improvement. For vision and code use cases, both models also perform better (ranging from ~2-7%) across evals measuring visual understanding and Python code generation.
We also improved the overall helpfulness of model responses, while continuing to uphold our content safety policies and standards. This means less punting/fewer refusals and more helpful responses across many topics.
Both models now have a more concise style in response to developer feedback which is intended to make these models easier to use and reduce costs. For use cases like summarization, question answering, and extraction, the default output length of the updated models is ~5-20% shorter than previous models. For chat-based products where users might prefer longer responses by default, you can read our prompting strategies guide to learn more about how to make the models more verbose and conversational.
For more details on migrating to the latest versions of Gemini 1.5 Pro and 1.5 Flash, check out the Gemini API models page.
We continue to be blown away with the creative and useful applications of Gemini 1.5 Pro‚Äôs 2 million token long context window and multimodal capabilities. From video understanding to processing 1000 page PDFs, there are so many new use cases still to be built. Today we are announcing a 64% price reduction on input tokens, a 52% price reduction on output tokens, and a 64% price reduction on incremental cached tokens for our strongest 1.5 series model, Gemini 1.5 Pro, effective October 1st, 2024, on prompts less than 128K tokens. Coupled with context caching, this continues to drive the cost of building with Gemini down.
To make it even easier for developers to build with Gemini, we are increasing the paid tier rate limits for 1.5 Flash to 2,000 RPM and increasing 1.5 Pro to 1,000 RPM, up from 1,000 and 360, respectively. In the coming weeks, we expect to continue to increase the Gemini API rate limits so developers can build more with Gemini.
Along with core improvements to our latest models, over the last few weeks we have driven down the latency with 1.5 Flash and significantly increased the output tokens per second, enabling new use cases with our most powerful models.
Since the first launch of Gemini in December of 2023, building a safe and reliable model has been a key focus. With the latest versions of Gemini (-002 models), we‚Äôve made improvements to the model&#39;s ability to follow user instructions while balancing safety. We will continue to offer a suite of safety filters that developers may apply to Google‚Äôs models. For the models released today, the filters will not be applied by default so that developers can determine the configuration best suited for their use case.
We are releasing a further improved version of the Gemini 1.5 model we announced in August called ‚ÄúGemini-1.5-Flash-8B-Exp-0924.‚Äù This improved version includes significant performance increases across both text and multimodal use cases. It is available now via Google AI Studio and the Gemini API.
The overwhelmingly positive feedback developers have shared about 1.5 Flash-8B has been incredible to see, and we will continue to shape our experimental to production release pipeline based on developer feedback.
We&#39;re excited about these updates and can&#39;t wait to see what you&#39;ll build with the new Gemini models! And for Gemini Advanced users, you will soon be able to access a chat optimized version of Gemini 1.5 Pro-002. </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
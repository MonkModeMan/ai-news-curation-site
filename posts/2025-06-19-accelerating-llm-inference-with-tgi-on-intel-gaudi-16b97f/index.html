<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Accelerating LLM Inference with TGI on Intel Gaudi</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.Ba3Vwl3b.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- âœ… ã‚¿ã‚¤ãƒˆãƒ« --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">ğŸ“° Accelerating LLM Inference with TGI on Intel Gaudi</h1> <p class="text-sm text-gray-500"> 2025/3/28 â€“ Hugging Face Blog  <a href="https://huggingface.co/blog/intel-gaudi-backend-for-tgi" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
å…ƒè¨˜äº‹
</a>  </p> </header> <!-- âœ… æœ¬æ–‡ --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> ğŸš€ Accelerating LLM Inference with TGI on Intel Gaudi
We&#39;re excited to announce the native integration of Intel Gaudi hardware support directly into Text Generation Inference (TGI), our production-ready serving solution for Large Language Models (LLMs). This integration brings the power of Intel&#39;s specialized AI accelerators to our high-performance inference stack, enabling more deployment options for the open-source AI community ğŸ‰
âœ¨ What&#39;s New?
We&#39;ve fully integrated Gaudi support into TGI&#39;s main codebase in PR #3091. Previously, we maintained a separate fork for Gaudi devices at tgi-gaudi. This was cumbersome for users and prevented us from supporting the latest TGI features at launch. Now using the new TGI multi-backend architecture, we support Gaudi directly on TGI â€“ no more finicking on a custom repository ğŸ™Œ
This integration supports Intel&#39;s full line of Gaudi hardware:
- Gaudi1 ğŸ’»: Available on AWS EC2 DL1 instances
- Gaudi2 ğŸ’»ğŸ’»: Available on Intel Tiber AI Cloud and Denvr Dataworks
- Gaudi3 ğŸ’»ğŸ’»ğŸ’»: Available on Intel Tiber AI Cloud, IBM Cloud and from OEM such as Dell, HP and Supermicro
You can also find more information on Gaudi hardware on Intel&#39;s Gaudi product page
ğŸŒŸ Why This Matters
The Gaudi backend for TGI provides several key benefits:
- Hardware Diversity ğŸ”„: More options for deploying LLMs in production beyond traditional GPUs
- Cost Efficiency ğŸ’°: Gaudi hardware often provides compelling price-performance for specific workloads
- Production-Ready âš™ï¸: All the robustness of TGI (dynamic batching, streamed responses, etc.) now available on Gaudi
- Model Support ğŸ¤–: Run popular models like Llama 3.1, Mixtral, Mistral, and more on Gaudi hardware
- Advanced Features ğŸ”¥: Support for multi-card inference (sharding), vision-language models, and FP8 precision
ğŸš¦ Getting Started with TGI on Gaudi
The easiest way to run TGI on Gaudi is to use our official Docker image. You need to run the image on a Gaudi hardware machine. Here&#39;s a basic example to get you started:
model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run
hf_token=YOUR_HF_ACCESS_TOKEN
docker run --runtime=habana --cap-add=sys_nice --ipc=host \
-p 8080:80 \
-v $volume:/data \
-e HF_TOKEN=$hf_token \
-e HABANA_VISIBLE_DEVICES=all \
ghcr.io/huggingface/text-generation-inference:3.2.1-gaudi \
--model-id $model
Once the server is running, you can send inference requests:
curl 127.0.0.1:8080/generate
-X POST
-d &#39;{&quot;inputs&quot;:&quot;What is Deep Learning?&quot;,&quot;parameters&quot;:{&quot;max_new_tokens&quot;:32}}&#39;
-H &#39;Content-Type: application/json&#39;
For comprehensive documentation on using TGI with Gaudi, including how-to guides and advanced configurations, refer to the new dedicated Gaudi backend documentation.
ğŸ‰ Top features
We have optimized the following models for both single and multi-card configurations. This means these models run as fast as possible on Intel Gaudi. We&#39;ve specifically optimized the modeling code to target Intel Gaudi hardware, ensuring we offer the best performance and fully utilize Gaudi&#39;s capabilities:
- Llama 3.1 (8B and 70B)
- Llama 3.3 (70B)
- Llama 3.2 Vision (11B)
- Mistral (7B)
- Mixtral (8x7B)
- CodeLlama (13B)
- Falcon (180B)
- Qwen2 (72B)
- Starcoder and Starcoder2
- Gemma (7B)
- Llava-v1.6-Mistral-7B
- Phi-2
ğŸƒâ™‚ï¸ We also offer many advanced features on Gaudi hardware, such as FP8 quantization thanks to Intel Neural Compressor (INC), enabling even greater performance optimizations.
âœ¨ Coming soon! We&#39;re excited to expand our model lineup with cutting-edge additions including DeepSeek-r1/v3, QWen-VL, and more powerful models to power your AI applications! ğŸš€
ğŸ’ª Getting Involved
We invite the community to try out TGI on Gaudi hardware and provide feedback. The full documentation is available in the TGI Gaudi backend documentation. ğŸ“š If you&#39;re interested in contributing, check out our contribution guidelines or open an issue with your feedback on GitHub. ğŸ¤ By bringing Intel Gaudi support directly into TGI, we&#39;re continuing our mission to provide flexible, efficient, and production-ready tools for deploying LLMs. We&#39;re excited to see what you&#39;ll build with this new capability! ğŸ‰ </article> <!-- âœ… æˆ»ã‚‹ãƒœã‚¿ãƒ³ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
â† ä¸€è¦§ã¸æˆ»ã‚‹
</a> </div> </div> <!-- âœ… base ã‚’æ­£ã—ãåŸ‹ã‚è¾¼ã‚€ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- âœ… æˆ»ã‚‹ãƒªãƒ³ã‚¯ã‚’æ­£ã—ãæ§‹ç¯‰ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("âœ… base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("âœ… backLink.href:", backLink.href);
      } else {
        console.warn("âš ï¸ backLink not found");
      }
    </script> </body> </html>
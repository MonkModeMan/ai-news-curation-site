<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Welcome Stable-baselines3 to the Hugging Face Hub ü§ó</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/index.BoXAN-Xr.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Welcome Stable-baselines3 to the Hugging Face Hub ü§ó</h1> <p class="text-sm text-gray-500"> 2022/1/21 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/sb3" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Welcome Stable-baselines3 to the Hugging Face Hub ü§ó
At Hugging Face, we are contributing to the ecosystem for Deep Reinforcement Learning researchers and enthusiasts. That‚Äôs why we‚Äôre happy to announce that we integrated Stable-Baselines3 to the Hugging Face Hub.
Stable-Baselines3 is one of the most popular PyTorch Deep Reinforcement Learning library that makes it easy to train and test your agents in a variety of environments (Gym, Atari, MuJoco, Procgen...). With this integration, you can now host your saved models üíæ and load powerful models from the community.
In this article, we‚Äôre going to show how you can do it.
Installation
To use stable-baselines3 with Hugging Face Hub, you just need to install these 2 libraries:
pip install huggingface_hub
pip install huggingface_sb3
Finding Models
We‚Äôre currently uploading saved models of agents playing Space Invaders, Breakout, LunarLander and more. On top of this, you can find all stable-baselines-3 models from the community here
When you found the model you need, you just have to copy the repository id:
Download a model from the Hub
The coolest feature of this integration is that you can now very easily load a saved model from Hub to Stable-baselines3.
In order to do that you just need to copy the repo-id that contains your saved model and the name of the saved model zip file in the repo.
For instancesb3/demo-hf-CartPole-v1
:
import gym
from huggingface_sb3 import load_from_hub
from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy
# Retrieve the model from the hub
## repo_id = id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name})
## filename = name of the model zip file from the repository including the extension .zip
checkpoint = load_from_hub(
repo_id=&quot;sb3/demo-hf-CartPole-v1&quot;,
filename=&quot;ppo-CartPole-v1.zip&quot;,
)
model = PPO.load(checkpoint)
# Evaluate the agent and watch it
eval_env = gym.make(&quot;CartPole-v1&quot;)
mean_reward, std_reward = evaluate_policy(
model, eval_env, render=True, n_eval_episodes=5, deterministic=True, warn=False
)
print(f&quot;mean_reward={mean_reward:.2f} +/- {std_reward}&quot;)
Sharing a model to the Hub
In just a minute, you can get your saved model in the Hub.
First, you need to be logged in to Hugging Face to upload a model:
- If you&#39;re using Colab/Jupyter Notebooks:
from huggingface_hub import notebook_login
notebook_login()
- Else:
huggingface-cli login
Then, in this example, we train a PPO agent to play CartPole-v1 and push it to a new repo ThomasSimonini/demo-hf-CartPole-v1
`
from huggingface_sb3 import push_to_hub
from stable_baselines3 import PPO
# Define a PPO model with MLP policy network
model = PPO(&quot;MlpPolicy&quot;, &quot;CartPole-v1&quot;, verbose=1)
# Train it for 10000 timesteps
model.learn(total_timesteps=10_000)
# Save the model
model.save(&quot;ppo-CartPole-v1&quot;)
# Push this saved model to the hf repo
# If this repo does not exists it will be created
## repo_id = id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name})
## filename: the name of the file == &quot;name&quot; inside model.save(&quot;ppo-CartPole-v1&quot;)
push_to_hub(
repo_id=&quot;ThomasSimonini/demo-hf-CartPole-v1&quot;,
filename=&quot;ppo-CartPole-v1.zip&quot;,
commit_message=&quot;Added Cartpole-v1 model trained with PPO&quot;,
)
Try it out and share your models with the community!
What&#39;s next?
In the coming weeks and months, we will be extending the ecosystem by:
- Integrating RL-baselines3-zoo
- Uploading RL-trained-agents models into the Hub: a big collection of pre-trained Reinforcement Learning agents using stable-baselines3
- Integrating other Deep Reinforcement Learning libraries
- Implementing Decision Transformers üî•
- And more to come ü•≥
The best way to keep in touch is to join our discord server to exchange with us and with the community.
And if you want to dive deeper, we wrote a tutorial where you‚Äôll learn:
- How to train a Deep Reinforcement Learning lander agent to land correctly on the Moon üåï
- How to upload it to the Hub üöÄ
- How to download and use a saved model from the Hub that plays Space Invaders üëæ.
Conclusion
We&#39;re excited to see what you&#39;re working on with Stable-baselines3 and try your models in the Hub üòç.
And we would love to hear your feedback üíñ. üìß Feel free to reach us.
Finally, we would like to thank the SB3 team and in particular Antonin Raffin for their precious help for the integration of the library ü§ó.
Would you like to integrate your library to the Hub?
This integration is possible thanks to the huggingface_hub
library which has all our widgets and the API for all our supported libraries. If you would like to integrate your library to the Hub, we have a guide for you! </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
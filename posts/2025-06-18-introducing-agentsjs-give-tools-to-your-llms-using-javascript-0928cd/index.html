<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Introducing Agents.js: Give tools to your LLMs using JavaScript</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.Ba3Vwl3b.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Introducing Agents.js: Give tools to your LLMs using JavaScript</h1> <p class="text-sm text-gray-500"> 2023/7/24 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/agents-js" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Introducing Agents.js: Give tools to your LLMs using JavaScript
We have recently been working on Agents.js at huggingface.js. It&#39;s a new library for giving tool access to LLMs from JavaScript in either the browser or the server. It ships with a few multi-modal tools out of the box and can easily be extended with your own tools and language models.
Installation
Getting started is very easy, you can grab the library from npm with the following:
npm install @huggingface/agents
Usage
The library exposes the HfAgent
object which is the entry point to the library. You can instantiate it like this:
import { HfAgent } from &quot;@huggingface/agents&quot;;
const HF_ACCESS_TOKEN = &quot;hf_...&quot;; // get your token at https://huggingface.co/settings/tokens
const agent = new HfAgent(HF_ACCESS_TOKEN);
Afterward, using the agent is easy. You give it a plain-text command and it will return some messages.
const code = await agent.generateCode(
&quot;Draw a picture of a rubber duck with a top hat, then caption this picture.&quot;
);
which in this case generated the following code
// code generated by the LLM
async function generate() {
const output = await textToImage(&quot;rubber duck with a top hat&quot;);
message(&quot;We generate the duck picture&quot;, output);
const caption = await imageToText(output);
message(&quot;Now we caption the image&quot;, caption);
return output;
}
Then the code can be evaluated as such:
const messages = await agent.evaluateCode(code);
The messages returned by the agent are objects with the following shape:
export interface Update {
message: string;
data: undefined | string | Blob;
where message
is an info text and data
can contain either a string or a blob. The blob can be used to display images or audio.
If you trust your environment (see warning), you can also run the code directly from the prompt with run
:
const messages = await agent.run(
&quot;Draw a picture of a rubber duck with a top hat, then caption this picture.&quot;
);
Usage warning
Currently using this library will mean evaluating arbitrary code in the browser (or in Node). This is a security risk and should not be done in an untrusted environment. We recommend that you use generateCode
and evaluateCode
instead of run
in order to check what code you are running.
Custom LLMs üí¨
By default HfAgent
will use OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5 hosted Inference API as the LLM. This can be customized however.
When instancing your HfAgent
you can pass a custom LLM. A LLM in this context is any async function that takes a string input and returns a promise for a string. For example if you have an OpenAI API key you could make use of it like this:
import { Configuration, OpenAIApi } from &quot;openai&quot;;
const HF_ACCESS_TOKEN = &quot;hf_...&quot;;
const api = new OpenAIApi(new Configuration({ apiKey: &quot;sk-...&quot; }));
const llmOpenAI = async (prompt: string): Promise&lt;string&gt; =&gt; {
return (
(
await api.createCompletion({
model: &quot;text-davinci-003&quot;,
prompt: prompt,
max_tokens: 1000,
})
).data.choices[0].text ?? &quot;&quot;
);
};
const agent = new HfAgent(HF_ACCESS_TOKEN, llmOpenAI);
Custom Tools üõ†Ô∏è
Agents.js was designed to be easily expanded with custom tools &amp; examples. For example if you wanted to add a tool that would translate text from English to German you could do it like this:
import type { Tool } from &quot;@huggingface/agents/src/types&quot;;
const englishToGermanTool: Tool = {
name: &quot;englishToGerman&quot;,
description:
&quot;Takes an input string in english and returns a german translation. &quot;,
examples: [
{
prompt: &quot;translate the string &#39;hello world&#39; to german&quot;,
code: `const output = englishToGerman(&quot;hello world&quot;)`,
tools: [&quot;englishToGerman&quot;],
},
{
prompt:
&quot;translate the string &#39;The quick brown fox jumps over the lazy dog` into german&quot;,
code: `const output = englishToGerman(&quot;The quick brown fox jumps over the lazy dog&quot;)`,
tools: [&quot;englishToGerman&quot;],
},
],
call: async (input, inference) =&gt; {
const data = await input;
if (typeof data !== &quot;string&quot;) {
throw new Error(&quot;Input must be a string&quot;);
}
const result = await inference.translation({
model: &quot;t5-base&quot;,
inputs: input,
});
return result.translation_text;
},
};
Now this tool can be added to the list of tools when initiating your agent.
import { HfAgent, LLMFromHub, defaultTools } from &quot;@huggingface/agents&quot;;
const HF_ACCESS_TOKEN = &quot;hf_...&quot;;
const agent = new HfAgent(HF_ACCESS_TOKEN, LLMFromHub(&quot;hf_...&quot;), [
englishToGermanTool,
...defaultTools,
]);
Passing input files to the agent üñºÔ∏è
The agent can also take input files to pass along to the tools. You can pass an optional FileList
to generateCode
and evaluateCode
as such:
If you have the following html:
&lt;input id=&quot;fileItem&quot; type=&quot;file&quot; /&gt;
Then you can do:
const agent = new HfAgent(HF_ACCESS_TOKEN);
const files = document.getElementById(&quot;fileItem&quot;).files; // FileList type
const code = agent.generateCode(
&quot;Caption the image and then read the text out loud.&quot;,
files
);
Which generated the following code when passing an image:
// code generated by the LLM
async function generate(image) {
const caption = await imageToText(image);
message(&quot;First we caption the image&quot;, caption);
const output = await textToSpeech(caption);
message(&quot;Then we read the caption out loud&quot;, output);
return output;
}
Demo üéâ
We&#39;ve been working on a demo for Agents.js that you can try out here. It&#39;s powered by the same Open Assistant 30B model that we use on HuggingChat and uses tools called from the hub. üöÄ </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
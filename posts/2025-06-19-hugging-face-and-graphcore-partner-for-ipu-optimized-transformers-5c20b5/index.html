<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Hugging Face and Graphcore partner for IPU-optimized Transformers</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.Ba3Vwl3b.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Hugging Face and Graphcore partner for IPU-optimized Transformers</h1> <p class="text-sm text-gray-500"> 2021/9/14 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/graphcore" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Hugging Face and Graphcore partner for IPU-optimized Transformers
Speaking at the 2021 AI Hardware Summit, Hugging Face announced the launch of their new Hardware Partner Program, including device-optimized models and software integrations. Here, Graphcore - creators of the Intelligence Processing Unit (IPU) and a founding member of the program ‚Äì explain how their partnership with Hugging Face will allow developers to easily accelerate their use of state-of-the-art Transformer models.
Graphcore and Hugging Face are two companies with a common goal ‚Äì to make it easier for innovators to harness the power of machine intelligence.
Hugging Face‚Äôs Hardware Partner Program will allow developers using Graphcore systems to deploy state-of-the-art Transformer models, optimised for our Intelligence Processing Unit (IPU), at production scale, with minimum coding complexity.
What is an Intelligence Processing Unit?
IPUs are the processors that power Graphcore‚Äôs IPU-POD datacenter compute systems. This new type of processor is designed to support the very specific computational requirements of AI and machine learning. Characteristics such as fine-grained parallelism, low precision arithmetic, and the ability to handle sparsity have been built into our silicon.
Instead of adopting a SIMD/SIMT architecture like GPUs, Graphcore‚Äôs IPU uses a massively parallel, MIMD architecture, with ultra-high bandwidth memory placed adjacent to the processor cores, right on the silicon die.
This design delivers high performance and new levels of efficiency, whether running today‚Äôs most popular models, such as BERT and EfficientNet, or exploring next-generation AI applications.
Software plays a vital role in unlocking the IPU‚Äôs capabilities. Our Poplar SDK has been co-designed with the processor since Graphcore‚Äôs inception. Today it fully integrates with standard machine learning frameworks, including PyTorch and TensorFlow, as well as orchestration and deployment tools such as Docker and Kubernetes.
Making Poplar compatible with these widely used, third-party systems allows developers to easily port their models from their other compute platforms and start taking advantage of the IPU‚Äôs advanced AI capabilities.
Optimising Transformers for Production
Transformers have completely transformed (pun intended) the field of AI. Models such as BERT are widely used by Graphcore customers in a huge array of applications, across NLP and beyond. These multi-talented models can perform feature extraction, text generation, sentiment analysis, translation and many more functions.
Already, Hugging Face plays host to hundreds of Transformers, from the French-language CamemBERT to ViT which applies lessons learned in NLP to computer vision. The Transformers library is downloaded an average of 2 million times every month and demand is growing.
With a user base of more than 50,000 developers ‚Äì Hugging Face has seen the fastest ever adoption of an open-source project.
Now, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today&#39;s most advanced AI hardware.
Using Optimum, a new open-source library and toolkit, developers will be able to access hardware-optimized models certified by Hugging Face.
These are being developed in a collaboration between Graphcore and Hugging Face, with the first IPU-optimized models appearing on Optimum later this year. Ultimately, these will cover a wide range of applications, from vision and speech to translation and text generation.
Hugging Face CEO Cl√©ment Delangue said: ‚ÄúDevelopers all want access to the latest and greatest hardware ‚Äì like the Graphcore IPU, but there‚Äôs always that question of whether they‚Äôll have to learn new code or processes. With Optimum and the Hugging Face Hardware Program, that‚Äôs just not an issue. It‚Äôs essentially plug-and-play&quot;.
SOTA Models meet SOTA Hardware
Prior to the announcement of the Hugging Face Partnership, we had demonstrated the power of the IPU to accelerate state-of-the-art Transformer models with a special Graphcore-optimised implementation of Hugging Face BERT using Pytorch.
Full details of this example can be found in the Graphcore blog BERT-Large training on the IPU explained.
The dramatic benchmark results for BERT running on a Graphcore system, compared with a comparable GPU-based system are surely a tantalising prospect for anyone currently running the popular NLP model on something other than the IPU.
This type of acceleration can be game changing for machine learning researchers and engineers, winning them back valuable hours of training time and allowing them many more iterations when developing new models.
Now Graphcore users will be able to unlock such performance advantages, through the Hugging Face platform, with its elegant simplicity and superlative range of models.
Together, Hugging Face and Graphcore are helping even more people to access the power of Transformers and accelerate the AI revolution.
Visit the Hugging Face Hardware Partner portal to learn more about Graphcore IPU systems and how to gain access </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
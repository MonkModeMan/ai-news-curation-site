<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>CinePile 2.0 - making stronger datasets with adversarial refinement</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.BNX7D8gk.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ CinePile 2.0 - making stronger datasets with adversarial refinement</h1> <p class="text-sm text-gray-500"> 2024/10/23 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/cinepile2" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> CinePile 2.0 - making stronger datasets with adversarial refinement
In this blog post we share the journey of releasing CinePile 2.0, a significantly improved version of our long video QA dataset. The improvements in the new dataset rely on a new approach that we coined adversarial dataset refinement.
We&#39;re excited to share both CinePile 2.0 and our adversarial refinement method implementation, which we believe can strengthen many existing datasets and directly be part of future dataset creation pipelines.
If you are mainly interested in the adversarial refinement method, you can jump directly to the Adversarial Refinement section.
Wait. What is CinePile?
In May 2024, we launched CinePile, a long video QA dataset with about 300,000 training samples and 5,000 test samples.
The first release stood out from other datasets in two aspects:
- Question diversity: It covers temporal understanding, plot analysis, character dynamics, setting, and themes.
- Question difficulty: In our benchmark, humans outperformed the best commercial vision models by 25% and open-source ones by 65%.
Taking a look at a data sample
Part of the secret sauce behind it is that it relies on movie clips from YouTube and Q&amp;A distilled from precise audio descriptions designed for visually impaired audiences. These descriptions offer rich context beyond basic visuals (e.g., &quot;What color is the car?&quot;), helping us create more complex questions.
Tell me more. How did you put together the original dataset?
To automate question creation, we first built question templates by inspecting existing datasets like MovieQA and TVQA. We clustered the questions in these datasets using a textual similarity model WhereIsAI/UAE-Large-V1 and then prompted GPT-4 with 10 random examples from each cluster to generate a question template and a prototypical question for each:
| Category | Question template | Prototypical question |
|---|---|---|
| Character and Relationship Dynamics (CRD) | Interpersonal Dynamics | What changes occur in the relationship between person A and person B following a shared experience or actions? |
| Character and Relationship Dynamics (CRD) | Decision Justification | What reasons did the character give for making their decision? |
| Narrative and Plot Analysis (NPA) | Crisis Event | What major event leads to the character&#39;s drastic action? |
| Narrative and Plot Analysis (NPA) | Mysteries Unveiled | What secret does character A reveal about event B? |
| Setting and Technical Analysis (STA) | Physical Possessions | What is [Character Name] holding? |
| Setting and Technical Analysis (STA) | Environmental Details | What does the [setting/location] look like [during/at] [specific time/place/event]? |
| Temporal (TEMP) | Critical Time-Sensitive Actions | What must [Character] do quickly, and what are the consequences otherwise? |
| Temporal (Temp) | Frequency | How many times does a character attempt [action A]? |
| Thematic Exploration (TH) | Symbolism and Motif Tracking | Are there any symbols or motifs introduced in Scene A that reappear or evolve in Scene B, and what do they signify? |
| Thematic Exploration (TH) | Thematic Parallels | What does the chaos in the scene parallel in terms of the movie&#39;s themes? |
Since templates aren&#39;t always relevant to every movie clip, we used Gemini 1.0 Pro to select the most appropriate ones for each scene. Next, we feed a language model the scene&#39;s text, selected template names (e.g., &quot;Physical Possession&quot;), sample questions, and a system prompt to create scene-specific questions. A well-designed prompt helps the model focus on the entire scene, generating deeper questions while avoiding superficial ones. We found that:
- Providing prototypical examples and including timestamps for dialogues and visual descriptions prevents GPT-4 from hallucinating
- This approach leads to more plausible multiple-choice question (MCQ) distractors
- Asking the model to provide a rationale for its answers improves the quality of the questions
Using this approach, we generate approximately 32 questions per video. Prior to releasing CinePile, we implemented several mechanisms to ensure the quality of the dataset/benchmark that we cover in the next section.
Inspecting the quality of the first results
While our process typically generates well-formed, answerable questions, some turn out to be trivial or rely on basic concepts that don&#39;t require watching the clip. To address this, we used several large language models (LLMs) to identify and filter three types of issues:
Degeneracy Issues
- A question is considered &quot;degenerate&quot; if its answer is obvious from the question itself (e.g., &quot;What is the color of the pink house?&quot;)
- These comprised only a small portion of our dataset
- Since manual review wasn&#39;t feasible at our scale, we employed three LLMs‚ÄîGemini, GPT-3.5, and Phi-1.5‚Äîfor automated detection
- Questions were excluded from the evaluation set if all three models answered correctly without any context
Vision Reliance Issues
- Some multiple-choice questions could be answered using dialogue alone, without requiring visual information
- We used the Gemini model to determine if questions could be answered using only dialogue
- Questions received a binary score: 0 if answerable without visuals, 1 if visual information was required
Difficulty Assessment
- To evaluate question difficulty, we tested whether models could answer correctly even when given full context (both visual descriptions and subtitles)
Through continued use of the benchmark by our team and the broader community, we identified several areas for improvement that drove us to consider CinePile 2.0.
CinePile 2.0
For CinePile&#39;s second release, we worked together with Hugging Face (following their successful experimentation with fine-tuning Video Llava 7B on CinePile) to identify and prioritize several areas of improvement.
Issues in CinePile 1.0
While the degeneracy filtering was useful in CinePile 1.0, it had several limitations:
- Some questions could be answered using just the Q&amp;A pairs, without requiring transcripts or visual content
- Many flagged questions contained valuable insights from the video - rather than discarding them, they could have been rephrased to better capture their value
- Degeneracy checks were limited to the test set: running multiple models‚Äîespecially proprietary ones‚Äîwas too expensive at scale for CinePile 1.0&#39;s training set
To address these issues, we introduced a new Adversarial Refinement pipeline that helps improve weak questions rather than simply discarding them. This approach can be more easily applied at scale. Throughout this post, we&#39;ll refer to the model(s) that identify degenerate questions (using only the question and answer choices, without visual or dialogue information) as the &quot;Deaf-Blind LLM.&quot;
Adversarial Refinement
The Adversarial Refinement pipeline aims to modify questions or answers until a Deaf-Blind LLM cannot easily predict the correct answer. Here&#39;s how it works:
- The Deaf-Blind LLM provides both an answer and a rationale explaining its choice based solely on the question
- These rationales help identify implicit cues or biases embedded in the question
- Our question-generation model uses this rationale to modify the question and/or answer choices, removing implicit clues
- This process repeats up to five times per question until the Deaf-Blind LLM&#39;s performance drops to random chance
Given the computational demands of this iterative process, we needed a powerful yet accessible LLM that could run locally to avoid API usage limits, delays, and cloud service costs. We chose:
- LLaMA 3.1 70B (open-source model) as the Deaf-Blind LLM
- GPT-4 for question modification generation
To account for random chance, we:
- Tested all five permutations of answer choice order
- Marked a question as degenerate if the model answered correctly in three out of five attempts
Results of the adversarial refinement
Briefly, this was the impact of running adversarial refinement in CinePile:
- Successfully modified 90.24% of degenerate Q&amp;A pairs in the test set
- Manually reviewed unfixable Q&amp;A pairs (~80 out of 800)
- Modified when possible
- Otherwise excluded from evaluation split
- Corrected 90.94% of weak pairs in the training set
- Retained unfixable ones as they don&#39;t negatively impact performance
Implementation
In this release, we&#39;re publishing both our adversarial refinement pipeline and the code for identifying weak questions. The complete implementation, including all prompts, is available in our public repository.
Evaluations
After testing both previously evaluated models and 16 new Video-LLMs on the modified test set, we‚Äôve highlighted the top performers in the figure below. Here‚Äôs what the results show:
Gemini 1.5 Pro led among commercial Vision Language Models (VLMs)
- Excelled particularly in &quot;Setting and Technical Analysis&quot;
- Best performance on visually-driven questions about movie environments and character interactions
GPT-based models showed competitive performance
- Strong in &quot;Narrative and Plot Analysis&quot;
- Performed well on questions about storylines and character interactions
Gemini 1.5 Flash, a lighter version of Gemini 1.5 Pro
- Achieved 58.75% overall accuracy
- Performed particularly well in &quot;Setting and Technical Analysis&quot;
Open Source models
The open-source video-LLM community has made significant progress from the first to the current release of CinePile. This is what we learned:
LLaVa-One Vision leads open-source models
- Achieved 49.34% accuracy
- A dramatic improvement from CinePile 1.0&#39;s best performer (Video LLaVA at 22.51%)
Smaller models showed competitive performance
- LLaVa-OV (7B parameters)
- MiniCPM-V 2.6 (8B parameters)
- Both outperformed InternVL2 (26B parameters)
There is room to improve
- Nearly all models showed 15-20% accuracy drop on the hard-split
- Indicates significant room for improvement
Hard Split
The hard-split results in CinePile clearly demonstrate that current models still lag far behind human capability in understanding visual narratives and story elements. This gap highlights the value of CinePile&#39;s new release as a benchmark for measuring progress toward more sophisticated visual understanding.
Leaderboard
We&#39;ve launched a new CinePile Leaderboard that will be continuously updated as new models emerge. Visit the space to learn how to submit your own models for evaluation. </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
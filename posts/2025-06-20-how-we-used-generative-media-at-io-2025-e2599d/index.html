<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>How we used generative media at I/O 2025</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/index.BoXAN-Xr.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ How we used generative media at I/O 2025</h1> <p class="text-sm text-gray-500"> 2025/6/10 ‚Äì Google AI Blog  <a href="https://blog.google/technology/ai/generative-ai-io-keynote-2025/" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> How we used generative media at I/O 2025
When it came to generative AI at I/O 2025, our goal was to both show and tell. We shared exciting news about our newest video and image generation models, Veo 3 and Imagen 4, and expanded access to Lyria 2. And we announced Flow, an AI filmmaking tool that lets you seamlessly create cinematic clips, scenes and stories with consistency using our most capable generative models.
But we also put some of these tools to work for our biggest event of the year ‚Äî 219 slides from the main I/O keynote were generated by AI, 48% of the keynote visuals were made with Imagen and 80% of the keynote videos used Veo or Imagen in some way. Here‚Äôs a deeper dive into how we used our own AI tools ‚Äî including how we wrote our prompts ‚Äî to bring I/O 2025 to life:
Veo 2: Creating speaker title cards with style
As Elizabeth Reid, our VP and Head of Search, walked up to share big Search updates, the on-stage screen projected a short video clip that showed Liz opening up her hand to reveal a delicate orange origami crane as it spun and levitated a few inches above her palm.
We used the image to video feature in AI Studio and Vertex AI Studio ‚Äî powered by Veo 2 ‚Äî to create this and other speaker title card videos. We gathered headshots from our speakers, as well as a few details on their interests and hobbies to use in the prompts. Here‚Äôs the prompt we used for research scientist Jason Baldridge‚Äôs title card:
Here&#39;s what Veo 2 gave us to work with, combined with some simple animations, and what you saw on stage:
Imagen 4 and Veo 3: Storyboarding and producing films
Several of the I/O films used AI tools in the creation process. The opening film took viewers on a wild ride through a Western-themed town before panning up to colorful balloons spelling out ‚ÄúWelcome to I/O‚Äù to kick off the keynote.
The team used a few different AI tools to make it. They began with Imagen 4 to generate images for the video prompts. They also turned to the Google DeepMind team, who asked Gemini to rewrite their prompt so they could find the language that would give them the result they wanted. Here‚Äôs the prompt for the first scene:
This is what they got:
The team added this image to the Veo 3 prompt, which included instructions like:
Once they had the images and prompt language, they started generating the film using our newest Veo 3 technology (available through our Google AI Ultra plan!). They also used Veo 3 to make changes like adding overhead shots, wider angles and different visuals as they reviewed the output in real time. Instead of reshooting a scene like they‚Äôd have had to in traditional filmmaking, the crew was able to move much faster. No coding experience was required: While some of the Googlers who worked on this video used the Colab function to work in code while editing the video, plenty of others were using text-to-video prompting, writing out their edits and requests in natural language.
Here‚Äôs the prompt used to create one of the film‚Äôs final moments:
Lyria RealTime: Jamming with Toro y Moi in real time
In our I/O pre-show, the artist Toro y Moi used Lyria RealTime in a musical set to showcase the model‚Äôs latest features and capabilities. Lyria RealTime is our new interactive music generation model that allows you to interactively create, control and perform generative music in real time. You can blend and warp instruments, sounds, genres and more. ‚ÄúThis is going to be a performance and an experiment,‚Äù he said before diving in. ‚ÄúEverything I‚Äôm playing is completely improvised, and I‚Äôm basically jamming with the computer. The computer will also be jamming with me.‚Äù
The team gave Toro y Moi a prototype of Lyria RealTime in the weeks leading up to the performance so he could try out and refine his prompts. This helped him come up with a creative concept and prompts to use during the show. The 16 sound prompts he came up with, including ‚Äúchaotic conga player‚Äù and ‚Äúpitch shift U.K. jungle drums,‚Äù were assigned to knobs on a physical MIDI controller.
Getting Lyria RealTime running on a local machine via a MIDI controller let Toro y Moi use the model onstage and allowed viewers to see the interface at work.
During the show, an app we built using the Lyria RealTime API and running on the MIDI controller flashed on screen behind Toro y Moi. The audience got a glimpse of the app‚Äôs options for creating and controlling a continuous stream of music ‚Äî as well as the prompts he used ‚Äî as he added layers of keyboards and vocal flourishes.
Try it yourself using the Lyria RealTime API in AI Studio.
AI Studio: Designing AI-generated swag
We used several Gemini models in Google AI Studio ‚Äî our free-to-use interface that lets you quickly try out models and experiment with different prompts ‚Äî to help us make this year‚Äôs I/O shirt with a brand new custom logo. The creative prompt, as the shirt‚Äôs tag explains, was ‚ÄúThe Google I/O logo melting into the ground, rainbow colors, illustration, studio lighting.‚Äù Here‚Äôs how we made it:
First, the team selected Gemini 2.5 Pro in AI Studio, and outlined their goal with a thorough system instruction prompt.
After setting up these guidelines, they entered their aforementioned creative prompt. With this, Gemini 2.5 Pro generated a block of text listing requirements and style ideas for the desired image outputs. For example: ‚ÄúCore shapes: The fundamental geometric forms of the three original shapes (the overall rectangle, the thin slash/rectangle, and the cylinder) must still be clearly recognizable and visually distinct, even with the melting effect applied to their sides. Do not distort the core structure beyond the melting paint effect.‚Äù
Once the team had these written instructions, they switched to using our workhorse model, Gemini 2.0 Flash with image generation, to generate a range of proposed image outputs, based on the previously-generated list.
After lots of experimentation, they landed on several final designs ‚Äî and this final one.
It‚Äôs worth noting that the team did also try skipping the initial prompt rewriting with Gemini 2.5 Pro, and going straight to Gemini 2.0 Flash in AI Studio ‚Äî and while they felt like they got close to what they wanted, using the 2.5 Pro rewrite consistently led to better results that worked in more creative detail. And ultimately, some very cool T-shirts!
Ready to run it back? Catch up on our biggest I/O 2025 announcements here, then try your hand at our I/O quiz, too. </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>
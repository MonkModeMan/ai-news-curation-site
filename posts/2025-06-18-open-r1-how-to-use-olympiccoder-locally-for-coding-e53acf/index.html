<!DOCTYPE html><html lang="ja"> <head><meta charset="UTF-8"><title>Open R1: How to use OlympicCoder locally for coding?</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/ai-news-curation-site/_astro/_page_.CO8YC2Z5.css"></head> <body class="bg-gray-100 text-gray-800 font-sans px-4 py-6"> <div class="max-w-3xl mx-auto"> <!-- ‚úÖ „Çø„Ç§„Éà„É´ --> <header class="mb-6"> <h1 class="text-3xl font-extrabold text-sky-500 mb-2">üì∞ Open R1: How to use OlympicCoder locally for coding?</h1> <p class="text-sm text-gray-500"> 2025/3/20 ‚Äì Hugging Face Blog  <a href="https://huggingface.co/blog/olympic-coder-lmstudio" target="_blank" rel="noopener noreferrer" class="text-sky-500 hover:text-gray-500 no-underline border-b border-transparent hover:border-gray-300 transition">
ÂÖÉË®ò‰∫ã
</a>  </p> </header> <!-- ‚úÖ Êú¨Êñá --> <article class="prose prose-sm sm:prose lg:prose-lg max-w-none bg-white rounded-lg shadow p-6"> Open R1: How to use OlympicCoder locally for coding
Everyone‚Äôs been using Claude and OpenAI as coding assistants for the last few years, but there‚Äôs less appeal if you look at the developments coming out of open source projects like Open R1. If we look at the evaluation on LiveCodeBench below, we can see that the 7B parameter variant outperforms Claude 3.7 Sonnet and GPT-4o. These models are the daily drivers of many engineers in applications like Cursor and VSCode.
Evals are great and all, but I want to get my hands dirty and feel the commits! This blog post focuses on how you can integrate these models in your IDE now. We will set up OlympicCoder 7B, the smaller of the two OlympicCoder variants, and we‚Äôll use a quantized variant for optimum local inference. Here‚Äôs the stack we‚Äôre going to use:
- OlympicCoder 7B. The 4bit GGUF version from the LMStudio Community
- LM Studio: A tool that simplifies running AI models
- Visual Studio Code (VS Code)
- Continue a VS Code extension for local models
It‚Äôs important to say that we chose this stack purely for simplicity. You might want to experiment with the larger model and/or different GGUF files. Or even alternative inference engines like llama.cpp.
1. Install LM Studio
LM Studio is like a control panel for AI models. It integrates with the Hugging Face hub to pull models, helps you find the right GGUF file, and exposes an API that other applications can use to interact with the model.
In short, it lets you download and run them without any complicated setup.
- Go to the LM Studio website: Open your web browser and go to https://lmstudio.ai/download.
- Choose your operating system: Click the download button for your computer (Windows, Mac, or Linux).
- Install LM Studio: Run the downloaded file and follow the instructions. It‚Äôs just like installing any other program.
2. Get OlympicCoder 7B
The GGUF files that we need are hosted on the hub. We can open the model from the hub in LMStudio, using the ‚ÄòUse this model‚Äô button:
This will link to the LMStudio application and open it on your machine. You‚Äôll just need to Choose a Quantization. I went for Q4_K_M
because it will perform well on most devices. If you have more compute, you might want to try out one of the options with Q8_*
.
If you want to skip the UI, you can also load models with LMStudio
via the command line:
lms get lmstudio-community/OlympicCoder-7B-GGUF
lms load olympiccoder-7b
lms server start
3. Connect LM Studio to VS Code
This is the important part. We now need to integrate VScode with the model served by LMStudio.
- In LM Studio, activate the server on the ‚ÄòDeveloper‚Äô tab. This will expose the endpoints at
http://localhost:1234/v1
.
- Install the VS Code Extension to connect to our local server. I went for Continue.dev, but there are other options too.
- In VSCode, go to the Extensions view (click the square icon on the left sidebar, or press Ctrl+Shift+X / Cmd+Shift+X).
- Search for ‚ÄúContinue‚Äù and install the extension from ‚ÄúContinue Dev‚Äù.
- Configure a New Model in Continue.dev
- Open the Continue tab and in the models dropdown, select ‚Äòadd new chat model‚Äô.
- This will open a json configuration file. You‚Äôll need to specify the model name. I.e. olympiccoder-7b
üöÄ You‚Äôve got a local coding assistant!
Most of the core AI features in vscode are available via this setup, for example:
- Code Completion: Start typing, and the AI will suggest how to finish your code.
- Generate Code: Ask it to write a function or a whole block of code. For example, you could type (in a comment or a chat window, depending on the extension): // Write a function to reverse a string in JavaScript
- Explain Code: Select some code and ask the AI to explain what it does.
- Refactor Code: Ask the AI to make your code cleaner or more efficient.
- Write Tests: Ask the AI to create unit tests for your code.
üèãÔ∏è‚ôÄÔ∏è What‚Äôs the vibe of OlympicCoder?
OlympicCoder is not Claude. It‚Äôs optimised on the CodeForces-CoTs dataset which is based on competitive coding challenges. That means that you should not expect it to be super friendly and explanatory. Instead, roll up your sleeves and expect a no-holds barred competitive coder ready to deal with tough problems.
You might want to mix up OlympicCoder with other models to get a rounded coding experience. For example, if you‚Äôre trying to squeeze milliseconds out of a binary search, try OlympicCoder. If you want to design a user facing API, go for Claude-3.7-sonnet or Qwen-2.5-Coder.
Next Steps
- Share your favorite generations in the comments below
- Try out another variant of OlympicCoder from the hub.
- Experiment with quantization types based on your hardware.
- Try out multiple models in LM Studio for different coding vibes! Check out the model catalog https://lmstudio.ai/models
- Experiment with other VS Code extensions like Cline which have agentic functionality </article> <!-- ‚úÖ Êàª„Çã„Éú„Çø„É≥ --> <div class="mt-10 text-center"> <a id="backLink" href="#" class="inline-block px-4 py-2 border border-sky-600 text-sky-600 rounded hover:bg-gray-100 transition">
‚Üê ‰∏ÄË¶ß„Å∏Êàª„Çã
</a> </div> </div> <!-- ‚úÖ base „ÇíÊ≠£„Åó„ÅèÂüã„ÇÅËæº„ÇÄ --> <script id="baseScript" data-base="/ai-news-curation-site"></script> <!-- ‚úÖ Êàª„Çã„É™„É≥„ÇØ„ÇíÊ≠£„Åó„ÅèÊßãÁØâ --> <script>
      const base = document.getElementById('baseScript')?.dataset.base || '';
      console.log("‚úÖ base:", base);

      const params = new URL(window.location.href).searchParams;
      const fromPage = params.get("fromPage") || "1";
      const fromSort = params.get("fromSort") || "date";

      const backLink = document.getElementById("backLink");
      if (backLink) {
        backLink.href = `${base}/page/${fromSort}/${fromPage}`;
        console.log("‚úÖ backLink.href:", backLink.href);
      } else {
        console.warn("‚ö†Ô∏è backLink not found");
      }
    </script> </body> </html>